{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13eb0543-86b4-40a3-93fc-969242b79349",
   "metadata": {},
   "source": [
    "# Lexos Visualizations\n",
    "\n",
    "This script does the following:\n",
    "\n",
    "1. Creates spaCy docs from a list of text files.\n",
    "2. Converts the tokens to lower case and filters them to remove digits, punctuation, and whitespace.\n",
    "3. Tests that the loader is working properly.\n",
    "4. Store data in a document term matrix.\n",
    "5. Generates a dendogram from the dtm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bf0c8-bb4c-4cd7-aca5-4e30dd94c522",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure a list of file paths, the labels you wish to use for each document, and the language model you wish to use to parse the texts.\n",
    "\n",
    "Note that converting long texts to spaCy docs can take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6c0ea8-f075-4cd5-81a5-61402b5f4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own data\n",
    "data = [\n",
    "    r\"C:\\Users\\jack\\OneDrive\\Documents\\School\\summer22\\LexosRepo\\lexos\\tests\\test_data\\txt\\Austen_Pride.txt\",\n",
    "    r\"C:\\Users\\jack\\OneDrive\\Documents\\School\\summer22\\LexosRepo\\lexos\\tests\\test_data\\txt\\Austen_Pride.txt\"\n",
    "]\n",
    "labels = [\"Pride\", \"Sense\"]\n",
    "model = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb3b32-adca-49ee-88b0-0d907c29de90",
   "metadata": {},
   "source": [
    "## Import Lexos API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a065ff-e19c-4b1c-acda-e0b2222b54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local import path\n",
    "import os\n",
    "import sys\n",
    "LEXOS_PATH = \"lexos\"\n",
    "if \"NOTEBOOK_INITIATED_FLAG\" not in globals():\n",
    "    NOTEBOOK_INITIATED_FLAG = True\n",
    "    try:\n",
    "        module_path = os.path.join(os.path.dirname(__file__), os.pardir)\n",
    "    except:\n",
    "        module_path = os.path.abspath(os.path.join(LEXOS_PATH))\n",
    "        %cd lexos\n",
    "        %pwd\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "        \n",
    "# Import Lexos API modules\n",
    "from lexos.io.basic import Loader\n",
    "from lexos import tokenizer\n",
    "from lexos.dtm import DTM\n",
    "from lexos.cutter import Ginsu\n",
    "try:\n",
    "    from lexos.cluster.dendrogram import Dendrogram\n",
    "except ImportError:\n",
    "    print(\"Dendogram not imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb3bfd-0393-451d-82d5-e98a4ade28c6",
   "metadata": {},
   "source": [
    "## Load Texts and Convert to spaCy Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47e8055-22b9-4f83-bc0a-ece1d20c58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loader and load the data\n",
    "loader = Loader()\n",
    "loader.load(data)\n",
    "\n",
    "# Make the docs -- currently takes a long time with full novels\n",
    "docs = tokenizer.make_docs(loader.texts, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5a284-dd21-42f9-bc0b-db4e5c1191d4",
   "metadata": {},
   "source": [
    "## Ensure Loader is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bccfbfd-e43d-407d-92bc-ed1293573ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pride and Prejudice\n",
      "by Jane Austen\n",
      "Chapter 1\n",
      "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
      "However little known the feelings or views of such a man\n",
      "\n",
      "\n",
      " Pride and Prejudice\n",
      "by Jane Austen\n",
      "Chapter 1\n",
      "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
      "However little known the feelings or views of such a man\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(docs):\n",
    "    print(text[0:50])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2907daf-db84-4c42-99c7-2aec9b564976",
   "metadata": {},
   "source": [
    "## Cut texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42af011c-2801-4b66-a5ee-050a3675dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.cutter import Ginsu\n",
    "cutter = Ginsu()\n",
    "\n",
    "result = cutter.splitn(docs, n=3, merge_threshold=0.5, overlap=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa1e23-2477-421e-9051-f9d6c9360582",
   "metadata": {},
   "source": [
    "## Initiate DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363d00a4-6aae-4e45-af19-dd2cc2ff832a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cytoolz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlexos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTM\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPride_and_Prejudice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSense_and_Sensibility\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m dtm \u001b[38;5;241m=\u001b[39m \u001b[43mDTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPride1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPride2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPride3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSense1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSense2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSense3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#result = [doc.text for doc in result]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#print (result[0])\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\School\\summer22\\LexosRepo\\lexos\\lexos\\dtm\\__init__.py:23\u001b[0m, in \u001b[0;36mDTM.__init__\u001b[1;34m(self, docs, labels)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_settings \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild()\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\School\\summer22\\LexosRepo\\lexos\\lexos\\dtm\\__init__.py:181\u001b[0m, in \u001b[0;36mDTM.set_vectorizer\u001b[1;34m(self, tf_type, idf_type, dl_type, norm, min_df, max_df, max_n_terms, vocabulary_terms, new)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_vectorizer\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    167\u001b[0m                     tf_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    168\u001b[0m                     idf_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m                     new: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;124;03m\"\"\"Set the vectorizer.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    By default, returns a vectorizer that gets raw counts.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepresentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vectorizer\n\u001b[0;32m    182\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m Vectorizer(\n\u001b[0;32m    183\u001b[0m         tf_type \u001b[38;5;241m=\u001b[39m tf_type,\n\u001b[0;32m    184\u001b[0m         idf_type \u001b[38;5;241m=\u001b[39m idf_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         vocabulary_terms \u001b[38;5;241m=\u001b[39m vocabulary_terms\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer_settings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: tf_type,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midf_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: idf_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_n_terms\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_n_terms,\n\u001b[0;32m    199\u001b[0m     }\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textacy\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_DATA_DIR\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Corpus\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_id\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m identify_lang\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspacier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_spacy_lang, make_spacy_doc\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textacy\\corpus.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcytoolz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m itertoolz\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cytoolz'"
     ]
    }
   ],
   "source": [
    "from lexos.dtm import DTM\n",
    "labels = [\"Pride_and_Prejudice\", \"Sense_and_Sensibility\"]\n",
    "dtm = DTM(docs, labels)\n",
    "\n",
    "labels = [\"Pride1\", \"Pride2\", \"Pride3\", \"Sense1\", \"Sense2\", \"Sense3\"]\n",
    "#result = [doc.text for doc in result]\n",
    "#print (result[0])\n",
    "dtm2 = DTM(result[0], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62f4d8-131f-4052-a5c0-e44d84486fb8",
   "metadata": {},
   "source": [
    "## Create Dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3233b1-0782-4807-939d-d5845178899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = Dendrogram(dtm, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea2fb9-2679-4028-967f-cf7d05431736",
   "metadata": {},
   "source": [
    "## Create WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e865e9-ca95-48d5-8399-726aa7ca28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.visualization.cloud.wordcloud import multicloud\n",
    "labels = dtm.get_table().columns.tolist()[1:]\n",
    "multicloud(dtm, docs=None, opts=None, ncols=3, title=None, labels=None, show=True, figure_opts=None, round=None, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418622ab-0ee6-419a-88c9-3284132253c9",
   "metadata": {},
   "source": [
    "## Create BubbleViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012ab72-6ddd-4067-84be-7613c052e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.visualization.bubbleviz import bubbleviz\n",
    "bubbleviz(dtm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2aa7953f390f28e7e50d668f5dd71a50654ba94f4b9e6876e74ed7ed7caa2611"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
