{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Tokenizer\n",
    "   \n",
    "This notebook is to show examples of how to use the `tokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Lexos to the Jupyter `sys.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System path set to `../../../lexos`.\n"
     ]
    }
   ],
   "source": [
    "%run jupyter_local_setup.py ../../../lexos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lexos Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.io.smart import Loader\n",
    "from lexos import tokenizer\n",
    "from lexos.cutter.ginsu import Ginsu\n",
    "from lexos.cutter.machete import Machete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"../test_data/txt/Austen_Pride.txt\"\n",
    "loader = Loader()\n",
    "loader.load(data)\n",
    "doc = tokenizer.make_doc(loader.texts[0], model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting with Ginsu\n",
    "\n",
    "Ginsu is used for cutting spaCy docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ginsu.split()`\n",
    "\n",
    "Splits doc(s) into segments of n tokens.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: A spaCy doc or list of spaCy docs.\n",
    "- `n`: The number of tokens to split on (default = 1000).\n",
    "- `merge_threshold`: The threshold to merge the last segment (default = 0.5).\n",
    "- `overlap`: The number of tokens to overlap (default = 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Ginsu()\n",
    "\n",
    "# Returns a list of lists of docs\n",
    "pride_segments = cutter.split(doc, n=7500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ginsu.splitn()`\n",
    "\n",
    "Splits doc(s) into a specific number of segments.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: A spaCy doc or list of spaCy docs.\n",
    "- `n`: The number of segments to create (default = 2).\n",
    "- `merge_threshold`: The threshold to merge the last segment (default = 0.5).\n",
    "- `overlap`: The number of tokens to overlap (default = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of lists of docs\n",
    "pride_segments = cutter.splitn([doc], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ginsu.split_on_milestones()`\n",
    "\n",
    "Splits doc(s) on milestone patterns using patterns or pre-processed `token._.is_milestone` attributes.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: The document(s) to be split.\n",
    "- `milestone`: A variable representing the value(s) to be matched.\n",
    "- `preserve_milestones`: If True (the default), the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted.\n",
    "\n",
    "Milestones can be strings, lists, or complex patterns expressed in a dict. See <a href=\"https://scottkleinman.github.io/lexos/tutorial/cutting_docs/#splitting-documents-on-milestones\" target=\"_blank\">Splitting Documents on Milestones</a> for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of lists of docs\n",
    "pride_segments = cutter.split_on_milestones([doc], milestone=\"Chapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below preprocesses the document with the `Milestones` class and splits the document into segments based on the `token._.is_milestone` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.cutter.milestones import Milestones\n",
    "\n",
    "Milestones().set(doc, \"Chapter\")\n",
    "\n",
    "pride_segments = cutter.split_on_milestones([doc], milestone={\"is_milestone\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.split()`\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `texts`: A text string or list of text strings.\n",
    "- `n`: The number of tokens to split on (default = 1000).\n",
    "- `merge_threshold`: The threshold to merge the last segment (default = 0.5).\n",
    "- `overlap`: The number of tokens to overlap (default = 0).\n",
    "- `tokenizer`: The name of the tokenizer function to use (default = \"whitespace\").\n",
    "- `as_string`: Whether to return the segments as a list of strings (default = True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting with Machete\n",
    "\n",
    "Machete is used for cutting text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Machete()\n",
    "\n",
    "text = loader.texts[0]\n",
    "\n",
    "# Returns a list of str lists\n",
    "segments = cutter.split(text, n=7500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.splitn()`\n",
    "\n",
    "Splits text(s) into a specific number of segments.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `texts`: A text string or list of text strings.\n",
    "- `n`: The number of segments to create (default = 2).\n",
    "- `merge_threshold`: The threshold to merge the last segment (default = 0.5).\n",
    "- `overlap`: The number of tokens to overlap (default = 0).\n",
    "- `tokenizer`: The name of the tokenizer function to use (default = \"whitespace\").\n",
    "- `as_string`: Whether to return the segments as a list of strings (default = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of str lists\n",
    "segments = cutter.splitn(text, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.split_on_milestones()`\n",
    "\n",
    "Splits text(s) on milestone patterns.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: The document(s) to be split.\n",
    "- `milestone`: A variable representing the value(s) to be matched.\n",
    "- `preserve_milestones`: If True (the default), the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted.\n",
    "- `tokenizer`: The name of the tokenizer function to use (default = \"whitespace\").\n",
    "- `as_string`: Whether to return the segments as a list of strings (default = True).\n",
    "\n",
    "Milestone patterns are evaluated as regular expressions and searched from the beginning of the token string using Python's `re.match()` function. See <a href=\"https://scottkleinman.github.io/lexos/tutorial/cutting_docs/#splitting-documents-with-machete\" target=\"_blank\">Splitting Documents with Machete</a> for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of str lists\n",
    "segments = cutter.split_on_milestones(text, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.split_list()`\n",
    "\n",
    "Splits a list of tokens into segments.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `doc`: The text to be split.\n",
    "- `n`: The number of tokens to split on (default = 1000).\n",
    "- `merge_threshold`: The threshold to merge the last segment (default = 0.5).\n",
    "- `overlap`: The number of tokens to overlap (default = 0).\n",
    "- `as_string`: Whether to return the segments as a list of strings (default = True).\n",
    "\n",
    "See <a href=\"https://scottkleinman.github.io/lexos/tutorial/cutting_docs/#splitting-lists-of-tokens-with-machete\" target=\"_blank\">Splitting Lists of Tokens with Machete</a> for important considerations when using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split()\n",
    "\n",
    "# Returns a list of str lists\n",
    "segments = cutter.split_list(text, n=7500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting with `FileSplit` (Chainsaw)\n",
    "\n",
    "`FileSplit` (AKA \"Chainsaw\") is used for cutting binary files into smaller files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Filesplit` Class\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `man_filename`: The path to the manifest filename (default = \"fs_manifest.csv\").\n",
    "- `buffer_size`: The maximum file size for each segment (default = 1000000, 1 MB).\n",
    "\n",
    "The class is initialised with the defaults in the cells below.\n",
    "\n",
    "### `Filesplit.split()`\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `file`: The path to the file to be split.\n",
    "- `split_size`: The maximum file size for each segment (default = 1000000, 1 MB).\n",
    "- `output_dir`: The path to the directory where the segments will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.cutter.filesplit import Filesplit\n",
    "\n",
    "fs = Filesplit()\n",
    "\n",
    "fs.split(\n",
    "    file=\"/filesplit_test/longfile.txt\",\n",
    "    split_size=30000000,\n",
    "    output_dir=\"/filesplit_test/splits/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Filesplit.merge()`\n",
    "\n",
    "The `Filesplit.merge()` method uses the saved metadata file to merge segments of a file previously split using `Filesplit.split()`.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `input_dir`: The path to the directory containing the split files and the manifest file.\n",
    "- `sep`: The separator string used in the file names (default = \"_\").\n",
    "- `output_file`: The path to the file which will contain the merged segments. If not provided, the final merged filename is derived from the split filename and placed in the same `input_dir`.\n",
    "- `manifest_file`: The path to the manifest file. If not provided, the process will look for the file within the `input_dir`.\n",
    "- `callback`: A callback function that accepts two arguments: the path to the destination file and the size the file in bytes.\n",
    "- `cleanup`: If `True`, all the split files and the manifest file will be deleted after the merge, leaving behind only the merged file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fs.merge(\"/filesplit_test/splits/\", cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff93cd05c7a11458fc6e692c465602a12d07b4d86c038fa25d5e533c12dcd222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
