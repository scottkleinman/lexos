{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5169fa6-7341-47e7-bd77-d1b236c3f48d",
   "metadata": {},
   "source": [
    "# Dendrogram Tutorial\n",
    "\n",
    "This notebook is test to show how a dendrogram would be constructed using three novels. You can substitute any data you like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4c5fa-a30f-4a94-97b8-d90dd4326f52",
   "metadata": {},
   "source": [
    "## Import Lexos Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b841f-14c5-4174-bdf0-c57c895b5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lexos.io.smart import Loader\n",
    "from lexos import tokenizer\n",
    "from lexos.dtm import DTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf84bd4-fa8c-45c7-960d-31abb12f314c",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We will load _Pride and Prejudice_ and _Sense and Sensibility_ from the repository's test data, but we'll also download _Frankenstein_ from Project Gutenberg.\n",
    "\n",
    "For quick run times, we'll take only the first 10000 characters from each novel. We're going to run a quick function called `clean_text()` to remove unwanted line breaks and spaces. If you are using your own data, you may want to do some preprocessing with `Scrubber`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0e735-6e9a-4780-95c2-852c54064a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"../test_data/txt/Austen_Pride.txt\",\n",
    "    \"../test_data/txt/Austen_Sense.txt\",\n",
    "    \"https://www.gutenberg.org/files/84/84-0.txt\"\n",
    "]\n",
    "\n",
    "# Create the loader and load the data\n",
    "loader = Loader()\n",
    "loader.load(data)\n",
    "\n",
    "# Shorten the texts\n",
    "texts = [text[0:10000] for text in loader.texts]\n",
    "\n",
    "# We'll do a little cleanup to get rid of line breaks\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Convert line breaks to spaces and remove trailing spaces.\"\"\"\n",
    "    return re.sub(\"[\\r\\n|\\n]+\", \" \", text).strip()\n",
    "\n",
    "texts = [clean_text(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a282673-b33a-4be6-9a50-351d6b6431a6",
   "metadata": {},
   "source": [
    "## Make spaCy Docs\n",
    "\n",
    "Since we are dealing with full novels, this might take a while to process. If you are using a language model, it is recommended that you disable pipeline components you are not using. However, the default multilanguage model should work fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52600b25-7b71-4d11-9131-7b90d1964903",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Pride_and_Prejudice\", \"Sense_and_Sensibility\", \"Frankenstein\"]\n",
    "docs = tokenizer.make_docs(texts, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8e5ff-3a13-4f77-8f0a-6e7672db7259",
   "metadata": {},
   "source": [
    "## Generate the Document-Term Matrix and Show the DTM Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ca962-9cbc-4d3b-b2c6-a7f845421532",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = DTM(docs, labels)\n",
    "df = dtm.get_table()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb3112",
   "metadata": {},
   "source": [
    "### Yikes!\n",
    "\n",
    "There are a lot of spaces, punctuation marks, and digits in our table. Do we _really_ want those in our analysis? Maybe we should filter them out and then re-generate the DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a58847",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_texts = []\n",
    "for doc in docs:\n",
    "    filtered_tokens = [\n",
    "        token.text for token in doc\n",
    "        if not token.is_space\n",
    "        and not token.is_punct\n",
    "        and not token.is_digit\n",
    "    ]\n",
    "    filtered_texts.append(\" \".join(filtered_tokens))\n",
    "\n",
    "docs = tokenizer.make_docs(filtered_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a85f2c",
   "metadata": {},
   "source": [
    "You can experiment with the cell above to try different filters. For instance, try removing stop words with `token.is_stop`. You can also convert to lower case by changing `token.text` to `token.norm_`.\n",
    "\n",
    "In the next cell, you'll rebuild the DTM based on your new spaCy docs. If you're satisfied, you can move on to generate your dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88796585",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = DTM(docs, labels)\n",
    "df = dtm.get_table()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc8ca7-2bb6-47d8-8688-9cff56fa431a",
   "metadata": {},
   "source": [
    "## Make a Dendrogram\n",
    "\n",
    "A dendrogram is a graph based on a form of cluster analysis called hierarchical agglomerative clustering. This technique measures the term vectors for each document and determines the \"distance\" between them. Based on this distance, the algorithm assigns documents to clusters. In the dendrogram, the \"leaves\" (document labels) that are most directly connected by \"branches\" have the closest vectors. The height of branches indicates how close different clusters of connected leaves are to other clusters.\n",
    "\n",
    "There are a number of ways to determine how to measure the distance between documents and how to assign documents to clusters (known as \"linkage\"). In this example, we will use the default settings of Euclidean distance and average linkage to generate a dendrogram.\n",
    "\n",
    "We start by importing the `Dendrogram` class from the Lexos `cluster` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa9620-3b2b-42b1-be8d-a96436cce866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.cluster.dendrogram import Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711b4a5",
   "metadata": {},
   "source": [
    "We are now ready to build the dendrogram by feeding it our DTM. The `show=True` flag will display the dendrogram in our notebook. For some applications, you may wish to suppress the display and do something else with the dendrogram, which we are here assigning to the `dendrogram` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dd410-137a-48aa-94da-4056a2e99088",
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = Dendrogram(dtm, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823abb1",
   "metadata": {},
   "source": [
    "If you have followed the default settings, _Pride and Prejudice_ groups with _Frankenstein_, rather than _Sense and Sensibility_. This is surprising since _Pride and Prejudice_ and _Sense and Sensibility_ are by Jane Austen, whereas _Frankenstein_ is by Mary Shelley. We need to decide if this is a meaningful result or if it has something to do with our data. We may want to go back and reconsider how we have preprocessed the data. Have we done anything that might have influenced the result?\n",
    "\n",
    "Even if we are satisfied with our data, we may also have influenced the result by our choice of distance metric and linkage method. It is a good idea to try other options to see how robust the result is. Valid <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\" target=\"_blank\">distance metrics</a> and <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\" target=\"_blank\">linkage methods</a>, along with technical explanations, can be found in the scipy documentation. However, the following guide may be useful.\n",
    "\n",
    "One of the most important (and least well-documented) aspects of the hierarchical clustering method is the distance metric. Since we are representing texts as document vectors, it makes sense to define document similarity by comparing the vectors. One way to do this is to measure the distance between each pair of vectors. For example, if two vectors are visualized as lines in a triangle, the hypotenuse between these lines can be used as a measure of the distance between the two documents. This method of measuring how far apart two documents are is known as Euclidean distance, which is the default distance metric used in Lexos. The table below provides some suggestions for distance metrics to try with different types of data.\n",
    "\n",
    "\n",
    "| Vocabulary Size  | Small Number of terms per document                         | Large Number of terms per document                                |\n",
    "|------------------|-----------------------------------------------------------|------------------------------------------------------------------|\n",
    "| Small | `Bray-Curtis`, `Hamming`               | `Chebyshev`, `Euclidean`, `Standardized Euclidean` |\n",
    "| Large | `Correlation`, `Jaccard`, `Squared Euclidean` | `Canberra`, `Cosine`, `Manhattan`     \n",
    "\n",
    "At each stage of the clustering process, a choice must be made about whether two clusters should be joined (a single document itself forms a cluster at the lowest level of the hierarchy). You may choose any of the linkage methods listed below:\n",
    "\n",
    "- `average`: Average linkage is a compromise between single and complete linkage. It takes the average distance of all the points in each cluster and uses the shortest average distance for deciding which cluster should be joined to the current one. This is the default linkage method in Lexos.\n",
    "- `single`: Single linkage joins the cluster containing a point (e.g. a term frequency) closest to the current cluster. Single linkage joins clusters based on only a single point and does not take into account the rest of the points in the cluster. The resulting dendrograms tend to have spread out clusters. This process is called \"chaining\".\n",
    "- `complete`: Complete linkage uses the opposite approach to single linkage. It takes the two points furthest apart between the current cluster and the others. The cluster with the shortest distance to the current cluster is joined to it. Complete linkage thus takes into account all the points on the vector that come before the one with the maximum distance. It tends to produce compact, evenly distributed clusters in the resulting dendrograms.\n",
    "- `weighted`: The weighted average linkage performs the average linkage calculation but weights the distances based on the number of terms in the cluster. It, therefore, may be a good option when there is significant variation in the size of the documents under examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc1ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = Dendrogram(dtm, show=True, metric=\"cosine\", method=\"average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b20b4",
   "metadata": {},
   "source": [
    "You can change the orientation of the dendrogram or the angle of the labels, which is especially useful if you have a dendrogram with a lot of leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram = Dendrogram(dtm, show=True, orientation=\"left\", leaf_rotation=-45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19adef",
   "metadata": {},
   "source": [
    "You can also save the dendrogram to a file. In the cell below, replace \"dendrogram.png\" with a filepath of your choice. If you change the file extension to \"jpg\" or \".pdf\", the file will be saved in that format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca7b34",
   "metadata": {},
   "source": [
    "dendrogram.savefig(\"dendrogram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac2a5c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lexos",
   "language": "python",
   "name": "lexos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff93cd05c7a11458fc6e692c465602a12d07b4d86c038fa25d5e533c12dcd222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
