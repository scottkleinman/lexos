{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutter Tutorial\n",
    "   \n",
    "This notebook is to show examples of how to use the `cutter` module.\n",
    "\n",
    "`Cutter` used to divide files, texts, or documents into segments. It uses three Python classes (identified by cute codenames), depending on the type of data you are working with:\n",
    "\n",
    "- `Ginsu` is used to split spaCy documents.\n",
    "- `Machete` is used to split raw text strings.\n",
    "- `Filesplit` (codename `Chainsaw`) is used to split files based on byte size.\n",
    "\n",
    "Each is used by importing the class and instantiating the class before using its methods. By convention, we assign the class instance to the name `cutter`. Here is an example using `Ginsu` to split a document into two segments:\n",
    "\n",
    "```python\n",
    "    from lexos.cutter.ginsu import Ginsu\n",
    "    cutter = Ginsu()\n",
    "    segments = cutter.splitn(doc, n=2)\n",
    "```\n",
    "\n",
    "The `splitn()` method takes the `n` argument to indicate the number of desired segments. Each takes multiple arguments. We will examine each method in turn with examples using a minimum number of arguments. However, the full list of arguments available is listed for each method so that you can easily try them out in the example code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lexos Modules and Load Some Data\n",
    "\n",
    "For brevity, we will import all the modules we need below and load some data to be used in our example cells. We'll use Jane Austen's _Pride and Prejudice_, which we will split in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lexos.io.smart import Loader\n",
    "from lexos import tokenizer\n",
    "from lexos.cutter.ginsu import Ginsu\n",
    "from lexos.cutter.machete import Machete\n",
    "from lexos.cutter.filesplit import Filesplit\n",
    "from lexos.cutter.milestones import Milestones\n",
    "\n",
    "data = \"../test_data/txt/Austen_Pride.txt\"\n",
    "loader = Loader()\n",
    "loader.load(data)\n",
    "\n",
    "# Here we convert line break characters to spaces to make\n",
    "# our results more readable in the examples below.\n",
    "text = re.sub(\"[\\r\\n|\\n]+\", \" \",loader.texts[0]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting with `Ginsu`\n",
    "\n",
    "`Ginsu` has three methods:\n",
    "\n",
    "- `split()`: Splits a spaCy doc or a list of spaCy docs into segments every `n` tokens.\n",
    "- `splitn()`: Splits a spaCy doc or a list of spaCy docs into a specified number of segments.\n",
    "- `split_on_milestones()`: Splits a spaCy doc or a list of spaCy docs whenever a specific token is encountered.\n",
    "\n",
    "We'll explore each of these in turn. Since `Ginsu` operates on spaCy docs, we must first convert our text using `Tokenizer`. This will take a bit of time because we are processing it with an English language model to gain access to attributes like parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizer.make_doc(text, model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now look at each function in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ginsu.split()`\n",
    "\n",
    "Splits doc(s) into segments of n tokens.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: A spaCy doc or list of spaCy docs.\n",
    "- `n`: The number of tokens to split on. Default = 1000.\n",
    "- `merge_threshold`: The threshold to merge the last segment. Default = 0.5.\n",
    "- `overlap`: The number of tokens to overlap. Default = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Ginsu()\n",
    "\n",
    "segments = cutter.split(doc, n=7500)\n",
    "\n",
    "print(\"Number of segments:\", len(segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ginsu.splitn()`\n",
    "\n",
    "Splits doc(s) into a specific number of segments.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: A spaCy doc or list of spaCy docs.\n",
    "- `n`: The number of tokens to split on. Default = 2.\n",
    "- `merge_threshold`: The threshold to merge the last segment. Default = 0.5.\n",
    "- `overlap`: The number of tokens to overlap. Default = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = cutter.splitn(doc, n=10)\n",
    "\n",
    "print(\"Number of segments:\", len(segments), \"\\n\")\n",
    "\n",
    "for segment in segments:\n",
    "    print(f\"- {segment[0:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ginsu.split_on_milestones()`\n",
    "\n",
    "Splits doc(s) on milestone patterns using patterns or token attributes.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: The document(s) to be split.\n",
    "- `milestone`: A variable representing the value(s) to be matched.\n",
    "- `preserve_milestones`: If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. Default = True\n",
    "\n",
    "Milestones can be strings, lists, or complex patterns expressed in a dict. See <a href=\"https://scottkleinman.github.io/lexos/tutorial/cutting_docs/#splitting-documents-on-milestones\" target=\"_blank\">Splitting Documents on Milestones</a> for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = cutter.split_on_milestones(doc, milestone=\"Chapter\")\n",
    "\n",
    "print(f\"Number of segments: {len(segments)} (first 10 shown)\\n\")\n",
    "\n",
    "for segment in segments[0:10]:\n",
    "    print(f\"- {segment[0:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below preprocesses the document with the `Milestones` class (imported at the beginning of the notebook) and splits the document into segments based on the `token._.is_milestone` attribute. As you will see from the example below, this gives the same result as the previous cell. The difference is that `split_on_milestones()` simply splits the document whenever it matches the specified pattern, whereas the `Milestones` class adds the `is_milestone` attribute, allowing it to be saved with the text and re-used. \n",
    "\n",
    "Note: The `._.` prefix before `is_milestone` indicates that this is a custom extension, not a built-in spaCy attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set _is_milestone=True for all tokens matching \"Chapter\"\n",
    "Milestones().set(doc, \"Chapter\")\n",
    "\n",
    "# Split doc wherever a token's \"is_milestone\" attribute is True\n",
    "segments = cutter.split_on_milestones(doc, milestone={\"is_milestone\": True})\n",
    "\n",
    "print(f\"Number of segments: {len(segments)} (first 10 shown)\\n\")\n",
    "\n",
    "for segment in segments[0:10]:\n",
    "    print(f\"- {segment[0:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting with Machete\n",
    "\n",
    "`Machete` is used for cutting raw text strings. It has three methods:\n",
    "\n",
    "- `split()`: Splits a text or a list of texts into segments every `n` tokens.\n",
    "- `splitn()`: Splits a text or a list of texts into a specified number of segments.\n",
    "- `split_on_milestones()`: Splits a text or a list of texts whenever a specific string pattern is encountered.\n",
    "- `split_list()`: Splits a pre-tokenised list of tokens into segments.\n",
    "\n",
    "We'll explore each of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.split()`\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `texts`: A text string or list of text strings.\n",
    "- `n`: The number of tokens to split on. Default = 1000.\n",
    "- `merge_threshold`: The threshold to merge the last segment. Default = 0.5.\n",
    "- `overlap`: The number of tokens to overlap. Default = 0.\n",
    "- `tokenizer`: The name of the tokenizer function to use. Default = \"whitespace\".\n",
    "- `as_string`: Whether to return the segments as a list of strings. Default = True.\n",
    "\n",
    "**Important:** `Machete.split()` returns a list of lists where each outer list represents a text. Therefore, you must reference the index of the text to get its segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutter = Machete()\n",
    "\n",
    "segments = cutter.split(text, n=7500)\n",
    "\n",
    "print(\"Number of segments:\", len(segments[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.splitn()`\n",
    "\n",
    "Splits text(s) into a specific number of segments.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `texts`: A text string or list of text strings.\n",
    "- `n`: The number of segments to create. Default = 2.\n",
    "- `merge_threshold`: The threshold to merge the last segment. Default = 0.5.\n",
    "- `overlap`: The number of tokens to overlap. Default = 0.\n",
    "- `tokenizer`: The name of the tokenizer function to use. Default = \"whitespace\".\n",
    "- `as_string`: Whether to return the segments as a list of strings. Default = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = cutter.splitn(text, n=10)\n",
    "\n",
    "print(\"Number of segments:\", len(segments[0]), \"\\n\")\n",
    "\n",
    "for segment in segments[0]:\n",
    "    print(f\"- {segment[0:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.split_on_milestones()`\n",
    "\n",
    "Splits text(s) on milestone patterns.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `docs`: The document(s) to be split.\n",
    "- `milestone`: A variable representing the value(s) to be matched.\n",
    "- `preserve_milestones`: If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. Default = True.\n",
    "- `tokenizer`: The name of the tokenizer function to use. Default = \"whitespace\".\n",
    "- `as_string`: Whether to return the segments as a list of strings. Default = True.\n",
    "\n",
    "Pay close attention to the effect of `preserve_milestones`.\n",
    "\n",
    "Milestone patterns are evaluated as regular expressions and searched from the beginning of the token string using Python's `re.match()` function. See <a href=\"https://scottkleinman.github.io/lexos/tutorial/cutting_docs/#splitting-documents-with-machete\" target=\"_blank\">Splitting Documents with Machete</a> for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = cutter.split_on_milestones(text, milestone=\"Chapter\")\n",
    "\n",
    "print(\"Number of segments:\", len(segments[0]), \" (first 10 shown)\\n\")\n",
    "\n",
    "for segment in segments[0][0:10]:\n",
    "    print(f\"- {segment[0:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Machete.split_list()`\n",
    "\n",
    "Splits a list of tokens into segments.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `doc`: The text to be split.\n",
    "- `n`: The number of tokens to split on (default = 1000).\n",
    "- `merge_threshold`: The threshold to merge the last segment (default = 0.5).\n",
    "- `overlap`: The number of tokens to overlap (default = 0).\n",
    "- `as_string`: Whether to return the segments as a list of strings (default = True).\n",
    "\n",
    "See <a href=\"https://scottkleinman.github.io/lexos/tutorial/cutting_docs/#splitting-lists-of-tokens-with-machete\" target=\"_blank\">Splitting Lists of Tokens with Machete</a> for important considerations when using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our sample text into a list based on whitespace\n",
    "token_list = text.split()\n",
    "\n",
    "segments = cutter.split_list(token_list, n=7500)\n",
    "\n",
    "print(\"Number of segments:\", len(segments[0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting with `FileSplit` (Chainsaw)\n",
    "\n",
    "`FileSplit` (AKA \"Chainsaw\") is used for cutting binary files into smaller files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Filesplit` Class\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `man_filename`: The path to the manifest filename. Default = \"fs_manifest.csv\".\n",
    "- `buffer_size`: The maximum file size for each segment. Default = 1000000 (1 MB).\n",
    "\n",
    "The class is initialised with the defaults in the cells below.\n",
    "\n",
    "### `Filesplit.split()`\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `file`: The path to the manifest filename. Default = \"fs_manifest.csv\".\n",
    "- `split_size`: The maximum file size for each segment. Default = 1000000 (1 MB).\n",
    "- `output_dir`: The path to the directory where the segments will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.cutter.filesplit import Filesplit\n",
    "\n",
    "fs = Filesplit()\n",
    "\n",
    "fs.split(\n",
    "    file=\"/filesplit_test/longfile.txt\",\n",
    "    split_size=30000000,\n",
    "    output_dir=\"/filesplit_test/splits/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Filesplit.merge()`\n",
    "\n",
    "The `Filesplit.merge()` method uses the saved metadata file to merge segments of a file previously split using `Filesplit.split()`.\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    "- `input_dir`: The path to the directory containing the split files and the manifest file.\n",
    "- `sep`: The separator string used in the file names (default = \"_\").\n",
    "- `output_file`: The path to the file which will contain the merged segments. If not provided, the final merged filename is derived from the split filename and placed in the same `input_dir`.\n",
    "- `manifest_file`: The path to the manifest file. If not provided, the process will look for the file within the `input_dir`.\n",
    "- `callback`: A callback function that accepts two arguments: the path to the destination file and the size the file in bytes.\n",
    "- `cleanup`: If `True`, all the split files and the manifest file will be deleted after the merge, leaving behind only the merged file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.merge(\"/filesplit_test/splits/\", cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lexos",
   "language": "python",
   "name": "lexos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff93cd05c7a11458fc6e692c465602a12d07b4d86c038fa25d5e533c12dcd222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
