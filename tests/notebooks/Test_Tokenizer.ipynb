{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Tokenizer\n",
        "   \n",
        "This notebook is to show examples of how to use the `tokenizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Lexos to the Jupyter `sys.path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System path set to `../../../lexos`.\n"
          ]
        }
      ],
      "source": [
        "%run jupyter_local_setup.py ../../../lexos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Lexos Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lexos import tokenizer\n",
        "from lexos.io.smart import Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = \"../test_data/txt/Austen_Pride.txt\"\n",
        "loader = Loader()\n",
        "loader.load(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making a Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = tokenizer.make_doc(loader.texts[0])\n",
        "#by default the language model used is \"xx_sent_ud_sm\" which is a multilanguage model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#You can specify which language model to use when tokenizing\n",
        "doc = tokenizer.make_doc(loader.texts[0],model=\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Multiple texts can be tokenized in one call using tokenizer.make_docs. Reuturns a list of docs\n",
        "docs = tokenizer.make_docs(loader.texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Disabling or Excluding Components\n",
        "Tokenizing using a language model can take a relatively long time. To increase efficiency you can disable/exclude components you do not intend to use. Disabled components will be loaded but unused, and excluded components will not be loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = tokenizer.make_doc(loader.texts[0],model=\"en_core_web_sm\", disable=[\"tagger\",\"parser\"])\n",
        "#doc = tokenizer.make_doc(loader.texts[0],model=\"en_core_web_sm\", exclude=[\"tagger\",\"parser\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stop Words\n",
        "Stop words can be added or removed with `add_stopwords` and `remove_stopwords`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is an example string to test the tokenizer\"\n",
        "txtDoc = tokenizer.make_doc(\n",
        "    text,\n",
        "    add_stopwords=[\"an\", \"the\", \"is\"]\n",
        ")\n",
        "for token in txtDoc:\n",
        "    print(token.text, token.is_stop)\n",
        "\n",
        "#language models will usually have default stop words. remove_stopwords is used to remove unwanted\n",
        "#stopwords from the list marked as default by a language model.\n",
        "text = \"This is an example string to test the tokenizer\"\n",
        "txtDoc = tokenizer.make_doc(\n",
        "    text,\n",
        "    model = \"en_core_web_sm\",\n",
        "    remove_stopwords=[\"is\",\"the\"]\n",
        ")\n",
        "print(\"\\nRemove:\")\n",
        "for token in txtDoc:\n",
        "    print(token.text, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Word Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is an example string to test the tokenizer component\"\n",
        "doc = tokenizer.make_doc(text)\n",
        "ngrams = tokenizer.ngrams_from_doc(doc, size=2)\n",
        "for ngram in ngrams:\n",
        "    print(ngram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# an alternative method to create word ngrams is to use textacy directly. This method has additional options\n",
        "# documented here: https://textacy.readthedocs.io/en/latest/api_reference/extract.html#textacy.extract.basics.ngrams\n",
        "from textacy.extract.basics import ngrams as ng\n",
        "text = \"The end is nigh.\"\n",
        "doc = tokenizer.make_doc(text)\n",
        "ngrams = list(ng(doc, 2, min_freq=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Docs From Ngrams\n",
        "ngrams_from_doc generates a list of ngrams from a doc. If you want to use the ngrams as a doc you will need to generate a new doc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nDoc = tokenizer.doc_from_ngrams(ngrams, strict = True, model =\"en_core_web_sm\")\n",
        "for token in nDoc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Character Ngrams\n",
        "Character ngrams are generated from untokenized text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is an example string to test the tokenizer\"\n",
        "chNgrams = tokenizer.generate_character_ngrams(text, 2, drop_whitespace=False)\n",
        "for ngram in chNgrams:\n",
        "    print(ngram)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c1a182fbbe39991ab3c9f482a16703f4da6dfd24ce0191d5b4794b62c97dd21c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
