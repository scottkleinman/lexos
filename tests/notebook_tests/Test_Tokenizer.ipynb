{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `Tokenizer` Tutorial\n",
        "   \n",
        "This notebook is to show examples of how to use the `tokenizer` to divide texts into \"tokens\". A token is a countable entity, which serves as the basis for computational analysis. Most of the time, tokens will correspond to words, but they may also be characters, punctuation marks, or even spaces.\n",
        "\n",
        "Once a text is tokenised, it is possible to generate a list of unique token forms in the text, generally known as \"types\" or \"terms\" (we use the latter here). The frequency with which individual terms occur in a text is often revealing about the style, meaning, or authorship of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## About `Tokenizer`\n",
        "\n",
        "It is possible to produce a list of word tokens for an English text by dividing words on every white space in the text using a tool like Python's `split()` function. However, this will not work 100% of the time and may work far less well for some other languages. The Lexos `Tokenizer` takes advantage of \"language models\" to automate the process. Language models can implement both rule-based and probabilistic strategies for separating document strings into tokens. Because they have built-in procedures appropriate to specific languages, language models can often do a better job of tokenisation than the approach used in the Lexos app.\n",
        "\n",
        "There are some trade-offs to using language models. Because the algorithm does more than split strings, processing times can be greater. In addition, tokenisation is no longer (explicitly) language agnostic. A language model is \"opinionated\" and it may overfit the data. Likewise, if no language model exists for the language being tokenised, the results may not be satisfactory. The Lexos strategy for handling this situation is described below.\n",
        "\n",
        "Behind the scenes, Lexos uses the Python spaCy library to tokenise texts. The result is called a spaCy doc (short for document). Each spaCy doc has a `text` attribute (which is the original text) and a list of tokens, each with their own attributes. spaCy has a lot of built-in attributes: things like `is_punct` (whether or not the token is a punctuation mark) or `is_digit` (whether or not the token is a digit). You can see a <a href=\"https://spacy.io/api/token#attributes\" target=\"_blank\">complete list</a> in spaCy documentation. Depending on how the language model has been trained, you may get more or less information. For instance, spaCy's \"en_web_core_sm\" English-language model tags the part of speech of every word. You can load this (or another) model into the Lexos `Tokenizer` if you want. However, Lexos does not assume that you are working in English, so the default model is spaCy's \"xx_sent_ud_sm\" multilanguage model, which does a good job finding sentence and token boundaries for a wide variety of languages but does not provide as much information as some of the other models.\n",
        "\n",
        "You can see whether spaCy has a model for your language on the <a href=\"https://spacy.io/models\" target=\"_blank\">spaCy models</a> webpage. You can load any of these models into the Lexos `Tokenizer`, but you will need to download the model first by copying the code provided on that page.\n",
        "\n",
        "This is probably enough information to get started, so let's get to work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Some Data\n",
        "\n",
        "We'll start by loading some data using the `Loader` module. We're going to take the first 1245 characters of Jane Austen's _Pride and Prejudice_? Why 1245? Because it's a relatively small passage that we can process quickly (the full novel would take much longer) and because character 1245 comes at the end of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Pride and Prejudice\\nby Jane Austen\\nChapter 1\\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\\nHowever little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\\n\"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\"\\nMr. Bennet replied that he had not.\\n\"But it is,\" returned she; \"for Mrs. Long has just been here, and she told me all about it.\"\\nMr. Bennet made no answer.\\n\"Do you not want to know who has taken it?\" cried his wife impatiently.\\n\"_You_ want to tell me, and I have no objection to hearing it.\"\\nThis was invitation enough.\\n\"Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it, that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and some of his servants are to be in the house by the end of next week.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from lexos.io.smart import Loader\n",
        "\n",
        "loader = Loader()\n",
        "loader.load(\"../test_data/txt/Austen_Pride.txt\")\n",
        "text = loader.texts[0].strip()[0:1245]\n",
        "text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import `Tokenizer`\n",
        "\n",
        "Now we'll attempt to tokenise this text. We'll start by importing the `tokenizer` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lexos import tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making a Doc\n",
        "\n",
        "`Tokenizer` has a function called `make_doc()` to which we can feed our text. Remember that by default, `Tokenizer` uses spaCy's \"xx_sent_ud_sm\" multilanguage model.\n",
        "\n",
        "We can view the doc's original text by referencing `doc.text`, or we can print out the text of each token in the document using a `for` loop. In the example below, we will only print out snippets of the text and tokens. We enclose tokens in angle brackets for greater visibility and to show that some tokens are line breaks or punctuation marks (something we may have to deal with by scrubbing our text first or by filtering our tokens later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Text:\n",
            "=====\n",
            "Pride and Prejudice\n",
            "by Jane Austen\n",
            "Chapter 1\n",
            "It is a truth universally acknowledged, that a single m\n",
            "\n",
            "Tokens:\n",
            "=======\n",
            "<Pride>\n",
            "<and>\n",
            "<Prejudice>\n",
            "<\n",
            ">\n",
            "<by>\n",
            "<Jane>\n",
            "<Austen>\n",
            "<\n",
            ">\n",
            "<Chapter>\n",
            "<1>\n",
            "<\n",
            ">\n",
            "<It>\n",
            "<is>\n",
            "<a>\n",
            "<truth>\n",
            "<universally>\n",
            "<acknowledged>\n",
            "<,>\n",
            "<that>\n",
            "<a>\n",
            "<single>\n",
            "<man>\n",
            "<in>\n",
            "<possession>\n",
            "<of>\n",
            "<a>\n",
            "<good>\n",
            "<fortune>\n",
            "<,>\n",
            "<must>\n",
            "<be>\n",
            "<in>\n",
            "<want>\n",
            "<of>\n",
            "<a>\n",
            "<wife>\n",
            "<.>\n",
            "<\n",
            ">\n",
            "<However>\n",
            "<little>\n",
            "<known>\n",
            "<the>\n",
            "<feelings>\n",
            "<or>\n",
            "<views>\n",
            "<of>\n",
            "<such>\n",
            "<a>\n",
            "<man>\n",
            "<may>\n",
            "<be>\n",
            "<on>\n",
            "<his>\n",
            "<first>\n",
            "<entering>\n",
            "<a>\n",
            "<neighbourhood>\n",
            "<,>\n",
            "<this>\n",
            "<truth>\n"
          ]
        }
      ],
      "source": [
        "doc = tokenizer.make_doc(text)\n",
        "print(\"\\nText:\")\n",
        "print(\"=====\")\n",
        "\n",
        "print(doc.text[0:100])\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(\"=======\")\n",
        "for token in doc[0:60]:\n",
        "    print(f\"<{token.text}>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specifying a Language Model\n",
        "\n",
        "You can specify a language model with the `model` parameter. In the example below, we load the \"en_core_web_sm\" model. Notice how much longer it takes to tokenise. But notice the information that we get out of the model (in this example, we are printing out the part of speech for each token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Pride>: PROPN\n",
            "<and>: CCONJ\n",
            "<Prejudice>: PROPN\n",
            "<\n",
            ">: SPACE\n",
            "<by>: ADP\n",
            "<Jane>: PROPN\n",
            "<Austen>: PROPN\n",
            "<\n",
            ">: SPACE\n",
            "<Chapter>: NOUN\n",
            "<1>: NUM\n",
            "<\n",
            ">: SPACE\n",
            "<It>: PRON\n",
            "<is>: AUX\n",
            "<a>: DET\n",
            "<truth>: NOUN\n",
            "<universally>: ADV\n",
            "<acknowledged>: VERB\n",
            "<,>: PUNCT\n",
            "<that>: SCONJ\n",
            "<a>: DET\n",
            "<single>: ADJ\n",
            "<man>: NOUN\n",
            "<in>: ADP\n",
            "<possession>: NOUN\n",
            "<of>: ADP\n",
            "<a>: DET\n",
            "<good>: ADJ\n",
            "<fortune>: NOUN\n",
            "<,>: PUNCT\n",
            "<must>: AUX\n",
            "<be>: AUX\n",
            "<in>: ADP\n",
            "<want>: NOUN\n",
            "<of>: ADP\n",
            "<a>: DET\n",
            "<wife>: NOUN\n",
            "<.>: PUNCT\n",
            "<\n",
            ">: SPACE\n",
            "<However>: ADV\n",
            "<little>: ADJ\n",
            "<known>: VERB\n",
            "<the>: DET\n",
            "<feelings>: NOUN\n",
            "<or>: CCONJ\n",
            "<views>: NOUN\n",
            "<of>: ADP\n",
            "<such>: DET\n",
            "<a>: DET\n",
            "<man>: NOUN\n",
            "<may>: AUX\n",
            "<be>: AUX\n",
            "<on>: ADP\n",
            "<his>: PRON\n",
            "<first>: ADJ\n",
            "<entering>: VERB\n",
            "<a>: DET\n",
            "<neighbourhood>: NOUN\n",
            "<,>: PUNCT\n",
            "<this>: DET\n",
            "<truth>: NOUN\n"
          ]
        }
      ],
      "source": [
        "doc = tokenizer.make_doc(text, model=\"en_core_web_sm\")\n",
        "for token in doc[0:60]:\n",
        "    print(f\"<{token.text}>: {token.pos_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenising Multiple Texts\n",
        "\n",
        "You can use the `Tokenizer.make_docs()` function to make process multiple texts at once. In this example, we are just going to cut our text roughly in half to make two separate texts and then convert them to spaCy docs. Although we are going to feed our texts to `make_docs()` in a list in this example, recall that `loader.texts` _is_ a list, so a common procedure would be to call `make_docs(loader.texts)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Pride and Prejudice\\nby Jane Austen\\nChapter 1\\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\\nHowever little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\\n\"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\"\\nMr. Bennet replied that he had not.\\n\"'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1 = text[0:565]\n",
        "text2 = text[565:1245]\n",
        "\n",
        "docs = tokenizer.make_docs([text1, text2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Multiple texts can be tokenized in one call using tokenizer.make_docs. Reuturns a list of docs\n",
        "docs = tokenizer.make_docs(loader.texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Disabling or Excluding Components\n",
        "\n",
        "Part of the reason some language models take a long time is that they have numerous components that handle tasks such as tagging parts of speech, identifying syntactic dependencies, or labelling named entities like people and places. The documentation on the <a href=\"https://spacy.io/models\" target=\"_blank\">spaCy models</a> webpage will identify which components are available in the model's pipeline. If you do not need a component, you can speed up processing times by disabling or excluding components you do not intend to use. Disabled components (listed with the `disable` parameter) will be loaded but unused, and excluded components (listed with the `exclude` parameter) will not be loaded.\n",
        "\n",
        "Try out the examples below by commenting and uncommenting them to compare how long they take to the process with all pipeline components enabled and included. You won't notice a lot of difference on a short text but you will on a longer one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = tokenizer.make_doc(loader.text, model=\"en_core_web_sm\", disable=[\"tagger\",\"parser\"])\n",
        "\n",
        "#doc = tokenizer.make_doc(loader.text, model=\"en_core_web_sm\", exclude=[\"tagger\",\"parser\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stop Words\n",
        "\n",
        "A stop word (or \"stopword\") is a token that you typically wish to remove from your analysis, generally because the token is not a carrier of meaning. Stop words are generally small function words like \"and\" or \"the\", but they can also be words like personal names, where the inclusion of those names might skew your data in your intended analysis.\n",
        "\n",
        "Stop words can be added or removed with `add_stopwords` and `remove_stopwords`. Since the default language model is a multilanguage model, it has no built-in stop words. Models for specific languages generally have built-in stop word lists which you can modify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Default model with stop words added:\n",
            "\n",
            "This False\n",
            "is True\n",
            "an True\n",
            "example False\n",
            "string False\n",
            "to False\n",
            "test False\n",
            "the True\n",
            "tokenizer False\n",
            ". False\n",
            "\n",
            "English language model with stop words removed:\n",
            "\n",
            "This True\n",
            "is False\n",
            "an True\n",
            "example False\n",
            "string False\n",
            "to True\n",
            "test False\n",
            "the False\n",
            "tokenizer False\n",
            ". False\n"
          ]
        }
      ],
      "source": [
        "text = \"This is an example string to test the tokenizer.\"\n",
        "\n",
        "doc1 = tokenizer.make_doc(\n",
        "    text,\n",
        "    add_stopwords=[\"an\", \"the\", \"is\"]\n",
        ")\n",
        "print(\"\\nDefault model with stop words added:\\n\")\n",
        "for token in doc1:\n",
        "    print(token.text, token.is_stop)\n",
        "\n",
        "doc2 = tokenizer.make_doc(\n",
        "    text,\n",
        "    model=\"en_core_web_sm\",\n",
        "    remove_stopwords=[\"is\", \"the\"]\n",
        ")\n",
        "print(\"\\nEnglish language model with stop words removed:\\n\")\n",
        "for token in doc2:\n",
        "    print(token.text, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Word Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is an example string to test the tokenizer component\"\n",
        "doc = tokenizer.make_doc(text)\n",
        "ngrams = tokenizer.ngrams_from_doc(doc, size=2)\n",
        "for ngram in ngrams:\n",
        "    print(ngram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# an alternative method to create word ngrams is to use textacy directly. This method has additional options\n",
        "# documented here: https://textacy.readthedocs.io/en/latest/api_reference/extract.html#textacy.extract.basics.ngrams\n",
        "from textacy.extract.basics import ngrams as ng\n",
        "text = \"The end is nigh.\"\n",
        "doc = tokenizer.make_doc(text)\n",
        "ngrams = list(ng(doc, 2, min_freq=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Docs From Ngrams\n",
        "ngrams_from_doc generates a list of ngrams from a doc. If you want to use the ngrams as a doc you will need to generate a new doc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nDoc = tokenizer.doc_from_ngrams(ngrams, strict = True, model =\"en_core_web_sm\")\n",
        "for token in nDoc:\n",
        "    print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Character Ngrams\n",
        "Character ngrams are generated from untokenized text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This is an example string to test the tokenizer\"\n",
        "chNgrams = tokenizer.generate_character_ngrams(text, 2, drop_whitespace=False)\n",
        "for ngram in chNgrams:\n",
        "    print(ngram)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Lexos",
      "language": "python",
      "name": "lexos"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ff93cd05c7a11458fc6e692c465602a12d07b4d86c038fa25d5e533c12dcd222"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
