{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00a4 The Lexos API is a library of methods for programmatically implementating and extending the functionality in the Lexos web app. Eventually, the web app will be rewritten to use the API directly. For the moment, much of the thinking behind the API is explained in the Tutorial . Current Status (v0.0.1) \u00a4 I have mostly built out the basic architecture of the API so far. The basic Loader will accept any local file, regardless of format, and it will also download text from URLs. Obviously, there are some security features that need to be added. It would also be nice to load from different file formats (json, docx, zip, etc.), which is not currently supported. All of the functionality of the Lexos app's scrubber module has been ported over, and the basic tokenizer module works. However, there needs to be some error checking in both modules. The cutter module will need some consideration, as it will probably require a combination of features from scrubber and tokenizer , depending on whether the user wants to cut based on some pattern or cut by token ranges.","title":"Home"},{"location":"#introduction","text":"The Lexos API is a library of methods for programmatically implementating and extending the functionality in the Lexos web app. Eventually, the web app will be rewritten to use the API directly. For the moment, much of the thinking behind the API is explained in the Tutorial .","title":"Introduction"},{"location":"#current-status-v001","text":"I have mostly built out the basic architecture of the API so far. The basic Loader will accept any local file, regardless of format, and it will also download text from URLs. Obviously, there are some security features that need to be added. It would also be nice to load from different file formats (json, docx, zip, etc.), which is not currently supported. All of the functionality of the Lexos app's scrubber module has been ported over, and the basic tokenizer module works. However, there needs to be some error checking in both modules. The cutter module will need some consideration, as it will probably require a combination of features from scrubber and tokenizer , depending on whether the user wants to cut based on some pattern or cut by token ranges.","title":"Current Status (v0.0.1)"},{"location":"installation/","text":"Installation \u00a4 During the development process, dependency management and packaging are handled using Poetry . When the API is released, you will be able to install it with pip install lexos-api . Other methods of installation will be added to this page at a later date. Installing the Lexos API with Poetry \u00a4 A Mac-centric approach to installing Poetry is as follows: $ brew install poetry $ poetry --version Poetry version 1.1.11 The Poetry website contains extensive documentation on how to install it on different operating systems. My running theory is that they way to install the Lexos API prior to publication is to complete following steps: Install Poetry. Clone the repo. cd to the local repo directory. Run poetry init . This is as yet untested.","title":"Installation"},{"location":"installation/#installation","text":"During the development process, dependency management and packaging are handled using Poetry . When the API is released, you will be able to install it with pip install lexos-api . Other methods of installation will be added to this page at a later date.","title":"Installation"},{"location":"installation/#installing-the-lexos-api-with-poetry","text":"A Mac-centric approach to installing Poetry is as follows: $ brew install poetry $ poetry --version Poetry version 1.1.11 The Poetry website contains extensive documentation on how to install it on different operating systems. My running theory is that they way to install the Lexos API prior to publication is to complete following steps: Install Poetry. Clone the repo. cd to the local repo directory. Run poetry init . This is as yet untested.","title":"Installing the Lexos API with Poetry"},{"location":"tutorial/","text":"Tutorial \u00a4 This page is a rough overview of the usage of the API. Important Aspects of the API may change before the tutorial is updated. At this stage, the tutorial should only be taken as a general guideline to the API's usage. Getting Started \u00a4 Begin by importing in some modules in the Lexos API. from lexos.io.basic import Loader from lexos.scrubber.pipeline import make_pipeline , pipe from lexos.scrubber.registry import scrubber_components , load_components Here are some explanations: The io module contains IO functions. Right now, there is a \"basic\" Loader class that takes a file path, url, list of file paths or urls, or a directory name indicating where the source data is. More sophisticated loaders can be created later. The scrubber module consists of thematic \"components\": normalize , remove , replace , and so on. Each component has a number of functions, such as converting to lower case, removing digits, stripping tags, etc. Component functions are registered in a registry. They can be loaded into memory as needed and applied to texts in any order. Loading Texts \u00a4 A typical workflow would create a lexos.io.basic.Loader object and call lexos.io.basic.Loader.load to load the data from disk or download it from the internet. You can access all loaded texts by calling Loader.texts . Note It is more efficient simply to use Python's open() to load texts into a list if you know the file's encoding. Currently, the main advantage of the Loader class is that it automatically coerces the data to Unicode. At this stage of development, the user or application developer is expected to maintain their data folders and know their file locations. More sophisticated project/corpus management methods could be added to the API at a later date. Here is a sample of the code for loading a single text file: # Data source data = \"tests/test_data/Austen_Pride.txt\" # Create the loader and load the data loader = Loader () loader . load ( data ) # Print the first text in the Loader text = loader . texts [ 0 ] print ( text ) Scrubbing Texts \u00a4 Scrubber is now explicitly defined as a destructive preprocessor. In other words, it changes the text as loaded in ways that potentially make mapping the results onto the original text potentially impossible. It is therefore best used before other procedures so that the scrubbed text is essentially treated as the \"original\" text. The importance of this will be seen below when we see the implementation of the tokeniser. But, to be short, Scrubber does not play a role in tokenisation by separating tokens by whitespace. Scrubbing works by applying a single function or a pipeline of functions to the text. We have already loaded the Scrubber components registry above, so let's load the components we want. We can load them individually, as in the first example below, or we can specify multiple components in a tuple, as in the second example. In both cases, the returned variable is a function, which we can then feed to a scrubbing pipeline. # Load a component from the registry lower_case = scrubber_components . get ( \"lower_case\" ) # Or, if you want to do several at once... title_case , remove_digits = load_components (( \"title_case\" , \"remove_digits\" )) Now let's make the pipeline. We simply feed our component function names into the make_pipeline() function in the order we want them to be implemented. Notice that the remove_digits function has to be passed through the pipe() function, which enables arguments to be passed. The value returned is a function that implements the full pipeline when called on a text, as shown below. # Make the pipeline scrub = make_pipeline ( lower_case , title_case , pipe ( remove_digits , only = [ \"1\" ]) ) # Scrub the text scrubbed_text = scrub ( \"Lexos is the number 12 text analysis tool.\" ) This will return \"Lexos Is The Number 2 Text Analysis Tool\". You can also call component functions without a pipeline. For instance, scrubbed_text = remove_digits ( \"Lexos123\" , only = [ \"2\" , \"3\" ]) This will return \"Lexos1\". Custom Scrubbing Components \u00a4 The title_case function in the code in the pipeline above will not work because title_case is a custom component. To use it, we need to add it to the registry. # Define the custom function def title_case ( text : str ) -> str : \"\"\"Our custom function to convert text to title case.\"\"\" return text . title () # Register the custom function scrubber_components . register ( \"title_case\" , func = title_case ) Users can add whatever scrubbing functions they want. For development purposes, we can start by creating custom functions, and, if we use them a lot, migrate them to the permanent registry. Important To use a custom scrubbing function, you must register it before you call lexos.scrubber.registry.load_components . Tokenising Texts \u00a4 The tokenizer module is a big change for Lexos, as it formally separates tokenisation from preprocessing. At present, the Lexos user uses Scrubber to massage the text into shape using his or her implicit knowledge about the text's language. The text is then separated into tokens on whitespace by sklearn.feature_extraction.text.CountVectorizer . The API instead uses a language model to tokenize the text. The language model formalises the implicit rules the user supplies and thus automates the process. Built-in procedures appropriate to specific languages can often do a better job of tokenisation than the current Lexos approach. There some other advantages as well, which we'll see below. So the new Lexos tokenizer will load a language model and use its rules and predictions to separate the text into a list of tokens. Many filtering procedures will then be easy to implement with list comprehensions like no_stopwords = [ token for token in tokens if token not in stopwords ] Using language models will have other benefits. If the language of the text is English, for instance, an English language model can be loaded which allows tokens to be annotated automatically with labels for parts of speech information, lemmas, stop words, and other grammatical information at the time the text is tokenised. It then becomes possible to filter by part of speech, for instance, with something like only_nouns = [ token for token in tokens if token . pos_ == \"NOUN\" ] If no language model exists for the text's language, it will only be possible to tokenise using general rules, and it will not be possible to add other labels (at the tokenisation stage). But new language models, including models for historical languages, are being produced all the time, and this is a growing area of interest in DH. The Lexos API wraps the spaCy NLP library for loading models and tokenising texts. Because spaCy has excellent documentation and fairly wide acceptance in the DH community, it is a good tool to use under the bonnet. But it should be possible to add procedures for using libraries like NLTK. As a side note, the architecture of the scrubber module is built on top of the preprocessing functions in Textacy , which also accesses and extends spaCy. So let's see an example of how we tokenise a text. from lexos import tokenizer doc = tokenizer.make_doc(text) In spaCy, sequences of tokens are stored in a Doc object. So the Lexos tokenizer's make_doc() function creates a spaCy Doc (or just \"doc\"). Note that by default the tokenizer uses a small language model that has been trained for tokenisation and sentence segmentation on multiple languages. If you were making a document from a text in a language with a more sophisticated model, you would specify the model to be used. For instance, to use spaCy's small English model trained on web texts, you would call doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" ) The following are some examples of iterations that can be made over a spaCy doc. # Get a list of tokens tokens = [ token . text for token in doc ] # Get a list of non-punctuation tokens non_punct_tokens = [ token . text for token in doc if not token . is_punct ] The spaCy Doc is non-destructive because it preserves the original text alongside the list of tokens and their attributes. Indeed, calling doc.to_json() will return a JSON representation which gives the start and end position of each token in the original text! SpaCy docs are produced by a pipeline of components which can annotated with labels like parts of speech if the information is available in the language model. Each of these annotations is stored in the document's attributes. It is possible to extend spaCy's Doc object with its extension attribute. The Lexos API has a sample is_fruit extension, which is illustrated below. Note that extensions are accessed via the underscore prefix, as shown. # Indicate whether the token is labelled as fruit for token in doc : print ( token . _ . is_fruit ) In addition, there is a LexosDoc class, which provides a wrapper for spaCy docs. Its use is illustrated below. from lexos.tokenizer.lexosdoc import LexosDoc lexos_doc = LexosDoc ( doc ) tokens = lexos_doc . get_tokens () The example above just returns [token.text for token in doc] but it can be useful for producing clean code. In other cases, it might be useful to manipulate spaCy docs with methods that do not access their built-in or extended attributes or method. For instance, there is a method to check what attributes are available for tokens in the doc and a method for exporting the tokens and their attributes in a pandas dataframe. Note Notice in the code above that tokenizer has a make_docs() function to parse a list of texts into spaCy docs. Summary \u00a4 Here is a summary of the procedure so far. # Create the loader and load the data loader = Loader () loader . load ( data ) # Load Scrubber components, make a pipeline, and scrub the texts lower_case , remove_digits = load_components ( ( 'lower_case' , 'remove_digits' ) ) scrub = make_pipeline ( lower_case , pipe ( remove_digits , only = [ \"1\" ]) ) scrubbed_texts = [ scrub ( text ) for text in loader . texts ] # Tokenise the texts docs = tokenizer . make_docs ( scrubbed_texts ) Creating a Document-Term Matrix \u00a4 The function to generate a DTM from these docs has not yet been written, but it could already be done easily with Textacy as shown below (notice the LexosDoc(doc).get_tokens() is from the Lexos API \u2014 the rest is Textacy's Vectorizer , which is pretty sophisticated). from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = \"linear\" , idf_type = \"smooth\" , norm = \"l2\" , min_df = 3 , max_df = 0.95 ) tokenised_docs = ( LexosDoc ( doc ) . get_tokens () for doc in docs ) doc_term_matrix = vectorizer . fit_transform ( tokenised_docs ) The main difference here from the current procedure in Lexos, is that the text is pre-tokenised, rather than relying on sklearn.feature_extraction.text.CountVectorizer to do the dirty work.","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This page is a rough overview of the usage of the API. Important Aspects of the API may change before the tutorial is updated. At this stage, the tutorial should only be taken as a general guideline to the API's usage.","title":"Tutorial"},{"location":"tutorial/#getting-started","text":"Begin by importing in some modules in the Lexos API. from lexos.io.basic import Loader from lexos.scrubber.pipeline import make_pipeline , pipe from lexos.scrubber.registry import scrubber_components , load_components Here are some explanations: The io module contains IO functions. Right now, there is a \"basic\" Loader class that takes a file path, url, list of file paths or urls, or a directory name indicating where the source data is. More sophisticated loaders can be created later. The scrubber module consists of thematic \"components\": normalize , remove , replace , and so on. Each component has a number of functions, such as converting to lower case, removing digits, stripping tags, etc. Component functions are registered in a registry. They can be loaded into memory as needed and applied to texts in any order.","title":"Getting Started"},{"location":"tutorial/#loading-texts","text":"A typical workflow would create a lexos.io.basic.Loader object and call lexos.io.basic.Loader.load to load the data from disk or download it from the internet. You can access all loaded texts by calling Loader.texts . Note It is more efficient simply to use Python's open() to load texts into a list if you know the file's encoding. Currently, the main advantage of the Loader class is that it automatically coerces the data to Unicode. At this stage of development, the user or application developer is expected to maintain their data folders and know their file locations. More sophisticated project/corpus management methods could be added to the API at a later date. Here is a sample of the code for loading a single text file: # Data source data = \"tests/test_data/Austen_Pride.txt\" # Create the loader and load the data loader = Loader () loader . load ( data ) # Print the first text in the Loader text = loader . texts [ 0 ] print ( text )","title":"Loading Texts"},{"location":"tutorial/#scrubbing-texts","text":"Scrubber is now explicitly defined as a destructive preprocessor. In other words, it changes the text as loaded in ways that potentially make mapping the results onto the original text potentially impossible. It is therefore best used before other procedures so that the scrubbed text is essentially treated as the \"original\" text. The importance of this will be seen below when we see the implementation of the tokeniser. But, to be short, Scrubber does not play a role in tokenisation by separating tokens by whitespace. Scrubbing works by applying a single function or a pipeline of functions to the text. We have already loaded the Scrubber components registry above, so let's load the components we want. We can load them individually, as in the first example below, or we can specify multiple components in a tuple, as in the second example. In both cases, the returned variable is a function, which we can then feed to a scrubbing pipeline. # Load a component from the registry lower_case = scrubber_components . get ( \"lower_case\" ) # Or, if you want to do several at once... title_case , remove_digits = load_components (( \"title_case\" , \"remove_digits\" )) Now let's make the pipeline. We simply feed our component function names into the make_pipeline() function in the order we want them to be implemented. Notice that the remove_digits function has to be passed through the pipe() function, which enables arguments to be passed. The value returned is a function that implements the full pipeline when called on a text, as shown below. # Make the pipeline scrub = make_pipeline ( lower_case , title_case , pipe ( remove_digits , only = [ \"1\" ]) ) # Scrub the text scrubbed_text = scrub ( \"Lexos is the number 12 text analysis tool.\" ) This will return \"Lexos Is The Number 2 Text Analysis Tool\". You can also call component functions without a pipeline. For instance, scrubbed_text = remove_digits ( \"Lexos123\" , only = [ \"2\" , \"3\" ]) This will return \"Lexos1\".","title":"Scrubbing Texts"},{"location":"tutorial/#custom-scrubbing-components","text":"The title_case function in the code in the pipeline above will not work because title_case is a custom component. To use it, we need to add it to the registry. # Define the custom function def title_case ( text : str ) -> str : \"\"\"Our custom function to convert text to title case.\"\"\" return text . title () # Register the custom function scrubber_components . register ( \"title_case\" , func = title_case ) Users can add whatever scrubbing functions they want. For development purposes, we can start by creating custom functions, and, if we use them a lot, migrate them to the permanent registry. Important To use a custom scrubbing function, you must register it before you call lexos.scrubber.registry.load_components .","title":"Custom Scrubbing Components"},{"location":"tutorial/#tokenising-texts","text":"The tokenizer module is a big change for Lexos, as it formally separates tokenisation from preprocessing. At present, the Lexos user uses Scrubber to massage the text into shape using his or her implicit knowledge about the text's language. The text is then separated into tokens on whitespace by sklearn.feature_extraction.text.CountVectorizer . The API instead uses a language model to tokenize the text. The language model formalises the implicit rules the user supplies and thus automates the process. Built-in procedures appropriate to specific languages can often do a better job of tokenisation than the current Lexos approach. There some other advantages as well, which we'll see below. So the new Lexos tokenizer will load a language model and use its rules and predictions to separate the text into a list of tokens. Many filtering procedures will then be easy to implement with list comprehensions like no_stopwords = [ token for token in tokens if token not in stopwords ] Using language models will have other benefits. If the language of the text is English, for instance, an English language model can be loaded which allows tokens to be annotated automatically with labels for parts of speech information, lemmas, stop words, and other grammatical information at the time the text is tokenised. It then becomes possible to filter by part of speech, for instance, with something like only_nouns = [ token for token in tokens if token . pos_ == \"NOUN\" ] If no language model exists for the text's language, it will only be possible to tokenise using general rules, and it will not be possible to add other labels (at the tokenisation stage). But new language models, including models for historical languages, are being produced all the time, and this is a growing area of interest in DH. The Lexos API wraps the spaCy NLP library for loading models and tokenising texts. Because spaCy has excellent documentation and fairly wide acceptance in the DH community, it is a good tool to use under the bonnet. But it should be possible to add procedures for using libraries like NLTK. As a side note, the architecture of the scrubber module is built on top of the preprocessing functions in Textacy , which also accesses and extends spaCy. So let's see an example of how we tokenise a text. from lexos import tokenizer doc = tokenizer.make_doc(text) In spaCy, sequences of tokens are stored in a Doc object. So the Lexos tokenizer's make_doc() function creates a spaCy Doc (or just \"doc\"). Note that by default the tokenizer uses a small language model that has been trained for tokenisation and sentence segmentation on multiple languages. If you were making a document from a text in a language with a more sophisticated model, you would specify the model to be used. For instance, to use spaCy's small English model trained on web texts, you would call doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" ) The following are some examples of iterations that can be made over a spaCy doc. # Get a list of tokens tokens = [ token . text for token in doc ] # Get a list of non-punctuation tokens non_punct_tokens = [ token . text for token in doc if not token . is_punct ] The spaCy Doc is non-destructive because it preserves the original text alongside the list of tokens and their attributes. Indeed, calling doc.to_json() will return a JSON representation which gives the start and end position of each token in the original text! SpaCy docs are produced by a pipeline of components which can annotated with labels like parts of speech if the information is available in the language model. Each of these annotations is stored in the document's attributes. It is possible to extend spaCy's Doc object with its extension attribute. The Lexos API has a sample is_fruit extension, which is illustrated below. Note that extensions are accessed via the underscore prefix, as shown. # Indicate whether the token is labelled as fruit for token in doc : print ( token . _ . is_fruit ) In addition, there is a LexosDoc class, which provides a wrapper for spaCy docs. Its use is illustrated below. from lexos.tokenizer.lexosdoc import LexosDoc lexos_doc = LexosDoc ( doc ) tokens = lexos_doc . get_tokens () The example above just returns [token.text for token in doc] but it can be useful for producing clean code. In other cases, it might be useful to manipulate spaCy docs with methods that do not access their built-in or extended attributes or method. For instance, there is a method to check what attributes are available for tokens in the doc and a method for exporting the tokens and their attributes in a pandas dataframe. Note Notice in the code above that tokenizer has a make_docs() function to parse a list of texts into spaCy docs.","title":"Tokenising Texts"},{"location":"tutorial/#summary","text":"Here is a summary of the procedure so far. # Create the loader and load the data loader = Loader () loader . load ( data ) # Load Scrubber components, make a pipeline, and scrub the texts lower_case , remove_digits = load_components ( ( 'lower_case' , 'remove_digits' ) ) scrub = make_pipeline ( lower_case , pipe ( remove_digits , only = [ \"1\" ]) ) scrubbed_texts = [ scrub ( text ) for text in loader . texts ] # Tokenise the texts docs = tokenizer . make_docs ( scrubbed_texts )","title":"Summary"},{"location":"tutorial/#creating-a-document-term-matrix","text":"The function to generate a DTM from these docs has not yet been written, but it could already be done easily with Textacy as shown below (notice the LexosDoc(doc).get_tokens() is from the Lexos API \u2014 the rest is Textacy's Vectorizer , which is pretty sophisticated). from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = \"linear\" , idf_type = \"smooth\" , norm = \"l2\" , min_df = 3 , max_df = 0.95 ) tokenised_docs = ( LexosDoc ( doc ) . get_tokens () for doc in docs ) doc_term_matrix = vectorizer . fit_transform ( tokenised_docs ) The main difference here from the current procedure in Lexos, is that the text is pre-tokenised, rather than relying on sklearn.feature_extraction.text.CountVectorizer to do the dirty work.","title":"Creating a Document-Term Matrix"},{"location":"api/","text":"Overview \u00a4 A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. io A set of functions for handling input-output processes. utils A set of utility functions shared by multiple modules.","title":"Index"},{"location":"api/#overview","text":"A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. io A set of functions for handling input-output processes. utils A set of utility functions shared by multiple modules.","title":"Overview"},{"location":"api/overview/","text":"Overview \u00a4 A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. io A set of functions for handling input-output processes. utils A set of utility functions shared by multiple modules.","title":"Overview"},{"location":"api/overview/#overview","text":"A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. io A set of functions for handling input-output processes. utils A set of utility functions shared by multiple modules.","title":"Overview"},{"location":"api/utils/","text":"Utils \u00a4 This module contains helper functions used by multiple modules. lexos . utils . _decode_bytes ( raw_bytes ) private \u00a4 Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: Type Description str The decoded string. Source code in lexos\\utils.py def _decode_bytes ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: The decoded string. \"\"\" if isinstance ( raw_bytes , bytes ): try : decoded_str = _try_decode_bytes_ ( raw_bytes ) except ( UnicodeDecodeError , TypeError ): raise LexosException ( 'Chardet failed to detect encoding of your ' 'file. Please make sure your file is in ' 'utf-8 encoding.' ) else : decoded_str = raw_bytes return decoded_str lexos . utils . _try_decode_bytes_ ( raw_bytes ) private \u00a4 Try to decode raw bytes (helper function for decode_bytes(). Parameters: Name Type Description Default raw_bytes bytes The bytes you want to decode to string. required Returns: Type Description str A decoded string. Source code in lexos\\utils.py def _try_decode_bytes_ ( raw_bytes : bytes ) -> str : \"\"\"Try to decode raw bytes (helper function for decode_bytes(). Args: raw_bytes (bytes): The bytes you want to decode to string. Returns: A decoded string. \"\"\" # Detect the encoding with only the first couple of bytes encoding_detect = chardet . detect ( raw_bytes [: constants . MIN_ENCODING_DETECT ]) # Get the encoding encoding_type = encoding_detect [ 'encoding' ] if encoding_type is None : encoding_detect = chardet . detect ( raw_bytes ) encoding_type = encoding_detect [ 'encoding' ] try : # Try to decode the string using the original encoding decoded_string = raw_bytes . decode ( encoding_type ) except ( UnicodeDecodeError , TypeError ): # Try UnicodeDammit if chardet didn't work if encoding_type == \"ascii\" : dammit = UnicodeDammit ( raw_bytes , [ \"iso-8859-1\" , \"iso-8859-15\" , \"windows-1252\" ]) else : dammit = UnicodeDammit ( raw_bytes ) decoded_string = dammit . unicode_markup return decoded_string lexos . utils . ensure_list ( item ) \u00a4 Ensure string is converted to a Path. Parameters: Name Type Description Default item Any Anything. required Returns: Type Description List The item inside a list if it is not already a list. Source code in lexos\\utils.py def ensure_list ( item : Any ) -> List : \"\"\"Ensure string is converted to a Path. Args: item (Any): Anything. Returns: The item inside a list if it is not already a list. \"\"\" if not isinstance ( item , list ): item = [ item ] return item lexos . utils . ensure_path ( path ) \u00a4 Ensure string is converted to a Path. Parameters: Name Type Description Default path Any Anything. If string, it's converted to Path. required Returns: Type Description Any Path or original argument. Source code in lexos\\utils.py def ensure_path ( path : Any ) -> Any : \"\"\"Ensure string is converted to a Path. Args: path (Any): Anything. If string, it's converted to Path. Returns: Path or original argument. \"\"\" if isinstance ( path , str ): return Path ( path . replace ( ' \\\\ ' , '/' )) else : return path lexos . utils . is_url ( s ) \u00a4 Check if string is a URL. Source code in lexos\\utils.py def is_url ( s : str ) -> bool : \"\"\"Check if string is a URL.\"\"\" return bool ( re . match ( r \"(https?|ftp)://\" # protocol r \"(\\w+(\\-\\w+)*\\.)?\" # host (optional) r \"((\\w+(\\-\\w+)*)\\.(\\w+))\" # domain r \"(\\.\\w+)*\" # top-level domain (optional, can have > 1) r \"([\\w\\-\\._\\~/]*)*(?<!\\.)\" # path, params, anchors, etc. (optional) , s )) lexos . utils . unzip_archive ( archive_path , extract_dir ) \u00a4 Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python . Parameters: Name Type Description Default archive_path str The path to the archive file to be unzipped. required extract_dir str The path to folder where the archive will be extracted. required Source code in lexos\\utils.py def unzip_archive ( archive_path : str , extract_dir : str ): \"\"\"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python. Args: archive_path (str): The path to the archive file to be unzipped. extract_dir (str): The path to folder where the archive will be extracted. \"\"\" zf = zipfile . ZipFile ( archive_path , 'r' ) progress = Progress () with progress : for file in progress . track ( zf . infolist (), description = \"Processing...\" ): zf . extract ( file , path = extract_dir ) sleep ( 0.1 ) lexos . utils . zip_folder ( source_dir , archive_file ) \u00a4 Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Parameters: Name Type Description Default source_dir Path The path to the source directory. required archive_file Path The path to the archive file to be created (including file extension). required Source code in lexos\\utils.py def zip_folder ( source_dir : Path , archive_file : Path ): \"\"\"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Args: source_dir (Path): The path to the source directory. archive_file (Path): The path to the archive file to be created (including file extension). \"\"\" progress = Progress () with zipfile . ZipFile ( archive_file , mode = \"w\" , compression = zipfile . ZIP_DEFLATED , compresslevel = 7 ) as zip : files = list ( source_dir . rglob ( \"*\" )) with progress : for file in progress . track ( files , description = \"Processing...\" ): relative_path = file . relative_to ( source_dir ) zip . write ( file , arcname = relative_path ) sleep ( 0.1 ) lexos . utils . get_encoding ( input_string ) \u00a4 Use chardet to return the encoding type of a string. Parameters: Name Type Description Default input_string bytes A bytestring. required Returns: Type Description str The string's encoding type. Source code in lexos\\utils.py def get_encoding ( input_string : bytes ) -> str : \"\"\"Use chardet to return the encoding type of a string. Args: input_string (bytes): A bytestring. Returns: The string's encoding type. \"\"\" encoding_detect = chardet . detect ( input_string [ : constants . MIN_ENCODING_DETECT ]) encoding_type = encoding_detect [ 'encoding' ] return encoding_type lexos . utils . normalize ( raw_bytes ) \u00a4 Normalise a string to LexosFile format. Parameters: Name Type Description Default raw_bytes str The input string. required Returns: Type Description str Normalised version of the input string. Source code in lexos\\utils.py def normalize ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Normalise a string to LexosFile format. Args: raw_bytes (str): The input string. Returns: Normalised version of the input string. \"\"\" s = _decode_bytes ( raw_bytes ) return s lexos . utils . normalize_files ( filepaths , destination_dir = '.' ) \u00a4 Normalise a list of files to LexosFile format and save the files. Parameters: Name Type Description Default filepaths list The list of paths to input files. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_files ( filepaths : List [ Union [ Path , str ]], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a list of files to LexosFile format and save the files. Args: filepaths (list): The list of paths to input files. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" for filepath in filepaths : filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc )) lexos . utils . normalize_file ( filepath , destination_dir = '.' ) \u00a4 Normalise a file to LexosFile format and save the file. Parameters: Name Type Description Default filepath Path, str The path to the input file. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_file ( filepath : Union [ Path , str ], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a file to LexosFile format and save the file. Args: filepath (Path, str): The path to the input file. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc )) lexos . utils . normalize_strings ( strings ) \u00a4 Normalise a list of strings to LexosFile format. Parameters: Name Type Description Default strings list The list of input strings. required Returns: Type Description str A list of normalised versions of the input strings. Source code in lexos\\utils.py def normalize_strings ( strings : List [ Union [ bytes , str ]]) -> str : \"\"\"Normalise a list of strings to LexosFile format. Args: strings (list): The list of input strings. Returns: A list of normalised versions of the input strings. \"\"\" normalized_strings = [] for s in strings : normalized_strings . append ( normalize ( s )) return normalized_strings","title":"Utils"},{"location":"api/utils/#utils","text":"This module contains helper functions used by multiple modules.","title":"Utils"},{"location":"api/utils/#lexos.utils._decode_bytes","text":"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: Type Description str The decoded string. Source code in lexos\\utils.py def _decode_bytes ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: The decoded string. \"\"\" if isinstance ( raw_bytes , bytes ): try : decoded_str = _try_decode_bytes_ ( raw_bytes ) except ( UnicodeDecodeError , TypeError ): raise LexosException ( 'Chardet failed to detect encoding of your ' 'file. Please make sure your file is in ' 'utf-8 encoding.' ) else : decoded_str = raw_bytes return decoded_str","title":"_decode_bytes()"},{"location":"api/utils/#lexos.utils._try_decode_bytes_","text":"Try to decode raw bytes (helper function for decode_bytes(). Parameters: Name Type Description Default raw_bytes bytes The bytes you want to decode to string. required Returns: Type Description str A decoded string. Source code in lexos\\utils.py def _try_decode_bytes_ ( raw_bytes : bytes ) -> str : \"\"\"Try to decode raw bytes (helper function for decode_bytes(). Args: raw_bytes (bytes): The bytes you want to decode to string. Returns: A decoded string. \"\"\" # Detect the encoding with only the first couple of bytes encoding_detect = chardet . detect ( raw_bytes [: constants . MIN_ENCODING_DETECT ]) # Get the encoding encoding_type = encoding_detect [ 'encoding' ] if encoding_type is None : encoding_detect = chardet . detect ( raw_bytes ) encoding_type = encoding_detect [ 'encoding' ] try : # Try to decode the string using the original encoding decoded_string = raw_bytes . decode ( encoding_type ) except ( UnicodeDecodeError , TypeError ): # Try UnicodeDammit if chardet didn't work if encoding_type == \"ascii\" : dammit = UnicodeDammit ( raw_bytes , [ \"iso-8859-1\" , \"iso-8859-15\" , \"windows-1252\" ]) else : dammit = UnicodeDammit ( raw_bytes ) decoded_string = dammit . unicode_markup return decoded_string","title":"_try_decode_bytes_()"},{"location":"api/utils/#lexos.utils.ensure_list","text":"Ensure string is converted to a Path. Parameters: Name Type Description Default item Any Anything. required Returns: Type Description List The item inside a list if it is not already a list. Source code in lexos\\utils.py def ensure_list ( item : Any ) -> List : \"\"\"Ensure string is converted to a Path. Args: item (Any): Anything. Returns: The item inside a list if it is not already a list. \"\"\" if not isinstance ( item , list ): item = [ item ] return item","title":"ensure_list()"},{"location":"api/utils/#lexos.utils.ensure_path","text":"Ensure string is converted to a Path. Parameters: Name Type Description Default path Any Anything. If string, it's converted to Path. required Returns: Type Description Any Path or original argument. Source code in lexos\\utils.py def ensure_path ( path : Any ) -> Any : \"\"\"Ensure string is converted to a Path. Args: path (Any): Anything. If string, it's converted to Path. Returns: Path or original argument. \"\"\" if isinstance ( path , str ): return Path ( path . replace ( ' \\\\ ' , '/' )) else : return path","title":"ensure_path()"},{"location":"api/utils/#lexos.utils.is_url","text":"Check if string is a URL. Source code in lexos\\utils.py def is_url ( s : str ) -> bool : \"\"\"Check if string is a URL.\"\"\" return bool ( re . match ( r \"(https?|ftp)://\" # protocol r \"(\\w+(\\-\\w+)*\\.)?\" # host (optional) r \"((\\w+(\\-\\w+)*)\\.(\\w+))\" # domain r \"(\\.\\w+)*\" # top-level domain (optional, can have > 1) r \"([\\w\\-\\._\\~/]*)*(?<!\\.)\" # path, params, anchors, etc. (optional) , s ))","title":"is_url()"},{"location":"api/utils/#lexos.utils.unzip_archive","text":"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python . Parameters: Name Type Description Default archive_path str The path to the archive file to be unzipped. required extract_dir str The path to folder where the archive will be extracted. required Source code in lexos\\utils.py def unzip_archive ( archive_path : str , extract_dir : str ): \"\"\"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python. Args: archive_path (str): The path to the archive file to be unzipped. extract_dir (str): The path to folder where the archive will be extracted. \"\"\" zf = zipfile . ZipFile ( archive_path , 'r' ) progress = Progress () with progress : for file in progress . track ( zf . infolist (), description = \"Processing...\" ): zf . extract ( file , path = extract_dir ) sleep ( 0.1 )","title":"unzip_archive()"},{"location":"api/utils/#lexos.utils.zip_folder","text":"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Parameters: Name Type Description Default source_dir Path The path to the source directory. required archive_file Path The path to the archive file to be created (including file extension). required Source code in lexos\\utils.py def zip_folder ( source_dir : Path , archive_file : Path ): \"\"\"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Args: source_dir (Path): The path to the source directory. archive_file (Path): The path to the archive file to be created (including file extension). \"\"\" progress = Progress () with zipfile . ZipFile ( archive_file , mode = \"w\" , compression = zipfile . ZIP_DEFLATED , compresslevel = 7 ) as zip : files = list ( source_dir . rglob ( \"*\" )) with progress : for file in progress . track ( files , description = \"Processing...\" ): relative_path = file . relative_to ( source_dir ) zip . write ( file , arcname = relative_path ) sleep ( 0.1 )","title":"zip_folder()"},{"location":"api/utils/#lexos.utils.get_encoding","text":"Use chardet to return the encoding type of a string. Parameters: Name Type Description Default input_string bytes A bytestring. required Returns: Type Description str The string's encoding type. Source code in lexos\\utils.py def get_encoding ( input_string : bytes ) -> str : \"\"\"Use chardet to return the encoding type of a string. Args: input_string (bytes): A bytestring. Returns: The string's encoding type. \"\"\" encoding_detect = chardet . detect ( input_string [ : constants . MIN_ENCODING_DETECT ]) encoding_type = encoding_detect [ 'encoding' ] return encoding_type","title":"get_encoding()"},{"location":"api/utils/#lexos.utils.normalize","text":"Normalise a string to LexosFile format. Parameters: Name Type Description Default raw_bytes str The input string. required Returns: Type Description str Normalised version of the input string. Source code in lexos\\utils.py def normalize ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Normalise a string to LexosFile format. Args: raw_bytes (str): The input string. Returns: Normalised version of the input string. \"\"\" s = _decode_bytes ( raw_bytes ) return s","title":"normalize()"},{"location":"api/utils/#lexos.utils.normalize_files","text":"Normalise a list of files to LexosFile format and save the files. Parameters: Name Type Description Default filepaths list The list of paths to input files. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_files ( filepaths : List [ Union [ Path , str ]], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a list of files to LexosFile format and save the files. Args: filepaths (list): The list of paths to input files. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" for filepath in filepaths : filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc ))","title":"normalize_files()"},{"location":"api/utils/#lexos.utils.normalize_file","text":"Normalise a file to LexosFile format and save the file. Parameters: Name Type Description Default filepath Path, str The path to the input file. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_file ( filepath : Union [ Path , str ], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a file to LexosFile format and save the file. Args: filepath (Path, str): The path to the input file. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc ))","title":"normalize_file()"},{"location":"api/utils/#lexos.utils.normalize_strings","text":"Normalise a list of strings to LexosFile format. Parameters: Name Type Description Default strings list The list of input strings. required Returns: Type Description str A list of normalised versions of the input strings. Source code in lexos\\utils.py def normalize_strings ( strings : List [ Union [ bytes , str ]]) -> str : \"\"\"Normalise a list of strings to LexosFile format. Args: strings (list): The list of input strings. Returns: A list of normalised versions of the input strings. \"\"\" normalized_strings = [] for s in strings : normalized_strings . append ( normalize ( s )) return normalized_strings","title":"normalize_strings()"},{"location":"api/io/","text":"Basic \u00a4 Currently, the IO module contains a basic Loader class. lexos.io.basic.Loader \u00a4 Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\basic.py class Loader (): \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text.\"\"\" return utils . _decode_bytes ( text ) def _download_text ( self , url : str ) -> str : \"\"\"Download a text from a url.\"\"\" try : r = requests . get ( url ) r . raise_for_status () return self . _decode ( r . text ) except requests . exceptions . HTTPError as e : raise LexosException ( e . response . text ) def _validate_source ( self , source : Any ) -> bool : \"\"\"Validate a source.\"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): self . errors . append ( source ) return False else : return True def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" ) __init__ ( self ) special \u00a4 init method. Source code in lexos\\io\\basic.py def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True load ( self , source , decode = True ) \u00a4 Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union[List[Path, str], Path, str] A source or list of sources. required decode bool Whether to decode the source. True Exceptions: Type Description LexosException An error message. Source code in lexos\\io\\basic.py def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"Index"},{"location":"api/io/#basic","text":"Currently, the IO module contains a basic Loader class.","title":"Basic"},{"location":"api/io/#lexos.io.basic.Loader","text":"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\basic.py class Loader (): \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text.\"\"\" return utils . _decode_bytes ( text ) def _download_text ( self , url : str ) -> str : \"\"\"Download a text from a url.\"\"\" try : r = requests . get ( url ) r . raise_for_status () return self . _decode ( r . text ) except requests . exceptions . HTTPError as e : raise LexosException ( e . response . text ) def _validate_source ( self , source : Any ) -> bool : \"\"\"Validate a source.\"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): self . errors . append ( source ) return False else : return True def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"Loader"},{"location":"api/io/#lexos.io.basic.Loader.__init__","text":"init method. Source code in lexos\\io\\basic.py def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True","title":"__init__()"},{"location":"api/io/#lexos.io.basic.Loader.load","text":"Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union[List[Path, str], Path, str] A source or list of sources. required decode bool Whether to decode the source. True Exceptions: Type Description LexosException An error message. Source code in lexos\\io\\basic.py def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"load()"},{"location":"api/io/basic/","text":"Basic \u00a4 Currently, the IO module contains a basic Loader class. lexos.io.basic.Loader \u00a4 Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\basic.py class Loader (): \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text.\"\"\" return utils . _decode_bytes ( text ) def _download_text ( self , url : str ) -> str : \"\"\"Download a text from a url.\"\"\" try : r = requests . get ( url ) r . raise_for_status () return self . _decode ( r . text ) except requests . exceptions . HTTPError as e : raise LexosException ( e . response . text ) def _validate_source ( self , source : Any ) -> bool : \"\"\"Validate a source.\"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): self . errors . append ( source ) return False else : return True def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" ) __init__ ( self ) special \u00a4 init method. Source code in lexos\\io\\basic.py def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True load ( self , source , decode = True ) \u00a4 Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union[List[Path, str], Path, str] A source or list of sources. required decode bool Whether to decode the source. True Exceptions: Type Description LexosException An error message. Source code in lexos\\io\\basic.py def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"Basic"},{"location":"api/io/basic/#basic","text":"Currently, the IO module contains a basic Loader class.","title":"Basic"},{"location":"api/io/basic/#lexos.io.basic.Loader","text":"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\basic.py class Loader (): \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text.\"\"\" return utils . _decode_bytes ( text ) def _download_text ( self , url : str ) -> str : \"\"\"Download a text from a url.\"\"\" try : r = requests . get ( url ) r . raise_for_status () return self . _decode ( r . text ) except requests . exceptions . HTTPError as e : raise LexosException ( e . response . text ) def _validate_source ( self , source : Any ) -> bool : \"\"\"Validate a source.\"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): self . errors . append ( source ) return False else : return True def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"Loader"},{"location":"api/io/basic/#lexos.io.basic.Loader.__init__","text":"init method. Source code in lexos\\io\\basic.py def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True","title":"__init__()"},{"location":"api/io/basic/#lexos.io.basic.Loader.load","text":"Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union[List[Path, str], Path, str] A source or list of sources. required decode bool Whether to decode the source. True Exceptions: Type Description LexosException An error message. Source code in lexos\\io\\basic.py def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"load()"},{"location":"api/scrubber/","text":"Scrubber \u00a4 Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into five submodules: normalize A set of functions for massaging text into standardised forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Constains the lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Index"},{"location":"api/scrubber/#scrubber","text":"Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into five submodules: normalize A set of functions for massaging text into standardised forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Constains the lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Scrubber"},{"location":"api/scrubber/normalize/","text":"Normalize \u00a4 The normalize component of Scrubber contains functions to perform a variety of text manipulations. The functions are frequently applied at the beginning of a scrubbing pipeline. lexos . scrubber . normalize . bullet_points ( text ) \u00a4 Normalize bullet points. Normalises all \"fancy\" bullet point symbols in text to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's utils.normalize_bullets . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def bullet_points ( text : str ) -> str : \"\"\"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in `text` to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's `utils.normalize_bullets`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_BULLET_POINTS . sub ( r \"\\1-\" , text ) lexos . scrubber . normalize . hyphenated_words ( text ) \u00a4 Normalize hyphenated words. Normalize words in text that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's utils.normalize_hyphens . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def hyphenated_words ( text : str ) -> str : \"\"\"Normalize hyphenated words. Normalize words in `text` that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's `utils.normalize_hyphens`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_HYPHENATED_WORD . sub ( r \"\\1\\2\" , text ) lexos . scrubber . normalize . lower_case ( text ) \u00a4 Convert text to lower case. Parameters: Name Type Description Default text str The text to convert to lower case. required Returns: Type Description str The converted text. Source code in lexos\\scrubber\\normalize.py def lower_case ( text : str ) -> str : \"\"\"Convert `text` to lower case. Args: text (str): The text to convert to lower case. Returns: The converted text. \"\"\" return text . lower () lexos . scrubber . normalize . quotation_marks ( text ) \u00a4 Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in text to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's utils.normalize_quotation_marks . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def quotation_marks ( text : str ) -> str : \"\"\"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in `text` to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's `utils.normalize_quotation_marks`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return text . translate ( resources . QUOTE_TRANSLATION_TABLE ) lexos . scrubber . normalize . repeating_chars ( text , * , chars , maxn = 1 ) \u00a4 Normalize repeating characters in text . Truncating their number of consecutive repetitions to maxn . Duplicates Textacy's utils.normalize_repeating_chars . Parameters: Name Type Description Default text str The text to normalize. required chars str One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". required maxn int Maximum number of consecutive repetitions of chars to which longer repetitions will be truncated. 1 Returns: Type Description str str Source code in lexos\\scrubber\\normalize.py def repeating_chars ( text : str , * , chars : str , maxn : int = 1 ) -> str : \"\"\"Normalize repeating characters in `text`. Truncating their number of consecutive repetitions to `maxn`. Duplicates Textacy's `utils.normalize_repeating_chars`. Args: text (str): The text to normalize. chars: One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". maxn: Maximum number of consecutive repetitions of `chars` to which longer repetitions will be truncated. Returns: str \"\"\" return re . sub ( r \"( {} ){{ {} ,}}\" . format ( re . escape ( chars ), maxn + 1 ), chars * maxn , text ) lexos . scrubber . normalize . unicode ( text , * , form = 'NFC' ) \u00a4 Normalize unicode characters in text into canonical forms. Duplicates Textacy's utils.normalize_unicode . Parameters: Name Type Description Default text str The text to normalize. required form Literal['NFC', 'NFD', 'NFKC', 'NFKD'] Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. 'NFC' See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize Source code in lexos\\scrubber\\normalize.py def unicode ( text : str , * , form : Literal [ \"NFC\" , \"NFD\" , \"NFKC\" , \"NFKD\" ] = \"NFC\" ) -> str : \"\"\"Normalize unicode characters in `text` into canonical forms. Duplicates Textacy's `utils.normalize_unicode`. Args: text (str): The text to normalize. form: Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"\"\" return unicodedata . normalize ( form , text ) lexos . scrubber . normalize . whitespace ( text ) \u00a4 Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def whitespace ( text : str ) -> str : \"\"\"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" text = resources . RE_ZWSP . sub ( \"\" , text ) text = resources . RE_LINEBREAK . sub ( r \"\\n\" , text ) text = resources . RE_NONBREAKING_SPACE . sub ( \" \" , text ) return text . strip ()","title":"Normalize"},{"location":"api/scrubber/normalize/#normalize","text":"The normalize component of Scrubber contains functions to perform a variety of text manipulations. The functions are frequently applied at the beginning of a scrubbing pipeline.","title":"Normalize"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.bullet_points","text":"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in text to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's utils.normalize_bullets . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def bullet_points ( text : str ) -> str : \"\"\"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in `text` to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's `utils.normalize_bullets`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_BULLET_POINTS . sub ( r \"\\1-\" , text )","title":"bullet_points()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.hyphenated_words","text":"Normalize hyphenated words. Normalize words in text that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's utils.normalize_hyphens . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def hyphenated_words ( text : str ) -> str : \"\"\"Normalize hyphenated words. Normalize words in `text` that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's `utils.normalize_hyphens`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_HYPHENATED_WORD . sub ( r \"\\1\\2\" , text )","title":"hyphenated_words()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.lower_case","text":"Convert text to lower case. Parameters: Name Type Description Default text str The text to convert to lower case. required Returns: Type Description str The converted text. Source code in lexos\\scrubber\\normalize.py def lower_case ( text : str ) -> str : \"\"\"Convert `text` to lower case. Args: text (str): The text to convert to lower case. Returns: The converted text. \"\"\" return text . lower ()","title":"lower_case()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.quotation_marks","text":"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in text to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's utils.normalize_quotation_marks . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def quotation_marks ( text : str ) -> str : \"\"\"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in `text` to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's `utils.normalize_quotation_marks`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return text . translate ( resources . QUOTE_TRANSLATION_TABLE )","title":"quotation_marks()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.repeating_chars","text":"Normalize repeating characters in text . Truncating their number of consecutive repetitions to maxn . Duplicates Textacy's utils.normalize_repeating_chars . Parameters: Name Type Description Default text str The text to normalize. required chars str One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". required maxn int Maximum number of consecutive repetitions of chars to which longer repetitions will be truncated. 1 Returns: Type Description str str Source code in lexos\\scrubber\\normalize.py def repeating_chars ( text : str , * , chars : str , maxn : int = 1 ) -> str : \"\"\"Normalize repeating characters in `text`. Truncating their number of consecutive repetitions to `maxn`. Duplicates Textacy's `utils.normalize_repeating_chars`. Args: text (str): The text to normalize. chars: One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". maxn: Maximum number of consecutive repetitions of `chars` to which longer repetitions will be truncated. Returns: str \"\"\" return re . sub ( r \"( {} ){{ {} ,}}\" . format ( re . escape ( chars ), maxn + 1 ), chars * maxn , text )","title":"repeating_chars()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.unicode","text":"Normalize unicode characters in text into canonical forms. Duplicates Textacy's utils.normalize_unicode . Parameters: Name Type Description Default text str The text to normalize. required form Literal['NFC', 'NFD', 'NFKC', 'NFKD'] Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. 'NFC' See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize Source code in lexos\\scrubber\\normalize.py def unicode ( text : str , * , form : Literal [ \"NFC\" , \"NFD\" , \"NFKC\" , \"NFKD\" ] = \"NFC\" ) -> str : \"\"\"Normalize unicode characters in `text` into canonical forms. Duplicates Textacy's `utils.normalize_unicode`. Args: text (str): The text to normalize. form: Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"\"\" return unicodedata . normalize ( form , text )","title":"unicode()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.whitespace","text":"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def whitespace ( text : str ) -> str : \"\"\"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" text = resources . RE_ZWSP . sub ( \"\" , text ) text = resources . RE_LINEBREAK . sub ( r \"\\n\" , text ) text = resources . RE_NONBREAKING_SPACE . sub ( \" \" , text ) return text . strip ()","title":"whitespace()"},{"location":"api/scrubber/overview/","text":"Scrubber \u00a4 Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into five submodules: normalize A set of functions for massaging text into standardised forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Constains the api_lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Overview"},{"location":"api/scrubber/overview/#scrubber","text":"Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into five submodules: normalize A set of functions for massaging text into standardised forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Constains the api_lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Scrubber"},{"location":"api/scrubber/pipeline/","text":"Pipeline \u00a4 The pipeline component of Scrubber is used to manage an ordered application of Scrubber component functions to text. lexos . scrubber . pipeline . make_pipeline ( * funcs ) \u00a4 Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. python -m pip install cytoolz is required for this function to work. Use pipe (an alias for functools.partial ) to pass arguments to preprocessors. from lexos import scrubber scrubber = Scrubber . pipeline . make_pipeline ( scrubber . replace . hashtags , scrubber . replace . emojis , pipe ( scrubber . remove . punctuation , only = [ \".\" , \"?\" , \"!\" ]) ) scrubber ( \"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\" ) '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' Parameters: Name Type Description Default *funcs dict A series of functions to be applied to the text. () Returns: Type Description Callable[[str], str] Pipeline composed of *funcs that applies each in sequential order. Source code in lexos\\scrubber\\pipeline.py def make_pipeline ( * funcs : Callable [[ str ], str ]) -> Callable [[ str ], str ]: \"\"\"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. `python -m pip install cytoolz` is required for this function to work. Use `pipe` (an alias for `functools.partial`) to pass arguments to preprocessors. ```python from lexos import scrubber scrubber = Scrubber.pipeline.make_pipeline( scrubber.replace.hashtags, scrubber.replace.emojis, pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"]) ) scrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\") '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' ``` Args: *funcs (dict): A series of functions to be applied to the text. Returns: Pipeline composed of ``*funcs`` that applies each in sequential order. \"\"\" return functoolz . compose_left ( * funcs ) lexos . scrubber . pipeline . make_pipeline_from_tuple ( funcs ) \u00a4 Return a pipeline from a tuple. Parameters: Name Type Description Default funcs tuple A tuple containing callables or string names of functions. required Returns a tuple of functions. Source code in lexos\\scrubber\\pipeline.py def make_pipeline_from_tuple ( funcs : tuple ) -> tuple : \"\"\"Return a pipeline from a tuple. Args: funcs (tuple): A tuple containing callables or string names of functions. Returns a tuple of functions. \"\"\" return make_pipeline ( * [ eval ( x ) if isinstance ( x , str ) else x for x in funcs ]) Note lexos.scrubber.pipeline.make_pipeline_from_tuple is deprecated. It should not be necessary if you are using lexos.scrubber.registry . lexos . scrubber . pipeline . pipe ( func , * args , ** kwargs ) \u00a4 Apply functool.partial and add __name__ to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Parameters: Name Type Description Default func Callable A callable. required Returns: Type Description Callable A partial function with __name__ set to the name of the function. Source code in lexos\\scrubber\\pipeline.py def pipe ( func : Callable , * args , ** kwargs ) -> Callable : \"\"\"Apply functool.partial and add `__name__` to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Args: func (Callable): A callable. Returns: A partial function with `__name__` set to the name of the function. \"\"\" if not args and not kwargs : return func else : partial_func = partial ( func , * args , ** kwargs ) update_wrapper ( partial_func , func ) return partial_func","title":"Pipeline"},{"location":"api/scrubber/pipeline/#pipeline","text":"The pipeline component of Scrubber is used to manage an ordered application of Scrubber component functions to text.","title":"Pipeline"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline","text":"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. python -m pip install cytoolz is required for this function to work. Use pipe (an alias for functools.partial ) to pass arguments to preprocessors. from lexos import scrubber scrubber = Scrubber . pipeline . make_pipeline ( scrubber . replace . hashtags , scrubber . replace . emojis , pipe ( scrubber . remove . punctuation , only = [ \".\" , \"?\" , \"!\" ]) ) scrubber ( \"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\" ) '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' Parameters: Name Type Description Default *funcs dict A series of functions to be applied to the text. () Returns: Type Description Callable[[str], str] Pipeline composed of *funcs that applies each in sequential order. Source code in lexos\\scrubber\\pipeline.py def make_pipeline ( * funcs : Callable [[ str ], str ]) -> Callable [[ str ], str ]: \"\"\"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. `python -m pip install cytoolz` is required for this function to work. Use `pipe` (an alias for `functools.partial`) to pass arguments to preprocessors. ```python from lexos import scrubber scrubber = Scrubber.pipeline.make_pipeline( scrubber.replace.hashtags, scrubber.replace.emojis, pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"]) ) scrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\") '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' ``` Args: *funcs (dict): A series of functions to be applied to the text. Returns: Pipeline composed of ``*funcs`` that applies each in sequential order. \"\"\" return functoolz . compose_left ( * funcs )","title":"make_pipeline()"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline_from_tuple","text":"Return a pipeline from a tuple. Parameters: Name Type Description Default funcs tuple A tuple containing callables or string names of functions. required Returns a tuple of functions. Source code in lexos\\scrubber\\pipeline.py def make_pipeline_from_tuple ( funcs : tuple ) -> tuple : \"\"\"Return a pipeline from a tuple. Args: funcs (tuple): A tuple containing callables or string names of functions. Returns a tuple of functions. \"\"\" return make_pipeline ( * [ eval ( x ) if isinstance ( x , str ) else x for x in funcs ]) Note lexos.scrubber.pipeline.make_pipeline_from_tuple is deprecated. It should not be necessary if you are using lexos.scrubber.registry .","title":"make_pipeline_from_tuple()"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.pipe","text":"Apply functool.partial and add __name__ to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Parameters: Name Type Description Default func Callable A callable. required Returns: Type Description Callable A partial function with __name__ set to the name of the function. Source code in lexos\\scrubber\\pipeline.py def pipe ( func : Callable , * args , ** kwargs ) -> Callable : \"\"\"Apply functool.partial and add `__name__` to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Args: func (Callable): A callable. Returns: A partial function with `__name__` set to the name of the function. \"\"\" if not args and not kwargs : return func else : partial_func = partial ( func , * args , ** kwargs ) update_wrapper ( partial_func , func ) return partial_func","title":"pipe()"},{"location":"api/scrubber/registry/","text":"Registry \u00a4 The registry component of Scrubber maintains a catalogue of registered functions that can be imported individually as needed. The registry enables the functions to be referenced by name using string values. The code registry is created and accessed using the catalogue library by Explosion. Registered functions can be retrieved individually using lower_case = scrubber_components.get(\"lower_case\") . Multiple functions can be loaded using the load_components function: lexos . scrubber . registry . load_components ( t ) \u00a4 Load components from a tuple. Parameters: Name Type Description Default t tuple A tuple containing string names of functions. required Source code in lexos\\scrubber\\registry.py def load_components ( t : tuple ): \"\"\"Load components from a tuple. Args: t: A tuple containing string names of functions. \"\"\" for item in t : yield scrubber_components . get ( item ) Note Custom functions can be registered by first creating the function and then adding it to the registry. An example is given below: from lexos.scrubber.registry import scrubber_components def title_case ( text ): \"\"\"Convert text to title case using `title()`\"\"\" return text . title () scrubber_components . register ( \"title_case\" , func = title_case )","title":"Registry"},{"location":"api/scrubber/registry/#registry","text":"The registry component of Scrubber maintains a catalogue of registered functions that can be imported individually as needed. The registry enables the functions to be referenced by name using string values. The code registry is created and accessed using the catalogue library by Explosion. Registered functions can be retrieved individually using lower_case = scrubber_components.get(\"lower_case\") . Multiple functions can be loaded using the load_components function:","title":"Registry"},{"location":"api/scrubber/registry/#lexos.scrubber.registry.load_components","text":"Load components from a tuple. Parameters: Name Type Description Default t tuple A tuple containing string names of functions. required Source code in lexos\\scrubber\\registry.py def load_components ( t : tuple ): \"\"\"Load components from a tuple. Args: t: A tuple containing string names of functions. \"\"\" for item in t : yield scrubber_components . get ( item ) Note Custom functions can be registered by first creating the function and then adding it to the registry. An example is given below: from lexos.scrubber.registry import scrubber_components def title_case ( text ): \"\"\"Convert text to title case using `title()`\"\"\" return text . title () scrubber_components . register ( \"title_case\" , func = title_case )","title":"load_components()"},{"location":"api/scrubber/remove/","text":"Remove \u00a4 The remove component of Scrubber contains a set of functions for removing strings and patterns from text. lexos . scrubber . remove . accents ( text , * , fast = False , accents = None ) \u00a4 Remove accents from any accented unicode characters in text , either by replacing them with ASCII equivalents or removing them entirely. Parameters: Name Type Description Default text str The text from which accents will be removed. required fast bool If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. False accents Union[str, tuple] An optional string or tuple of strings indicating the names of diacritics to be stripped. None Returns: Type Description str str fast=True can be significantly faster than fast=False , but its transformation of text is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out unidecode : https://github.com/avian2/unidecode Source code in lexos\\scrubber\\remove.py def accents ( text : str , * , fast : bool = False , accents : Union [ str , tuple ] = None ) -> str : \"\"\" Remove accents from any accented unicode characters in `text`, either by replacing them with ASCII equivalents or removing them entirely. Args: text (str): The text from which accents will be removed. fast: If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. accents: An optional string or tuple of strings indicating the names of diacritics to be stripped. Returns: str Note: `fast=True` can be significantly faster than `fast=False`, but its transformation of `text` is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out `unidecode`: https://github.com/avian2/unidecode \"\"\" if fast is False : if accents : if isinstance ( accents , str ): accents = set ( unicodedata . lookup ( accents )) elif len ( accents ) == 1 : accents = set ( unicodedata . lookup ( accents [ 0 ])) else : accents = set ( map ( unicodedata . lookup , accents )) return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if char not in accents ) else : return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if not unicodedata . combining ( char ) ) else : return ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ascii\" , errors = \"ignore\" ) . decode ( \"ascii\" ) ) lexos . scrubber . remove . brackets ( text , * , only = None ) \u00a4 Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Parameters: Name Type Description Default text str The text from which brackets will be removed. required only Optional[str | Collection[str]] Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, \"square\" removes only those contents found between square brackets, while [\"round\", \"square\"] removes those contents found between square or round brackets, but not curly. None Returns: Type Description str str Note This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. Source code in lexos\\scrubber\\remove.py def brackets ( text : str , * , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Args: text (str): The text from which brackets will be removed. only: Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, `\"square\"` removes only those contents found between square brackets, while `[\"round\", \"square\"]` removes those contents found between square or round brackets, but not curly. Returns: str Note: This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. \"\"\" only = utils . to_collection ( only , val_type = str , col_type = set ) if only is None or \"curly\" in only : text = resources . RE_BRACKETS_CURLY . sub ( \"\" , text ) if only is None or \"square\" in only : text = resources . RE_BRACKETS_SQUARE . sub ( \"\" , text ) if only is None or \"round\" in only : text = resources . RE_BRACKETS_ROUND . sub ( \"\" , text ) return text lexos . scrubber . remove . digits ( text , * , only = None ) \u00a4 Remove digits. Remove digits from text by replacing all instances of digits (or a subset thereof specified by only ) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Parameters: Name Type Description Default text str The text from which digits will be removed. required only Optional[str | Collection[str]] Remove only those digits specified here. For example, \"9\" removes only 9, while [\"1\", \"2\", \"3\"] removes 1, 2, 3; if None, all unicode digits marks are removed. None Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def digits ( text : str , * , only : Optional [ str | Collection [ str ]] = None ) -> str : \"\"\"Remove digits. Remove digits from `text` by replacing all instances of digits (or a subset thereof specified by `only`) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Args: text (str): The text from which digits will be removed. only: Remove only those digits specified here. For example, `\"9\"` removes only 9, while `[\"1\", \"2\", \"3\"]` removes 1, 2, 3; if None, all unicode digits marks are removed. Returns: str \"\"\" if only : if isinstance ( only , list ): pattern = re . compile ( f '[ { \"\" . join ( only ) } ]' ) else : pattern = re . compile ( only ) else : # Using \".\" to represent any unicode character used to indicate # a decimal number, and \"***\" to represent any sequence of # unicode digits, this pattern will match: # 1) *** # 2) ***.*** unicode_digits = \"\" for i in range ( sys . maxunicode ): if unicodedata . category ( chr ( i )) . startswith ( 'N' ): unicode_digits = unicode_digits + chr ( i ) pattern = re . compile ( r \"([+-]?[\" + re . escape ( unicode_digits ) + r \"])|((?<=\" + re . escape ( unicode_digits ) + r \")[\\u0027|\\u002C|\\u002E|\\u00B7|\" r \"\\u02D9|\\u066B|\\u066C|\\u2396][\" + re . escape ( unicode_digits ) + r \"]+)\" , re . UNICODE ) return str ( re . sub ( pattern , r \" \" , text )) lexos . scrubber . remove . new_lines ( text ) \u00a4 Remove new lines. Remove all line-breaking spaces. Parameters: Name Type Description Default text str The text from which new lines will be removed. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\remove.py def new_lines ( text : str ) -> str : \"\"\"Remove new lines. Remove all line-breaking spaces. Args: text (str): The text from which new lines will be removed. Returns: The normalized text. \"\"\" return resources . RE_LINEBREAK . sub ( \"\" , text ) . strip () lexos . scrubber . remove . pattern ( text , * , pattern ) \u00a4 Remove strings from text using a regex pattern. Parameters: Name Type Description Default text str The text from which patterns will be removed. required pattern Union[str, Collection[str]] The pattern to match. required Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def pattern ( text : str , * , pattern : Union [ str , Collection [ str ]] ) -> str : \"\"\"Remove strings from `text` using a regex pattern. Args: text (str): The text from which patterns will be removed. pattern: The pattern to match. Returns: str \"\"\" if isinstance ( pattern , list ): pattern = \"|\" . join ( pattern ) pat = re . compile ( pattern ) return re . sub ( pat , \"\" , text ) lexos . scrubber . remove . punctuation ( text , * , exclude = None , only = None ) \u00a4 Remove punctuation from text . Removes all instances of punctuation (or a subset thereof specified by only ). Parameters: Name Type Description Default text str The text from which punctuation will be removed. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\remove.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove punctuation from `text`. Removes all instances of punctuation (or a subset thereof specified by `only`). Args: text (str): The text from which punctuation will be removed. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \"\" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) else : exclude = [] # Note: We can't use the cached translation table because it replaces # the punctuation with whitespace, so we have to build a new one. translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \"\" ) return text . translate ( translation_table ) lexos . scrubber . remove . tabs ( text ) \u00a4 Remove tabs. If you want to replace tabs with a single space, use normalize.whitespace() instead. Parameters: Name Type Description Default text str The text from which tabs will be removed. required Returns: Type Description str The stripped text. Source code in lexos\\scrubber\\remove.py def tabs ( text : str ) -> str : \"\"\"Remove tabs. If you want to replace tabs with a single space, use `normalize.whitespace()` instead. Args: text (str): The text from which tabs will be removed. Returns: The stripped text. \"\"\" return resources . RE_TAB . sub ( \"\" , text ) lexos . scrubber . remove . tags ( text , sep = ' ' , remove_whitespace = True ) \u00a4 Remove tags from text . Parameters: Name Type Description Default text str The text from which tags will be removed. required sep str A string to insert between tags and text found between them. ' ' remove_whitespace bool If True, remove extra whitespace between text after tags are removed. True Returns: Type Description str A string containing just the text found between tags and other non-data elements. Note If you want to perfom selective removal of tags, use replace.tag_map instead. This function relies on the stdlib html.parser.HTMLParser . It appears to work for stripping tags from both html and xml. Using lxml or BeautifulSoup might be faster, but this is untested. This function preserves text in comments, as well as tags Source code in lexos\\scrubber\\remove.py def tags ( text : str , sep : str = \" \" , remove_whitespace : bool = True ) -> str : \"\"\"Remove tags from `text`. Args: text (str): The text from which tags will be removed. sep: A string to insert between tags and text found between them. remove_whitespace: If True, remove extra whitespace between text after tags are removed. Returns: A string containing just the text found between tags and other non-data elements. Note: - If you want to perfom selective removal of tags, use `replace.tag_map` instead. - This function relies on the stdlib `html.parser.HTMLParser`. It appears to work for stripping tags from both html and xml. Using `lxml` or BeautifulSoup might be faster, but this is untested. - This function preserves text in comments, as well as tags \"\"\" parser = resources . HTMLTextExtractor () parser . feed ( text ) text = parser . get_text ( sep = sep ) if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , sep , text , re . UNICODE ) return text","title":"Remove"},{"location":"api/scrubber/remove/#remove","text":"The remove component of Scrubber contains a set of functions for removing strings and patterns from text.","title":"Remove"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.accents","text":"Remove accents from any accented unicode characters in text , either by replacing them with ASCII equivalents or removing them entirely. Parameters: Name Type Description Default text str The text from which accents will be removed. required fast bool If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. False accents Union[str, tuple] An optional string or tuple of strings indicating the names of diacritics to be stripped. None Returns: Type Description str str fast=True can be significantly faster than fast=False , but its transformation of text is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out unidecode : https://github.com/avian2/unidecode Source code in lexos\\scrubber\\remove.py def accents ( text : str , * , fast : bool = False , accents : Union [ str , tuple ] = None ) -> str : \"\"\" Remove accents from any accented unicode characters in `text`, either by replacing them with ASCII equivalents or removing them entirely. Args: text (str): The text from which accents will be removed. fast: If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. accents: An optional string or tuple of strings indicating the names of diacritics to be stripped. Returns: str Note: `fast=True` can be significantly faster than `fast=False`, but its transformation of `text` is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out `unidecode`: https://github.com/avian2/unidecode \"\"\" if fast is False : if accents : if isinstance ( accents , str ): accents = set ( unicodedata . lookup ( accents )) elif len ( accents ) == 1 : accents = set ( unicodedata . lookup ( accents [ 0 ])) else : accents = set ( map ( unicodedata . lookup , accents )) return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if char not in accents ) else : return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if not unicodedata . combining ( char ) ) else : return ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ascii\" , errors = \"ignore\" ) . decode ( \"ascii\" ) )","title":"accents()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.brackets","text":"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Parameters: Name Type Description Default text str The text from which brackets will be removed. required only Optional[str | Collection[str]] Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, \"square\" removes only those contents found between square brackets, while [\"round\", \"square\"] removes those contents found between square or round brackets, but not curly. None Returns: Type Description str str Note This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. Source code in lexos\\scrubber\\remove.py def brackets ( text : str , * , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Args: text (str): The text from which brackets will be removed. only: Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, `\"square\"` removes only those contents found between square brackets, while `[\"round\", \"square\"]` removes those contents found between square or round brackets, but not curly. Returns: str Note: This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. \"\"\" only = utils . to_collection ( only , val_type = str , col_type = set ) if only is None or \"curly\" in only : text = resources . RE_BRACKETS_CURLY . sub ( \"\" , text ) if only is None or \"square\" in only : text = resources . RE_BRACKETS_SQUARE . sub ( \"\" , text ) if only is None or \"round\" in only : text = resources . RE_BRACKETS_ROUND . sub ( \"\" , text ) return text","title":"brackets()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.digits","text":"Remove digits. Remove digits from text by replacing all instances of digits (or a subset thereof specified by only ) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Parameters: Name Type Description Default text str The text from which digits will be removed. required only Optional[str | Collection[str]] Remove only those digits specified here. For example, \"9\" removes only 9, while [\"1\", \"2\", \"3\"] removes 1, 2, 3; if None, all unicode digits marks are removed. None Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def digits ( text : str , * , only : Optional [ str | Collection [ str ]] = None ) -> str : \"\"\"Remove digits. Remove digits from `text` by replacing all instances of digits (or a subset thereof specified by `only`) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Args: text (str): The text from which digits will be removed. only: Remove only those digits specified here. For example, `\"9\"` removes only 9, while `[\"1\", \"2\", \"3\"]` removes 1, 2, 3; if None, all unicode digits marks are removed. Returns: str \"\"\" if only : if isinstance ( only , list ): pattern = re . compile ( f '[ { \"\" . join ( only ) } ]' ) else : pattern = re . compile ( only ) else : # Using \".\" to represent any unicode character used to indicate # a decimal number, and \"***\" to represent any sequence of # unicode digits, this pattern will match: # 1) *** # 2) ***.*** unicode_digits = \"\" for i in range ( sys . maxunicode ): if unicodedata . category ( chr ( i )) . startswith ( 'N' ): unicode_digits = unicode_digits + chr ( i ) pattern = re . compile ( r \"([+-]?[\" + re . escape ( unicode_digits ) + r \"])|((?<=\" + re . escape ( unicode_digits ) + r \")[\\u0027|\\u002C|\\u002E|\\u00B7|\" r \"\\u02D9|\\u066B|\\u066C|\\u2396][\" + re . escape ( unicode_digits ) + r \"]+)\" , re . UNICODE ) return str ( re . sub ( pattern , r \" \" , text ))","title":"digits()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.new_lines","text":"Remove new lines. Remove all line-breaking spaces. Parameters: Name Type Description Default text str The text from which new lines will be removed. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\remove.py def new_lines ( text : str ) -> str : \"\"\"Remove new lines. Remove all line-breaking spaces. Args: text (str): The text from which new lines will be removed. Returns: The normalized text. \"\"\" return resources . RE_LINEBREAK . sub ( \"\" , text ) . strip ()","title":"new_lines()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.pattern","text":"Remove strings from text using a regex pattern. Parameters: Name Type Description Default text str The text from which patterns will be removed. required pattern Union[str, Collection[str]] The pattern to match. required Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def pattern ( text : str , * , pattern : Union [ str , Collection [ str ]] ) -> str : \"\"\"Remove strings from `text` using a regex pattern. Args: text (str): The text from which patterns will be removed. pattern: The pattern to match. Returns: str \"\"\" if isinstance ( pattern , list ): pattern = \"|\" . join ( pattern ) pat = re . compile ( pattern ) return re . sub ( pat , \"\" , text )","title":"pattern()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.punctuation","text":"Remove punctuation from text . Removes all instances of punctuation (or a subset thereof specified by only ). Parameters: Name Type Description Default text str The text from which punctuation will be removed. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\remove.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove punctuation from `text`. Removes all instances of punctuation (or a subset thereof specified by `only`). Args: text (str): The text from which punctuation will be removed. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \"\" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) else : exclude = [] # Note: We can't use the cached translation table because it replaces # the punctuation with whitespace, so we have to build a new one. translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \"\" ) return text . translate ( translation_table )","title":"punctuation()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tabs","text":"Remove tabs. If you want to replace tabs with a single space, use normalize.whitespace() instead. Parameters: Name Type Description Default text str The text from which tabs will be removed. required Returns: Type Description str The stripped text. Source code in lexos\\scrubber\\remove.py def tabs ( text : str ) -> str : \"\"\"Remove tabs. If you want to replace tabs with a single space, use `normalize.whitespace()` instead. Args: text (str): The text from which tabs will be removed. Returns: The stripped text. \"\"\" return resources . RE_TAB . sub ( \"\" , text )","title":"tabs()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tags","text":"Remove tags from text . Parameters: Name Type Description Default text str The text from which tags will be removed. required sep str A string to insert between tags and text found between them. ' ' remove_whitespace bool If True, remove extra whitespace between text after tags are removed. True Returns: Type Description str A string containing just the text found between tags and other non-data elements. Note If you want to perfom selective removal of tags, use replace.tag_map instead. This function relies on the stdlib html.parser.HTMLParser . It appears to work for stripping tags from both html and xml. Using lxml or BeautifulSoup might be faster, but this is untested. This function preserves text in comments, as well as tags Source code in lexos\\scrubber\\remove.py def tags ( text : str , sep : str = \" \" , remove_whitespace : bool = True ) -> str : \"\"\"Remove tags from `text`. Args: text (str): The text from which tags will be removed. sep: A string to insert between tags and text found between them. remove_whitespace: If True, remove extra whitespace between text after tags are removed. Returns: A string containing just the text found between tags and other non-data elements. Note: - If you want to perfom selective removal of tags, use `replace.tag_map` instead. - This function relies on the stdlib `html.parser.HTMLParser`. It appears to work for stripping tags from both html and xml. Using `lxml` or BeautifulSoup might be faster, but this is untested. - This function preserves text in comments, as well as tags \"\"\" parser = resources . HTMLTextExtractor () parser . feed ( text ) text = parser . get_text ( sep = sep ) if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , sep , text , re . UNICODE ) return text","title":"tags()"},{"location":"api/scrubber/replace/","text":"Replace \u00a4 The replace component of Scrubber contains a set of functions for replacing strings and patterns in text. Important Some functions have the same names as functions in the remove component. To distinguish them in the registry, replace functions with the same names are prefixed with re_ . When loaded into a script, they can be given any name the user desires. lexos . scrubber . replace . currency_symbols ( text , repl = '_CUR_' ) \u00a4 Replace all currency symbols in text with repl . Parameters: Name Type Description Default text str The text in which currency symbols will be replaced. required repl str The replacement value for currency symbols. '_CUR_' Returns: Type Description str The text with currency symbols replaced. Source code in lexos\\scrubber\\replace.py def currency_symbols ( text : str , repl : str = \"_CUR_\" ) -> str : \"\"\"Replace all currency symbols in `text` with `repl`. Args: text (str): The text in which currency symbols will be replaced. repl (str): The replacement value for currency symbols. Returns: str: The text with currency symbols replaced. \"\"\" return resources . RE_CURRENCY_SYMBOL . sub ( repl , text ) lexos . scrubber . replace . digits ( text , repl = '_DIGIT_' ) \u00a4 Replace all digits in text with repl . Parameters: Name Type Description Default text str The text in which digits will be replaced. required repl str The replacement value for digits. '_DIGIT_' Returns: Type Description str The text with digits replaced. Source code in lexos\\scrubber\\replace.py def digits ( text : str , repl : str = \"_DIGIT_\" ) -> str : \"\"\"Replace all digits in `text` with `repl`. Args: text (str): The text in which digits will be replaced. repl (str): The replacement value for digits. Returns: str: The text with digits replaced. \"\"\" return resources . RE_NUMBER . sub ( repl , text ) lexos . scrubber . replace . emails ( text , repl = '_EMAIL_' ) \u00a4 Replace all email addresses in text with repl . Parameters: Name Type Description Default text str The text in which emails will be replaced. required repl str The replacement value for emails. '_EMAIL_' Returns: Type Description str The text with emails replaced. Source code in lexos\\scrubber\\replace.py def emails ( text : str , repl : str = \"_EMAIL_\" ) -> str : \"\"\"Replace all email addresses in `text` with `repl`. Args: text (str): The text in which emails will be replaced. repl (str): The replacement value for emails. Returns: str: The text with emails replaced. \"\"\" return resources . RE_EMAIL . sub ( repl , text ) lexos . scrubber . replace . emojis ( text , repl = '_EMOJI_' ) \u00a4 Replace all emoji and pictographs in text with repl . Parameters: Name Type Description Default text str The text in which emojis will be replaced. required repl str The replacement value for emojis. '_EMOJI_' Returns: Type Description str The text with emojis replaced. Note If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! Source code in lexos\\scrubber\\replace.py def emojis ( text : str , repl : str = \"_EMOJI_\" ) -> str : \"\"\" Replace all emoji and pictographs in `text` with `repl`. Args: text (str): The text in which emojis will be replaced. repl (str): The replacement value for emojis. Returns: str: The text with emojis replaced. Note: If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! \"\"\" return resources . RE_EMOJI . sub ( repl , text ) lexos . scrubber . replace . hashtags ( text , repl = '_HASHTAG_' ) \u00a4 Replace all hashtags in text with repl . Parameters: Name Type Description Default text str The text in which hashtags will be replaced. required repl str The replacement value for hashtags. '_HASHTAG_' Returns: Type Description str The text with currency hashtags replaced. Source code in lexos\\scrubber\\replace.py def hashtags ( text : str , repl : str = \"_HASHTAG_\" ) -> str : \"\"\"Replace all hashtags in `text` with `repl`. Args: text (str): The text in which hashtags will be replaced. repl (str): The replacement value for hashtags. Returns: str: The text with currency hashtags replaced. \"\"\" return resources . RE_HASHTAG . sub ( repl , text ) lexos . scrubber . replace . pattern ( text , * , pattern ) \u00a4 Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which a pattern or pattern will be replaced. required pattern Union[dict, Collection[dict]] (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). required Returns: Type Description str The text with pattern(s) replaced. Source code in lexos\\scrubber\\replace.py def pattern ( text : str , * , pattern : Union [ dict , Collection [ dict ]] ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which a pattern or pattern will be replaced. pattern: (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). Returns: str: The text with pattern(s) replaced. \"\"\" pattern = utils . ensure_list ( pattern ) for pat in pattern : k = str ( * pat ) match = re . compile ( k ) text = re . sub ( match , pat [ k ], text ) return text lexos . scrubber . replace . phone_numbers ( text , repl = '_PHONE_' ) \u00a4 Replace all phone numbers in text with repl . Parameters: Name Type Description Default text str The text in which phone numbers will be replaced. required repl str The replacement value for phone numbers. '_PHONE_' Returns: Type Description str The text with phone numbers replaced. Source code in lexos\\scrubber\\replace.py def phone_numbers ( text : str , repl : str = \"_PHONE_\" ) -> str : \"\"\"Replace all phone numbers in `text` with `repl`. Args: text (str): The text in which phone numbers will be replaced. repl (str): The replacement value for phone numbers. Returns: str: The text with phone numbers replaced. \"\"\" return resources . RE_PHONE_NUMBER . sub ( repl , text ) lexos . scrubber . replace . process_tag_replace_options ( orig_text , tag , action , attribute ) \u00a4 Replace html-style tags in text files according to user options. Parameters: Name Type Description Default orig_text str The user's text containing the original tag. required tag str The particular tag to be processed. required action str A string specifying the action to be performed on the tag. required attribute str Replacement value for tag when \"replace_with_attribute\" is specified. required Action options are required - \"remove_tag\" Remove the tag required - \"remove_element\" Remove the element and contents required - \"replace_element\" Replace the tag with the specified attribute required Returns: Type Description str The text after the specified tag is processed. Source code in lexos\\scrubber\\replace.py def process_tag_replace_options ( orig_text : str , tag : str , action : str , attribute : str ) -> str : \"\"\"Replace html-style tags in text files according to user options. Args: orig_text: The user's text containing the original tag. tag: The particular tag to be processed. action: A string specifying the action to be performed on the tag. attribute: Replacement value for tag when \"replace_with_attribute\" is specified. Action options are: - \"remove_tag\": Remove the tag - \"remove_element\": Remove the element and contents - \"replace_element\": Replace the tag with the specified attribute Returns: str: The text after the specified tag is processed. Note: The replacement of a tag with the value of an attribute may not be supported. This needs a second look. \"\"\" if action == \"remove_tag\" : # searching for variants this specific tag: <tag> ... pattern = re . compile ( r '<(?:' + tag + r '(?=\\s)(?!(?:[^>\" \\' ]|\"[^\"]*\"| \\' [^ \\' ]* \\' )*?(?<=\\s)' r '\\s*=)(?!\\s*/?>)\\s+(?:\".*?\"| \\' .*? \\' |[^>]*?)+|/?' + tag + r '\\s*/?)>' , re . MULTILINE | re . DOTALL | re . UNICODE ) # substitute all matching patterns with one space processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"remove_element\" : # <[whitespaces] TAG [SPACE attributes]> contents </[whitespaces]TAG> # as applied across newlines, (re.MULTILINE), on re.UNICODE, # and .* includes newlines (re.DOTALL) pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \"( .+?>|>).+?</\\s*\" + re . escape ( tag ) + \">\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"replace_element\" : pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \".*?>.+?</\\s*\" + re . escape ( tag ) + \".*?>\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , attribute , orig_text ) else : processed_text = orig_text # Leave Tag Alone return processed_text lexos . scrubber . replace . punctuation ( text , * , exclude = None , only = None ) \u00a4 Replace punctuation from text . Replaces all instances of punctuation (or a subset thereof specified by only ) with whitespace. Parameters: Name Type Description Default text str The text in which punctuation will be replaced. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\replace.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Replace punctuation from `text`. Replaces all instances of punctuation (or a subset thereof specified by `only`) with whitespace. Args: text (str): The text in which punctuation will be replaced. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \" \" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \" \" ) else : translation_table = resources . PUNCT_TRANSLATION_TABLE return text . translate ( translation_table ) lexos . scrubber . replace . special_characters ( text , * , is_html = False , ruleset = None ) \u00a4 Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which special characters will be replaced. required is_html bool Whether to replace HTML entities. False ruleset dict A dict containing the special characters to match and their replacements. None Returns: Type Description str str Source code in lexos\\scrubber\\replace.py def special_characters ( text : str , * , is_html : bool = False , ruleset : dict = None , ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which special characters will be replaced. is_html (bool): Whether to replace HTML entities. ruleset (dict): A dict containing the special characters to match and their replacements. Returns: str \"\"\" if is_html : text = html . unescape ( text ) else : for k , v in ruleset . items (): match = re . compile ( k ) text = re . sub ( match , v , text ) return text lexos . scrubber . replace . tag_map ( text , map , remove_comments = True , remove_doctype = True , remove_whitespace = False ) \u00a4 Handle tags that are found in the text. Parameters: Name Type Description Default text str The text in which tags will be replaced. required remove_comments bool Whether to remove comments. True remove_doctype bool Whether to remove the doctype or xml declaration. True remove_whitespace bool Whether to remove whitespace. False Returns: Type Description str The text after tags have been replaced. Source code in lexos\\scrubber\\replace.py def tag_map ( text : str , # xmlhandlingoptions: List[dict], map : Dict [ str ], remove_comments : bool = True , remove_doctype : bool = True , remove_whitespace : bool = False ) -> str : \"\"\"Handle tags that are found in the text. Args: text (str): The text in which tags will be replaced. remove_comments (bool): Whether to remove comments. remove_doctype (bool): Whether to remove the doctype or xml declaration. remove_whitespace (bool): Whether to remove whitespace. Returns: str: The text after tags have been replaced. \"\"\" if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) # Remove extra white space if remove_doctype : doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations text = re . sub ( r \"(<\\?.*?>)\" , \"\" , text ) # Remove xml declarations if remove_comments : text = re . sub ( r \"(<!--.*?-->)\" , \"\" , text ) # Remove comments # This matches the DOCTYPE and all internal entity declarations doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations # Visit each tag: for tag , opts in map . items (): action = opts [ \"action\" ] attribute = opts [ \"attribute\" ] text = process_tag_replace_options ( text , tag , action , attribute ) # One last catch-all removes extra whitespace from all the removed tags if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) return text lexos . scrubber . replace . urls ( text , repl = '_URL_' ) \u00a4 Replace all URLs in text with repl . Parameters: Name Type Description Default text str The text in which urls will be replaced. required repl str The replacement value for urls. '_URL_' Returns: Type Description str The text with urls replaced. Source code in lexos\\scrubber\\replace.py def urls ( text : str , repl : str = \"_URL_\" ) -> str : \"\"\"Replace all URLs in `text` with `repl`. Args: text (str): The text in which urls will be replaced. repl (str): The replacement value for urls. Returns: str: The text with urls replaced. \"\"\" return resources . RE_SHORT_URL . sub ( repl , resources . RE_URL . sub ( repl , text )) lexos . scrubber . replace . user_handles ( text , repl = '_USER_' ) \u00a4 Replace all (Twitter-style) user handles in text with repl . Parameters: Name Type Description Default text str The text in which user handles will be replaced. required repl str The replacement value for user handles. '_USER_' Returns: Type Description str The text with user handles replaced. Source code in lexos\\scrubber\\replace.py def user_handles ( text : str , repl : str = \"_USER_\" ) -> str : \"\"\"Replace all (Twitter-style) user handles in `text` with `repl`. Args: text (str): The text in which user handles will be replaced. repl (str): The replacement value for user handles. Returns: str: The text with user handles replaced. \"\"\" return resources . RE_USER_HANDLE . sub ( repl , text )","title":"Replace"},{"location":"api/scrubber/replace/#replace","text":"The replace component of Scrubber contains a set of functions for replacing strings and patterns in text. Important Some functions have the same names as functions in the remove component. To distinguish them in the registry, replace functions with the same names are prefixed with re_ . When loaded into a script, they can be given any name the user desires.","title":"Replace"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.currency_symbols","text":"Replace all currency symbols in text with repl . Parameters: Name Type Description Default text str The text in which currency symbols will be replaced. required repl str The replacement value for currency symbols. '_CUR_' Returns: Type Description str The text with currency symbols replaced. Source code in lexos\\scrubber\\replace.py def currency_symbols ( text : str , repl : str = \"_CUR_\" ) -> str : \"\"\"Replace all currency symbols in `text` with `repl`. Args: text (str): The text in which currency symbols will be replaced. repl (str): The replacement value for currency symbols. Returns: str: The text with currency symbols replaced. \"\"\" return resources . RE_CURRENCY_SYMBOL . sub ( repl , text )","title":"currency_symbols()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.digits","text":"Replace all digits in text with repl . Parameters: Name Type Description Default text str The text in which digits will be replaced. required repl str The replacement value for digits. '_DIGIT_' Returns: Type Description str The text with digits replaced. Source code in lexos\\scrubber\\replace.py def digits ( text : str , repl : str = \"_DIGIT_\" ) -> str : \"\"\"Replace all digits in `text` with `repl`. Args: text (str): The text in which digits will be replaced. repl (str): The replacement value for digits. Returns: str: The text with digits replaced. \"\"\" return resources . RE_NUMBER . sub ( repl , text )","title":"digits()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emails","text":"Replace all email addresses in text with repl . Parameters: Name Type Description Default text str The text in which emails will be replaced. required repl str The replacement value for emails. '_EMAIL_' Returns: Type Description str The text with emails replaced. Source code in lexos\\scrubber\\replace.py def emails ( text : str , repl : str = \"_EMAIL_\" ) -> str : \"\"\"Replace all email addresses in `text` with `repl`. Args: text (str): The text in which emails will be replaced. repl (str): The replacement value for emails. Returns: str: The text with emails replaced. \"\"\" return resources . RE_EMAIL . sub ( repl , text )","title":"emails()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emojis","text":"Replace all emoji and pictographs in text with repl . Parameters: Name Type Description Default text str The text in which emojis will be replaced. required repl str The replacement value for emojis. '_EMOJI_' Returns: Type Description str The text with emojis replaced. Note If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! Source code in lexos\\scrubber\\replace.py def emojis ( text : str , repl : str = \"_EMOJI_\" ) -> str : \"\"\" Replace all emoji and pictographs in `text` with `repl`. Args: text (str): The text in which emojis will be replaced. repl (str): The replacement value for emojis. Returns: str: The text with emojis replaced. Note: If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! \"\"\" return resources . RE_EMOJI . sub ( repl , text )","title":"emojis()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.hashtags","text":"Replace all hashtags in text with repl . Parameters: Name Type Description Default text str The text in which hashtags will be replaced. required repl str The replacement value for hashtags. '_HASHTAG_' Returns: Type Description str The text with currency hashtags replaced. Source code in lexos\\scrubber\\replace.py def hashtags ( text : str , repl : str = \"_HASHTAG_\" ) -> str : \"\"\"Replace all hashtags in `text` with `repl`. Args: text (str): The text in which hashtags will be replaced. repl (str): The replacement value for hashtags. Returns: str: The text with currency hashtags replaced. \"\"\" return resources . RE_HASHTAG . sub ( repl , text )","title":"hashtags()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.pattern","text":"Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which a pattern or pattern will be replaced. required pattern Union[dict, Collection[dict]] (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). required Returns: Type Description str The text with pattern(s) replaced. Source code in lexos\\scrubber\\replace.py def pattern ( text : str , * , pattern : Union [ dict , Collection [ dict ]] ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which a pattern or pattern will be replaced. pattern: (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). Returns: str: The text with pattern(s) replaced. \"\"\" pattern = utils . ensure_list ( pattern ) for pat in pattern : k = str ( * pat ) match = re . compile ( k ) text = re . sub ( match , pat [ k ], text ) return text","title":"pattern()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.phone_numbers","text":"Replace all phone numbers in text with repl . Parameters: Name Type Description Default text str The text in which phone numbers will be replaced. required repl str The replacement value for phone numbers. '_PHONE_' Returns: Type Description str The text with phone numbers replaced. Source code in lexos\\scrubber\\replace.py def phone_numbers ( text : str , repl : str = \"_PHONE_\" ) -> str : \"\"\"Replace all phone numbers in `text` with `repl`. Args: text (str): The text in which phone numbers will be replaced. repl (str): The replacement value for phone numbers. Returns: str: The text with phone numbers replaced. \"\"\" return resources . RE_PHONE_NUMBER . sub ( repl , text )","title":"phone_numbers()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.process_tag_replace_options","text":"Replace html-style tags in text files according to user options. Parameters: Name Type Description Default orig_text str The user's text containing the original tag. required tag str The particular tag to be processed. required action str A string specifying the action to be performed on the tag. required attribute str Replacement value for tag when \"replace_with_attribute\" is specified. required Action options are required - \"remove_tag\" Remove the tag required - \"remove_element\" Remove the element and contents required - \"replace_element\" Replace the tag with the specified attribute required Returns: Type Description str The text after the specified tag is processed. Source code in lexos\\scrubber\\replace.py def process_tag_replace_options ( orig_text : str , tag : str , action : str , attribute : str ) -> str : \"\"\"Replace html-style tags in text files according to user options. Args: orig_text: The user's text containing the original tag. tag: The particular tag to be processed. action: A string specifying the action to be performed on the tag. attribute: Replacement value for tag when \"replace_with_attribute\" is specified. Action options are: - \"remove_tag\": Remove the tag - \"remove_element\": Remove the element and contents - \"replace_element\": Replace the tag with the specified attribute Returns: str: The text after the specified tag is processed. Note: The replacement of a tag with the value of an attribute may not be supported. This needs a second look. \"\"\" if action == \"remove_tag\" : # searching for variants this specific tag: <tag> ... pattern = re . compile ( r '<(?:' + tag + r '(?=\\s)(?!(?:[^>\" \\' ]|\"[^\"]*\"| \\' [^ \\' ]* \\' )*?(?<=\\s)' r '\\s*=)(?!\\s*/?>)\\s+(?:\".*?\"| \\' .*? \\' |[^>]*?)+|/?' + tag + r '\\s*/?)>' , re . MULTILINE | re . DOTALL | re . UNICODE ) # substitute all matching patterns with one space processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"remove_element\" : # <[whitespaces] TAG [SPACE attributes]> contents </[whitespaces]TAG> # as applied across newlines, (re.MULTILINE), on re.UNICODE, # and .* includes newlines (re.DOTALL) pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \"( .+?>|>).+?</\\s*\" + re . escape ( tag ) + \">\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"replace_element\" : pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \".*?>.+?</\\s*\" + re . escape ( tag ) + \".*?>\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , attribute , orig_text ) else : processed_text = orig_text # Leave Tag Alone return processed_text","title":"process_tag_replace_options()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.punctuation","text":"Replace punctuation from text . Replaces all instances of punctuation (or a subset thereof specified by only ) with whitespace. Parameters: Name Type Description Default text str The text in which punctuation will be replaced. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\replace.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Replace punctuation from `text`. Replaces all instances of punctuation (or a subset thereof specified by `only`) with whitespace. Args: text (str): The text in which punctuation will be replaced. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \" \" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \" \" ) else : translation_table = resources . PUNCT_TRANSLATION_TABLE return text . translate ( translation_table )","title":"punctuation()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.special_characters","text":"Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which special characters will be replaced. required is_html bool Whether to replace HTML entities. False ruleset dict A dict containing the special characters to match and their replacements. None Returns: Type Description str str Source code in lexos\\scrubber\\replace.py def special_characters ( text : str , * , is_html : bool = False , ruleset : dict = None , ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which special characters will be replaced. is_html (bool): Whether to replace HTML entities. ruleset (dict): A dict containing the special characters to match and their replacements. Returns: str \"\"\" if is_html : text = html . unescape ( text ) else : for k , v in ruleset . items (): match = re . compile ( k ) text = re . sub ( match , v , text ) return text","title":"special_characters()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.tag_map","text":"Handle tags that are found in the text. Parameters: Name Type Description Default text str The text in which tags will be replaced. required remove_comments bool Whether to remove comments. True remove_doctype bool Whether to remove the doctype or xml declaration. True remove_whitespace bool Whether to remove whitespace. False Returns: Type Description str The text after tags have been replaced. Source code in lexos\\scrubber\\replace.py def tag_map ( text : str , # xmlhandlingoptions: List[dict], map : Dict [ str ], remove_comments : bool = True , remove_doctype : bool = True , remove_whitespace : bool = False ) -> str : \"\"\"Handle tags that are found in the text. Args: text (str): The text in which tags will be replaced. remove_comments (bool): Whether to remove comments. remove_doctype (bool): Whether to remove the doctype or xml declaration. remove_whitespace (bool): Whether to remove whitespace. Returns: str: The text after tags have been replaced. \"\"\" if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) # Remove extra white space if remove_doctype : doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations text = re . sub ( r \"(<\\?.*?>)\" , \"\" , text ) # Remove xml declarations if remove_comments : text = re . sub ( r \"(<!--.*?-->)\" , \"\" , text ) # Remove comments # This matches the DOCTYPE and all internal entity declarations doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations # Visit each tag: for tag , opts in map . items (): action = opts [ \"action\" ] attribute = opts [ \"attribute\" ] text = process_tag_replace_options ( text , tag , action , attribute ) # One last catch-all removes extra whitespace from all the removed tags if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) return text","title":"tag_map()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.urls","text":"Replace all URLs in text with repl . Parameters: Name Type Description Default text str The text in which urls will be replaced. required repl str The replacement value for urls. '_URL_' Returns: Type Description str The text with urls replaced. Source code in lexos\\scrubber\\replace.py def urls ( text : str , repl : str = \"_URL_\" ) -> str : \"\"\"Replace all URLs in `text` with `repl`. Args: text (str): The text in which urls will be replaced. repl (str): The replacement value for urls. Returns: str: The text with urls replaced. \"\"\" return resources . RE_SHORT_URL . sub ( repl , resources . RE_URL . sub ( repl , text ))","title":"urls()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.user_handles","text":"Replace all (Twitter-style) user handles in text with repl . Parameters: Name Type Description Default text str The text in which user handles will be replaced. required repl str The replacement value for user handles. '_USER_' Returns: Type Description str The text with user handles replaced. Source code in lexos\\scrubber\\replace.py def user_handles ( text : str , repl : str = \"_USER_\" ) -> str : \"\"\"Replace all (Twitter-style) user handles in `text` with `repl`. Args: text (str): The text in which user handles will be replaced. repl (str): The replacement value for user handles. Returns: str: The text with user handles replaced. \"\"\" return resources . RE_USER_HANDLE . sub ( repl , text )","title":"user_handles()"},{"location":"api/scrubber/resources/","text":"Resources \u00a4 The resources component of Scrubber contains a set of functions for replacing strings and patterns in text. lexos.scrubber.resources.HTMLTextExtractor ( HTMLParser ) \u00a4 Simple subclass of :class: html.parser.HTMLParser to collect data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via :meth: HTMLTextExtractor.get_text() . Note Users probably shouldn't deal with this class directly; instead, use :func: remove.remove_html_tags()`. Source code in lexos\\scrubber\\resources.py class HTMLTextExtractor ( html . parser . HTMLParser ): \"\"\" Simple subclass of :class:`html.parser.HTMLParser` to collect data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via :meth:`HTMLTextExtractor.get_text()`. Note: Users probably shouldn't deal with this class directly; instead, use `:func:`remove.remove_html_tags()`. \"\"\" def __init__ ( self ): super () . __init__ () self . data = [] def handle_data ( self , data ): self . data . append ( data ) def get_text ( self , sep : str = \"\" ) -> str : return sep . join ( self . data ) . strip () lexos . scrubber . resources . _get_punct_translation_table () private \u00a4 Source code in lexos\\scrubber\\resources.py @functools . lru_cache ( maxsize = None ) def _get_punct_translation_table (): return dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) ), \" \" ) lexos . scrubber . resources . __getattr__ ( name ) \u00a4 Source code in lexos\\scrubber\\resources.py def __getattr__ ( name : str ) -> Any : if name == \"PUNCT_TRANSLATION_TABLE\" : return _get_punct_translation_table () else : raise AttributeError ( f \"module { __name__ !r} has no attribute { name !r} \" ) Constants \u00a4 There are also a number of constants: QUOTE_TRANSLATION_TABLE RE_BRACKETS_CURLY RE_BRACKETS_ROUND RE_BRACKETS_SQUARE RE_BULLET_POINTS RE_CURRENCY_SYMBOL RE_EMAIL RE_EMOJI RE_HASHTAG RE_HYPHENATED_WORD RE_LINEBREAK RE_NONBREAKING_SPACE RE_NUMBER RE_PHONE_NUMBER RE_SHORT_URL RE_TAB RE_URL RE_USER_HANDLE RE_ZWSP","title":"Resources"},{"location":"api/scrubber/resources/#resources","text":"The resources component of Scrubber contains a set of functions for replacing strings and patterns in text.","title":"Resources"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor","text":"Simple subclass of :class: html.parser.HTMLParser to collect data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via :meth: HTMLTextExtractor.get_text() . Note Users probably shouldn't deal with this class directly; instead, use :func: remove.remove_html_tags()`. Source code in lexos\\scrubber\\resources.py class HTMLTextExtractor ( html . parser . HTMLParser ): \"\"\" Simple subclass of :class:`html.parser.HTMLParser` to collect data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via :meth:`HTMLTextExtractor.get_text()`. Note: Users probably shouldn't deal with this class directly; instead, use `:func:`remove.remove_html_tags()`. \"\"\" def __init__ ( self ): super () . __init__ () self . data = [] def handle_data ( self , data ): self . data . append ( data ) def get_text ( self , sep : str = \"\" ) -> str : return sep . join ( self . data ) . strip ()","title":"HTMLTextExtractor"},{"location":"api/scrubber/resources/#lexos.scrubber.resources._get_punct_translation_table","text":"Source code in lexos\\scrubber\\resources.py @functools . lru_cache ( maxsize = None ) def _get_punct_translation_table (): return dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) ), \" \" )","title":"_get_punct_translation_table()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.__getattr__","text":"Source code in lexos\\scrubber\\resources.py def __getattr__ ( name : str ) -> Any : if name == \"PUNCT_TRANSLATION_TABLE\" : return _get_punct_translation_table () else : raise AttributeError ( f \"module { __name__ !r} has no attribute { name !r} \" )","title":"__getattr__()"},{"location":"api/scrubber/resources/#constants","text":"There are also a number of constants: QUOTE_TRANSLATION_TABLE RE_BRACKETS_CURLY RE_BRACKETS_ROUND RE_BRACKETS_SQUARE RE_BULLET_POINTS RE_CURRENCY_SYMBOL RE_EMAIL RE_EMOJI RE_HASHTAG RE_HYPHENATED_WORD RE_LINEBREAK RE_NONBREAKING_SPACE RE_NUMBER RE_PHONE_NUMBER RE_SHORT_URL RE_TAB RE_URL RE_USER_HANDLE RE_ZWSP","title":"Constants"},{"location":"api/scrubber/scrubber/","text":"Scrubber \u00a4 The scrubber component of Scrubber contains a class for managing scrubbing pipelines. lexos.scrubber.scrubber.Scrubber \u00a4 Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) Source code in lexos\\scrubber\\scrubber.py class Scrubber : \"\"\"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) \"\"\" def __init__ ( self ): self . texts = [] self . pipeline = None def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline ) def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts add_pipeline ( self , * funcs ) \u00a4 Add a pipeline. Parameters: Name Type Description Default *funcs Callable[[str], str] The functions to add to the pipeline. () Source code in lexos\\scrubber\\scrubber.py def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) get_pipeline ( self ) \u00a4 Return a tuple representation of the pipeline. Source code in lexos\\scrubber\\scrubber.py def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) scrub ( self , data ) \u00a4 Scrub a text or list of texts. Parameters: Name Type Description Default data Union[List[str], str] The text or list of texts to scrub. required Returns: Type Description list A list of scrubbed texts. Source code in lexos\\scrubber\\scrubber.py def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts set_pipeline ( self , pipeline ) \u00a4 Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of get_pipeline() , rather than passing callables. Parameters: Name Type Description Default pipeline tuple A tuple of functions. required Source code in lexos\\scrubber\\scrubber.py def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline )","title":"Scrubber"},{"location":"api/scrubber/scrubber/#scrubber","text":"The scrubber component of Scrubber contains a class for managing scrubbing pipelines.","title":"Scrubber"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber","text":"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) Source code in lexos\\scrubber\\scrubber.py class Scrubber : \"\"\"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) \"\"\" def __init__ ( self ): self . texts = [] self . pipeline = None def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline ) def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts","title":"Scrubber"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.add_pipeline","text":"Add a pipeline. Parameters: Name Type Description Default *funcs Callable[[str], str] The functions to add to the pipeline. () Source code in lexos\\scrubber\\scrubber.py def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs )","title":"add_pipeline()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.get_pipeline","text":"Return a tuple representation of the pipeline. Source code in lexos\\scrubber\\scrubber.py def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline )","title":"get_pipeline()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.scrub","text":"Scrub a text or list of texts. Parameters: Name Type Description Default data Union[List[str], str] The text or list of texts to scrub. required Returns: Type Description list A list of scrubbed texts. Source code in lexos\\scrubber\\scrubber.py def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts","title":"scrub()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.set_pipeline","text":"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of get_pipeline() , rather than passing callables. Parameters: Name Type Description Default pipeline tuple A tuple of functions. required Source code in lexos\\scrubber\\scrubber.py def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline )","title":"set_pipeline()"},{"location":"api/scrubber/utils/","text":"Utils \u00a4 The utils component of Scrubber contains helper functions shared by the other components. lexos . scrubber . utils . get_tags ( text ) \u00a4 Get information about the tags in a text. Parameters: Name Type Description Default text str The text to be analyzed. required Returns: Type Description dict A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. it falls back to BeautifulSoup's parser. Source code in lexos\\scrubber\\utils.py def get_tags ( text ): \"\"\"Get information about the tags in a text. Args: text (str): The text to be analyzed. Returns: dict: A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. Note: The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. \"\"\" import json import re from natsort import humansorted from xml.etree import ElementTree tags = [] attributes = [] try : root = ElementTree . fromstring ( text ) for element in root . iter (): if re . sub ( \"{.+}\" , \"\" , element . tag ) not in tags : tags . append ( re . sub ( \"{.+}\" , \"\" , element . tag )) if element . attrib != {}: attributes . append ({ re . sub ( \"{.+}\" , \"\" , element . tag ): element . attrib }) tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) except ElementTree . ParseError : import bs4 from bs4 import BeautifulSoup soup = BeautifulSoup ( text , \"xml\" ) for e in soup : if isinstance ( e , bs4 . element . ProcessingInstruction ): e . extract () [ tags . append ( tag . name ) for tag in soup . find_all ()] [ attributes . append ({ tag . name : tag . attrs }) for tag in soup . find_all ()] tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) return { \"tags\" : tags , \"attributes\" : attributes }","title":"Utils"},{"location":"api/scrubber/utils/#utils","text":"The utils component of Scrubber contains helper functions shared by the other components.","title":"Utils"},{"location":"api/scrubber/utils/#lexos.scrubber.utils.get_tags","text":"Get information about the tags in a text. Parameters: Name Type Description Default text str The text to be analyzed. required Returns: Type Description dict A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. it falls back to BeautifulSoup's parser. Source code in lexos\\scrubber\\utils.py def get_tags ( text ): \"\"\"Get information about the tags in a text. Args: text (str): The text to be analyzed. Returns: dict: A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. Note: The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. \"\"\" import json import re from natsort import humansorted from xml.etree import ElementTree tags = [] attributes = [] try : root = ElementTree . fromstring ( text ) for element in root . iter (): if re . sub ( \"{.+}\" , \"\" , element . tag ) not in tags : tags . append ( re . sub ( \"{.+}\" , \"\" , element . tag )) if element . attrib != {}: attributes . append ({ re . sub ( \"{.+}\" , \"\" , element . tag ): element . attrib }) tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) except ElementTree . ParseError : import bs4 from bs4 import BeautifulSoup soup = BeautifulSoup ( text , \"xml\" ) for e in soup : if isinstance ( e , bs4 . element . ProcessingInstruction ): e . extract () [ tags . append ( tag . name ) for tag in soup . find_all ()] [ attributes . append ({ tag . name : tag . attrs }) for tag in soup . find_all ()] tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) return { \"tags\" : tags , \"attributes\" : attributes }","title":"get_tags()"},{"location":"api/tokenizer/","text":"Overview \u00a4 The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used. lexos . tokenizer . make_doc ( text , model = 'xx_sent_ud_sm' , disable = []) \u00a4 Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: object: A spaCy doc object. \"\"\" nlp = spacy . load ( model ) return nlp ( text , disable = disable ) lexos . tokenizer . make_docs ( texts , model = 'xx_sent_ud_sm' , disable = []) \u00a4 Return a list of docs from a text or list of texts. Parameters: Name Type Description Default text Union[List[str], str] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description list A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> List : \"\"\"Return a list of docs from a text or list of texts. Args: text (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: list: A list of spaCy doc objects. \"\"\" nlp = spacy . load ( model ) return list ( nlp . pipe ( utils . ensure_list ( texts ), disable = disable ))","title":"Index"},{"location":"api/tokenizer/#overview","text":"The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used.","title":"Overview"},{"location":"api/tokenizer/#lexos.tokenizer.make_doc","text":"Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: object: A spaCy doc object. \"\"\" nlp = spacy . load ( model ) return nlp ( text , disable = disable )","title":"make_doc()"},{"location":"api/tokenizer/#lexos.tokenizer.make_docs","text":"Return a list of docs from a text or list of texts. Parameters: Name Type Description Default text Union[List[str], str] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description list A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> List : \"\"\"Return a list of docs from a text or list of texts. Args: text (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: list: A list of spaCy doc objects. \"\"\" nlp = spacy . load ( model ) return list ( nlp . pipe ( utils . ensure_list ( texts ), disable = disable ))","title":"make_docs()"},{"location":"api/tokenizer/extensions/","text":"Extensions \u00a4 This is a set of extensions to spaCy docs allowing custom attributes and methods. Typically, they woudld be accessed with an underscore prefix like doc._.is_fruit or doc._.get(\"is_fruit\") . Extensions are set with code like fruits = [ \"apple\" , \"pear\" , \"banana\" , \"orange\" , \"strawberry\" ] is_fruit_getter = lambda token : token . text in fruits Token . set_extension ( \"is_fruit\" , getter = is_fruit_getter ) See the spaCy custom attributes documentation for full details. lexos . tokenizer . extensions . is_fruit_getter ( token ) \u00a4 Source code in lexos\\tokenizer\\extensions.py is_fruit_getter = lambda token : token . text in fruits Note This is really a proof of concept function. A better example can be added in the future.","title":"Extensions"},{"location":"api/tokenizer/extensions/#extensions","text":"This is a set of extensions to spaCy docs allowing custom attributes and methods. Typically, they woudld be accessed with an underscore prefix like doc._.is_fruit or doc._.get(\"is_fruit\") . Extensions are set with code like fruits = [ \"apple\" , \"pear\" , \"banana\" , \"orange\" , \"strawberry\" ] is_fruit_getter = lambda token : token . text in fruits Token . set_extension ( \"is_fruit\" , getter = is_fruit_getter ) See the spaCy custom attributes documentation for full details.","title":"Extensions"},{"location":"api/tokenizer/extensions/#lexos.tokenizer.extensions.is_fruit_getter","text":"Source code in lexos\\tokenizer\\extensions.py is_fruit_getter = lambda token : token . text in fruits Note This is really a proof of concept function. A better example can be added in the future.","title":"is_fruit_getter()"},{"location":"api/tokenizer/lexosdoc/","text":"LexosDoc \u00a4 A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. lexos.tokenizer.lexosdoc.LexosDoc \u00a4 A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. Source code in lexos\\tokenizer\\lexosdoc.py class LexosDoc (): \"\"\"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called `tokenizer.py` and import them. If certain functions get used commonly, they can be turned into Doc extensions. \"\"\" def __init__ ( self , doc : object ): self . doc = doc def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols ) get_token_attrs ( self ) \u00a4 Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. Source code in lexos\\tokenizer\\lexosdoc.py def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } get_tokens ( self ) \u00a4 Return a list of tokens in the doc. Source code in lexos\\tokenizer\\lexosdoc.py def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] to_dataframe ( self , cols = [ 'text' ], show_ranges = True ) \u00a4 Get a pandas dataframe of the doc attributes. Parameters: Name Type Description Default cols List[str] A list of columns to include in the dataframe. ['text'] show_ranges bool Whether to include the token start and end positions in the dataframe. True Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call LexosDoc.get_token_attrs() first to check which attributes are available for the doc. Source code in lexos\\tokenizer\\lexosdoc.py def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols )","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexosdoc","text":"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions.","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc","text":"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. Source code in lexos\\tokenizer\\lexosdoc.py class LexosDoc (): \"\"\"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called `tokenizer.py` and import them. If certain functions get used commonly, they can be turned into Doc extensions. \"\"\" def __init__ ( self , doc : object ): self . doc = doc def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols )","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_token_attrs","text":"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. Source code in lexos\\tokenizer\\lexosdoc.py def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts }","title":"get_token_attrs()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_tokens","text":"Return a list of tokens in the doc. Source code in lexos\\tokenizer\\lexosdoc.py def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ]","title":"get_tokens()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.to_dataframe","text":"Get a pandas dataframe of the doc attributes. Parameters: Name Type Description Default cols List[str] A list of columns to include in the dataframe. ['text'] show_ranges bool Whether to include the token start and end positions in the dataframe. True Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call LexosDoc.get_token_attrs() first to check which attributes are available for the doc. Source code in lexos\\tokenizer\\lexosdoc.py def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols )","title":"to_dataframe()"},{"location":"api/tokenizer/overview/","text":"Overview \u00a4 The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used. lexos . tokenizer . make_doc ( text , model = 'xx_sent_ud_sm' , disable = []) \u00a4 Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: object: A spaCy doc object. \"\"\" nlp = spacy . load ( model ) return nlp ( text , disable = disable ) lexos . tokenizer . make_docs ( texts , model = 'xx_sent_ud_sm' , disable = []) \u00a4 Return a list of docs from a text or list of texts. Parameters: Name Type Description Default text Union[List[str], str] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description list A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> List : \"\"\"Return a list of docs from a text or list of texts. Args: text (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: list: A list of spaCy doc objects. \"\"\" nlp = spacy . load ( model ) return list ( nlp . pipe ( utils . ensure_list ( texts ), disable = disable ))","title":"Overview"},{"location":"api/tokenizer/overview/#overview","text":"The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used.","title":"Overview"},{"location":"api/tokenizer/overview/#lexos.tokenizer.make_doc","text":"Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: object: A spaCy doc object. \"\"\" nlp = spacy . load ( model ) return nlp ( text , disable = disable )","title":"make_doc()"},{"location":"api/tokenizer/overview/#lexos.tokenizer.make_docs","text":"Return a list of docs from a text or list of texts. Parameters: Name Type Description Default text Union[List[str], str] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description list A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> List : \"\"\"Return a list of docs from a text or list of texts. Args: text (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: list: A list of spaCy doc objects. \"\"\" nlp = spacy . load ( model ) return list ( nlp . pipe ( utils . ensure_list ( texts ), disable = disable ))","title":"make_docs()"}]}