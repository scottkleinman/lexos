{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Lexos Python library reproduces and extends most of the text analysis tools in the Lexos web app. Lexos is designed to implement many common text analysis procedures in a way that saves the user having to re-invent the wheel or figure out how to combine multiple Python packages to achieve a given result. It is intended to be used as a library in other projects to build backend functions for applications, but it can be used in standalone scripts or in Jupyter notebooks. As with the original web app, it is designed to accessible to entry-level users whilst offering power functionality for students and researchers, particularly in the Humanities. It is also designed to be as language-agnostic as possible so that it can be used for a wide variety of historical and under-resourced languages.</p> <p>For the moment, much of the thinking behind the API's architecture is explained in the User Guide.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Loads texts from a variety of sources into a common data structure.</li> <li>Manages a corpus of texts and generates stastics about the corpus.</li> <li>Performs text pre-processing (\"scrubbing\") and splitting (\"cutting\").</li> <li>Performs tokenization and trains language models using spaCy.</li> <li>Creates assorted visualizations of term vectors.</li> <li>Performs hierarchical and kmeans clustering.</li> <li>Generates topic models and topic model visualizations using MALLET and DFR Browser 2.</li> </ul> <p>And more!</p>"},{"location":"#project-status","title":"Project Status","text":"<p>The Lexos API is currently in beta. Most of the core functionality of the Lexos web app, along with new features, has been implemented and documented. As of January 2026, the API is considered feature complete and stable for general use, but some rough edges remain. Feedback is welcome.</p> <p>I will continue to fix bugs and improve the documentation as issues arise, but no major new features are planned at this time. The beta release coincides with at a time when the landscape of digital tools is rapidly evolving and AI-assisted coding is becoming more prevalent. I am waiting to see whether there is significant adoption of the Lexos library before investing more time in developing new features. If you like Lexos, you can help by requesting new features in the GitHub issues (labelled as \"enhancement\") or contributing them yourself.</p>"},{"location":"#questions","title":"Questions","text":"<p>If are looking for help using Lexos, please post you question on the GitHub Discussions board.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Lexos is an open-source project, and contributions from the public are welcome. If you are interested in contributing or have a bug to report, see the Development pages for information on how to get started.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#citation-information","title":"Citation Information","text":"<p>Kleinman, S., (2026). Lexos. v0.1.0b3 https://github.com/scottkleinman/lexos. doi:10.5281/zenodo.1403869.</p>"},{"location":"credits/","title":"Credits","text":"<p>The Lexos Python library stands on the shoulders of many giants, including (but not limited to)</p> <ul> <li>The Lexos web app, developed previously with support from the National Endowment for the Humanities</li> <li>spaCy</li> <li>Textacy</li> </ul> <p>Both the Lexos web app and the Lexos Python library were developed over many years with the help of many students at Wheaton College, MA under the direction of Professor Mark LeBlanc. Thanks go out to Wheaton College's many years of support for students to work on the project. Students who made contributions to the Lexos Python library are listed below:</p> <p>Gabriel Albernaz, Will Allan, Kshitij Kesharwani, Toushar Koushik, Colin Martin-Brown, Jack Murphy, Hanna Ondrasek, Viviana Schroeder, Kevin Smith, Aaron Stange, Jeffrey Stewart, Thea West</p> <p>Parts of the <code>rolling_windows</code> module received a a code review by DHTech, by Cole Crawford (Harvard University) and Ryan Muther (Harvard University), facilitated by Julia Damerow (Arizona State University).</p> <p>The <code>dfr_browser2</code> module was developed as a separate application, DFR Browser 2, and derives from Andrew Goldstone's dfr-browser.</p>"},{"location":"installation/","title":"Installation","text":"<p>The information below describes how to install Lexos as a user. If you are interested in contributing to the Lexos source code or documentation, see the separate Development documentation.</p>"},{"location":"installation/#installing-python","title":"Installing Python","text":"<p>Lexos requires Python 3.12 or greater. Our development environment is <code>uv</code>, and Lexos should work in a Python virtual environment created using that tool. If you are using a different Python environment, you can install Lexos using <code>pip</code>.</p>"},{"location":"installation/#installing-the-lexos-package","title":"Installing the Lexos Package","text":"<p>You can install Lexos using pip:</p> <pre><code>pip install lexos\n</code></pre> <p>If you are using <code>uv</code>, run</p> <pre><code>uv add lexos\n</code></pre> <p>Note</p> <p>Lexos depends on a pre-release version of the Python <code>puremagic</code> library. In some cases, <code>uv</code> may prevent installation. If you encounter this problem, try <code>uv add lexos --prerelease=allow</code>. If you are using Lexos as a dependency in your project, add the following to your <code>pyproject.toml</code> file:</p> <pre><code>[tool.uv]\nprerelease = \"allow\"\n</code></pre> <p>This will install the Lexos API and all of its dependencies.</p> <p>By default, <code>uv</code> installs the latest version of Lexos. To update to the latest version with <code>pip</code>, use</p> <pre><code>pip install -U lexos\n</code></pre>"},{"location":"installation/#downloading-language-models","title":"Downloading Language Models","text":"<p>Many features of Lexos use language models created for the Python spaCy natural language processing library. When you install Lexos, spaCy's multi-language model <code>xx_sent_ud_sm</code> and small English model <code>en_core_web_sm</code> are installed. For information on how Lexos uses language models, see Tokenizing Texts.</p>"},{"location":"installation/#downloading-additional-language-models-optional","title":"Downloading Additional Language Models (Optional)","text":"<p>The <code>xx_sent_ud_sm</code> model is a minimal model that can be used for sentence and token segmentation in a variety of languages, while the <code>en_core_web_sm</code> model is specifically for English text. If you are working in another language or need a larger language, you may need to download additional language models. You can find information on available models on the spaCy models page.</p> <p>To download a model (for instance, the small Chinese model <code>zh_core_web_sm</code>), you can run the following commands in your terminal.</p> <p>If you are using <code>uv</code>, run:</p> <pre><code>uv run python -m spacy download zh_core_web_sm\n</code></pre> <p>or, if you are not using <code>uv</code>, you can run:</p> <pre><code>python -m spacy download zh_core_web_sm\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>To verify that Lexos is installed correctly, you can run the following command in your terminal:</p> <p>If you are using <code>uv</code>:</p> <pre><code>uv run python -m lexos --version\n</code></pre> <p>or, if you are not using <code>uv</code>:</p> <pre><code>python -m lexos --version\n</code></pre> <p>If you are using a Jupyter notebook, you can also check the installation by running the following code in a cell:</p> <pre><code>import lexos\nprint(lexos.__version__)\n</code></pre> <p>This should display the version of Lexos that is installed. If you see an error, please check your installation steps or refer to the Troubleshooting section below.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>Below are some common issues and solutions you may encounter during installation:</p>"},{"location":"installation/#1-python-version-error","title":"1. Python Version Error","text":"<p>Issue: You see an error like <code>ModuleNotFoundError</code> or <code>SyntaxError</code> when installing or running Lexos.</p> <p>Solution: Lexos requires Python 3.12 or greater. Check your Python version with:</p> <pre><code>python --version\n</code></pre> <p>If your version is lower than 3.12, please install a compatible version and create a new virtual environment.</p>"},{"location":"installation/#2-uv-or-pip-not-found","title":"2. <code>uv</code> or <code>pip</code> Not Found","text":"<p>Issue: The terminal says <code>uv: command not found</code> or <code>pip: command not found</code>.</p> <p>Solution: Make sure you have installed <code>uv</code> or <code>pip</code>. If not, follow the official installation instructions for your platform.</p>"},{"location":"installation/#3-permission-denied-errors","title":"3. Permission Denied Errors","text":"<p>Issue: You see <code>Permission denied</code> when installing packages.</p> <p>Solution: Always use a virtual environment for Lexos. If you must install globally, you may need to use <code>sudo</code> on Mac and Linux systems, but this is not recommended. Prefer using a virtual environment to avoid permission issues.</p>"},{"location":"installation/#4-spacy-model-not-found","title":"4. spaCy Model Not Found","text":"<p>Issue: You see an error like <code>OSError: [E050] Can't find model 'en_core_web_sm'</code> or similar when running Lexos features that use spaCy.</p> <p>Solution: Install the required spaCy model using one of the following commands:</p> <pre><code>uv run python -m spacy download en_core_web_sm\n# or, if not using uv:\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"installation/#5-lexos-not-found-after-installation","title":"5. Lexos Not Found After Installation","text":"<p>Issue: Running <code>python -m lexos --version</code> or <code>import lexos</code> fails with <code>ModuleNotFoundError</code>.</p> <p>Solution: Ensure you are in the correct virtual environment where Lexos was installed. Activate your environment and try again. If the problem persists, reinstall Lexos using <code>uv add lexos</code> or <code>pip install lexos</code>.</p>"},{"location":"installation/#6-outdated-pip-or-uv","title":"6. Outdated <code>pip</code> or <code>uv</code>","text":"<p>Issue: Installation fails with errors about incompatible or missing dependencies.</p> <p>Solution: Update your package manager:</p> <pre><code>pip install --upgrade pip\n# or\nuv pip install --upgrade uv\n</code></pre> <p>If you encounter any other problems not covered here, please consider reaching out to the Lexos community Discussion forum on GitHub or checking the GitHub Issues page for assistance.</p>"},{"location":"api/","title":"API","text":"<p>A full overview will be added in the future. In the meantime, here is a table of the Lexos API modules:</p> <code>cluster</code> A module for cluster analysis. <code>constants</code> A set of constants available to all modules. <code>corpus</code> Manages a corpus of documents. <code>cutter</code> Splits documents into segments. <code>DTM</code> Creates a document-term matrix. <code>exceptions</code> An exception handling class for raising custom error messages. <code>filter</code> A set of functions for applying filters to spaCy `Doc` objects. <code>io</code> A set of functions for handling input-output processes. <code>milestones</code> Sets structural divisions in documents. <code>rolling_windows</code> Analyzes documents by calculating term frequencies over sliding token windows. <code>scrubber</code> A destructive preprocessor normally used on texts before             they are tokenised. <code>tokenizer</code> A set of functions used to convert texts into spaCy tokenised             spaCy docs and to manipulate those docs. <code>topic_modeling</code> Provides methods for creating and visualizing topic models. <code>util</code> A set of utility functions available to all modules. <code>visualization</code> A set of functions for visualising data generated from documents."},{"location":"api/constants/","title":"Constants","text":""},{"location":"api/constants/#lexos.constants","title":"<code>constants</code>","text":"<p>constants.py.</p> <p>Lexos constants.</p>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#lexos.exceptions","title":"<code>exceptions</code>","text":"<p>exceptions.py.</p> <p>LexosException class.</p> <p>Classes:</p> Name Description <code>LexosException</code> <p>Base class for all exceptions in Lexos.</p>"},{"location":"api/exceptions/#lexos.exceptions.LexosException","title":"<code>LexosException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all exceptions in Lexos.</p> Source code in <code>lexos/exceptions.py</code> <pre><code>class LexosException(Exception):\n    \"\"\"Base class for all exceptions in Lexos.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/util/","title":"Util","text":""},{"location":"api/util/#lexos.util","title":"<code>util</code>","text":"<p>utils.py.</p> <p>This file contains helper functions used by multiple modules.</p> <p>Last Updated: June 24, 2025 Lasty Tested: June 24, 2025</p> <p>Functions:</p> Name Description <code>ensure_list</code> <p>Ensure string is converted to a Path.</p> <code>ensure_path</code> <p>Ensure string is converted to a Path.</p> <code>get_encoding</code> <p>Use chardet to return the encoding type of a string.</p> <code>get_paths</code> <p>Get a list paths in a directory.</p> <code>get_token_extension_names</code> <p>Get the names of token extensions from a spaCy Doc.</p> <code>is_valid_colour</code> <p>Check if a string is a valid colour.</p> <code>load_spacy_model</code> <p>Load a spaCy language model.</p> <code>normalize</code> <p>Normalise a string to LexosFile format.</p> <code>normalize_file</code> <p>Normalise a file to LexosFile format and save the file.</p> <code>normalize_files</code> <p>Normalise a list of files to LexosFile format and save the files.</p> <code>normalize_strings</code> <p>Normalise a list of strings to LexosFile format.</p> <code>strip_doc</code> <p>Strip leading and normalise trailing whitespace in a spaCy Doc.</p> <code>to_collection</code> <p>Validate and cast a value or values to a collection.</p>"},{"location":"api/util/#lexos.util.ensure_list","title":"<code>ensure_list(item: Any) -&gt; list</code>","text":"<p>Ensure string is converted to a Path.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>Anything.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The item inside a list if it is not already a list.</p> Source code in <code>lexos/util.py</code> <pre><code>def ensure_list(item: Any) -&gt; list:\n    \"\"\"Ensure string is converted to a Path.\n\n    Args:\n        item (Any): Anything.\n\n    Returns:\n        The item inside a list if it is not already a list.\n    \"\"\"\n    if not isinstance(item, list):\n        item = [item]\n    return item\n</code></pre>"},{"location":"api/util/#lexos.util.ensure_path","title":"<code>ensure_path(path: Any) -&gt; Any</code>","text":"<p>Ensure string is converted to a Path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Any</code> <p>Anything. If string, it's converted to Path.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Path or original argument.</p> Source code in <code>lexos/util.py</code> <pre><code>def ensure_path(path: Any) -&gt; Any:\n    \"\"\"Ensure string is converted to a Path.\n\n    Args:\n        path (Any): Anything. If string, it's converted to Path.\n\n    Returns:\n        Path or original argument.\n    \"\"\"\n    if isinstance(path, str):\n        return Path(path.replace(\"\\\\\", \"/\"))\n    else:\n        return path\n</code></pre>"},{"location":"api/util/#lexos.util.get_encoding","title":"<code>get_encoding(input_string: bytes) -&gt; str</code>","text":"<p>Use chardet to return the encoding type of a string.</p> <p>Parameters:</p> Name Type Description Default <code>input_string</code> <code>bytes</code> <p>A bytestring.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string's encoding type.</p> Source code in <code>lexos/util.py</code> <pre><code>def get_encoding(input_string: bytes) -&gt; str:\n    \"\"\"Use chardet to return the encoding type of a string.\n\n    Args:\n        input_string (bytes): A bytestring.\n\n    Returns:\n        The string's encoding type.\n    \"\"\"\n    encoding_detect = chardet.detect(input_string[: constants.MIN_ENCODING_DETECT])\n    encoding_type = encoding_detect[\"encoding\"]\n    if encoding_type is None:\n        encoding_type = \"utf-8\"\n    return encoding_type\n</code></pre>"},{"location":"api/util/#lexos.util.get_paths","title":"<code>get_paths(path: Path | str) -&gt; list</code>","text":"<p>Get a list paths in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the directory.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of file paths.</p> Source code in <code>lexos/util.py</code> <pre><code>def get_paths(path: Path | str) -&gt; list:\n    \"\"\"Get a list paths in a directory.\n\n    Args:\n        path (Path | str): The path to the directory.\n\n    Returns:\n        list: A list of file paths.\n    \"\"\"\n    return list(Path(path).glob(\"**/*\"))\n</code></pre>"},{"location":"api/util/#lexos.util.get_token_extension_names","title":"<code>get_token_extension_names(doc: Doc) -&gt; list[str]</code>","text":"<p>Get the names of token extensions from a spaCy Doc.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>spaCy Doc to analyze.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: a list of token extensions.</p> Source code in <code>lexos/util.py</code> <pre><code>def get_token_extension_names(doc: Doc) -&gt; list[str]:\n    \"\"\"Get the names of token extensions from a spaCy Doc.\n\n    Args:\n        doc: spaCy Doc to analyze.\n\n    Returns:\n        list[str]: a list of token extensions.\n    \"\"\"\n    return [ext for ext in doc[0]._.__dict__[\"_extensions\"].keys()]\n</code></pre>"},{"location":"api/util/#lexos.util.is_valid_colour","title":"<code>is_valid_colour(color: str) -&gt; bool</code>","text":"<p>Check if a string is a valid colour.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>str</code> <p>A string representing a colour.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the string is a valid colour, False otherwise.</p> <p>Note: Implements Pydantic's Color type for validation. See https://docs.pydantic.dev/2.0/usage/types/extra_types/color_types/ for more information.</p> Source code in <code>lexos/util.py</code> <pre><code>def is_valid_colour(color: str) -&gt; bool:\n    \"\"\"Check if a string is a valid colour.\n\n    Args:\n        color: A string representing a colour.\n\n    Returns:\n        True if the string is a valid colour, False otherwise.\n\n    Note: Implements Pydantic's Color type for validation.\n    See https://docs.pydantic.dev/2.0/usage/types/extra_types/color_types/ for more information.\n    \"\"\"\n    try:\n        Color(color)\n    except PydanticCustomError:\n        return False\n    return True\n</code></pre>"},{"location":"api/util/#lexos.util.load_spacy_model","title":"<code>load_spacy_model(model: Language | str) -&gt; Language</code>","text":"<p>Load a spaCy language model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Language | str</code> <p>The spaCy model to load, either as a Language object or a string representing the model name.</p> required <p>Returns:</p> Name Type Description <code>Language</code> <code>Language</code> <p>The loaded spaCy language model.</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If the model cannot be loaded or if the model type is incorrect.</p> Source code in <code>lexos/util.py</code> <pre><code>def load_spacy_model(model: Language | str) -&gt; Language:\n    \"\"\"Load a spaCy language model.\n\n    Args:\n        model (Language | str): The spaCy model to load, either as a Language object or a string representing the model name.\n\n    Returns:\n        Language: The loaded spaCy language model.\n\n    Raises:\n        LexosException: If the model cannot be loaded or if the model type is incorrect.\n    \"\"\"\n    if not isinstance(model, (Language, str)):\n        raise LexosException(\"Model must be a string or a spaCy Language object.\")\n\n    if isinstance(model, Language):\n        return model\n    else:\n        try:\n            return spacy.load(model)\n        except OSError:\n            raise LexosException(\n                f\"Error loading model '{model}'. Please check the name and try again. You may need to install the model on your system.\"\n            )\n</code></pre>"},{"location":"api/util/#lexos.util.normalize","title":"<code>normalize(raw_bytes: bytes | str) -&gt; str</code>","text":"<p>Normalise a string to LexosFile format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_bytes</code> <code>bytes | str</code> <p>The input bytestring.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalised version of the input string.</p> Source code in <code>lexos/util.py</code> <pre><code>def normalize(raw_bytes: bytes | str) -&gt; str:\n    \"\"\"Normalise a string to LexosFile format.\n\n    Args:\n        raw_bytes (bytes | str): The input bytestring.\n\n    Returns:\n        Normalised version of the input string.\n    \"\"\"\n    s = _decode_bytes(raw_bytes)\n    return s\n</code></pre>"},{"location":"api/util/#lexos.util.normalize_file","title":"<code>normalize_file(filepath: Path | str, destination_dir: Path | str = '.') -&gt; None</code>","text":"<p>Normalise a file to LexosFile format and save the file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>The path to the input file.</p> required <code>destination_dir</code> <code>Path | str</code> <p>The path to the directory where the files. will be saved.</p> <code>'.'</code> Source code in <code>lexos/util.py</code> <pre><code>def normalize_file(filepath: Path | str, destination_dir: Path | str = \".\") -&gt; None:\n    \"\"\"Normalise a file to LexosFile format and save the file.\n\n    Args:\n        filepath (Path | str): The path to the input file.\n        destination_dir (Path | str): The path to the directory where the files.\n            will be saved.\n    \"\"\"\n    # filepath = ensure_path(filepath)\n    filepath = Path(filepath)\n    destination_dir = ensure_path(destination_dir)\n    with open(filepath, \"rb\") as f:\n        doc = f.read()\n    with open(destination_dir / Path(filepath.name), \"w\") as f:\n        f.write(normalize(doc))\n</code></pre>"},{"location":"api/util/#lexos.util.normalize_files","title":"<code>normalize_files(filepaths: list[Path | str], destination_dir: Path | str = '.') -&gt; None</code>","text":"<p>Normalise a list of files to LexosFile format and save the files.</p> <p>Parameters:</p> Name Type Description Default <code>filepaths</code> <code>list[Path | str]</code> <p>The list of paths to input files.</p> required <code>destination_dir</code> <code>Path | str</code> <p>The path to the directory where the files. will be saved.</p> <code>'.'</code> Source code in <code>lexos/util.py</code> <pre><code>def normalize_files(\n    filepaths: list[Path | str], destination_dir: Path | str = \".\"\n) -&gt; None:\n    \"\"\"Normalise a list of files to LexosFile format and save the files.\n\n    Args:\n        filepaths (list[Path | str]): The list of paths to input files.\n        destination_dir (Path | str): The path to the directory where the files.\n            will be saved.\n    \"\"\"\n    for filepath in filepaths:\n        filepath = ensure_path(filepath)\n        with open(filepath, \"rb\") as f:\n            doc = f.read()\n        with open(destination_dir / filepath.name, \"w\") as f:\n            f.write(normalize(doc))\n</code></pre>"},{"location":"api/util/#lexos.util.normalize_strings","title":"<code>normalize_strings(strings: list[str]) -&gt; list[str]</code>","text":"<p>Normalise a list of strings to LexosFile format.</p> <p>Parameters:</p> Name Type Description Default <code>strings</code> <code>list[Path | str]</code> <p>The list of input strings.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of normalised versions of the input strings.</p> Source code in <code>lexos/util.py</code> <pre><code>def normalize_strings(strings: list[str]) -&gt; list[str]:\n    \"\"\"Normalise a list of strings to LexosFile format.\n\n    Args:\n        strings (list[Path | str]): The list of input strings.\n\n    Returns:\n        A list of normalised versions of the input strings.\n    \"\"\"\n    normalized_strings = []\n    for s in strings:\n        normalized_strings.append(normalize(s))\n    return normalized_strings\n</code></pre>"},{"location":"api/util/#lexos.util.strip_doc","title":"<code>strip_doc(doc: Doc) -&gt; Doc</code>","text":"<p>Strip leading and normalise trailing whitespace in a spaCy Doc.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>spaCy Doc to analyze</p> required <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>the Doc with leading and trailing whitespace removed.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Doc is empty or contains only whitespace.</p> If the final token has trailing whitespace, this will be preserved. <p>You can remove the space with:</p> <p>```python words = [t.text for t in doc] spaces = [t.whitespace_ for t in doc] spaces[-1] = \"\" doc = Doc(doc.vocab, words=words, spaces=spaces)</p> <p>But you will lose all entities and custom extensions. So it makes more sense to call doc.text.strip() when needed instead.</p> Source code in <code>lexos/util.py</code> <pre><code>def strip_doc(doc: Doc) -&gt; Doc:\n    \"\"\"Strip leading and normalise trailing whitespace in a spaCy Doc.\n\n    Args:\n        doc: spaCy Doc to analyze\n\n    Returns:\n        Doc: the Doc with leading and trailing whitespace removed.\n\n    Raises:\n        ValueError: If Doc is empty or contains only whitespace.\n\n    Note: If the final token has trailing whitespace, this will be preserved.\n          You can remove the space with:\n\n          ```python\n          words = [t.text for t in doc]\n          spaces = [t.whitespace_ for t in doc]\n          spaces[-1] = \"\"\n          doc = Doc(doc.vocab, words=words, spaces=spaces)\n\n          But you will lose all entities and custom extensions. So it makes more\n          sense to call doc.text.strip() when needed instead.\n    \"\"\"\n    if not doc:\n        raise LexosException(\"Document is empty.\")\n\n    # Find first non-whitespace token\n    start_idx = 0\n    for token in doc:\n        if not token.is_space:\n            start_idx = token.i\n            break\n\n    # Find last non-whitespace token\n    end_idx = len(doc) - 1\n    for i in range(len(doc) - 1, -1, -1):  # list(doc)[::-1]\n        if not doc[i].is_space:\n            end_idx = i\n            break\n\n    return doc[start_idx : end_idx + 1].as_doc()\n</code></pre>"},{"location":"api/util/#lexos.util.to_collection","title":"<code>to_collection(val: AnyVal | Collection[AnyVal], val_type: type[Any] | tuple[type[Any], ...], col_type: type[Any]) -&gt; Collection[AnyVal]</code>","text":"<p>Validate and cast a value or values to a collection.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>AnyVal | Collection[AnyVal]</code> <p>Value or values to validate and cast.</p> required <code>val_type</code> <code>type[Any] | tuple[type[Any], ...]</code> <p>Type of each value in collection, e.g. <code>int</code> or <code>(str, bytes)</code>.</p> required <code>col_type</code> <code>type[Any]</code> <p>Type of collection to return, e.g. <code>tuple</code> or <code>set</code>.</p> required <p>Returns:</p> Type Description <code>Collection[AnyVal]</code> <p>Collection[AnyVal]: Collection of type <code>col_type</code> with values all of type <code>val_type</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>An invalid value was passed.</p> Source code in <code>lexos/util.py</code> <pre><code>def to_collection(\n    val: AnyVal | Collection[AnyVal],\n    val_type: type[Any] | tuple[type[Any], ...],\n    col_type: type[Any],\n) -&gt; Collection[AnyVal]:\n    \"\"\"Validate and cast a value or values to a collection.\n\n    Args:\n        val (AnyVal | Collection[AnyVal]): Value or values to validate and cast.\n        val_type (type[Any] | tuple[type[Any], ...]): Type of each value in collection, e.g. ``int`` or ``(str, bytes)``.\n        col_type (type[Any]): Type of collection to return, e.g. ``tuple`` or ``set``.\n\n    Returns:\n        Collection[AnyVal]: Collection of type ``col_type`` with values all of type ``val_type``.\n\n    Raises:\n        TypeError: An invalid value was passed.\n    \"\"\"\n    if val is None:\n        return []\n    if isinstance(val, val_type):\n        return col_type([val])\n    elif isinstance(val, (tuple, list, set, frozenset)):\n        if not all(isinstance(v, val_type) for v in val):\n            raise TypeError(f\"not all values are of type {val_type}\")\n        return col_type(val)\n    else:\n        # TODO: use standard error message, maybe?\n        raise TypeError(\n            f\"values must be {val_type} or a collection thereof, not {type(val)}\"\n        )\n</code></pre>"},{"location":"api/cluster/","title":"Cluster","text":"<p>The <code>cluster</code> module provides methods for generating and visualizing cluster analyses. The <code>dendrogram</code> module performs hierarchical agglomerative clustering, and the <code>kmeans</code> module perfors k-means clustering.</p> <p>The <code>plotly_dendrogram</code> module performs the same function as <code>dendrogram</code> but uses the Python Plotly library to generate dendrograms.</p>"},{"location":"api/cluster/bootstrap_consensus/","title":"Boostrap Consensus Trees","text":""},{"location":"api/cluster/bootstrap_consensus/#the-bct-class","title":"The <code>BCT</code> Class","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT","title":"<code>BCT</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The Bootstrap Consensus Tree Class.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>dtm</code>                 (<code>DTM</code>)             </li> <li> <code>metric</code>                 (<code>Optional[str]</code>)             </li> <li> <code>method</code>                 (<code>Optional[str]</code>)             </li> <li> <code>cutoff</code>                 (<code>Optional[float]</code>)             </li> <li> <code>iterations</code>                 (<code>Optional[int]</code>)             </li> <li> <code>replace</code>                 (<code>Optional[str]</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[int | str] | dict[int, str]]</code>)             </li> <li> <code>text_color</code>                 (<code>Optional[str]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>layout</code>                 (<code>Optional[str]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_text_color</code>                 \u2192                   <code>text_color</code> </li> </ul> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>class BCT(BaseModel):\n    \"\"\"The Bootstrap Consensus Tree Class.\"\"\"\n\n    dtm: DTM = Field(None, description=\"The document term matrix.\")\n    metric: Optional[str] = Field(\"euclidean\", description=\"The distance metric.\")\n    method: Optional[str] = Field(\"average\", description=\"The linkage method.\")\n    cutoff: Optional[float] = Field(0.5, description=\"The cutoff value.\")\n    iterations: Optional[int] = Field(\n        100, description=\"The number of iterations to run the bootstrap.\"\n    )\n    replace: Optional[str] = Field(\"without\", description=\"The replacement method.\")\n    labels: Optional[list[int | str] | dict[int, str]] = Field(\n        None, description=\"The document labels.\"\n    )\n    text_color: Optional[str] = Field(\"rgb(0, 0, 0)\", description=\"The text colour.\")\n    title: Optional[str] = Field(\n        \"Bootstrap Consensus Tree Result\", description=\"The title of the dendrogram.\"\n    )\n    layout: Optional[str] = Field(\n        \"rectangular\", description=\"Tree visualization layout: 'rectangular' or 'fan'.\"\n    )\n    fig: Optional[Figure] = Field(None, description=\"The figure for the dendrogram.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def _doc_term_matrix(self) -&gt; pd.DataFrame:\n        \"\"\"Return a dataframe of the document term matrix.\n\n        Returns:\n            pd.DataFrame: The document term matrix with doc labels as the index and terms as the columns.\n\n\n        Note that the web app uses doc ids as the index.\n        \"\"\"\n        if self.dtm is None:\n            raise LexosException(\"No document term matrix found.\")\n        return self.dtm.to_df().T\n\n    @property\n    def _document_label_map(self) -&gt; dict[int, str] | dict:\n        \"\"\"Return a dictionary of document label map.\n\n        Returns:\n            list[int | str] | dict[int, str]: A document label map or a list of indices or labels.\n        \"\"\"\n        if self.labels is not None and len(self.labels) &gt; 0:\n            if isinstance(self.labels, dict):\n                return self.labels\n            else:\n                if isinstance(self.labels[0], int):\n                    return {i: f\"doc{i + 1}\" for i, _ in enumerate(self.labels)}\n                else:\n                    return {i: label for i, label in enumerate(self.labels)}\n        return {}\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Construct the BCT instance and render the bootstrap consensus tree.\"\"\"\n        super().__init__(**data)\n\n        # Get the matplotlib figure for bootstrap consensus tree result\n        self.fig = self._get_bootstrap_consensus_tree_fig(layout=self.layout)\n        plt.close()\n\n    @field_validator(\"text_color\", mode=\"after\")\n    @classmethod\n    def _validate_text_color(cls, value):\n        if not is_valid_colour(value):\n            raise LexosException(\n                \"Value is not a valid colour: string not recognised as a valid colour.\"\n            )\n        return value\n\n    @staticmethod\n    def linkage_to_newick(matrix: np.ndarray, labels: list[str]) -&gt; str:\n        \"\"\"Convert a linkage matrix to a Newick formatted tree.\n\n        Args:\n            matrix (np.ndarray): The linkage matrix.\n            labels (list[str]): Names of the tree node.\n\n        Returns:\n            str: The Newick representation of the linkage matrix.\n        \"\"\"\n        # Convert the linkage matrix to a ClusterNode object\n        tree = to_tree(matrix, False)\n\n        # Define a recursive function to build the Newick tree\n        def _build_newick_tree(\n            node: ClusterNode, newick: str, parent_dist: float, leaf_names: list[str]\n        ) -&gt; str:\n            \"\"\"Recursively build the Newick tree.\n\n            Args:\n                node (ClusterNode): The tree node currently being converted to Newick.\n                newick (str): The current Newick representation of the tree.\n                parent_dist (float): The distance to parent node.\n                leaf_names (list[str]): Names of the tree node.\n\n            Returns:\n                str: The Newick representation of the tree.\n            \"\"\"\n            # If the node is a leaf, enclose\n            if node.is_leaf():\n                return f\"{leaf_names[node.id]}:{(parent_dist - node.dist) / 2}{newick}\"\n            else:\n                # Write the distance to the parent node\n                newick = (\n                    f\"):{(parent_dist - node.dist) / 2}{newick}\"\n                    if len(newick) &gt; 0\n                    else \");\"\n                )\n                # Recursive call to expand the tree\n                newick = _build_newick_tree(\n                    newick=newick,\n                    node=node.get_left(),\n                    parent_dist=node.dist,\n                    leaf_names=leaf_names,\n                )\n                newick = _build_newick_tree(\n                    newick=f\",{newick}\",\n                    node=node.get_right(),\n                    parent_dist=node.dist,\n                    leaf_names=leaf_names,\n                )\n                # Enclose the tree at the beginning\n                return f\"({newick}\"\n\n        # Trigger the recursive function\n        return _build_newick_tree(\n            node=tree, newick=\"\", parent_dist=tree.dist, leaf_names=labels\n        )\n\n    def _get_newick_tree(self, labels: list[str], sample_dtm: pd.DataFrame) -&gt; str:\n        \"\"\"Get Newick tree based on a subset of the DTM.\n\n        Args:\n            labels (list[str]): All file names from the DTM\n            sample_dtm (pd.DataFrame): An 80% subset of the complete DTM\n\n        Returns:\n            str: A Newick formatted tree representing the DTM subset\n        \"\"\"\n        # Get the linkage matrix for the sample doc term matrix\n        linkage_matrix = linkage(\n            sample_dtm.values, metric=self.metric, method=self.method\n        )\n\n        # Get the Newick representation of the tree\n        newick = self.linkage_to_newick(matrix=linkage_matrix, labels=labels)\n\n        # Convert linkage matrix to a tree node and return it\n        return Phylo.read(StringIO(newick), format=\"newick\")\n\n    def _get_bootstrap_trees(self) -&gt; list[str]:\n        \"\"\"Do bootstrap on the DTM to get a list of Newick trees.\n\n        Returns:\n            list[str]: A list of Newick formatted tree where each tree was based on an 80% subset of the complete DTM.\n        \"\"\"\n        # Save the DTM to avoid multiple calls\n        dtm = self._doc_term_matrix\n\n        # Get doc names, since tree nodes need labels\n        labels = [doc for doc in self._doc_term_matrix.index.values.tolist()]\n\n        # The bootstrap process to get all the trees.\n        return [\n            self._get_newick_tree(\n                sample_dtm=dtm.sample(\n                    axis=1,\n                    frac=0.8,\n                    replace=self.replace,\n                    random_state=np.random.RandomState(),\n                ),\n                labels=labels,\n            )\n            for _ in range(self.iterations)\n        ]\n\n    def _get_bootstrap_consensus_tree(self) -&gt; Phylo:\n        \"\"\"Get the consensus tree.\n\n        Returns:\n            Phylo: The consensus tree of the list of Newick trees.\n        \"\"\"\n        # Find the consensus of all the Newick trees\n        return majority_consensus(trees=self._get_bootstrap_trees(), cutoff=self.cutoff)\n\n    def _get_bootstrap_consensus_tree_fig(self, layout: str = \"rectangular\") -&gt; Figure:\n        \"\"\"Generate a bootstrap consensus tree figure.\n\n        Args:\n            layout: Tree visualization style. Options:\n                - \"rectangular\": Traditional rectangular tree layout\n                - \"fan\": Circular fan-style tree layout\n\n        Returns:\n            Figure: The matplotlib figure containing the tree visualization.\n        \"\"\"\n        # Get the colours\n        color = tuple(map(int, self.text_color[4:-1].split(\",\")))\n        normalized_color = tuple(x / 255 for x in color)\n\n        # Get the consensus tree\n        tree = self._get_bootstrap_consensus_tree()\n        tree.root.color = color\n\n        if layout == \"rectangular\":\n            return self._draw_rectangular_tree(tree, normalized_color)\n        elif layout in [\"fan\"]:\n            return self._draw_fan_tree(tree, normalized_color, layout)\n        else:\n            raise ValueError(f\"Unknown layout: {layout}. Use 'rectangular' or 'fan'.\")\n\n    def _draw_rectangular_tree(self, tree, normalized_color) -&gt; Figure:\n        \"\"\"Draw traditional rectangular tree layout.\"\"\"\n        fig, ax = plt.subplots()\n\n        Phylo.draw(\n            tree,\n            axes=ax,\n            do_show=False,\n            branch_labels=lambda clade: f\"{clade.branch_length:.{PRECISION}f}\\n\"\n            if clade.branch_length is not None\n            else \"\",\n        )\n\n        # Set labels for the plot\n        plt.xlabel(\"Branch Length\", color=normalized_color)\n        plt.ylabel(\"Documents\", color=normalized_color)\n\n        # Hide the two unused borders\n        plt.gca().spines[\"top\"].set_visible(False)\n        plt.gca().spines[\"right\"].set_visible(False)\n\n        # Set the colour of the used borders and labels\n        plt.gca().spines[\"bottom\"].set_color(normalized_color)\n        plt.gca().spines[\"left\"].set_color(normalized_color)\n        plt.gca().tick_params(colors=normalized_color)\n\n        # Extend the x-axis to the right to fit longer labels\n        x_left, x_right, y_low, y_high = plt.axis()\n        plt.axis((x_left, x_right * 1.25, y_low, y_high))\n\n        # Set the graph size, title, and tight layout\n        plt.gcf().set_size_inches(w=9.5, h=(len(self._document_label_map) * 0.3 + 1))\n        plt.title(self.title, color=normalized_color)\n        plt.gcf().tight_layout()\n\n        # Change the line spacing\n        for text in plt.gca().texts:\n            text.set_linespacing(spacing=0.1)\n            text.set_color(normalized_color)\n\n        return fig\n\n    def _draw_fan_tree(self, tree, normalized_color, style: str) -&gt; Figure:\n        \"\"\"Draw fan-style (circular) tree layout with color-coded labels by clade.\"\"\"\n        # Create figure with equal aspect ratio for circular plot\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_aspect(\"equal\")\n\n        # Get all terminal nodes and calculate their positions\n        terminals = list(tree.get_terminals())\n        num_terminals = len(terminals)\n\n        if num_terminals == 0:\n            raise ValueError(\"Tree has no terminal nodes\")\n\n        # Fan style: use 270 degrees (3/4 circle)\n        start_angle = -135  # Start at bottom-left\n        total_angle = 270  # 270 degrees total\n\n        # Assign angles to terminal nodes\n        terminal_angles = {}\n        for i, terminal in enumerate(terminals):\n            if num_terminals == 1:\n                angle = start_angle\n            else:\n                angle = start_angle + (i * total_angle / num_terminals)\n            terminal_angles[terminal] = math.radians(angle)\n\n        # Calculate tree depth for all nodes\n        node_depths = {}\n        max_depth = 0\n\n        def calculate_depths(node, depth=0):\n            \"\"\"Calculate depth from root for each node.\"\"\"\n            nonlocal max_depth\n            node_depths[node] = depth\n            max_depth = max(max_depth, depth)\n\n            if not node.is_terminal():\n                for child in node.clades:\n                    calculate_depths(child, depth + 1)\n\n        calculate_depths(tree.root)\n\n        # Generate color palette\n        def generate_colors(n):\n            \"\"\"Generate n distinct colors.\"\"\"\n            if n &lt;= 10:\n                # Use predefined colors for small numbers\n                colors = [\n                    \"#1f77b4\",\n                    \"#ff7f0e\",\n                    \"#2ca02c\",\n                    \"#d62728\",\n                    \"#9467bd\",\n                    \"#8c564b\",\n                    \"#e377c2\",\n                    \"#7f7f7f\",\n                    \"#bcbd22\",\n                    \"#17becf\",\n                ]\n                return colors[:n]\n            else:\n                # Generate colors using HSV color space for larger numbers\n                colors = []\n                for i in range(n):\n                    hue = i / n\n                    rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n                    colors.append(mcolors.rgb2hex(rgb))\n                return colors\n\n        # Find clade groupings for color assignment at higher levels\n        terminal_colors = {}\n\n        def assign_higher_level_colors():\n            \"\"\"Assign colors based on higher-level splits in the tree.\"\"\"\n            color_palette = generate_colors(10)\n            color_index = 0\n\n            def find_terminal_descendants(node):\n                \"\"\"Get all terminal nodes that descend from this node.\"\"\"\n                if node.is_terminal():\n                    return [node]\n\n                descendants = []\n                for child in node.clades:\n                    descendants.extend(find_terminal_descendants(child))\n                return descendants\n\n            def assign_colors_recursive(node, current_depth=0):\n                \"\"\"Recursively assign colors based on tree structure.\"\"\"\n                nonlocal color_index\n\n                if node.is_terminal():\n                    return\n\n                # For nodes at depth 1 (direct children of root), assign same color to all descendants\n                if current_depth == 1:\n                    descendants = find_terminal_descendants(node)\n                    current_color = color_palette[color_index % len(color_palette)]\n\n                    for terminal in descendants:\n                        if terminal not in terminal_colors:\n                            terminal_colors[terminal] = current_color\n\n                    color_index += 1\n\n                # Continue recursively for deeper nodes\n                for child in node.clades:\n                    assign_colors_recursive(child, current_depth + 1)\n\n            # Start from root\n            assign_colors_recursive(tree.root)\n\n        # Assign colors based on higher-level clades\n        assign_higher_level_colors()\n\n        # Set circumference radius for terminals\n        circumference_radius = 1.0\n\n        # Calculate positions for all nodes\n        node_positions = {}\n        terminal_parent_map = {}  # Store parent-child relationships for terminals\n\n        def calculate_positions(node):\n            \"\"\"Calculate x, y positions for all nodes.\"\"\"\n            if node.is_terminal():\n                # Terminal nodes: place directly on circumference\n                angle = terminal_angles[node]\n                x = circumference_radius * math.cos(angle)\n                y = circumference_radius * math.sin(angle)\n                node_positions[node] = (x, y, angle)\n                return angle, circumference_radius\n            else:\n                # Internal nodes: calculate based on children\n                child_angles = []\n                child_distances = []\n\n                for child in node.clades:\n                    child_angle, child_distance = calculate_positions(child)\n                    child_angles.append(child_angle)\n                    child_distances.append(child_distance)\n\n                    # Store parent-child relationship for terminals\n                    if child.is_terminal():\n                        terminal_parent_map[child] = node\n\n                # Internal node angle is the average of its children's angles\n                avg_angle = sum(child_angles) / len(child_angles)\n\n                # Position internal nodes VERY close to center for maximum clustering\n                depth = node_depths[node]\n\n                if depth == 0:  # Root node\n                    distance = 0.02  # Almost at center\n                else:\n                    # Use extremely aggressive scaling - internal nodes very close to root\n                    normalized_depth = depth / max_depth\n                    distance = circumference_radius * (\n                        0.05 + 0.15 * (normalized_depth**3.0)\n                    )\n\n                x = distance * math.cos(avg_angle)\n                y = distance * math.sin(avg_angle)\n                node_positions[node] = (x, y, avg_angle)\n\n                return avg_angle, distance\n\n        # Calculate all positions\n        calculate_positions(tree.root)\n\n        # Draw the tree with proper branching structure\n        def draw_branches(node):\n            \"\"\"Draw branches connecting nodes with proper tree structure.\"\"\"\n            if node not in node_positions:\n                return\n\n            node_x, node_y, node_angle = node_positions[node]\n\n            # For internal nodes with multiple children, we want to create proper branching\n            if not node.is_terminal() and len(node.clades) &gt; 1:\n                # First, draw lines from this node to each child\n                for child in node.clades:\n                    if child in node_positions:\n                        child_x, child_y, child_angle = node_positions[child]\n\n                        # Draw the branch\n                        if child.is_terminal():\n                            linewidth = 2.0\n                            alpha = 0.9\n                        else:\n                            linewidth = 1.8\n                            alpha = 0.8\n\n                        ax.plot(\n                            [node_x, child_x],\n                            [node_y, child_y],\n                            color=normalized_color,\n                            linewidth=linewidth,\n                            alpha=alpha,\n                        )\n\n                        # Recursively draw children\n                        draw_branches(child)\n            elif len(node.clades) == 1:\n                # Single child - direct connection\n                child = node.clades[0]\n                if child in node_positions:\n                    child_x, child_y, child_angle = node_positions[child]\n\n                    linewidth = 2.0 if child.is_terminal() else 1.8\n                    alpha = 0.9 if child.is_terminal() else 0.8\n\n                    ax.plot(\n                        [node_x, child_x],\n                        [node_y, child_y],\n                        color=normalized_color,\n                        linewidth=linewidth,\n                        alpha=alpha,\n                    )\n\n                    draw_branches(child)\n\n        # Draw all branches starting from root\n        draw_branches(tree.root)\n\n        # Draw nodes and labels with perfect alignment and color coding\n        for node in node_positions:\n            x, y, angle = node_positions[node]\n\n            if node.is_terminal():\n                # Terminal node: draw label perfectly aligned by extending the branch vector\n                label = str(node.name) if node.name else \"Unnamed\"\n\n                # Get the color for this terminal node\n                label_color = terminal_colors.get(node, normalized_color)\n\n                # Get the parent node position\n                if node in terminal_parent_map:\n                    parent = terminal_parent_map[node]\n                    parent_x, parent_y, parent_angle = node_positions[parent]\n\n                    # Calculate the direction vector from parent to terminal\n                    dx = x - parent_x\n                    dy = y - parent_y\n\n                    # Normalize the direction vector\n                    branch_length = math.sqrt(dx * dx + dy * dy)\n                    if branch_length &gt; 0:\n                        dx_norm = dx / branch_length\n                        dy_norm = dy / branch_length\n\n                        # Extend the label position along the same direction vector\n                        label_extension = 0.02  # Your preferred distance\n                        label_x = x + dx_norm * label_extension\n                        label_y = y + dy_norm * label_extension\n\n                        # Calculate rotation angle from the direction vector\n                        branch_angle_rad = math.atan2(dy, dx)\n                        angle_deg = math.degrees(branch_angle_rad)\n\n                        # Ensure text is readable (not upside down)\n                        if -90 &lt;= angle_deg &lt;= 90:\n                            rotation = angle_deg\n                            ha = \"left\"\n                            va = \"center\"\n                        else:\n                            rotation = angle_deg + 180\n                            ha = \"right\"\n                            va = \"center\"\n\n                        # Draw the label perfectly aligned with branch direction and colored by clade\n                        ax.text(\n                            label_x,\n                            label_y,\n                            label,\n                            rotation=rotation,\n                            rotation_mode=\"anchor\",\n                            ha=ha,\n                            va=va,\n                            color=label_color,\n                            fontsize=12,\n                            weight=\"bold\",\n                        )\n                    else:\n                        # Fallback if branch_length is zero (shouldn't happen)\n                        ax.text(\n                            x * 1.08,\n                            y * 1.08,\n                            label,\n                            color=label_color,\n                            fontsize=12,\n                            weight=\"bold\",\n                        )\n                else:\n                    # Fallback if no parent found (shouldn't happen)\n                    ax.text(\n                        x * 1.08,\n                        y * 1.08,\n                        label,\n                        color=label_color,\n                        fontsize=12,\n                        weight=\"bold\",\n                    )\n\n                # Don't draw terminal node markers - only show labels\n\n            elif node == tree.root:\n                # Only draw the root node marker - hide all other internal nodes\n                ax.plot(x, y, \"o\", color=normalized_color, markersize=6, alpha=1.0)\n\n        # Set up the plot limits\n        plot_limit = circumference_radius * 1.25\n        ax.set_xlim(-plot_limit, plot_limit)\n        ax.set_ylim(-plot_limit, plot_limit)\n\n        # Remove axes for cleaner look\n        ax.set_xticks([])\n        ax.set_yticks([])\n        for spine in ax.spines.values():\n            spine.set_visible(False)\n\n        # Add title\n        plt.title(\n            self.title,\n            color=normalized_color,\n            pad=30,\n            fontsize=16,\n            weight=\"bold\",\n        )\n\n        plt.tight_layout()\n\n        return fig\n\n    @validate_call(config=model_config)\n    def save(self, path: Path | str | None) -&gt; None:\n        \"\"\"Save the bootstrap consensus tree result to a file.\n\n        Args:\n            path (Path | str | None): The path to save the file.\n        \"\"\"\n        if not path or path == \"\":\n            raise LexosException(\"You must provide a valid path.\")\n        self.fig.savefig(path)\n\n    def show(self) -&gt; Figure:\n        \"\"\"Show the figure if it is hidden.\n\n        This is a helper method. You can also reference the figure using `BCT.fig`.\n        This will generally display in a Jupyter notebook.\n        \"\"\"\n        if self.fig is None:\n            raise LexosException(\n                \"You must call the instance before showing the figure.\"\n            )\n        return self.fig\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.cutoff","title":"<code>cutoff: Optional[float] = 0.5</code>  <code>pydantic-field</code>","text":"<p>The cutoff value.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.dtm","title":"<code>dtm: DTM = None</code>  <code>pydantic-field</code>","text":"<p>The document term matrix.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.iterations","title":"<code>iterations: Optional[int] = 100</code>  <code>pydantic-field</code>","text":"<p>The number of iterations to run the bootstrap.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.labels","title":"<code>labels: Optional[list[int | str] | dict[int, str]] = None</code>  <code>pydantic-field</code>","text":"<p>The document labels.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.layout","title":"<code>layout: Optional[str] = 'rectangular'</code>  <code>pydantic-field</code>","text":"<p>Tree visualization layout: 'rectangular' or 'fan'.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.method","title":"<code>method: Optional[str] = 'average'</code>  <code>pydantic-field</code>","text":"<p>The linkage method.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.metric","title":"<code>metric: Optional[str] = 'euclidean'</code>  <code>pydantic-field</code>","text":"<p>The distance metric.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.replace","title":"<code>replace: Optional[str] = 'without'</code>  <code>pydantic-field</code>","text":"<p>The replacement method.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.text_color","title":"<code>text_color: Optional[str] = 'rgb(0, 0, 0)'</code>  <code>pydantic-field</code>","text":"<p>The text colour.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.title","title":"<code>title: Optional[str] = 'Bootstrap Consensus Tree Result'</code>  <code>pydantic-field</code>","text":"<p>The title of the dendrogram.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Construct the BCT instance and render the bootstrap consensus tree.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Construct the BCT instance and render the bootstrap consensus tree.\"\"\"\n    super().__init__(**data)\n\n    # Get the matplotlib figure for bootstrap consensus tree result\n    self.fig = self._get_bootstrap_consensus_tree_fig(layout=self.layout)\n    plt.close()\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.linkage_to_newick","title":"<code>linkage_to_newick(matrix: np.ndarray, labels: list[str]) -&gt; str</code>  <code>staticmethod</code>","text":"<p>Convert a linkage matrix to a Newick formatted tree.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The linkage matrix.</p> required <code>labels</code> <code>list[str]</code> <p>Names of the tree node.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Newick representation of the linkage matrix.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>@staticmethod\ndef linkage_to_newick(matrix: np.ndarray, labels: list[str]) -&gt; str:\n    \"\"\"Convert a linkage matrix to a Newick formatted tree.\n\n    Args:\n        matrix (np.ndarray): The linkage matrix.\n        labels (list[str]): Names of the tree node.\n\n    Returns:\n        str: The Newick representation of the linkage matrix.\n    \"\"\"\n    # Convert the linkage matrix to a ClusterNode object\n    tree = to_tree(matrix, False)\n\n    # Define a recursive function to build the Newick tree\n    def _build_newick_tree(\n        node: ClusterNode, newick: str, parent_dist: float, leaf_names: list[str]\n    ) -&gt; str:\n        \"\"\"Recursively build the Newick tree.\n\n        Args:\n            node (ClusterNode): The tree node currently being converted to Newick.\n            newick (str): The current Newick representation of the tree.\n            parent_dist (float): The distance to parent node.\n            leaf_names (list[str]): Names of the tree node.\n\n        Returns:\n            str: The Newick representation of the tree.\n        \"\"\"\n        # If the node is a leaf, enclose\n        if node.is_leaf():\n            return f\"{leaf_names[node.id]}:{(parent_dist - node.dist) / 2}{newick}\"\n        else:\n            # Write the distance to the parent node\n            newick = (\n                f\"):{(parent_dist - node.dist) / 2}{newick}\"\n                if len(newick) &gt; 0\n                else \");\"\n            )\n            # Recursive call to expand the tree\n            newick = _build_newick_tree(\n                newick=newick,\n                node=node.get_left(),\n                parent_dist=node.dist,\n                leaf_names=leaf_names,\n            )\n            newick = _build_newick_tree(\n                newick=f\",{newick}\",\n                node=node.get_right(),\n                parent_dist=node.dist,\n                leaf_names=leaf_names,\n            )\n            # Enclose the tree at the beginning\n            return f\"({newick}\"\n\n    # Trigger the recursive function\n    return _build_newick_tree(\n        node=tree, newick=\"\", parent_dist=tree.dist, leaf_names=labels\n    )\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.save","title":"<code>save(path: Path | str | None) -&gt; None</code>","text":"<p>Save the bootstrap consensus tree result to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str | None</code> <p>The path to save the file.</p> required Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str | None) -&gt; None:\n    \"\"\"Save the bootstrap consensus tree result to a file.\n\n    Args:\n        path (Path | str | None): The path to save the file.\n    \"\"\"\n    if not path or path == \"\":\n        raise LexosException(\"You must provide a valid path.\")\n    self.fig.savefig(path)\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.show","title":"<code>show() -&gt; Figure</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. You can also reference the figure using <code>BCT.fig</code>. This will generally display in a Jupyter notebook.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def show(self) -&gt; Figure:\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. You can also reference the figure using `BCT.fig`.\n    This will generally display in a Jupyter notebook.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\n            \"You must call the instance before showing the figure.\"\n        )\n    return self.fig\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._doc_term_matrix","title":"<code>_doc_term_matrix: pd.DataFrame</code>  <code>property</code>","text":"<p>Return a dataframe of the document term matrix.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The document term matrix with doc labels as the index and terms as the columns.</p> <p>Note that the web app uses doc ids as the index.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._document_label_map","title":"<code>_document_label_map: dict[int, str] | dict</code>  <code>property</code>","text":"<p>Return a dictionary of document label map.</p> <p>Returns:</p> Type Description <code>dict[int, str] | dict</code> <p>list[int | str] | dict[int, str]: A document label map or a list of indices or labels.</p>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Construct the BCT instance and render the bootstrap consensus tree.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Construct the BCT instance and render the bootstrap consensus tree.\"\"\"\n    super().__init__(**data)\n\n    # Get the matplotlib figure for bootstrap consensus tree result\n    self.fig = self._get_bootstrap_consensus_tree_fig(layout=self.layout)\n    plt.close()\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._validate_text_color","title":"<code>_validate_text_color(value)</code>  <code>pydantic-validator</code>","text":"Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>@field_validator(\"text_color\", mode=\"after\")\n@classmethod\ndef _validate_text_color(cls, value):\n    if not is_valid_colour(value):\n        raise LexosException(\n            \"Value is not a valid colour: string not recognised as a valid colour.\"\n        )\n    return value\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.linkage_to_newick","title":"<code>linkage_to_newick(matrix: np.ndarray, labels: list[str]) -&gt; str</code>  <code>staticmethod</code>","text":"<p>Convert a linkage matrix to a Newick formatted tree.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The linkage matrix.</p> required <code>labels</code> <code>list[str]</code> <p>Names of the tree node.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Newick representation of the linkage matrix.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>@staticmethod\ndef linkage_to_newick(matrix: np.ndarray, labels: list[str]) -&gt; str:\n    \"\"\"Convert a linkage matrix to a Newick formatted tree.\n\n    Args:\n        matrix (np.ndarray): The linkage matrix.\n        labels (list[str]): Names of the tree node.\n\n    Returns:\n        str: The Newick representation of the linkage matrix.\n    \"\"\"\n    # Convert the linkage matrix to a ClusterNode object\n    tree = to_tree(matrix, False)\n\n    # Define a recursive function to build the Newick tree\n    def _build_newick_tree(\n        node: ClusterNode, newick: str, parent_dist: float, leaf_names: list[str]\n    ) -&gt; str:\n        \"\"\"Recursively build the Newick tree.\n\n        Args:\n            node (ClusterNode): The tree node currently being converted to Newick.\n            newick (str): The current Newick representation of the tree.\n            parent_dist (float): The distance to parent node.\n            leaf_names (list[str]): Names of the tree node.\n\n        Returns:\n            str: The Newick representation of the tree.\n        \"\"\"\n        # If the node is a leaf, enclose\n        if node.is_leaf():\n            return f\"{leaf_names[node.id]}:{(parent_dist - node.dist) / 2}{newick}\"\n        else:\n            # Write the distance to the parent node\n            newick = (\n                f\"):{(parent_dist - node.dist) / 2}{newick}\"\n                if len(newick) &gt; 0\n                else \");\"\n            )\n            # Recursive call to expand the tree\n            newick = _build_newick_tree(\n                newick=newick,\n                node=node.get_left(),\n                parent_dist=node.dist,\n                leaf_names=leaf_names,\n            )\n            newick = _build_newick_tree(\n                newick=f\",{newick}\",\n                node=node.get_right(),\n                parent_dist=node.dist,\n                leaf_names=leaf_names,\n            )\n            # Enclose the tree at the beginning\n            return f\"({newick}\"\n\n    # Trigger the recursive function\n    return _build_newick_tree(\n        node=tree, newick=\"\", parent_dist=tree.dist, leaf_names=labels\n    )\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._get_newick_tree","title":"<code>_get_newick_tree(labels: list[str], sample_dtm: pd.DataFrame) -&gt; str</code>","text":"<p>Get Newick tree based on a subset of the DTM.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>list[str]</code> <p>All file names from the DTM</p> required <code>sample_dtm</code> <code>DataFrame</code> <p>An 80% subset of the complete DTM</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A Newick formatted tree representing the DTM subset</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def _get_newick_tree(self, labels: list[str], sample_dtm: pd.DataFrame) -&gt; str:\n    \"\"\"Get Newick tree based on a subset of the DTM.\n\n    Args:\n        labels (list[str]): All file names from the DTM\n        sample_dtm (pd.DataFrame): An 80% subset of the complete DTM\n\n    Returns:\n        str: A Newick formatted tree representing the DTM subset\n    \"\"\"\n    # Get the linkage matrix for the sample doc term matrix\n    linkage_matrix = linkage(\n        sample_dtm.values, metric=self.metric, method=self.method\n    )\n\n    # Get the Newick representation of the tree\n    newick = self.linkage_to_newick(matrix=linkage_matrix, labels=labels)\n\n    # Convert linkage matrix to a tree node and return it\n    return Phylo.read(StringIO(newick), format=\"newick\")\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._get_bootstrap_trees","title":"<code>_get_bootstrap_trees() -&gt; list[str]</code>","text":"<p>Do bootstrap on the DTM to get a list of Newick trees.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of Newick formatted tree where each tree was based on an 80% subset of the complete DTM.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def _get_bootstrap_trees(self) -&gt; list[str]:\n    \"\"\"Do bootstrap on the DTM to get a list of Newick trees.\n\n    Returns:\n        list[str]: A list of Newick formatted tree where each tree was based on an 80% subset of the complete DTM.\n    \"\"\"\n    # Save the DTM to avoid multiple calls\n    dtm = self._doc_term_matrix\n\n    # Get doc names, since tree nodes need labels\n    labels = [doc for doc in self._doc_term_matrix.index.values.tolist()]\n\n    # The bootstrap process to get all the trees.\n    return [\n        self._get_newick_tree(\n            sample_dtm=dtm.sample(\n                axis=1,\n                frac=0.8,\n                replace=self.replace,\n                random_state=np.random.RandomState(),\n            ),\n            labels=labels,\n        )\n        for _ in range(self.iterations)\n    ]\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._get_bootstrap_consensus_tree","title":"<code>_get_bootstrap_consensus_tree() -&gt; Phylo</code>","text":"<p>Get the consensus tree.</p> <p>Returns:</p> Name Type Description <code>Phylo</code> <code>Phylo</code> <p>The consensus tree of the list of Newick trees.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def _get_bootstrap_consensus_tree(self) -&gt; Phylo:\n    \"\"\"Get the consensus tree.\n\n    Returns:\n        Phylo: The consensus tree of the list of Newick trees.\n    \"\"\"\n    # Find the consensus of all the Newick trees\n    return majority_consensus(trees=self._get_bootstrap_trees(), cutoff=self.cutoff)\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._get_bootstrap_consensus_tree_fig","title":"<code>_get_bootstrap_consensus_tree_fig(layout: str = 'rectangular') -&gt; Figure</code>","text":"<p>Generate a bootstrap consensus tree figure.</p> <p>Parameters:</p> Name Type Description Default <code>layout</code> <code>str</code> <p>Tree visualization style. Options: - \"rectangular\": Traditional rectangular tree layout - \"fan\": Circular fan-style tree layout</p> <code>'rectangular'</code> <p>Returns:</p> Name Type Description <code>Figure</code> <code>Figure</code> <p>The matplotlib figure containing the tree visualization.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def _get_bootstrap_consensus_tree_fig(self, layout: str = \"rectangular\") -&gt; Figure:\n    \"\"\"Generate a bootstrap consensus tree figure.\n\n    Args:\n        layout: Tree visualization style. Options:\n            - \"rectangular\": Traditional rectangular tree layout\n            - \"fan\": Circular fan-style tree layout\n\n    Returns:\n        Figure: The matplotlib figure containing the tree visualization.\n    \"\"\"\n    # Get the colours\n    color = tuple(map(int, self.text_color[4:-1].split(\",\")))\n    normalized_color = tuple(x / 255 for x in color)\n\n    # Get the consensus tree\n    tree = self._get_bootstrap_consensus_tree()\n    tree.root.color = color\n\n    if layout == \"rectangular\":\n        return self._draw_rectangular_tree(tree, normalized_color)\n    elif layout in [\"fan\"]:\n        return self._draw_fan_tree(tree, normalized_color, layout)\n    else:\n        raise ValueError(f\"Unknown layout: {layout}. Use 'rectangular' or 'fan'.\")\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._draw_rectangular_tree","title":"<code>_draw_rectangular_tree(tree, normalized_color) -&gt; Figure</code>","text":"<p>Draw traditional rectangular tree layout.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def _draw_rectangular_tree(self, tree, normalized_color) -&gt; Figure:\n    \"\"\"Draw traditional rectangular tree layout.\"\"\"\n    fig, ax = plt.subplots()\n\n    Phylo.draw(\n        tree,\n        axes=ax,\n        do_show=False,\n        branch_labels=lambda clade: f\"{clade.branch_length:.{PRECISION}f}\\n\"\n        if clade.branch_length is not None\n        else \"\",\n    )\n\n    # Set labels for the plot\n    plt.xlabel(\"Branch Length\", color=normalized_color)\n    plt.ylabel(\"Documents\", color=normalized_color)\n\n    # Hide the two unused borders\n    plt.gca().spines[\"top\"].set_visible(False)\n    plt.gca().spines[\"right\"].set_visible(False)\n\n    # Set the colour of the used borders and labels\n    plt.gca().spines[\"bottom\"].set_color(normalized_color)\n    plt.gca().spines[\"left\"].set_color(normalized_color)\n    plt.gca().tick_params(colors=normalized_color)\n\n    # Extend the x-axis to the right to fit longer labels\n    x_left, x_right, y_low, y_high = plt.axis()\n    plt.axis((x_left, x_right * 1.25, y_low, y_high))\n\n    # Set the graph size, title, and tight layout\n    plt.gcf().set_size_inches(w=9.5, h=(len(self._document_label_map) * 0.3 + 1))\n    plt.title(self.title, color=normalized_color)\n    plt.gcf().tight_layout()\n\n    # Change the line spacing\n    for text in plt.gca().texts:\n        text.set_linespacing(spacing=0.1)\n        text.set_color(normalized_color)\n\n    return fig\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT._draw_fan_tree","title":"<code>_draw_fan_tree(tree, normalized_color, style: str) -&gt; Figure</code>","text":"<p>Draw fan-style (circular) tree layout with color-coded labels by clade.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def _draw_fan_tree(self, tree, normalized_color, style: str) -&gt; Figure:\n    \"\"\"Draw fan-style (circular) tree layout with color-coded labels by clade.\"\"\"\n    # Create figure with equal aspect ratio for circular plot\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.set_aspect(\"equal\")\n\n    # Get all terminal nodes and calculate their positions\n    terminals = list(tree.get_terminals())\n    num_terminals = len(terminals)\n\n    if num_terminals == 0:\n        raise ValueError(\"Tree has no terminal nodes\")\n\n    # Fan style: use 270 degrees (3/4 circle)\n    start_angle = -135  # Start at bottom-left\n    total_angle = 270  # 270 degrees total\n\n    # Assign angles to terminal nodes\n    terminal_angles = {}\n    for i, terminal in enumerate(terminals):\n        if num_terminals == 1:\n            angle = start_angle\n        else:\n            angle = start_angle + (i * total_angle / num_terminals)\n        terminal_angles[terminal] = math.radians(angle)\n\n    # Calculate tree depth for all nodes\n    node_depths = {}\n    max_depth = 0\n\n    def calculate_depths(node, depth=0):\n        \"\"\"Calculate depth from root for each node.\"\"\"\n        nonlocal max_depth\n        node_depths[node] = depth\n        max_depth = max(max_depth, depth)\n\n        if not node.is_terminal():\n            for child in node.clades:\n                calculate_depths(child, depth + 1)\n\n    calculate_depths(tree.root)\n\n    # Generate color palette\n    def generate_colors(n):\n        \"\"\"Generate n distinct colors.\"\"\"\n        if n &lt;= 10:\n            # Use predefined colors for small numbers\n            colors = [\n                \"#1f77b4\",\n                \"#ff7f0e\",\n                \"#2ca02c\",\n                \"#d62728\",\n                \"#9467bd\",\n                \"#8c564b\",\n                \"#e377c2\",\n                \"#7f7f7f\",\n                \"#bcbd22\",\n                \"#17becf\",\n            ]\n            return colors[:n]\n        else:\n            # Generate colors using HSV color space for larger numbers\n            colors = []\n            for i in range(n):\n                hue = i / n\n                rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n                colors.append(mcolors.rgb2hex(rgb))\n            return colors\n\n    # Find clade groupings for color assignment at higher levels\n    terminal_colors = {}\n\n    def assign_higher_level_colors():\n        \"\"\"Assign colors based on higher-level splits in the tree.\"\"\"\n        color_palette = generate_colors(10)\n        color_index = 0\n\n        def find_terminal_descendants(node):\n            \"\"\"Get all terminal nodes that descend from this node.\"\"\"\n            if node.is_terminal():\n                return [node]\n\n            descendants = []\n            for child in node.clades:\n                descendants.extend(find_terminal_descendants(child))\n            return descendants\n\n        def assign_colors_recursive(node, current_depth=0):\n            \"\"\"Recursively assign colors based on tree structure.\"\"\"\n            nonlocal color_index\n\n            if node.is_terminal():\n                return\n\n            # For nodes at depth 1 (direct children of root), assign same color to all descendants\n            if current_depth == 1:\n                descendants = find_terminal_descendants(node)\n                current_color = color_palette[color_index % len(color_palette)]\n\n                for terminal in descendants:\n                    if terminal not in terminal_colors:\n                        terminal_colors[terminal] = current_color\n\n                color_index += 1\n\n            # Continue recursively for deeper nodes\n            for child in node.clades:\n                assign_colors_recursive(child, current_depth + 1)\n\n        # Start from root\n        assign_colors_recursive(tree.root)\n\n    # Assign colors based on higher-level clades\n    assign_higher_level_colors()\n\n    # Set circumference radius for terminals\n    circumference_radius = 1.0\n\n    # Calculate positions for all nodes\n    node_positions = {}\n    terminal_parent_map = {}  # Store parent-child relationships for terminals\n\n    def calculate_positions(node):\n        \"\"\"Calculate x, y positions for all nodes.\"\"\"\n        if node.is_terminal():\n            # Terminal nodes: place directly on circumference\n            angle = terminal_angles[node]\n            x = circumference_radius * math.cos(angle)\n            y = circumference_radius * math.sin(angle)\n            node_positions[node] = (x, y, angle)\n            return angle, circumference_radius\n        else:\n            # Internal nodes: calculate based on children\n            child_angles = []\n            child_distances = []\n\n            for child in node.clades:\n                child_angle, child_distance = calculate_positions(child)\n                child_angles.append(child_angle)\n                child_distances.append(child_distance)\n\n                # Store parent-child relationship for terminals\n                if child.is_terminal():\n                    terminal_parent_map[child] = node\n\n            # Internal node angle is the average of its children's angles\n            avg_angle = sum(child_angles) / len(child_angles)\n\n            # Position internal nodes VERY close to center for maximum clustering\n            depth = node_depths[node]\n\n            if depth == 0:  # Root node\n                distance = 0.02  # Almost at center\n            else:\n                # Use extremely aggressive scaling - internal nodes very close to root\n                normalized_depth = depth / max_depth\n                distance = circumference_radius * (\n                    0.05 + 0.15 * (normalized_depth**3.0)\n                )\n\n            x = distance * math.cos(avg_angle)\n            y = distance * math.sin(avg_angle)\n            node_positions[node] = (x, y, avg_angle)\n\n            return avg_angle, distance\n\n    # Calculate all positions\n    calculate_positions(tree.root)\n\n    # Draw the tree with proper branching structure\n    def draw_branches(node):\n        \"\"\"Draw branches connecting nodes with proper tree structure.\"\"\"\n        if node not in node_positions:\n            return\n\n        node_x, node_y, node_angle = node_positions[node]\n\n        # For internal nodes with multiple children, we want to create proper branching\n        if not node.is_terminal() and len(node.clades) &gt; 1:\n            # First, draw lines from this node to each child\n            for child in node.clades:\n                if child in node_positions:\n                    child_x, child_y, child_angle = node_positions[child]\n\n                    # Draw the branch\n                    if child.is_terminal():\n                        linewidth = 2.0\n                        alpha = 0.9\n                    else:\n                        linewidth = 1.8\n                        alpha = 0.8\n\n                    ax.plot(\n                        [node_x, child_x],\n                        [node_y, child_y],\n                        color=normalized_color,\n                        linewidth=linewidth,\n                        alpha=alpha,\n                    )\n\n                    # Recursively draw children\n                    draw_branches(child)\n        elif len(node.clades) == 1:\n            # Single child - direct connection\n            child = node.clades[0]\n            if child in node_positions:\n                child_x, child_y, child_angle = node_positions[child]\n\n                linewidth = 2.0 if child.is_terminal() else 1.8\n                alpha = 0.9 if child.is_terminal() else 0.8\n\n                ax.plot(\n                    [node_x, child_x],\n                    [node_y, child_y],\n                    color=normalized_color,\n                    linewidth=linewidth,\n                    alpha=alpha,\n                )\n\n                draw_branches(child)\n\n    # Draw all branches starting from root\n    draw_branches(tree.root)\n\n    # Draw nodes and labels with perfect alignment and color coding\n    for node in node_positions:\n        x, y, angle = node_positions[node]\n\n        if node.is_terminal():\n            # Terminal node: draw label perfectly aligned by extending the branch vector\n            label = str(node.name) if node.name else \"Unnamed\"\n\n            # Get the color for this terminal node\n            label_color = terminal_colors.get(node, normalized_color)\n\n            # Get the parent node position\n            if node in terminal_parent_map:\n                parent = terminal_parent_map[node]\n                parent_x, parent_y, parent_angle = node_positions[parent]\n\n                # Calculate the direction vector from parent to terminal\n                dx = x - parent_x\n                dy = y - parent_y\n\n                # Normalize the direction vector\n                branch_length = math.sqrt(dx * dx + dy * dy)\n                if branch_length &gt; 0:\n                    dx_norm = dx / branch_length\n                    dy_norm = dy / branch_length\n\n                    # Extend the label position along the same direction vector\n                    label_extension = 0.02  # Your preferred distance\n                    label_x = x + dx_norm * label_extension\n                    label_y = y + dy_norm * label_extension\n\n                    # Calculate rotation angle from the direction vector\n                    branch_angle_rad = math.atan2(dy, dx)\n                    angle_deg = math.degrees(branch_angle_rad)\n\n                    # Ensure text is readable (not upside down)\n                    if -90 &lt;= angle_deg &lt;= 90:\n                        rotation = angle_deg\n                        ha = \"left\"\n                        va = \"center\"\n                    else:\n                        rotation = angle_deg + 180\n                        ha = \"right\"\n                        va = \"center\"\n\n                    # Draw the label perfectly aligned with branch direction and colored by clade\n                    ax.text(\n                        label_x,\n                        label_y,\n                        label,\n                        rotation=rotation,\n                        rotation_mode=\"anchor\",\n                        ha=ha,\n                        va=va,\n                        color=label_color,\n                        fontsize=12,\n                        weight=\"bold\",\n                    )\n                else:\n                    # Fallback if branch_length is zero (shouldn't happen)\n                    ax.text(\n                        x * 1.08,\n                        y * 1.08,\n                        label,\n                        color=label_color,\n                        fontsize=12,\n                        weight=\"bold\",\n                    )\n            else:\n                # Fallback if no parent found (shouldn't happen)\n                ax.text(\n                    x * 1.08,\n                    y * 1.08,\n                    label,\n                    color=label_color,\n                    fontsize=12,\n                    weight=\"bold\",\n                )\n\n            # Don't draw terminal node markers - only show labels\n\n        elif node == tree.root:\n            # Only draw the root node marker - hide all other internal nodes\n            ax.plot(x, y, \"o\", color=normalized_color, markersize=6, alpha=1.0)\n\n    # Set up the plot limits\n    plot_limit = circumference_radius * 1.25\n    ax.set_xlim(-plot_limit, plot_limit)\n    ax.set_ylim(-plot_limit, plot_limit)\n\n    # Remove axes for cleaner look\n    ax.set_xticks([])\n    ax.set_yticks([])\n    for spine in ax.spines.values():\n        spine.set_visible(False)\n\n    # Add title\n    plt.title(\n        self.title,\n        color=normalized_color,\n        pad=30,\n        fontsize=16,\n        weight=\"bold\",\n    )\n\n    plt.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.save","title":"<code>save(path: Path | str | None) -&gt; None</code>","text":"<p>Save the bootstrap consensus tree result to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str | None</code> <p>The path to save the file.</p> required Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str | None) -&gt; None:\n    \"\"\"Save the bootstrap consensus tree result to a file.\n\n    Args:\n        path (Path | str | None): The path to save the file.\n    \"\"\"\n    if not path or path == \"\":\n        raise LexosException(\"You must provide a valid path.\")\n    self.fig.savefig(path)\n</code></pre>"},{"location":"api/cluster/bootstrap_consensus/#lexos.cluster.bootstrap_consensus.BCT.show","title":"<code>show() -&gt; Figure</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. You can also reference the figure using <code>BCT.fig</code>. This will generally display in a Jupyter notebook.</p> Source code in <code>lexos/cluster/bootstrap_consensus.py</code> <pre><code>def show(self) -&gt; Figure:\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. You can also reference the figure using `BCT.fig`.\n    This will generally display in a Jupyter notebook.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\n            \"You must call the instance before showing the figure.\"\n        )\n    return self.fig\n</code></pre>"},{"location":"api/cluster/clustermap/","title":"API Documentation: <code>clustermap.py</code>","text":"<p>The sync_script script synchronizes the heatmap and dendrogram axes in a Plotly clustermap. It is added to the HTML output of the clustermap to ensure that when the user zooms or pans on one axis, the corresponding axes are updated accordingly.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap","title":"<code>Clustermap</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Clustermap.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>dtm</code>                 (<code>ArrayLike | DTM | DataFrame</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>metric</code>                 (<code>Optional[str]</code>)             </li> <li> <code>method</code>                 (<code>Optional[str]</code>)             </li> <li> <code>hide_upper</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>hide_side</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> <li> <code>z_score</code>                 (<code>Optional[int]</code>)             </li> <li> <code>pivot_kws</code>                 (<code>Optional[dict[str, str]]</code>)             </li> <li> <code>standard_scale</code>                 (<code>Optional[int]</code>)             </li> <li> <code>figsize</code>                 (<code>Optional[tuple[int, int]]</code>)             </li> <li> <code>cbar_kws</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>row_cluster</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>col_cluster</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>row_linkage</code>                 (<code>Optional[ndarray]</code>)             </li> <li> <code>col_linkage</code>                 (<code>Optional[ndarray]</code>)             </li> <li> <code>row_colors</code>                 (<code>Optional[list | DataFrame | Series | str | ListedColormap]</code>)             </li> <li> <code>col_colors</code>                 (<code>Optional[list | DataFrame | Series | str | ListedColormap]</code>)             </li> <li> <code>mask</code>                 (<code>Optional[ndarray | DataFrame]</code>)             </li> <li> <code>dendrogram_ratio</code>                 (<code>Optional[float | tuple[float, float]]</code>)             </li> <li> <code>colors_ratio</code>                 (<code>Optional[float]</code>)             </li> <li> <code>cbar_pos</code>                 (<code>Optional[tuple[str | float]]</code>)             </li> <li> <code>tree_kws</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>center</code>                 (<code>Optional[float | int]</code>)             </li> <li> <code>cmap</code>                 (<code>Optional[str]</code>)             </li> <li> <code>linewidths</code>                 (<code>Optional[float]</code>)             </li> </ul> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>class Clustermap(BaseModel):\n    \"\"\"Clustermap.\"\"\"\n\n    dtm: ArrayLike | DTM | pd.DataFrame = Field(\n        ..., description=\"The document-term matrix.\"\n    )\n    labels: Optional[list[str]] = Field(\n        None, description=\"The labels for the clustermap.\"\n    )\n    metric: Optional[str] = Field(\n        \"euclidean\",\n        description=\"The metric to use for the dendrograms.\",\n    )\n    method: Optional[str] = Field(\n        \"average\",\n        description=\"The method to use for the dendrograms.\",\n    )\n    hide_upper: Optional[bool] = Field(False, description=\"Hide the upper dendrogram.\")\n    hide_side: Optional[bool] = Field(False, description=\"Hide the side dendrogram.\")\n    title: Optional[str] = Field(None, description=\"The title for the dendrogram.\")\n    fig: Optional[matplotlib.figure.Figure] = Field(\n        None, description=\"The figure for the dendrogram.\"\n    )\n    z_score: Optional[int] = Field(1, description=\"The z-score for the clustermap.\")\n    pivot_kws: Optional[dict[str, str]] = Field(\n        None, description=\"The pivot kwargs for the clustermap.\"\n    )\n    standard_scale: Optional[int] = Field(\n        None,\n        description=\"The standard scale for the clustermap.\",\n    )\n    figsize: Optional[tuple[int, int]] = Field(\n        (8, 8), description=\"The figure size for the clustermap.\"\n    )\n    cbar_kws: Optional[dict] = Field(\n        None, description=\"The cbar kwargs for the clustermap.\"\n    )\n    row_cluster: Optional[bool] = Field(\n        True, description=\"Whether to cluster the rows.\"\n    )\n    col_cluster: Optional[bool] = Field(\n        True, description=\"Whether to cluster the columns.\"\n    )\n    row_linkage: Optional[np.ndarray] = Field(\n        None,\n        description=\"Precomputed linkage matrix for the rows. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.\",\n    )\n    col_linkage: Optional[np.ndarray] = Field(\n        None,\n        description=\"Precomputed linkage matrix for the columns. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.\",\n    )\n    row_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = (\n        Field(None, description=\"The row colors.\")\n    )\n    col_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = (\n        Field(None, description=\"The column colors.\")\n    )\n    mask: Optional[np.ndarray | pd.DataFrame] = Field(\n        None, description=\"The mask for the clustermap.\"\n    )\n    dendrogram_ratio: Optional[float | tuple[float, float]] = Field(\n        (0.1, 0.2),\n        description=\"The dendrogram ratio for the clustermap.\",\n    )\n    colors_ratio: Optional[float] = Field(\n        0.03, description=\"The colors ratio for the clustermap.\"\n    )\n    cbar_pos: Optional[tuple[str | float]] = Field(\n        (0.02, 0.32, 0.03, 0.2),\n        description=\"The cbar position for the clustermap.\",\n    )\n    tree_kws: Optional[dict] = Field(\n        None, description=\"The tree kwargs for the dendrograms.\"\n    )\n    center: Optional[float | int] = Field(\n        0, description=\"The center for the clustermap.\"\n    )\n    cmap: Optional[str] = Field(\"vlag\", description=\"The cmap for the clustermap.\")\n    linewidths: Optional[float] = Field(\n        0.75, description=\"The linewidths for the dendrograms.\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(\n        self,\n        **data,\n    ) -&gt; None:\n        \"\"\"Initialize the Clustermap instance.\"\"\"\n        super().__init__(**data)\n\n        # Set the labels\n        self._set_labels()\n\n        # Get the matrix based on the data type\n        matrix = _get_matrix(self.dtm)\n\n        # Get colour palettes for the dendrograms\n        # Ensure that lists of colours are longer than the number of labels\n        # Not sure if this is necessary for column colours\n        # if isinstance(self.col_colors, list) and len(self.dtm.labels) &gt;= len(self.col_colors):\n        #     raise LexosException(\"The length of `col_colors` must have be greater than the number of labels.\")\n        if isinstance(self.row_colors, list) and len(self.labels) &gt;= len(\n            self.row_colors\n        ):\n            raise LexosException(\n                \"The length of `row_colors` must be greater than the number of labels.\"\n            )\n        col_colors, row_colors = self._get_colors()\n\n        # Validate the linkage matrices\n        self._validate_linkage_matrices()\n\n        # Perform the clustering\n        g = sns.clustermap(\n            matrix,\n            cmap=self.cmap,\n            method=self.method,\n            metric=self.metric,\n            figsize=self.figsize,\n            col_colors=col_colors,\n            row_colors=row_colors,\n            center=self.center,\n            linewidths=self.linewidths,\n            z_score=self.z_score,\n            pivot_kws=self.pivot_kws,\n            standard_scale=self.standard_scale,\n            cbar_kws=self.cbar_kws,\n            row_linkage=self.row_linkage,\n            col_linkage=self.col_linkage,\n            mask=self.mask,\n            dendrogram_ratio=self.dendrogram_ratio,\n            colors_ratio=self.colors_ratio,\n            cbar_pos=self.cbar_pos,\n            tree_kws=self.tree_kws,\n        )\n\n        # Remove the dendrogram on the top\n        if self.hide_upper:\n            g.ax_col_dendrogram.remove()\n\n        # Remove the dendrogram on the left\n        if self.hide_side:\n            g.ax_row_dendrogram.remove()\n\n        # Add the title\n        if self.title:\n            if self.hide_upper:\n                y = 0.95\n            else:\n                y = 1.05\n            g.figure.suptitle(self.title, y=y)\n\n        # Save the fig variable\n        self.fig = g.figure\n\n        # Do not automatically display -- require fig.show()\n        plt.close(self.fig)\n\n    def _get_colors(self) -&gt; ListedColormap | None:\n        \"\"\"Get the row and column colors for the clustermap.\n\n        Notes:\n        - For valid palettes, see https://seaborn.pydata.org/generated/seaborn.color_palette.html.\n        - The value \"default\" will use the husl palette with 8 colours.\n\n        Returns:\n            A matplotlib ListedColormap or None.\n        \"\"\"\n        # Convert palette to vectors drawn on the side of the matrix\n        # None means no colours, \"default\" means use the husl palette\n        if self.col_colors is None:\n            col_colors = None\n        elif isinstance(self.col_colors, (pd.DataFrame, pd.Series)):\n            col_colors = self.col_colors\n        elif self.col_colors == \"default\":\n            col_colors = sns.husl_palette(8, s=0.45)\n        else:\n            try:\n                col_colors = sns.color_palette(self.col_colors, len(self.col_colors))\n            except ValueError:\n                raise LexosException(\"Invalid column palette.\")\n\n        if self.row_colors is None:\n            row_colors = None\n        elif isinstance(self.row_colors, (pd.DataFrame, pd.Series)):\n            row_colors = self.row_colors\n        elif self.row_colors == \"default\":\n            row_colors = sns.husl_palette(8, s=0.45)\n        else:\n            try:\n                row_colors = sns.color_palette(self.row_colors, len(self.row_colors))\n            except ValueError:\n                raise LexosException(\"Invalid row palette.\")\n\n        return col_colors, row_colors\n\n    def _set_attrs(self, **kwargs: Any):\n        \"\"\"Set the attributes of the class.\n\n        Args:\n            **kwargs: The attributes to set.\n        \"\"\"\n        for key, value in kwargs.items():\n            if value is not None:\n                setattr(self, key, value)\n\n    def _set_labels(self):\n        \"\"\"Set the labels for the clustermap.\"\"\"\n        if not self.labels:\n            if isinstance(self.dtm, DTM):\n                self.labels = self.dtm.labels\n            elif isinstance(self.dtm, pd.DataFrame):\n                self.labels = self.dtm.columns.values.tolist()[1:]\n            else:\n                self.labels = [f\"Doc{i + 1}\" for i, _ in enumerate(self.dtm)]\n\n    def _validate_linkage_matrices(self):\n        \"\"\"Validate the linkage matrices.\"\"\"\n        # TODO: raise a LexosException if hierarchy.is_valid_linkage fails\n        if self.row_linkage is not None:\n            try:\n                hierarchy.is_valid_linkage(self.row_linkage, throw=True)\n            except (TypeError, ValueError) as e:\n                raise LexosException(f\"Invalid `row_linkage` value: {e}\")\n        if self.col_linkage is not None:\n            try:\n                hierarchy.is_valid_linkage(self.col_linkage, throw=True)\n            except (TypeError, ValueError) as e:\n                raise LexosException(f\"Invalid `col_linkage` value: {e}\")\n\n    def save(self, path: Path | str, **kwargs: Any):\n        \"\"\"Save the figure to a file.\n\n        Args:\n            path (Path | str): The path of the file to save.\n            **kwargs (Any): Additional keyword arguments for pyplot.savefig. See https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html.\n        \"\"\"\n        self.fig.savefig(path, **kwargs)\n\n    def show(self):\n        \"\"\"Show the figure if it is hidden.\n\n        This is a helper method. You can also reference the figure\n        using `ClusterMap.fig`. This will generally display in a\n        Jupyter notebook.\n        \"\"\"\n        return self.fig\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.cbar_kws","title":"<code>cbar_kws: Optional[dict] = None</code>  <code>pydantic-field</code>","text":"<p>The cbar kwargs for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.cbar_pos","title":"<code>cbar_pos: Optional[tuple[str | float]] = (0.02, 0.32, 0.03, 0.2)</code>  <code>pydantic-field</code>","text":"<p>The cbar position for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.center","title":"<code>center: Optional[float | int] = 0</code>  <code>pydantic-field</code>","text":"<p>The center for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.cmap","title":"<code>cmap: Optional[str] = 'vlag'</code>  <code>pydantic-field</code>","text":"<p>The cmap for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.col_cluster","title":"<code>col_cluster: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to cluster the columns.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.col_colors","title":"<code>col_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = None</code>  <code>pydantic-field</code>","text":"<p>The column colors.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.col_linkage","title":"<code>col_linkage: Optional[np.ndarray] = None</code>  <code>pydantic-field</code>","text":"<p>Precomputed linkage matrix for the columns. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.colors_ratio","title":"<code>colors_ratio: Optional[float] = 0.03</code>  <code>pydantic-field</code>","text":"<p>The colors ratio for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.dendrogram_ratio","title":"<code>dendrogram_ratio: Optional[float | tuple[float, float]] = (0.1, 0.2)</code>  <code>pydantic-field</code>","text":"<p>The dendrogram ratio for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.dtm","title":"<code>dtm: ArrayLike | DTM | pd.DataFrame</code>  <code>pydantic-field</code>","text":"<p>The document-term matrix.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.figsize","title":"<code>figsize: Optional[tuple[int, int]] = (8, 8)</code>  <code>pydantic-field</code>","text":"<p>The figure size for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.hide_side","title":"<code>hide_side: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Hide the side dendrogram.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.hide_upper","title":"<code>hide_upper: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Hide the upper dendrogram.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.labels","title":"<code>labels: Optional[list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>The labels for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.linewidths","title":"<code>linewidths: Optional[float] = 0.75</code>  <code>pydantic-field</code>","text":"<p>The linewidths for the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.mask","title":"<code>mask: Optional[np.ndarray | pd.DataFrame] = None</code>  <code>pydantic-field</code>","text":"<p>The mask for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.method","title":"<code>method: Optional[str] = 'average'</code>  <code>pydantic-field</code>","text":"<p>The method to use for the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.metric","title":"<code>metric: Optional[str] = 'euclidean'</code>  <code>pydantic-field</code>","text":"<p>The metric to use for the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.pivot_kws","title":"<code>pivot_kws: Optional[dict[str, str]] = None</code>  <code>pydantic-field</code>","text":"<p>The pivot kwargs for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.row_cluster","title":"<code>row_cluster: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to cluster the rows.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.row_colors","title":"<code>row_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = None</code>  <code>pydantic-field</code>","text":"<p>The row colors.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.row_linkage","title":"<code>row_linkage: Optional[np.ndarray] = None</code>  <code>pydantic-field</code>","text":"<p>Precomputed linkage matrix for the rows. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.standard_scale","title":"<code>standard_scale: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The standard scale for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title for the dendrogram.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.tree_kws","title":"<code>tree_kws: Optional[dict] = None</code>  <code>pydantic-field</code>","text":"<p>The tree kwargs for the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.z_score","title":"<code>z_score: Optional[int] = 1</code>  <code>pydantic-field</code>","text":"<p>The z-score for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize the Clustermap instance.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def __init__(\n    self,\n    **data,\n) -&gt; None:\n    \"\"\"Initialize the Clustermap instance.\"\"\"\n    super().__init__(**data)\n\n    # Set the labels\n    self._set_labels()\n\n    # Get the matrix based on the data type\n    matrix = _get_matrix(self.dtm)\n\n    # Get colour palettes for the dendrograms\n    # Ensure that lists of colours are longer than the number of labels\n    # Not sure if this is necessary for column colours\n    # if isinstance(self.col_colors, list) and len(self.dtm.labels) &gt;= len(self.col_colors):\n    #     raise LexosException(\"The length of `col_colors` must have be greater than the number of labels.\")\n    if isinstance(self.row_colors, list) and len(self.labels) &gt;= len(\n        self.row_colors\n    ):\n        raise LexosException(\n            \"The length of `row_colors` must be greater than the number of labels.\"\n        )\n    col_colors, row_colors = self._get_colors()\n\n    # Validate the linkage matrices\n    self._validate_linkage_matrices()\n\n    # Perform the clustering\n    g = sns.clustermap(\n        matrix,\n        cmap=self.cmap,\n        method=self.method,\n        metric=self.metric,\n        figsize=self.figsize,\n        col_colors=col_colors,\n        row_colors=row_colors,\n        center=self.center,\n        linewidths=self.linewidths,\n        z_score=self.z_score,\n        pivot_kws=self.pivot_kws,\n        standard_scale=self.standard_scale,\n        cbar_kws=self.cbar_kws,\n        row_linkage=self.row_linkage,\n        col_linkage=self.col_linkage,\n        mask=self.mask,\n        dendrogram_ratio=self.dendrogram_ratio,\n        colors_ratio=self.colors_ratio,\n        cbar_pos=self.cbar_pos,\n        tree_kws=self.tree_kws,\n    )\n\n    # Remove the dendrogram on the top\n    if self.hide_upper:\n        g.ax_col_dendrogram.remove()\n\n    # Remove the dendrogram on the left\n    if self.hide_side:\n        g.ax_row_dendrogram.remove()\n\n    # Add the title\n    if self.title:\n        if self.hide_upper:\n            y = 0.95\n        else:\n            y = 1.05\n        g.figure.suptitle(self.title, y=y)\n\n    # Save the fig variable\n    self.fig = g.figure\n\n    # Do not automatically display -- require fig.show()\n    plt.close(self.fig)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.save","title":"<code>save(path: Path | str, **kwargs: Any)</code>","text":"<p>Save the figure to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path of the file to save.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for pyplot.savefig. See https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html.</p> <code>{}</code> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def save(self, path: Path | str, **kwargs: Any):\n    \"\"\"Save the figure to a file.\n\n    Args:\n        path (Path | str): The path of the file to save.\n        **kwargs (Any): Additional keyword arguments for pyplot.savefig. See https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html.\n    \"\"\"\n    self.fig.savefig(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.show","title":"<code>show()</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. You can also reference the figure using <code>ClusterMap.fig</code>. This will generally display in a Jupyter notebook.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def show(self):\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. You can also reference the figure\n    using `ClusterMap.fig`. This will generally display in a\n    Jupyter notebook.\n    \"\"\"\n    return self.fig\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize the Clustermap instance.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def __init__(\n    self,\n    **data,\n) -&gt; None:\n    \"\"\"Initialize the Clustermap instance.\"\"\"\n    super().__init__(**data)\n\n    # Set the labels\n    self._set_labels()\n\n    # Get the matrix based on the data type\n    matrix = _get_matrix(self.dtm)\n\n    # Get colour palettes for the dendrograms\n    # Ensure that lists of colours are longer than the number of labels\n    # Not sure if this is necessary for column colours\n    # if isinstance(self.col_colors, list) and len(self.dtm.labels) &gt;= len(self.col_colors):\n    #     raise LexosException(\"The length of `col_colors` must have be greater than the number of labels.\")\n    if isinstance(self.row_colors, list) and len(self.labels) &gt;= len(\n        self.row_colors\n    ):\n        raise LexosException(\n            \"The length of `row_colors` must be greater than the number of labels.\"\n        )\n    col_colors, row_colors = self._get_colors()\n\n    # Validate the linkage matrices\n    self._validate_linkage_matrices()\n\n    # Perform the clustering\n    g = sns.clustermap(\n        matrix,\n        cmap=self.cmap,\n        method=self.method,\n        metric=self.metric,\n        figsize=self.figsize,\n        col_colors=col_colors,\n        row_colors=row_colors,\n        center=self.center,\n        linewidths=self.linewidths,\n        z_score=self.z_score,\n        pivot_kws=self.pivot_kws,\n        standard_scale=self.standard_scale,\n        cbar_kws=self.cbar_kws,\n        row_linkage=self.row_linkage,\n        col_linkage=self.col_linkage,\n        mask=self.mask,\n        dendrogram_ratio=self.dendrogram_ratio,\n        colors_ratio=self.colors_ratio,\n        cbar_pos=self.cbar_pos,\n        tree_kws=self.tree_kws,\n    )\n\n    # Remove the dendrogram on the top\n    if self.hide_upper:\n        g.ax_col_dendrogram.remove()\n\n    # Remove the dendrogram on the left\n    if self.hide_side:\n        g.ax_row_dendrogram.remove()\n\n    # Add the title\n    if self.title:\n        if self.hide_upper:\n            y = 0.95\n        else:\n            y = 1.05\n        g.figure.suptitle(self.title, y=y)\n\n    # Save the fig variable\n    self.fig = g.figure\n\n    # Do not automatically display -- require fig.show()\n    plt.close(self.fig)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.save","title":"<code>save(path: Path | str, **kwargs: Any)</code>","text":"<p>Save the figure to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path of the file to save.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for pyplot.savefig. See https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html.</p> <code>{}</code> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def save(self, path: Path | str, **kwargs: Any):\n    \"\"\"Save the figure to a file.\n\n    Args:\n        path (Path | str): The path of the file to save.\n        **kwargs (Any): Additional keyword arguments for pyplot.savefig. See https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html.\n    \"\"\"\n    self.fig.savefig(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap.show","title":"<code>show()</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. You can also reference the figure using <code>ClusterMap.fig</code>. This will generally display in a Jupyter notebook.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def show(self):\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. You can also reference the figure\n    using `ClusterMap.fig`. This will generally display in a\n    Jupyter notebook.\n    \"\"\"\n    return self.fig\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap._get_colors","title":"<code>_get_colors() -&gt; ListedColormap | None</code>","text":"<p>Get the row and column colors for the clustermap.</p> <p>Notes: - For valid palettes, see https://seaborn.pydata.org/generated/seaborn.color_palette.html. - The value \"default\" will use the husl palette with 8 colours.</p> <p>Returns:</p> Type Description <code>ListedColormap | None</code> <p>A matplotlib ListedColormap or None.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _get_colors(self) -&gt; ListedColormap | None:\n    \"\"\"Get the row and column colors for the clustermap.\n\n    Notes:\n    - For valid palettes, see https://seaborn.pydata.org/generated/seaborn.color_palette.html.\n    - The value \"default\" will use the husl palette with 8 colours.\n\n    Returns:\n        A matplotlib ListedColormap or None.\n    \"\"\"\n    # Convert palette to vectors drawn on the side of the matrix\n    # None means no colours, \"default\" means use the husl palette\n    if self.col_colors is None:\n        col_colors = None\n    elif isinstance(self.col_colors, (pd.DataFrame, pd.Series)):\n        col_colors = self.col_colors\n    elif self.col_colors == \"default\":\n        col_colors = sns.husl_palette(8, s=0.45)\n    else:\n        try:\n            col_colors = sns.color_palette(self.col_colors, len(self.col_colors))\n        except ValueError:\n            raise LexosException(\"Invalid column palette.\")\n\n    if self.row_colors is None:\n        row_colors = None\n    elif isinstance(self.row_colors, (pd.DataFrame, pd.Series)):\n        row_colors = self.row_colors\n    elif self.row_colors == \"default\":\n        row_colors = sns.husl_palette(8, s=0.45)\n    else:\n        try:\n            row_colors = sns.color_palette(self.row_colors, len(self.row_colors))\n        except ValueError:\n            raise LexosException(\"Invalid row palette.\")\n\n    return col_colors, row_colors\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap._set_attrs","title":"<code>_set_attrs(**kwargs: Any)</code>","text":"<p>Set the attributes of the class.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>The attributes to set.</p> <code>{}</code> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _set_attrs(self, **kwargs: Any):\n    \"\"\"Set the attributes of the class.\n\n    Args:\n        **kwargs: The attributes to set.\n    \"\"\"\n    for key, value in kwargs.items():\n        if value is not None:\n            setattr(self, key, value)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap._set_labels","title":"<code>_set_labels()</code>","text":"<p>Set the labels for the clustermap.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _set_labels(self):\n    \"\"\"Set the labels for the clustermap.\"\"\"\n    if not self.labels:\n        if isinstance(self.dtm, DTM):\n            self.labels = self.dtm.labels\n        elif isinstance(self.dtm, pd.DataFrame):\n            self.labels = self.dtm.columns.values.tolist()[1:]\n        else:\n            self.labels = [f\"Doc{i + 1}\" for i, _ in enumerate(self.dtm)]\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.Clustermap._validate_linkage_matrices","title":"<code>_validate_linkage_matrices()</code>","text":"<p>Validate the linkage matrices.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _validate_linkage_matrices(self):\n    \"\"\"Validate the linkage matrices.\"\"\"\n    # TODO: raise a LexosException if hierarchy.is_valid_linkage fails\n    if self.row_linkage is not None:\n        try:\n            hierarchy.is_valid_linkage(self.row_linkage, throw=True)\n        except (TypeError, ValueError) as e:\n            raise LexosException(f\"Invalid `row_linkage` value: {e}\")\n    if self.col_linkage is not None:\n        try:\n            hierarchy.is_valid_linkage(self.col_linkage, throw=True)\n        except (TypeError, ValueError) as e:\n            raise LexosException(f\"Invalid `col_linkage` value: {e}\")\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap","title":"<code>PlotlyClustermap</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Plotly version of the Clustermap.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>dtm</code>                 (<code>Optional[ArrayLike | DTM | DataFrame]</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>metric</code>                 (<code>Optional[str]</code>)             </li> <li> <code>method</code>                 (<code>Optional[str]</code>)             </li> <li> <code>hide_upper</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>hide_side</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> <li> <code>z_score</code>                 (<code>Optional[int]</code>)             </li> <li> <code>pivot_kws</code>                 (<code>Optional[dict[str, str]]</code>)             </li> <li> <code>standard_scale</code>                 (<code>Optional[int]</code>)             </li> <li> <code>figsize</code>                 (<code>Optional[tuple[int, int]]</code>)             </li> <li> <code>cbar_kws</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>row_cluster</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>col_cluster</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>row_linkage</code>                 (<code>Optional[ndarray]</code>)             </li> <li> <code>col_linkage</code>                 (<code>Optional[ndarray]</code>)             </li> <li> <code>row_colors</code>                 (<code>Optional[list | DataFrame | Series | str | ListedColormap]</code>)             </li> <li> <code>col_colors</code>                 (<code>Optional[list | DataFrame | Series | str | ListedColormap]</code>)             </li> <li> <code>mask</code>                 (<code>Optional[ndarray | DataFrame]</code>)             </li> <li> <code>dendrogram_ratio</code>                 (<code>Optional[float | tuple[float, float]]</code>)             </li> <li> <code>colors_ratio</code>                 (<code>Optional[float]</code>)             </li> <li> <code>cbar_pos</code>                 (<code>Optional[tuple[str]]</code>)             </li> <li> <code>colorbar</code>                 (<code>Optional[dict[str, Any]]</code>)             </li> <li> <code>tree_kws</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>center</code>                 (<code>Optional[float | int]</code>)             </li> <li> <code>cmap</code>                 (<code>Optional[str]</code>)             </li> <li> <code>linewidths</code>                 (<code>Optional[float]</code>)             </li> <li> <code>annot</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>fmt</code>                 (<code>Optional[str]</code>)             </li> <li> <code>show_dendrogram_labels</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>show_heatmap_labels</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>kwargs</code>                 (<code>Any</code>)             </li> </ul> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>class PlotlyClustermap(BaseModel):\n    \"\"\"Plotly version of the Clustermap.\"\"\"\n\n    dtm: Optional[ArrayLike | DTM | pd.DataFrame] = Field(\n        ..., description=\"The document-term matrix.\"\n    )\n    labels: Optional[list[str]] = Field(\n        None, description=\"The labels for the clustermap.\"\n    )\n    metric: Optional[str] = Field(\n        \"euclidean\",\n        description=\"The metric to use for the dendrograms.\",\n    )\n    method: Optional[str] = Field(\n        \"average\",\n        description=\"The method to use for the dendrograms.\",\n    )\n    hide_upper: Optional[bool] = Field(False, description=\"Hide the upper dendrogram.\")\n    hide_side: Optional[bool] = Field(False, description=\"Hide the side dendrogram.\")\n    title: Optional[str] = Field(None, description=\"The title for the dendrogram.\")\n    fig: Optional[go.Figure] = Field(None, description=\"The figure for the clustermap.\")\n    z_score: Optional[int] = Field(1, description=\"The z-score for the clustermap.\")\n    pivot_kws: Optional[dict[str, str]] = Field(\n        None, description=\"The pivot kwargs for the clustermap.\"\n    )\n    standard_scale: Optional[int] = Field(\n        None,\n        description=\"The standard scale for the clustermap.\",\n    )\n    figsize: Optional[tuple[int, int]] = Field(\n        (700, 700), description=\"The figure size for the clustermap in pixels.\"\n    )\n    cbar_kws: Optional[dict] = Field(\n        None, description=\"The cbar kwargs for the clustermap.\"\n    )\n    row_cluster: Optional[bool] = Field(\n        True, description=\"Whether to cluster the rows.\"\n    )\n    col_cluster: Optional[bool] = Field(\n        True, description=\"Whether to cluster the columns.\"\n    )\n    row_linkage: Optional[np.ndarray] = Field(\n        None,\n        description=\"Precomputed linkage matrix for the rows. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.\",\n    )\n    col_linkage: Optional[np.ndarray] = Field(\n        None,\n        description=\"Precomputed linkage matrix for the columns. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.\",\n    )\n    row_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = (\n        Field(None, description=\"The row colors.\")\n    )\n    col_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = (\n        Field(None, description=\"The column colors.\")\n    )\n    mask: Optional[np.ndarray | pd.DataFrame] = Field(\n        None, description=\"The mask for the clustermap.\"\n    )\n    dendrogram_ratio: Optional[float | tuple[float, float]] = Field(\n        (0.8, 0.2),\n        description=\"The dendrogram ratio for the clustermap.\",\n    )\n    colors_ratio: Optional[float] = Field(\n        0.03, description=\"The colors ratio for the clustermap.\"\n    )\n    cbar_pos: Optional[tuple[str]] = Field(\n        (0.02, 0.32, 0.03, 0.2),\n        description=\"The cbar position for the clustermap.\",\n    )\n    colorbar: Optional[dict[str, Any]] = Field(\n        dict(x=0.11, y=0.5, xref=\"container\", yref=\"container\", len=0.6),\n        description=\"The colorbar position for the clustermap. This is a more generic version than `cbar_pos` and can be used to set the position of the colorbar in a more flexible way.\",\n    )\n    tree_kws: Optional[dict] = Field(\n        None, description=\"The tree kwargs for the dendrograms.\"\n    )\n    center: Optional[float | int] = Field(\n        0, description=\"The center for the clustermap. Default could be None.\"\n    )\n    cmap: Optional[str] = Field(\"viridis\", description=\"The cmap for the clustermap.\")\n    linewidths: Optional[float] = Field(\n        0.75, description=\"The linewidths for the dendrograms. Default could be 0.\"\n    )\n    annot: Optional[bool] = Field(\n        False, description=\"Whether to annotate the clustermap.\"\n    )\n    fmt: Optional[str] = Field(\n        \".2g\", description=\"The format for the annotations in the clustermap.\"\n    )\n    show_dendrogram_labels: Optional[bool] = Field(\n        False, description=\"Whether to show the labels on the dendrograms.\"\n    )\n    show_heatmap_labels: Optional[bool] = Field(\n        True, description=\"Whether to show the labels on the heatmap.\"\n    )\n    kwargs: Any = Field(\n        {}, description=\"Additional keyword arguments for the clustermap.\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(\n        self,\n        **data,\n    ) -&gt; None:\n        \"\"\"Initialize the PlotlyClustermap instance.\"\"\"\n        super().__init__(**data)\n\n        # Set the labels\n        self._set_labels()\n\n        # Get the matrix based on the data type\n        matrix = _get_matrix(self.dtm)\n\n        # Extract our custom parameters from kwargs to prevent them being passed to plotly components\n        filtered_kwargs = self.kwargs.copy()\n        filtered_kwargs.pop(\n            \"show_dendrogram_labels\", None\n        )  # This is already a function parameter\n        filtered_kwargs.pop(\n            \"show_heatmap_labels\", None\n        )  # This is already a function parameter\n        filtered_kwargs.pop(\n            \"title\", None\n        )  # Title should go to layout, not heatmap trace\n\n        # Determine whether to show heatmap labels\n        if self.show_heatmap_labels is None:\n            # Auto-mode: hide y-axis (left) labels when row dendrogram is present,\n            # but always show x-axis (bottom) labels for readability\n            show_heatmap_x_labels = (\n                True  # Always show bottom labels unless explicitly disabled\n            )\n            show_heatmap_y_labels = (\n                not self.row_cluster\n            )  # Hide left labels only if row dendrogram present\n        else:\n            # Manual mode: use the same setting for both axes\n            show_heatmap_x_labels = self.show_heatmap_labels\n            show_heatmap_y_labels = self.show_heatmap_labels\n\n        # Initialize cluster grid\n        grid = PlotlyClusterGrid(\n            data=matrix,\n            z_score=self.z_score,\n            standard_scale=self.standard_scale,\n            mask=self.mask,\n            figsize=self.figsize,\n            dendrogram_ratio=self.dendrogram_ratio,\n        )\n\n        # Handle dendrogram ratios\n        if isinstance(self.dendrogram_ratio, (list, tuple)):\n            row_dendrogram_ratio, col_dendrogram_ratio = self.dendrogram_ratio\n        else:\n            row_dendrogram_ratio = col_dendrogram_ratio = self.dendrogram_ratio\n\n        # Handle tree styling\n        if self.tree_kws is None:\n            self.tree_kws = {}\n        tree_color = self.tree_kws.get(\"color\", \"rgb(50,50,50)\")\n        tree_width = self.tree_kws.get(\"linewidth\", 1.0)\n\n        # Calculate clustering\n        data_array = grid.data2d.values\n\n        # Row clustering\n        row_linkage = data.get(\"row_linkage\", None)\n        if self.row_cluster:\n            if row_linkage is None:\n                row_linkage = grid._calculate_linkage(\n                    data_array, self.method, self.metric\n                )\n            row_dendro_traces, row_dendro_data = _create_dendrogram_traces(\n                row_linkage,\n                labels=[str(x) for x in grid.data2d.index]\n                if self.show_dendrogram_labels\n                else None,\n                orientation=\"left\",\n                color=tree_color,\n                line_width=tree_width,\n            )\n            row_order = row_dendro_data[\"leaves\"]\n        else:\n            row_order = list(range(len(grid.data2d.index)))\n            row_dendro_traces = []\n            row_dendro_data = None\n\n        # Column clustering\n        col_linkage = data.get(\"col_linkage\", None)\n        if self.col_cluster:\n            if col_linkage is None:\n                col_linkage = grid._calculate_linkage(\n                    data_array.T, self.method, self.metric\n                )\n            col_dendro_traces, col_dendro_data = _create_dendrogram_traces(\n                col_linkage,\n                labels=[str(x) for x in grid.data2d.columns]\n                if self.show_dendrogram_labels\n                else None,\n                orientation=\"top\",\n                color=tree_color,\n                line_width=tree_width,\n            )\n            col_order = col_dendro_data[\"leaves\"]\n        else:\n            col_order = list(range(len(grid.data2d.columns)))\n            col_dendro_traces = []\n            col_dendro_data = None\n\n        # Reorder data\n        ordered_data = grid.data2d.iloc[row_order, col_order]\n\n        # Create subplot layout\n        n_rows = 2 if self.col_cluster else 1\n        n_cols = 2 if self.row_cluster else 1\n\n        # Calculate subplot dimensions\n        if self.row_cluster and self.col_cluster:\n            row_heights = [col_dendrogram_ratio, 1 - col_dendrogram_ratio]\n            col_widths = [1 - row_dendrogram_ratio, row_dendrogram_ratio]\n            # subplot_titles = [\"\", \"Column Dendrogram\", \"Row Dendrogram\", \"Heatmap\"]\n        elif self.col_cluster:\n            row_heights = [col_dendrogram_ratio, 1 - col_dendrogram_ratio]\n            col_widths = [1.0]\n            # subplot_titles = [\"Column Dendrogram\", \"Heatmap\"]\n        elif self.row_cluster:\n            row_heights = [1.0]\n            col_widths = [1 - row_dendrogram_ratio, row_dendrogram_ratio]\n            # subplot_titles = [\"Heatmap\", \"Row Dendrogram\"]\n        else:\n            row_heights = [1.0]\n            col_widths = [1.0]\n            # subplot_titles = [\"Heatmap\"]\n\n        # Create subplots\n        fig = make_subplots(\n            rows=n_rows,\n            cols=n_cols,\n            row_heights=row_heights,\n            column_widths=col_widths,\n            horizontal_spacing=0,  # Remove padding between dendrograms and heatmap\n            vertical_spacing=0,  # Remove padding between dendrograms and heatmap\n            # subplot_titles=None,  # We'll add custom titles if needed\n        )\n\n        # Determine subplot positions\n        heatmap_row = n_rows\n        heatmap_col = 1 if not self.row_cluster else n_cols\n\n        # Prepare heatmap data\n        z_data = ordered_data.values\n        x_labels = [str(x) for x in ordered_data.columns]\n        y_labels = [str(y) for y in ordered_data.index]\n\n        # Apply mask if provided\n        if grid.mask is not None:\n            mask_ordered = grid.mask.iloc[row_order, col_order]\n            z_data = np.where(mask_ordered.values, np.nan, z_data)\n\n        # Add heatmap\n        heatmap_trace = go.Heatmap(\n            z=z_data,\n            x=x_labels,\n            y=y_labels,\n            colorscale=self.cmap,\n            zmid=self.center,\n            showscale=True,\n            colorbar=self.colorbar,\n            name=\"\",  # Remove Trace 0 from hover\n            **filtered_kwargs,\n        )\n\n        fig.add_trace(heatmap_trace, row=heatmap_row, col=heatmap_col)\n\n        # Add column dendrogram\n        if not self.hide_upper:\n            if self.col_cluster and col_dendro_traces:\n                for trace in col_dendro_traces:\n                    fig.add_trace(trace, row=1, col=heatmap_col)\n\n        # Add row dendrogram\n        if not self.hide_side:\n            if self.row_cluster and row_dendro_traces:\n                for trace in row_dendro_traces:\n                    fig.add_trace(trace, row=heatmap_row, col=1)\n\n        # Reverse the traces for the row dendrogram to match the Seaborn dendrogram\n        fig.update_yaxes(row=heatmap_row, col=1, autorange=\"reversed\")\n\n        # Add annotations if requested\n        if self.annot:\n            annotations = []\n            for i, row in enumerate(y_labels):\n                for j, col in enumerate(x_labels):\n                    if not (grid.mask is not None and mask_ordered.iloc[i, j]):\n                        cell_value = z_data[i, j]\n                        if not np.isnan(cell_value):\n                            max_abs_val = np.nanmax(np.abs(z_data))\n                            text_color = (\n                                \"white\"\n                                if abs(cell_value) &gt; max_abs_val / 2\n                                else \"black\"\n                            )\n\n                            annotations.append(\n                                dict(\n                                    x=j,\n                                    y=i,\n                                    text=format(cell_value, self.fmt),\n                                    showarrow=False,\n                                    font=dict(color=text_color, size=10),\n                                    xref=f\"x{heatmap_col}\" if heatmap_col &gt; 1 else \"x\",\n                                    yref=f\"y{heatmap_row}\" if heatmap_row &gt; 1 else \"y\",\n                                )\n                            )\n\n            fig.update_layout(annotations=annotations)\n\n        # Update layout\n        fig.update_layout(\n            title=self.title if self.title else None,\n            width=self.figsize[0],\n            height=self.figsize[1],\n            showlegend=False,\n        )\n\n        # Configure axes for each subplot\n        for row in range(1, n_rows + 1):\n            for col in range(1, n_cols + 1):\n                # Generate xaxis and yaxis references\n                xaxis_ref = (\n                    f\"xaxis{col + (row - 1) * n_cols}\"\n                    if col + (row - 1) * n_cols &gt; 1\n                    else \"xaxis\"\n                )\n                yaxis_ref = (\n                    f\"yaxis{col + (row - 1) * n_cols}\"\n                    if col + (row - 1) * n_cols &gt; 1\n                    else \"yaxis\"\n                )\n\n                # Default settings for all subplots\n                fig.update_layout(\n                    {\n                        xaxis_ref: dict(\n                            showticklabels=False,\n                            showgrid=False,\n                            zeroline=False,\n                            showline=False,\n                            ticks=\"\",\n                        ),\n                        yaxis_ref: dict(\n                            showticklabels=False,\n                            showgrid=False,\n                            zeroline=False,\n                            showline=False,\n                            ticks=\"\",\n                        ),\n                    }\n                )\n\n        # Special configuration for heatmap\n        heatmap_xaxis = (\n            f\"xaxis{heatmap_col + (heatmap_row - 1) * n_cols}\"\n            if heatmap_col + (heatmap_row - 1) * n_cols &gt; 1\n            else \"xaxis\"\n        )\n        heatmap_yaxis = (\n            f\"yaxis{heatmap_col + (heatmap_row - 1) * n_cols}\"\n            if heatmap_col + (heatmap_row - 1) * n_cols &gt; 1\n            else \"yaxis\"\n        )\n\n        fig.update_layout(\n            {\n                heatmap_xaxis: dict(\n                    showticklabels=show_heatmap_x_labels,\n                    tickmode=\"array\" if show_heatmap_x_labels else \"linear\",\n                    tickvals=list(range(len(x_labels)))\n                    if show_heatmap_x_labels\n                    else [],\n                    ticktext=x_labels if show_heatmap_x_labels else [],\n                    tickangle=45 if show_heatmap_x_labels else 0,\n                    side=\"bottom\",\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not show_heatmap_x_labels else \"outside\",\n                ),\n                heatmap_yaxis: dict(\n                    showticklabels=show_heatmap_y_labels,\n                    tickmode=\"array\" if show_heatmap_y_labels else \"linear\",\n                    tickvals=list(range(len(y_labels)))\n                    if show_heatmap_y_labels\n                    else [],\n                    ticktext=y_labels if show_heatmap_y_labels else [],\n                    autorange=\"reversed\",  # Reverse to match typical heatmap orientation\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not show_heatmap_y_labels else \"outside\",\n                    side=\"right\",\n                ),\n            }\n        )\n\n        # Configure dendrogram axes ranges\n        if self.col_cluster and col_dendro_data:\n            col_dend_xaxis = f\"xaxis{heatmap_col}\" if heatmap_col &gt; 1 else \"xaxis\"\n            col_dend_yaxis = f\"yaxis{heatmap_col}\" if heatmap_col &gt; 1 else \"yaxis\"\n\n            # Set ranges for column dendrogram\n            fig.update_layout(\n                {\n                    col_dend_xaxis: dict(\n                        range=[0, len(ordered_data.columns) * 10 + 5],\n                        showticklabels=self.show_dendrogram_labels,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                    ),\n                    col_dend_yaxis: dict(\n                        range=[\n                            0,\n                            max(np.array(col_dendro_data[\"dcoord\"]).flatten()) * 1.00,\n                        ],\n                        showticklabels=self.show_dendrogram_labels,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                    ),\n                }\n            )\n\n        if self.row_cluster and row_dendro_data:\n            row_dend_xaxis = (\n                f\"xaxis{1 + (heatmap_row - 1) * n_cols}\"\n                if 1 + (heatmap_row - 1) * n_cols &gt; 1\n                else \"xaxis\"\n            )\n\n            row_dend_yaxis = (\n                f\"yaxis{1 + (heatmap_row - 1) * n_cols}\"\n                if 1 + (heatmap_row - 1) * n_cols &gt; 1\n                else \"yaxis\"\n            )\n\n            # Set ranges for row dendrogram\n            fig.update_layout(\n                {\n                    row_dend_xaxis: dict(\n                        range=[\n                            0,\n                            max(np.array(row_dendro_data[\"dcoord\"]).flatten()) * 1.01,\n                        ],\n                        showticklabels=self.show_dendrogram_labels,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                    ),\n                    row_dend_yaxis: dict(\n                        range=[0, len(ordered_data.index) * 10],\n                        showticklabels=self.show_dendrogram_labels,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                    ),\n                }\n            )\n\n        fig.update_layout(title_x=0.5)  # Automatically adjust x-axis margins\n\n        self.fig = fig\n\n        # Adjust layout if upper dendrogram is hidden\n        self._adjust_layout_for_hidden_upper()\n\n    def _adjust_layout_for_hidden_upper(self) -&gt; None:\n        \"\"\"Adjust the layout when the upper dendrogram is hidden to move components up.\"\"\"\n        if not self.hide_upper:\n            return\n\n        # Get the current layout\n        layout = self.fig.layout\n\n        # Find which subplot contains the heatmap and row dendrogram\n        heatmap_subplot = None\n        row_dendro_subplot = None\n\n        # Iterate through the traces to find the heatmap and row dendrogram\n        for i, trace in enumerate(self.fig.data):\n            if hasattr(trace, \"type\") and trace.type == \"heatmap\":\n                # This is the heatmap\n                heatmap_subplot = (\n                    getattr(trace, \"xaxis\", \"x\"),\n                    getattr(trace, \"yaxis\", \"y\"),\n                )\n            elif hasattr(trace, \"type\") and trace.type == \"scatter\":\n                # This might be the row dendrogram - check if it's on the side\n                x_axis = getattr(trace, \"xaxis\", \"x\")\n                y_axis = getattr(trace, \"yaxis\", \"y\")\n                if x_axis != heatmap_subplot[0] if heatmap_subplot else True:\n                    row_dendro_subplot = x_axis, y_axis\n\n        # Adjust the domains to move everything up\n        # The key is to expand the y-domain of the bottom subplots to fill the top space\n\n        updates = {}\n\n        # Move heatmap up by expanding its y-domain\n        if heatmap_subplot:\n            heatmap_xaxis = (\n                heatmap_subplot[0].replace(\"x\", \"xaxis\")\n                if \"x\" in heatmap_subplot[0]\n                else \"xaxis\"\n            )\n            heatmap_yaxis = (\n                heatmap_subplot[1].replace(\"y\", \"yaxis\")\n                if \"y\" in heatmap_subplot[1]\n                else \"yaxis\"\n            )\n\n            # Get current domain or use defaults\n            current_layout = getattr(layout, heatmap_yaxis, {})\n            current_domain = getattr(current_layout, \"domain\", [0.0, 0.8])\n\n            # Expand to fill the full height\n            updates[f\"{heatmap_yaxis}.domain\"] = [0.0, 1.0]\n\n        # Move row dendrogram up by expanding its y-domain\n        if row_dendro_subplot and not self.hide_side:\n            row_dendro_xaxis = (\n                row_dendro_subplot[0].replace(\"x\", \"xaxis\")\n                if \"x\" in row_dendro_subplot[0]\n                else \"xaxis\"\n            )\n            row_dendro_yaxis = (\n                row_dendro_subplot[1].replace(\"y\", \"yaxis\")\n                if \"y\" in row_dendro_subplot[1]\n                else \"yaxis\"\n            )\n\n            # Expand to fill the full height\n            updates[f\"{row_dendro_yaxis}.domain\"] = [0.0, 1.0]\n\n        # Apply the updates\n        if updates:\n            self.fig.update_layout(updates)\n\n    def _set_labels(self):\n        \"\"\"Set the labels for the clustermap.\"\"\"\n        if not self.labels:\n            if isinstance(self.dtm, DTM):\n                self.labels = self.dtm.labels\n            elif isinstance(self.dtm, pd.DataFrame):\n                self.labels = self.dtm.columns.values.tolist()[1:]\n            else:\n                self.labels = [f\"Doc{i + 1}\" for i, _ in enumerate(self.dtm)]\n\n    @validate_call(config=model_config)\n    def save(self, path: Path | str, **kwargs: Any) -&gt; None:\n        \"\"\"Save a static image of the figure to disk.\n\n        Alias of `write_image()`\n\n        Args:\n            path: The file path to save the image.\n            **kwargs (Any): Additional arguments to pass to the write_image method.\n        \"\"\"\n        self.write_image(path, **kwargs)\n\n    def show(self):\n        \"\"\"Show the clustermap.\"\"\"\n        config = dict(\n            displaylogo=False,\n            modeBarButtonsToRemove=[\"toggleSpikelines\"],\n            scrollZoom=True,\n        )\n        self.fig.show(config=config)\n\n    def to_html(self, include_sync=False, **kwargs: Any) -&gt; str:\n        \"\"\"Create an HTML representation of the figure with optional synchronization.\n\n        Wrapper from the Plotly Figure to_html method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n\n        Args:\n            include_sync (bool): Whether to include the synchronization script.\n            **kwargs (Any): Additional keyword arguments for the to_html method.\n\n        Returns:\n            str: The HTML representation of the figure.\n        \"\"\"\n        html = self.fig.to_html(**kwargs)\n\n        if include_sync:\n            # Insert the script before the closing &lt;/body&gt; tag\n            html = html.replace(\"&lt;/body&gt;\", f\"{SYNC_SCRIPT}&lt;/body&gt;\")\n\n        return html\n\n    def to_image(self, **kwargs: Any) -&gt; bytes:\n        \"\"\"Create a static image of the figure.\n\n        Args:\n            **kwargs (Any): Additional keyword arguments for the to_image method.\n\n        Returns:\n            bytes: The image in bytes.\n\n        Wrapper from the Plotly Figure to_html method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n        \"\"\"\n        return self.fig.to_image(**kwargs)\n\n    @validate_call(config=model_config)\n    def write_html(self, path: Path | str, **kwargs: Any) -&gt; None:\n        \"\"\"Save an HTML representation of the figure to disk.\n\n        Args:\n            path (Path | str): The file path to save the HTML.\n            **kwargs (Any): Additional arguments to pass to the write_html method.\n\n        Wrapper from the Plotly Figure write_html method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n        \"\"\"\n        return self.fig.write_html(path, **kwargs)\n\n    @validate_call(config=model_config)\n    def write_image(self, path: Path | str, **kwargs: Any) -&gt; None:\n        \"\"\"Save a static image of the figure to disk.\n\n        Args:\n            path (Path | str): The file path to save the image.\n            **kwargs (Any): Additional arguments to pass to the write_image method.\n\n        Wrapper from the Plotly Figure write_image method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n        \"\"\"\n        return self.fig.write_image(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.annot","title":"<code>annot: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to annotate the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.cbar_kws","title":"<code>cbar_kws: Optional[dict] = None</code>  <code>pydantic-field</code>","text":"<p>The cbar kwargs for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.cbar_pos","title":"<code>cbar_pos: Optional[tuple[str]] = (0.02, 0.32, 0.03, 0.2)</code>  <code>pydantic-field</code>","text":"<p>The cbar position for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.center","title":"<code>center: Optional[float | int] = 0</code>  <code>pydantic-field</code>","text":"<p>The center for the clustermap. Default could be None.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.cmap","title":"<code>cmap: Optional[str] = 'viridis'</code>  <code>pydantic-field</code>","text":"<p>The cmap for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.col_cluster","title":"<code>col_cluster: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to cluster the columns.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.col_colors","title":"<code>col_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = None</code>  <code>pydantic-field</code>","text":"<p>The column colors.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.col_linkage","title":"<code>col_linkage: Optional[np.ndarray] = None</code>  <code>pydantic-field</code>","text":"<p>Precomputed linkage matrix for the columns. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.colorbar","title":"<code>colorbar: Optional[dict[str, Any]] = dict(x=0.11, y=0.5, xref='container', yref='container', len=0.6)</code>  <code>pydantic-field</code>","text":"<p>The colorbar position for the clustermap. This is a more generic version than <code>cbar_pos</code> and can be used to set the position of the colorbar in a more flexible way.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.colors_ratio","title":"<code>colors_ratio: Optional[float] = 0.03</code>  <code>pydantic-field</code>","text":"<p>The colors ratio for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.dendrogram_ratio","title":"<code>dendrogram_ratio: Optional[float | tuple[float, float]] = (0.8, 0.2)</code>  <code>pydantic-field</code>","text":"<p>The dendrogram ratio for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.dtm","title":"<code>dtm: Optional[ArrayLike | DTM | pd.DataFrame]</code>  <code>pydantic-field</code>","text":"<p>The document-term matrix.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.figsize","title":"<code>figsize: Optional[tuple[int, int]] = (700, 700)</code>  <code>pydantic-field</code>","text":"<p>The figure size for the clustermap in pixels.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.fmt","title":"<code>fmt: Optional[str] = '.2g'</code>  <code>pydantic-field</code>","text":"<p>The format for the annotations in the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.hide_side","title":"<code>hide_side: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Hide the side dendrogram.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.hide_upper","title":"<code>hide_upper: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Hide the upper dendrogram.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.kwargs","title":"<code>kwargs: Any = {}</code>  <code>pydantic-field</code>","text":"<p>Additional keyword arguments for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.labels","title":"<code>labels: Optional[list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>The labels for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.linewidths","title":"<code>linewidths: Optional[float] = 0.75</code>  <code>pydantic-field</code>","text":"<p>The linewidths for the dendrograms. Default could be 0.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.mask","title":"<code>mask: Optional[np.ndarray | pd.DataFrame] = None</code>  <code>pydantic-field</code>","text":"<p>The mask for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.method","title":"<code>method: Optional[str] = 'average'</code>  <code>pydantic-field</code>","text":"<p>The method to use for the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.metric","title":"<code>metric: Optional[str] = 'euclidean'</code>  <code>pydantic-field</code>","text":"<p>The metric to use for the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.pivot_kws","title":"<code>pivot_kws: Optional[dict[str, str]] = None</code>  <code>pydantic-field</code>","text":"<p>The pivot kwargs for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.row_cluster","title":"<code>row_cluster: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to cluster the rows.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.row_colors","title":"<code>row_colors: Optional[list | pd.DataFrame | pd.Series | str | ListedColormap] = None</code>  <code>pydantic-field</code>","text":"<p>The row colors.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.row_linkage","title":"<code>row_linkage: Optional[np.ndarray] = None</code>  <code>pydantic-field</code>","text":"<p>Precomputed linkage matrix for the rows. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage for specific formats.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.show_dendrogram_labels","title":"<code>show_dendrogram_labels: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to show the labels on the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.show_heatmap_labels","title":"<code>show_heatmap_labels: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to show the labels on the heatmap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.standard_scale","title":"<code>standard_scale: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The standard scale for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title for the dendrogram.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.tree_kws","title":"<code>tree_kws: Optional[dict] = None</code>  <code>pydantic-field</code>","text":"<p>The tree kwargs for the dendrograms.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.z_score","title":"<code>z_score: Optional[int] = 1</code>  <code>pydantic-field</code>","text":"<p>The z-score for the clustermap.</p>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize the PlotlyClustermap instance.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def __init__(\n    self,\n    **data,\n) -&gt; None:\n    \"\"\"Initialize the PlotlyClustermap instance.\"\"\"\n    super().__init__(**data)\n\n    # Set the labels\n    self._set_labels()\n\n    # Get the matrix based on the data type\n    matrix = _get_matrix(self.dtm)\n\n    # Extract our custom parameters from kwargs to prevent them being passed to plotly components\n    filtered_kwargs = self.kwargs.copy()\n    filtered_kwargs.pop(\n        \"show_dendrogram_labels\", None\n    )  # This is already a function parameter\n    filtered_kwargs.pop(\n        \"show_heatmap_labels\", None\n    )  # This is already a function parameter\n    filtered_kwargs.pop(\n        \"title\", None\n    )  # Title should go to layout, not heatmap trace\n\n    # Determine whether to show heatmap labels\n    if self.show_heatmap_labels is None:\n        # Auto-mode: hide y-axis (left) labels when row dendrogram is present,\n        # but always show x-axis (bottom) labels for readability\n        show_heatmap_x_labels = (\n            True  # Always show bottom labels unless explicitly disabled\n        )\n        show_heatmap_y_labels = (\n            not self.row_cluster\n        )  # Hide left labels only if row dendrogram present\n    else:\n        # Manual mode: use the same setting for both axes\n        show_heatmap_x_labels = self.show_heatmap_labels\n        show_heatmap_y_labels = self.show_heatmap_labels\n\n    # Initialize cluster grid\n    grid = PlotlyClusterGrid(\n        data=matrix,\n        z_score=self.z_score,\n        standard_scale=self.standard_scale,\n        mask=self.mask,\n        figsize=self.figsize,\n        dendrogram_ratio=self.dendrogram_ratio,\n    )\n\n    # Handle dendrogram ratios\n    if isinstance(self.dendrogram_ratio, (list, tuple)):\n        row_dendrogram_ratio, col_dendrogram_ratio = self.dendrogram_ratio\n    else:\n        row_dendrogram_ratio = col_dendrogram_ratio = self.dendrogram_ratio\n\n    # Handle tree styling\n    if self.tree_kws is None:\n        self.tree_kws = {}\n    tree_color = self.tree_kws.get(\"color\", \"rgb(50,50,50)\")\n    tree_width = self.tree_kws.get(\"linewidth\", 1.0)\n\n    # Calculate clustering\n    data_array = grid.data2d.values\n\n    # Row clustering\n    row_linkage = data.get(\"row_linkage\", None)\n    if self.row_cluster:\n        if row_linkage is None:\n            row_linkage = grid._calculate_linkage(\n                data_array, self.method, self.metric\n            )\n        row_dendro_traces, row_dendro_data = _create_dendrogram_traces(\n            row_linkage,\n            labels=[str(x) for x in grid.data2d.index]\n            if self.show_dendrogram_labels\n            else None,\n            orientation=\"left\",\n            color=tree_color,\n            line_width=tree_width,\n        )\n        row_order = row_dendro_data[\"leaves\"]\n    else:\n        row_order = list(range(len(grid.data2d.index)))\n        row_dendro_traces = []\n        row_dendro_data = None\n\n    # Column clustering\n    col_linkage = data.get(\"col_linkage\", None)\n    if self.col_cluster:\n        if col_linkage is None:\n            col_linkage = grid._calculate_linkage(\n                data_array.T, self.method, self.metric\n            )\n        col_dendro_traces, col_dendro_data = _create_dendrogram_traces(\n            col_linkage,\n            labels=[str(x) for x in grid.data2d.columns]\n            if self.show_dendrogram_labels\n            else None,\n            orientation=\"top\",\n            color=tree_color,\n            line_width=tree_width,\n        )\n        col_order = col_dendro_data[\"leaves\"]\n    else:\n        col_order = list(range(len(grid.data2d.columns)))\n        col_dendro_traces = []\n        col_dendro_data = None\n\n    # Reorder data\n    ordered_data = grid.data2d.iloc[row_order, col_order]\n\n    # Create subplot layout\n    n_rows = 2 if self.col_cluster else 1\n    n_cols = 2 if self.row_cluster else 1\n\n    # Calculate subplot dimensions\n    if self.row_cluster and self.col_cluster:\n        row_heights = [col_dendrogram_ratio, 1 - col_dendrogram_ratio]\n        col_widths = [1 - row_dendrogram_ratio, row_dendrogram_ratio]\n        # subplot_titles = [\"\", \"Column Dendrogram\", \"Row Dendrogram\", \"Heatmap\"]\n    elif self.col_cluster:\n        row_heights = [col_dendrogram_ratio, 1 - col_dendrogram_ratio]\n        col_widths = [1.0]\n        # subplot_titles = [\"Column Dendrogram\", \"Heatmap\"]\n    elif self.row_cluster:\n        row_heights = [1.0]\n        col_widths = [1 - row_dendrogram_ratio, row_dendrogram_ratio]\n        # subplot_titles = [\"Heatmap\", \"Row Dendrogram\"]\n    else:\n        row_heights = [1.0]\n        col_widths = [1.0]\n        # subplot_titles = [\"Heatmap\"]\n\n    # Create subplots\n    fig = make_subplots(\n        rows=n_rows,\n        cols=n_cols,\n        row_heights=row_heights,\n        column_widths=col_widths,\n        horizontal_spacing=0,  # Remove padding between dendrograms and heatmap\n        vertical_spacing=0,  # Remove padding between dendrograms and heatmap\n        # subplot_titles=None,  # We'll add custom titles if needed\n    )\n\n    # Determine subplot positions\n    heatmap_row = n_rows\n    heatmap_col = 1 if not self.row_cluster else n_cols\n\n    # Prepare heatmap data\n    z_data = ordered_data.values\n    x_labels = [str(x) for x in ordered_data.columns]\n    y_labels = [str(y) for y in ordered_data.index]\n\n    # Apply mask if provided\n    if grid.mask is not None:\n        mask_ordered = grid.mask.iloc[row_order, col_order]\n        z_data = np.where(mask_ordered.values, np.nan, z_data)\n\n    # Add heatmap\n    heatmap_trace = go.Heatmap(\n        z=z_data,\n        x=x_labels,\n        y=y_labels,\n        colorscale=self.cmap,\n        zmid=self.center,\n        showscale=True,\n        colorbar=self.colorbar,\n        name=\"\",  # Remove Trace 0 from hover\n        **filtered_kwargs,\n    )\n\n    fig.add_trace(heatmap_trace, row=heatmap_row, col=heatmap_col)\n\n    # Add column dendrogram\n    if not self.hide_upper:\n        if self.col_cluster and col_dendro_traces:\n            for trace in col_dendro_traces:\n                fig.add_trace(trace, row=1, col=heatmap_col)\n\n    # Add row dendrogram\n    if not self.hide_side:\n        if self.row_cluster and row_dendro_traces:\n            for trace in row_dendro_traces:\n                fig.add_trace(trace, row=heatmap_row, col=1)\n\n    # Reverse the traces for the row dendrogram to match the Seaborn dendrogram\n    fig.update_yaxes(row=heatmap_row, col=1, autorange=\"reversed\")\n\n    # Add annotations if requested\n    if self.annot:\n        annotations = []\n        for i, row in enumerate(y_labels):\n            for j, col in enumerate(x_labels):\n                if not (grid.mask is not None and mask_ordered.iloc[i, j]):\n                    cell_value = z_data[i, j]\n                    if not np.isnan(cell_value):\n                        max_abs_val = np.nanmax(np.abs(z_data))\n                        text_color = (\n                            \"white\"\n                            if abs(cell_value) &gt; max_abs_val / 2\n                            else \"black\"\n                        )\n\n                        annotations.append(\n                            dict(\n                                x=j,\n                                y=i,\n                                text=format(cell_value, self.fmt),\n                                showarrow=False,\n                                font=dict(color=text_color, size=10),\n                                xref=f\"x{heatmap_col}\" if heatmap_col &gt; 1 else \"x\",\n                                yref=f\"y{heatmap_row}\" if heatmap_row &gt; 1 else \"y\",\n                            )\n                        )\n\n        fig.update_layout(annotations=annotations)\n\n    # Update layout\n    fig.update_layout(\n        title=self.title if self.title else None,\n        width=self.figsize[0],\n        height=self.figsize[1],\n        showlegend=False,\n    )\n\n    # Configure axes for each subplot\n    for row in range(1, n_rows + 1):\n        for col in range(1, n_cols + 1):\n            # Generate xaxis and yaxis references\n            xaxis_ref = (\n                f\"xaxis{col + (row - 1) * n_cols}\"\n                if col + (row - 1) * n_cols &gt; 1\n                else \"xaxis\"\n            )\n            yaxis_ref = (\n                f\"yaxis{col + (row - 1) * n_cols}\"\n                if col + (row - 1) * n_cols &gt; 1\n                else \"yaxis\"\n            )\n\n            # Default settings for all subplots\n            fig.update_layout(\n                {\n                    xaxis_ref: dict(\n                        showticklabels=False,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\",\n                    ),\n                    yaxis_ref: dict(\n                        showticklabels=False,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\",\n                    ),\n                }\n            )\n\n    # Special configuration for heatmap\n    heatmap_xaxis = (\n        f\"xaxis{heatmap_col + (heatmap_row - 1) * n_cols}\"\n        if heatmap_col + (heatmap_row - 1) * n_cols &gt; 1\n        else \"xaxis\"\n    )\n    heatmap_yaxis = (\n        f\"yaxis{heatmap_col + (heatmap_row - 1) * n_cols}\"\n        if heatmap_col + (heatmap_row - 1) * n_cols &gt; 1\n        else \"yaxis\"\n    )\n\n    fig.update_layout(\n        {\n            heatmap_xaxis: dict(\n                showticklabels=show_heatmap_x_labels,\n                tickmode=\"array\" if show_heatmap_x_labels else \"linear\",\n                tickvals=list(range(len(x_labels)))\n                if show_heatmap_x_labels\n                else [],\n                ticktext=x_labels if show_heatmap_x_labels else [],\n                tickangle=45 if show_heatmap_x_labels else 0,\n                side=\"bottom\",\n                showgrid=False,\n                zeroline=False,\n                showline=False,\n                ticks=\"\" if not show_heatmap_x_labels else \"outside\",\n            ),\n            heatmap_yaxis: dict(\n                showticklabels=show_heatmap_y_labels,\n                tickmode=\"array\" if show_heatmap_y_labels else \"linear\",\n                tickvals=list(range(len(y_labels)))\n                if show_heatmap_y_labels\n                else [],\n                ticktext=y_labels if show_heatmap_y_labels else [],\n                autorange=\"reversed\",  # Reverse to match typical heatmap orientation\n                showgrid=False,\n                zeroline=False,\n                showline=False,\n                ticks=\"\" if not show_heatmap_y_labels else \"outside\",\n                side=\"right\",\n            ),\n        }\n    )\n\n    # Configure dendrogram axes ranges\n    if self.col_cluster and col_dendro_data:\n        col_dend_xaxis = f\"xaxis{heatmap_col}\" if heatmap_col &gt; 1 else \"xaxis\"\n        col_dend_yaxis = f\"yaxis{heatmap_col}\" if heatmap_col &gt; 1 else \"yaxis\"\n\n        # Set ranges for column dendrogram\n        fig.update_layout(\n            {\n                col_dend_xaxis: dict(\n                    range=[0, len(ordered_data.columns) * 10 + 5],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n                col_dend_yaxis: dict(\n                    range=[\n                        0,\n                        max(np.array(col_dendro_data[\"dcoord\"]).flatten()) * 1.00,\n                    ],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n            }\n        )\n\n    if self.row_cluster and row_dendro_data:\n        row_dend_xaxis = (\n            f\"xaxis{1 + (heatmap_row - 1) * n_cols}\"\n            if 1 + (heatmap_row - 1) * n_cols &gt; 1\n            else \"xaxis\"\n        )\n\n        row_dend_yaxis = (\n            f\"yaxis{1 + (heatmap_row - 1) * n_cols}\"\n            if 1 + (heatmap_row - 1) * n_cols &gt; 1\n            else \"yaxis\"\n        )\n\n        # Set ranges for row dendrogram\n        fig.update_layout(\n            {\n                row_dend_xaxis: dict(\n                    range=[\n                        0,\n                        max(np.array(row_dendro_data[\"dcoord\"]).flatten()) * 1.01,\n                    ],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n                row_dend_yaxis: dict(\n                    range=[0, len(ordered_data.index) * 10],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n            }\n        )\n\n    fig.update_layout(title_x=0.5)  # Automatically adjust x-axis margins\n\n    self.fig = fig\n\n    # Adjust layout if upper dendrogram is hidden\n    self._adjust_layout_for_hidden_upper()\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.save","title":"<code>save(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save a static image of the figure to disk.</p> <p>Alias of <code>write_image()</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the image.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the write_image method.</p> <code>{}</code> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save a static image of the figure to disk.\n\n    Alias of `write_image()`\n\n    Args:\n        path: The file path to save the image.\n        **kwargs (Any): Additional arguments to pass to the write_image method.\n    \"\"\"\n    self.write_image(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.show","title":"<code>show()</code>","text":"<p>Show the clustermap.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def show(self):\n    \"\"\"Show the clustermap.\"\"\"\n    config = dict(\n        displaylogo=False,\n        modeBarButtonsToRemove=[\"toggleSpikelines\"],\n        scrollZoom=True,\n    )\n    self.fig.show(config=config)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.to_html","title":"<code>to_html(include_sync=False, **kwargs: Any) -&gt; str</code>","text":"<p>Create an HTML representation of the figure with optional synchronization.</p> <p>Wrapper from the Plotly Figure to_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> <p>Parameters:</p> Name Type Description Default <code>include_sync</code> <code>bool</code> <p>Whether to include the synchronization script.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the to_html method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The HTML representation of the figure.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def to_html(self, include_sync=False, **kwargs: Any) -&gt; str:\n    \"\"\"Create an HTML representation of the figure with optional synchronization.\n\n    Wrapper from the Plotly Figure to_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n\n    Args:\n        include_sync (bool): Whether to include the synchronization script.\n        **kwargs (Any): Additional keyword arguments for the to_html method.\n\n    Returns:\n        str: The HTML representation of the figure.\n    \"\"\"\n    html = self.fig.to_html(**kwargs)\n\n    if include_sync:\n        # Insert the script before the closing &lt;/body&gt; tag\n        html = html.replace(\"&lt;/body&gt;\", f\"{SYNC_SCRIPT}&lt;/body&gt;\")\n\n    return html\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.to_image","title":"<code>to_image(**kwargs: Any) -&gt; bytes</code>","text":"<p>Create a static image of the figure.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the to_image method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The image in bytes.</p> <p>Wrapper from the Plotly Figure to_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def to_image(self, **kwargs: Any) -&gt; bytes:\n    \"\"\"Create a static image of the figure.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments for the to_image method.\n\n    Returns:\n        bytes: The image in bytes.\n\n    Wrapper from the Plotly Figure to_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    return self.fig.to_image(**kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.write_html","title":"<code>write_html(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save an HTML representation of the figure to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the HTML.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the write_html method.</p> <code>{}</code> <p>Wrapper from the Plotly Figure write_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@validate_call(config=model_config)\ndef write_html(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save an HTML representation of the figure to disk.\n\n    Args:\n        path (Path | str): The file path to save the HTML.\n        **kwargs (Any): Additional arguments to pass to the write_html method.\n\n    Wrapper from the Plotly Figure write_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    return self.fig.write_html(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.write_image","title":"<code>write_image(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save a static image of the figure to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the image.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the write_image method.</p> <code>{}</code> <p>Wrapper from the Plotly Figure write_image method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@validate_call(config=model_config)\ndef write_image(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save a static image of the figure to disk.\n\n    Args:\n        path (Path | str): The file path to save the image.\n        **kwargs (Any): Additional arguments to pass to the write_image method.\n\n    Wrapper from the Plotly Figure write_image method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    return self.fig.write_image(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize the PlotlyClustermap instance.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def __init__(\n    self,\n    **data,\n) -&gt; None:\n    \"\"\"Initialize the PlotlyClustermap instance.\"\"\"\n    super().__init__(**data)\n\n    # Set the labels\n    self._set_labels()\n\n    # Get the matrix based on the data type\n    matrix = _get_matrix(self.dtm)\n\n    # Extract our custom parameters from kwargs to prevent them being passed to plotly components\n    filtered_kwargs = self.kwargs.copy()\n    filtered_kwargs.pop(\n        \"show_dendrogram_labels\", None\n    )  # This is already a function parameter\n    filtered_kwargs.pop(\n        \"show_heatmap_labels\", None\n    )  # This is already a function parameter\n    filtered_kwargs.pop(\n        \"title\", None\n    )  # Title should go to layout, not heatmap trace\n\n    # Determine whether to show heatmap labels\n    if self.show_heatmap_labels is None:\n        # Auto-mode: hide y-axis (left) labels when row dendrogram is present,\n        # but always show x-axis (bottom) labels for readability\n        show_heatmap_x_labels = (\n            True  # Always show bottom labels unless explicitly disabled\n        )\n        show_heatmap_y_labels = (\n            not self.row_cluster\n        )  # Hide left labels only if row dendrogram present\n    else:\n        # Manual mode: use the same setting for both axes\n        show_heatmap_x_labels = self.show_heatmap_labels\n        show_heatmap_y_labels = self.show_heatmap_labels\n\n    # Initialize cluster grid\n    grid = PlotlyClusterGrid(\n        data=matrix,\n        z_score=self.z_score,\n        standard_scale=self.standard_scale,\n        mask=self.mask,\n        figsize=self.figsize,\n        dendrogram_ratio=self.dendrogram_ratio,\n    )\n\n    # Handle dendrogram ratios\n    if isinstance(self.dendrogram_ratio, (list, tuple)):\n        row_dendrogram_ratio, col_dendrogram_ratio = self.dendrogram_ratio\n    else:\n        row_dendrogram_ratio = col_dendrogram_ratio = self.dendrogram_ratio\n\n    # Handle tree styling\n    if self.tree_kws is None:\n        self.tree_kws = {}\n    tree_color = self.tree_kws.get(\"color\", \"rgb(50,50,50)\")\n    tree_width = self.tree_kws.get(\"linewidth\", 1.0)\n\n    # Calculate clustering\n    data_array = grid.data2d.values\n\n    # Row clustering\n    row_linkage = data.get(\"row_linkage\", None)\n    if self.row_cluster:\n        if row_linkage is None:\n            row_linkage = grid._calculate_linkage(\n                data_array, self.method, self.metric\n            )\n        row_dendro_traces, row_dendro_data = _create_dendrogram_traces(\n            row_linkage,\n            labels=[str(x) for x in grid.data2d.index]\n            if self.show_dendrogram_labels\n            else None,\n            orientation=\"left\",\n            color=tree_color,\n            line_width=tree_width,\n        )\n        row_order = row_dendro_data[\"leaves\"]\n    else:\n        row_order = list(range(len(grid.data2d.index)))\n        row_dendro_traces = []\n        row_dendro_data = None\n\n    # Column clustering\n    col_linkage = data.get(\"col_linkage\", None)\n    if self.col_cluster:\n        if col_linkage is None:\n            col_linkage = grid._calculate_linkage(\n                data_array.T, self.method, self.metric\n            )\n        col_dendro_traces, col_dendro_data = _create_dendrogram_traces(\n            col_linkage,\n            labels=[str(x) for x in grid.data2d.columns]\n            if self.show_dendrogram_labels\n            else None,\n            orientation=\"top\",\n            color=tree_color,\n            line_width=tree_width,\n        )\n        col_order = col_dendro_data[\"leaves\"]\n    else:\n        col_order = list(range(len(grid.data2d.columns)))\n        col_dendro_traces = []\n        col_dendro_data = None\n\n    # Reorder data\n    ordered_data = grid.data2d.iloc[row_order, col_order]\n\n    # Create subplot layout\n    n_rows = 2 if self.col_cluster else 1\n    n_cols = 2 if self.row_cluster else 1\n\n    # Calculate subplot dimensions\n    if self.row_cluster and self.col_cluster:\n        row_heights = [col_dendrogram_ratio, 1 - col_dendrogram_ratio]\n        col_widths = [1 - row_dendrogram_ratio, row_dendrogram_ratio]\n        # subplot_titles = [\"\", \"Column Dendrogram\", \"Row Dendrogram\", \"Heatmap\"]\n    elif self.col_cluster:\n        row_heights = [col_dendrogram_ratio, 1 - col_dendrogram_ratio]\n        col_widths = [1.0]\n        # subplot_titles = [\"Column Dendrogram\", \"Heatmap\"]\n    elif self.row_cluster:\n        row_heights = [1.0]\n        col_widths = [1 - row_dendrogram_ratio, row_dendrogram_ratio]\n        # subplot_titles = [\"Heatmap\", \"Row Dendrogram\"]\n    else:\n        row_heights = [1.0]\n        col_widths = [1.0]\n        # subplot_titles = [\"Heatmap\"]\n\n    # Create subplots\n    fig = make_subplots(\n        rows=n_rows,\n        cols=n_cols,\n        row_heights=row_heights,\n        column_widths=col_widths,\n        horizontal_spacing=0,  # Remove padding between dendrograms and heatmap\n        vertical_spacing=0,  # Remove padding between dendrograms and heatmap\n        # subplot_titles=None,  # We'll add custom titles if needed\n    )\n\n    # Determine subplot positions\n    heatmap_row = n_rows\n    heatmap_col = 1 if not self.row_cluster else n_cols\n\n    # Prepare heatmap data\n    z_data = ordered_data.values\n    x_labels = [str(x) for x in ordered_data.columns]\n    y_labels = [str(y) for y in ordered_data.index]\n\n    # Apply mask if provided\n    if grid.mask is not None:\n        mask_ordered = grid.mask.iloc[row_order, col_order]\n        z_data = np.where(mask_ordered.values, np.nan, z_data)\n\n    # Add heatmap\n    heatmap_trace = go.Heatmap(\n        z=z_data,\n        x=x_labels,\n        y=y_labels,\n        colorscale=self.cmap,\n        zmid=self.center,\n        showscale=True,\n        colorbar=self.colorbar,\n        name=\"\",  # Remove Trace 0 from hover\n        **filtered_kwargs,\n    )\n\n    fig.add_trace(heatmap_trace, row=heatmap_row, col=heatmap_col)\n\n    # Add column dendrogram\n    if not self.hide_upper:\n        if self.col_cluster and col_dendro_traces:\n            for trace in col_dendro_traces:\n                fig.add_trace(trace, row=1, col=heatmap_col)\n\n    # Add row dendrogram\n    if not self.hide_side:\n        if self.row_cluster and row_dendro_traces:\n            for trace in row_dendro_traces:\n                fig.add_trace(trace, row=heatmap_row, col=1)\n\n    # Reverse the traces for the row dendrogram to match the Seaborn dendrogram\n    fig.update_yaxes(row=heatmap_row, col=1, autorange=\"reversed\")\n\n    # Add annotations if requested\n    if self.annot:\n        annotations = []\n        for i, row in enumerate(y_labels):\n            for j, col in enumerate(x_labels):\n                if not (grid.mask is not None and mask_ordered.iloc[i, j]):\n                    cell_value = z_data[i, j]\n                    if not np.isnan(cell_value):\n                        max_abs_val = np.nanmax(np.abs(z_data))\n                        text_color = (\n                            \"white\"\n                            if abs(cell_value) &gt; max_abs_val / 2\n                            else \"black\"\n                        )\n\n                        annotations.append(\n                            dict(\n                                x=j,\n                                y=i,\n                                text=format(cell_value, self.fmt),\n                                showarrow=False,\n                                font=dict(color=text_color, size=10),\n                                xref=f\"x{heatmap_col}\" if heatmap_col &gt; 1 else \"x\",\n                                yref=f\"y{heatmap_row}\" if heatmap_row &gt; 1 else \"y\",\n                            )\n                        )\n\n        fig.update_layout(annotations=annotations)\n\n    # Update layout\n    fig.update_layout(\n        title=self.title if self.title else None,\n        width=self.figsize[0],\n        height=self.figsize[1],\n        showlegend=False,\n    )\n\n    # Configure axes for each subplot\n    for row in range(1, n_rows + 1):\n        for col in range(1, n_cols + 1):\n            # Generate xaxis and yaxis references\n            xaxis_ref = (\n                f\"xaxis{col + (row - 1) * n_cols}\"\n                if col + (row - 1) * n_cols &gt; 1\n                else \"xaxis\"\n            )\n            yaxis_ref = (\n                f\"yaxis{col + (row - 1) * n_cols}\"\n                if col + (row - 1) * n_cols &gt; 1\n                else \"yaxis\"\n            )\n\n            # Default settings for all subplots\n            fig.update_layout(\n                {\n                    xaxis_ref: dict(\n                        showticklabels=False,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\",\n                    ),\n                    yaxis_ref: dict(\n                        showticklabels=False,\n                        showgrid=False,\n                        zeroline=False,\n                        showline=False,\n                        ticks=\"\",\n                    ),\n                }\n            )\n\n    # Special configuration for heatmap\n    heatmap_xaxis = (\n        f\"xaxis{heatmap_col + (heatmap_row - 1) * n_cols}\"\n        if heatmap_col + (heatmap_row - 1) * n_cols &gt; 1\n        else \"xaxis\"\n    )\n    heatmap_yaxis = (\n        f\"yaxis{heatmap_col + (heatmap_row - 1) * n_cols}\"\n        if heatmap_col + (heatmap_row - 1) * n_cols &gt; 1\n        else \"yaxis\"\n    )\n\n    fig.update_layout(\n        {\n            heatmap_xaxis: dict(\n                showticklabels=show_heatmap_x_labels,\n                tickmode=\"array\" if show_heatmap_x_labels else \"linear\",\n                tickvals=list(range(len(x_labels)))\n                if show_heatmap_x_labels\n                else [],\n                ticktext=x_labels if show_heatmap_x_labels else [],\n                tickangle=45 if show_heatmap_x_labels else 0,\n                side=\"bottom\",\n                showgrid=False,\n                zeroline=False,\n                showline=False,\n                ticks=\"\" if not show_heatmap_x_labels else \"outside\",\n            ),\n            heatmap_yaxis: dict(\n                showticklabels=show_heatmap_y_labels,\n                tickmode=\"array\" if show_heatmap_y_labels else \"linear\",\n                tickvals=list(range(len(y_labels)))\n                if show_heatmap_y_labels\n                else [],\n                ticktext=y_labels if show_heatmap_y_labels else [],\n                autorange=\"reversed\",  # Reverse to match typical heatmap orientation\n                showgrid=False,\n                zeroline=False,\n                showline=False,\n                ticks=\"\" if not show_heatmap_y_labels else \"outside\",\n                side=\"right\",\n            ),\n        }\n    )\n\n    # Configure dendrogram axes ranges\n    if self.col_cluster and col_dendro_data:\n        col_dend_xaxis = f\"xaxis{heatmap_col}\" if heatmap_col &gt; 1 else \"xaxis\"\n        col_dend_yaxis = f\"yaxis{heatmap_col}\" if heatmap_col &gt; 1 else \"yaxis\"\n\n        # Set ranges for column dendrogram\n        fig.update_layout(\n            {\n                col_dend_xaxis: dict(\n                    range=[0, len(ordered_data.columns) * 10 + 5],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n                col_dend_yaxis: dict(\n                    range=[\n                        0,\n                        max(np.array(col_dendro_data[\"dcoord\"]).flatten()) * 1.00,\n                    ],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n            }\n        )\n\n    if self.row_cluster and row_dendro_data:\n        row_dend_xaxis = (\n            f\"xaxis{1 + (heatmap_row - 1) * n_cols}\"\n            if 1 + (heatmap_row - 1) * n_cols &gt; 1\n            else \"xaxis\"\n        )\n\n        row_dend_yaxis = (\n            f\"yaxis{1 + (heatmap_row - 1) * n_cols}\"\n            if 1 + (heatmap_row - 1) * n_cols &gt; 1\n            else \"yaxis\"\n        )\n\n        # Set ranges for row dendrogram\n        fig.update_layout(\n            {\n                row_dend_xaxis: dict(\n                    range=[\n                        0,\n                        max(np.array(row_dendro_data[\"dcoord\"]).flatten()) * 1.01,\n                    ],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n                row_dend_yaxis: dict(\n                    range=[0, len(ordered_data.index) * 10],\n                    showticklabels=self.show_dendrogram_labels,\n                    showgrid=False,\n                    zeroline=False,\n                    showline=False,\n                    ticks=\"\" if not self.show_dendrogram_labels else \"outside\",\n                ),\n            }\n        )\n\n    fig.update_layout(title_x=0.5)  # Automatically adjust x-axis margins\n\n    self.fig = fig\n\n    # Adjust layout if upper dendrogram is hidden\n    self._adjust_layout_for_hidden_upper()\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.save","title":"<code>save(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save a static image of the figure to disk.</p> <p>Alias of <code>write_image()</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the image.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the write_image method.</p> <code>{}</code> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save a static image of the figure to disk.\n\n    Alias of `write_image()`\n\n    Args:\n        path: The file path to save the image.\n        **kwargs (Any): Additional arguments to pass to the write_image method.\n    \"\"\"\n    self.write_image(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.show","title":"<code>show()</code>","text":"<p>Show the clustermap.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def show(self):\n    \"\"\"Show the clustermap.\"\"\"\n    config = dict(\n        displaylogo=False,\n        modeBarButtonsToRemove=[\"toggleSpikelines\"],\n        scrollZoom=True,\n    )\n    self.fig.show(config=config)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.to_html","title":"<code>to_html(include_sync=False, **kwargs: Any) -&gt; str</code>","text":"<p>Create an HTML representation of the figure with optional synchronization.</p> <p>Wrapper from the Plotly Figure to_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> <p>Parameters:</p> Name Type Description Default <code>include_sync</code> <code>bool</code> <p>Whether to include the synchronization script.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the to_html method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The HTML representation of the figure.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def to_html(self, include_sync=False, **kwargs: Any) -&gt; str:\n    \"\"\"Create an HTML representation of the figure with optional synchronization.\n\n    Wrapper from the Plotly Figure to_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n\n    Args:\n        include_sync (bool): Whether to include the synchronization script.\n        **kwargs (Any): Additional keyword arguments for the to_html method.\n\n    Returns:\n        str: The HTML representation of the figure.\n    \"\"\"\n    html = self.fig.to_html(**kwargs)\n\n    if include_sync:\n        # Insert the script before the closing &lt;/body&gt; tag\n        html = html.replace(\"&lt;/body&gt;\", f\"{SYNC_SCRIPT}&lt;/body&gt;\")\n\n    return html\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.to_image","title":"<code>to_image(**kwargs: Any) -&gt; bytes</code>","text":"<p>Create a static image of the figure.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the to_image method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The image in bytes.</p> <p>Wrapper from the Plotly Figure to_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def to_image(self, **kwargs: Any) -&gt; bytes:\n    \"\"\"Create a static image of the figure.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments for the to_image method.\n\n    Returns:\n        bytes: The image in bytes.\n\n    Wrapper from the Plotly Figure to_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    return self.fig.to_image(**kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.write_html","title":"<code>write_html(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save an HTML representation of the figure to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the HTML.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the write_html method.</p> <code>{}</code> <p>Wrapper from the Plotly Figure write_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@validate_call(config=model_config)\ndef write_html(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save an HTML representation of the figure to disk.\n\n    Args:\n        path (Path | str): The file path to save the HTML.\n        **kwargs (Any): Additional arguments to pass to the write_html method.\n\n    Wrapper from the Plotly Figure write_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    return self.fig.write_html(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap.write_image","title":"<code>write_image(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save a static image of the figure to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the image.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the write_image method.</p> <code>{}</code> <p>Wrapper from the Plotly Figure write_image method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@validate_call(config=model_config)\ndef write_image(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save a static image of the figure to disk.\n\n    Args:\n        path (Path | str): The file path to save the image.\n        **kwargs (Any): Additional arguments to pass to the write_image method.\n\n    Wrapper from the Plotly Figure write_image method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    return self.fig.write_image(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap._adjust_layout_for_hidden_upper","title":"<code>_adjust_layout_for_hidden_upper() -&gt; None</code>","text":"<p>Adjust the layout when the upper dendrogram is hidden to move components up.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _adjust_layout_for_hidden_upper(self) -&gt; None:\n    \"\"\"Adjust the layout when the upper dendrogram is hidden to move components up.\"\"\"\n    if not self.hide_upper:\n        return\n\n    # Get the current layout\n    layout = self.fig.layout\n\n    # Find which subplot contains the heatmap and row dendrogram\n    heatmap_subplot = None\n    row_dendro_subplot = None\n\n    # Iterate through the traces to find the heatmap and row dendrogram\n    for i, trace in enumerate(self.fig.data):\n        if hasattr(trace, \"type\") and trace.type == \"heatmap\":\n            # This is the heatmap\n            heatmap_subplot = (\n                getattr(trace, \"xaxis\", \"x\"),\n                getattr(trace, \"yaxis\", \"y\"),\n            )\n        elif hasattr(trace, \"type\") and trace.type == \"scatter\":\n            # This might be the row dendrogram - check if it's on the side\n            x_axis = getattr(trace, \"xaxis\", \"x\")\n            y_axis = getattr(trace, \"yaxis\", \"y\")\n            if x_axis != heatmap_subplot[0] if heatmap_subplot else True:\n                row_dendro_subplot = x_axis, y_axis\n\n    # Adjust the domains to move everything up\n    # The key is to expand the y-domain of the bottom subplots to fill the top space\n\n    updates = {}\n\n    # Move heatmap up by expanding its y-domain\n    if heatmap_subplot:\n        heatmap_xaxis = (\n            heatmap_subplot[0].replace(\"x\", \"xaxis\")\n            if \"x\" in heatmap_subplot[0]\n            else \"xaxis\"\n        )\n        heatmap_yaxis = (\n            heatmap_subplot[1].replace(\"y\", \"yaxis\")\n            if \"y\" in heatmap_subplot[1]\n            else \"yaxis\"\n        )\n\n        # Get current domain or use defaults\n        current_layout = getattr(layout, heatmap_yaxis, {})\n        current_domain = getattr(current_layout, \"domain\", [0.0, 0.8])\n\n        # Expand to fill the full height\n        updates[f\"{heatmap_yaxis}.domain\"] = [0.0, 1.0]\n\n    # Move row dendrogram up by expanding its y-domain\n    if row_dendro_subplot and not self.hide_side:\n        row_dendro_xaxis = (\n            row_dendro_subplot[0].replace(\"x\", \"xaxis\")\n            if \"x\" in row_dendro_subplot[0]\n            else \"xaxis\"\n        )\n        row_dendro_yaxis = (\n            row_dendro_subplot[1].replace(\"y\", \"yaxis\")\n            if \"y\" in row_dendro_subplot[1]\n            else \"yaxis\"\n        )\n\n        # Expand to fill the full height\n        updates[f\"{row_dendro_yaxis}.domain\"] = [0.0, 1.0]\n\n    # Apply the updates\n    if updates:\n        self.fig.update_layout(updates)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClustermap._set_labels","title":"<code>_set_labels()</code>","text":"<p>Set the labels for the clustermap.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _set_labels(self):\n    \"\"\"Set the labels for the clustermap.\"\"\"\n    if not self.labels:\n        if isinstance(self.dtm, DTM):\n            self.labels = self.dtm.labels\n        elif isinstance(self.dtm, pd.DataFrame):\n            self.labels = self.dtm.columns.values.tolist()[1:]\n        else:\n            self.labels = [f\"Doc{i + 1}\" for i, _ in enumerate(self.dtm)]\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid","title":"<code>PlotlyClusterGrid</code>","text":"<p>Plotly implementation of clustered heatmap with dendrograms.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the cluster grid.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>class PlotlyClusterGrid:\n    \"\"\"Plotly implementation of clustered heatmap with dendrograms.\"\"\"\n\n    def __init__(\n        self,\n        data: pd.DataFrame | np.ndarray,\n        z_score: Optional[int] = None,\n        standard_scale: Optional[int] = None,\n        mask: Optional[np.ndarray | pd.DataFrame] = None,\n        figsize: tuple[int, int] = (800, 600),\n        dendrogram_ratio: float | tuple[float, float] = 0.2,\n    ) -&gt; None:\n        \"\"\"Initialize the cluster grid.\n\n        Args:\n            data (DataFrame or array-like): Rectangular data for clustering\n            z_score (int, optional): Whether to z-score rows (0) or columns (1)\n            standard_scale (int, optional): Whether to standard scale rows (0) or columns (1)\n            mask (bool array or DataFrame, optional): Mask for data visualization\n            figsize (tuple[int, int]): Figure size (width, height)\n            dendrogram_ratio (float | tuple[float, float]): Ratio of dendrogram size to heatmap size\n        \"\"\"\n        # Convert data to DataFrame if needed\n        if isinstance(data, pd.DataFrame):\n            self.data = data.copy()\n        else:\n            self.data = pd.DataFrame(data)\n\n        # Process data\n        self.data2d = self._format_data(z_score, standard_scale)\n        self.mask = self._process_mask(mask)\n\n        # Store configuration\n        self.figsize = figsize\n        self.dendrogram_ratio = dendrogram_ratio\n\n    def _format_data(\n        self, z_score: Optional[int] = None, standard_scale: Optional[int] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Format and normalize data.\n\n        Args:\n            z_score (int, optional): Whether to z-score rows (0) or columns (1)\n            standard_scale (int, optional): Whether to standard scale rows (0) or columns (1)\n\n        Returns:\n            pd.DataFrame: Formatted data\n        \"\"\"\n        data2d = self.data.copy()\n\n        if z_score is not None and standard_scale is not None:\n            raise ValueError(\n                \"Cannot perform both z-scoring and standard-scaling on data\"\n            )\n\n        if z_score is not None:\n            data2d = self._z_score(data2d, z_score)\n        if standard_scale is not None:\n            data2d = self._standard_scale(data2d, standard_scale)\n\n        return data2d\n\n    @staticmethod\n    def _z_score(data2d: pd.DataFrame, axis: int = 1) -&gt; pd.DataFrame:\n        \"\"\"Standardize the mean and variance of the data axis.\n\n        Args:\n            data2d (pd.DataFrame): Data to z-score\n        Returns:\n            pd.DataFrame: Z-scored data\n        \"\"\"\n        if axis == 1:\n            z_scored = data2d\n        else:\n            z_scored = data2d.T\n\n        z_scored = (z_scored - z_scored.mean()) / z_scored.std()\n\n        if axis == 1:\n            return z_scored\n        else:\n            return z_scored.T\n\n    @staticmethod\n    def _standard_scale(data2d: pd.DataFrame, axis: int = 1) -&gt; pd.DataFrame:\n        \"\"\"Divide the data by the difference between the max and min.\n\n        Args:\n            data2d (pd.DataFrame): Data to standard scale\n            axis (int, optional): Axis along which to scale (0 for rows, 1 for columns)\n\n        Returns:\n            pd.DataFrame: Standard scaled data\n        \"\"\"\n        if axis == 1:\n            standardized = data2d\n        else:\n            standardized = data2d.T\n\n        subtract = standardized.min()\n        standardized = (standardized - subtract) / (\n            standardized.max() - standardized.min()\n        )\n\n        if axis == 1:\n            return standardized\n        else:\n            return standardized.T\n\n    def _process_mask(\n        self, mask: Optional[np.ndarray | pd.DataFrame]\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Process mask for data visualization.\n\n        Args:\n            mask (np.ndarray | pd.DataFrame, optional): Mask to apply to the data\n\n        Returns:\n            pd.DataFrame: Processed mask\n        \"\"\"\n        if mask is None:\n            return None\n\n        if isinstance(mask, pd.DataFrame):\n            if not (\n                mask.index.equals(self.data2d.index)\n                and mask.columns.equals(self.data2d.columns)\n            ):\n                raise ValueError(\"Mask must have the same index and columns as data.\")\n        else:\n            mask = np.asarray(mask)\n            if mask.shape != self.data2d.shape:\n                raise ValueError(\"Mask must have the same shape as data.\")\n            mask = pd.DataFrame(\n                mask, index=self.data2d.index, columns=self.data2d.columns, dtype=bool\n            )\n\n        # Add missing data to mask\n        mask = mask | pd.isnull(self.data2d)\n        return mask\n\n    def _calculate_linkage(\n        self, data: np.ndarray, method: str = \"average\", metric: str = \"euclidean\"\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate linkage matrix.\n\n        Args:\n            data (np.ndarray): Data to cluster\n            method (str): Linkage method\n            metric (str): Distance metric\n\n        Returns:\n            np.ndarray: Linkage matrix\n        \"\"\"\n        euclidean_methods = (\"centroid\", \"median\", \"ward\")\n        euclidean = metric == \"euclidean\" and method in euclidean_methods\n        if euclidean or method == \"single\":\n            return fastcluster.linkage_vector(data, method=method, metric=metric)\n        else:\n            return fastcluster.linkage(data, method=method, metric=metric)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid.__init__","title":"<code>__init__(data: pd.DataFrame | np.ndarray, z_score: Optional[int] = None, standard_scale: Optional[int] = None, mask: Optional[np.ndarray | pd.DataFrame] = None, figsize: tuple[int, int] = (800, 600), dendrogram_ratio: float | tuple[float, float] = 0.2) -&gt; None</code>","text":"<p>Initialize the cluster grid.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame or array - like</code> <p>Rectangular data for clustering</p> required <code>z_score</code> <code>int</code> <p>Whether to z-score rows (0) or columns (1)</p> <code>None</code> <code>standard_scale</code> <code>int</code> <p>Whether to standard scale rows (0) or columns (1)</p> <code>None</code> <code>mask</code> <code>bool array or DataFrame</code> <p>Mask for data visualization</p> <code>None</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size (width, height)</p> <code>(800, 600)</code> <code>dendrogram_ratio</code> <code>float | tuple[float, float]</code> <p>Ratio of dendrogram size to heatmap size</p> <code>0.2</code> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame | np.ndarray,\n    z_score: Optional[int] = None,\n    standard_scale: Optional[int] = None,\n    mask: Optional[np.ndarray | pd.DataFrame] = None,\n    figsize: tuple[int, int] = (800, 600),\n    dendrogram_ratio: float | tuple[float, float] = 0.2,\n) -&gt; None:\n    \"\"\"Initialize the cluster grid.\n\n    Args:\n        data (DataFrame or array-like): Rectangular data for clustering\n        z_score (int, optional): Whether to z-score rows (0) or columns (1)\n        standard_scale (int, optional): Whether to standard scale rows (0) or columns (1)\n        mask (bool array or DataFrame, optional): Mask for data visualization\n        figsize (tuple[int, int]): Figure size (width, height)\n        dendrogram_ratio (float | tuple[float, float]): Ratio of dendrogram size to heatmap size\n    \"\"\"\n    # Convert data to DataFrame if needed\n    if isinstance(data, pd.DataFrame):\n        self.data = data.copy()\n    else:\n        self.data = pd.DataFrame(data)\n\n    # Process data\n    self.data2d = self._format_data(z_score, standard_scale)\n    self.mask = self._process_mask(mask)\n\n    # Store configuration\n    self.figsize = figsize\n    self.dendrogram_ratio = dendrogram_ratio\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid.__init__","title":"<code>__init__(data: pd.DataFrame | np.ndarray, z_score: Optional[int] = None, standard_scale: Optional[int] = None, mask: Optional[np.ndarray | pd.DataFrame] = None, figsize: tuple[int, int] = (800, 600), dendrogram_ratio: float | tuple[float, float] = 0.2) -&gt; None</code>","text":"<p>Initialize the cluster grid.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame or array - like</code> <p>Rectangular data for clustering</p> required <code>z_score</code> <code>int</code> <p>Whether to z-score rows (0) or columns (1)</p> <code>None</code> <code>standard_scale</code> <code>int</code> <p>Whether to standard scale rows (0) or columns (1)</p> <code>None</code> <code>mask</code> <code>bool array or DataFrame</code> <p>Mask for data visualization</p> <code>None</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size (width, height)</p> <code>(800, 600)</code> <code>dendrogram_ratio</code> <code>float | tuple[float, float]</code> <p>Ratio of dendrogram size to heatmap size</p> <code>0.2</code> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame | np.ndarray,\n    z_score: Optional[int] = None,\n    standard_scale: Optional[int] = None,\n    mask: Optional[np.ndarray | pd.DataFrame] = None,\n    figsize: tuple[int, int] = (800, 600),\n    dendrogram_ratio: float | tuple[float, float] = 0.2,\n) -&gt; None:\n    \"\"\"Initialize the cluster grid.\n\n    Args:\n        data (DataFrame or array-like): Rectangular data for clustering\n        z_score (int, optional): Whether to z-score rows (0) or columns (1)\n        standard_scale (int, optional): Whether to standard scale rows (0) or columns (1)\n        mask (bool array or DataFrame, optional): Mask for data visualization\n        figsize (tuple[int, int]): Figure size (width, height)\n        dendrogram_ratio (float | tuple[float, float]): Ratio of dendrogram size to heatmap size\n    \"\"\"\n    # Convert data to DataFrame if needed\n    if isinstance(data, pd.DataFrame):\n        self.data = data.copy()\n    else:\n        self.data = pd.DataFrame(data)\n\n    # Process data\n    self.data2d = self._format_data(z_score, standard_scale)\n    self.mask = self._process_mask(mask)\n\n    # Store configuration\n    self.figsize = figsize\n    self.dendrogram_ratio = dendrogram_ratio\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid._format_data","title":"<code>_format_data(z_score: Optional[int] = None, standard_scale: Optional[int] = None) -&gt; pd.DataFrame</code>","text":"<p>Format and normalize data.</p> <p>Parameters:</p> Name Type Description Default <code>z_score</code> <code>int</code> <p>Whether to z-score rows (0) or columns (1)</p> <code>None</code> <code>standard_scale</code> <code>int</code> <p>Whether to standard scale rows (0) or columns (1)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Formatted data</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _format_data(\n    self, z_score: Optional[int] = None, standard_scale: Optional[int] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Format and normalize data.\n\n    Args:\n        z_score (int, optional): Whether to z-score rows (0) or columns (1)\n        standard_scale (int, optional): Whether to standard scale rows (0) or columns (1)\n\n    Returns:\n        pd.DataFrame: Formatted data\n    \"\"\"\n    data2d = self.data.copy()\n\n    if z_score is not None and standard_scale is not None:\n        raise ValueError(\n            \"Cannot perform both z-scoring and standard-scaling on data\"\n        )\n\n    if z_score is not None:\n        data2d = self._z_score(data2d, z_score)\n    if standard_scale is not None:\n        data2d = self._standard_scale(data2d, standard_scale)\n\n    return data2d\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid._z_score","title":"<code>_z_score(data2d: pd.DataFrame, axis: int = 1) -&gt; pd.DataFrame</code>  <code>staticmethod</code>","text":"<p>Standardize the mean and variance of the data axis.</p> <p>Parameters:</p> Name Type Description Default <code>data2d</code> <code>DataFrame</code> <p>Data to z-score</p> required <p>Returns:     pd.DataFrame: Z-scored data</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@staticmethod\ndef _z_score(data2d: pd.DataFrame, axis: int = 1) -&gt; pd.DataFrame:\n    \"\"\"Standardize the mean and variance of the data axis.\n\n    Args:\n        data2d (pd.DataFrame): Data to z-score\n    Returns:\n        pd.DataFrame: Z-scored data\n    \"\"\"\n    if axis == 1:\n        z_scored = data2d\n    else:\n        z_scored = data2d.T\n\n    z_scored = (z_scored - z_scored.mean()) / z_scored.std()\n\n    if axis == 1:\n        return z_scored\n    else:\n        return z_scored.T\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid._standard_scale","title":"<code>_standard_scale(data2d: pd.DataFrame, axis: int = 1) -&gt; pd.DataFrame</code>  <code>staticmethod</code>","text":"<p>Divide the data by the difference between the max and min.</p> <p>Parameters:</p> Name Type Description Default <code>data2d</code> <code>DataFrame</code> <p>Data to standard scale</p> required <code>axis</code> <code>int</code> <p>Axis along which to scale (0 for rows, 1 for columns)</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Standard scaled data</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>@staticmethod\ndef _standard_scale(data2d: pd.DataFrame, axis: int = 1) -&gt; pd.DataFrame:\n    \"\"\"Divide the data by the difference between the max and min.\n\n    Args:\n        data2d (pd.DataFrame): Data to standard scale\n        axis (int, optional): Axis along which to scale (0 for rows, 1 for columns)\n\n    Returns:\n        pd.DataFrame: Standard scaled data\n    \"\"\"\n    if axis == 1:\n        standardized = data2d\n    else:\n        standardized = data2d.T\n\n    subtract = standardized.min()\n    standardized = (standardized - subtract) / (\n        standardized.max() - standardized.min()\n    )\n\n    if axis == 1:\n        return standardized\n    else:\n        return standardized.T\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid._process_mask","title":"<code>_process_mask(mask: Optional[np.ndarray | pd.DataFrame]) -&gt; Optional[pd.DataFrame]</code>","text":"<p>Process mask for data visualization.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray | DataFrame</code> <p>Mask to apply to the data</p> required <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>pd.DataFrame: Processed mask</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _process_mask(\n    self, mask: Optional[np.ndarray | pd.DataFrame]\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Process mask for data visualization.\n\n    Args:\n        mask (np.ndarray | pd.DataFrame, optional): Mask to apply to the data\n\n    Returns:\n        pd.DataFrame: Processed mask\n    \"\"\"\n    if mask is None:\n        return None\n\n    if isinstance(mask, pd.DataFrame):\n        if not (\n            mask.index.equals(self.data2d.index)\n            and mask.columns.equals(self.data2d.columns)\n        ):\n            raise ValueError(\"Mask must have the same index and columns as data.\")\n    else:\n        mask = np.asarray(mask)\n        if mask.shape != self.data2d.shape:\n            raise ValueError(\"Mask must have the same shape as data.\")\n        mask = pd.DataFrame(\n            mask, index=self.data2d.index, columns=self.data2d.columns, dtype=bool\n        )\n\n    # Add missing data to mask\n    mask = mask | pd.isnull(self.data2d)\n    return mask\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap.PlotlyClusterGrid._calculate_linkage","title":"<code>_calculate_linkage(data: np.ndarray, method: str = 'average', metric: str = 'euclidean') -&gt; np.ndarray</code>","text":"<p>Calculate linkage matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Data to cluster</p> required <code>method</code> <code>str</code> <p>Linkage method</p> <code>'average'</code> <code>metric</code> <code>str</code> <p>Distance metric</p> <code>'euclidean'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Linkage matrix</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _calculate_linkage(\n    self, data: np.ndarray, method: str = \"average\", metric: str = \"euclidean\"\n) -&gt; np.ndarray:\n    \"\"\"Calculate linkage matrix.\n\n    Args:\n        data (np.ndarray): Data to cluster\n        method (str): Linkage method\n        metric (str): Distance metric\n\n    Returns:\n        np.ndarray: Linkage matrix\n    \"\"\"\n    euclidean_methods = (\"centroid\", \"median\", \"ward\")\n    euclidean = metric == \"euclidean\" and method in euclidean_methods\n    if euclidean or method == \"single\":\n        return fastcluster.linkage_vector(data, method=method, metric=metric)\n    else:\n        return fastcluster.linkage(data, method=method, metric=metric)\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap._get_matrix","title":"<code>_get_matrix(matrix: ArrayLike | DTM | pd.DataFrame) -&gt; ArrayLike | pd.DataFrame</code>","text":"<p>Get a valid matrix from the input.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ArrayLike | DTM | DataFrame</code> <p>The input matrix, which can be an ArrayLike object, a DTM, or a pandas DataFrame.</p> required <p>Returns:</p> Type Description <code>ArrayLike | DataFrame</code> <p>ArrayLike | pd.DataFrame: A valid matrix that is not a sparse array and has more than one document.</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _get_matrix(matrix: ArrayLike | DTM | pd.DataFrame) -&gt; ArrayLike | pd.DataFrame:\n    \"\"\"Get a valid matrix from the input.\n\n    Args:\n        matrix (ArrayLike | DTM | pd.DataFrame): The input matrix, which can be an ArrayLike object, a DTM, or a pandas DataFrame.\n\n    Returns:\n        ArrayLike | pd.DataFrame: A valid matrix that is not a sparse array and has more than one document.\n    \"\"\"\n    if isinstance(matrix, DTM):\n        matrix = matrix.to_df()\n        matrix.index.name = \"terms\"\n\n    # Ensure that a DataFrame matrix is not a sparse array\n    # Let fastcluster make up for the processing time\n    if isinstance(matrix, pd.DataFrame) and hasattr(matrix, \"sparse\"):\n        matrix = matrix.sparse.to_dense()\n\n    if isinstance(matrix, list) and len(matrix) == 0:\n        raise LexosException(\"The document-term matrix cannot be empty.\")\n\n    if isinstance(matrix, list):\n        first_row = len(matrix[0])\n        first_row = len(matrix)\n    else:\n        first_row = matrix.shape[0]\n    if first_row &lt; 2:\n        raise LexosException(\n            \"The document-term matrix must have more than one document.\"\n        )\n\n    return matrix\n</code></pre>"},{"location":"api/cluster/clustermap/#lexos.cluster.clustermap._create_dendrogram_traces","title":"<code>_create_dendrogram_traces(linkage_matrix: np.ndarray, labels: Optional[list[str]] = None, orientation: str = 'bottom', color: str = 'rgb(50,50,50)', line_width: float = 1.0) -&gt; list[go.Scatter]</code>","text":"<p>Create dendrogram traces from linkage matrix.</p> <p>Parameters:</p> Name Type Description Default <code>linkage_matrix</code> <code>array - like</code> <p>Linkage matrix from scipy.cluster.hierarchy.linkage</p> required <code>labels</code> <code>list</code> <p>Labels for the leaves</p> <code>None</code> <code>orientation</code> <code>str</code> <p>Orientation of dendrogram ('top', 'bottom', 'left', 'right')</p> <code>'bottom'</code> <code>color</code> <code>str</code> <p>Color for dendrogram lines</p> <code>'rgb(50,50,50)'</code> <code>line_width</code> <code>float</code> <p>Width of dendrogram lines</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>traces</code> <code>list</code> <p>List of plotly scatter traces for dendrogram</p> Source code in <code>lexos/cluster/clustermap.py</code> <pre><code>def _create_dendrogram_traces(\n    linkage_matrix: np.ndarray,\n    labels: Optional[list[str]] = None,\n    orientation: str = \"bottom\",\n    color: str = \"rgb(50,50,50)\",\n    line_width: float = 1.0,\n) -&gt; list[go.Scatter]:\n    \"\"\"Create dendrogram traces from linkage matrix.\n\n    Args:\n        linkage_matrix (array-like): Linkage matrix from scipy.cluster.hierarchy.linkage\n        labels (list, optional): Labels for the leaves\n        orientation (str): Orientation of dendrogram ('top', 'bottom', 'left', 'right')\n        color (str): Color for dendrogram lines\n        line_width (float): Width of dendrogram lines\n\n    Returns:\n        traces (list): List of plotly scatter traces for dendrogram\n    \"\"\"\n    dendro_data = hierarchy.dendrogram(\n        linkage_matrix, labels=labels, no_plot=True, color_threshold=-np.inf\n    )\n\n    traces = []\n\n    # Extract coordinates\n    icoord = np.array(dendro_data[\"icoord\"])\n    dcoord = np.array(dendro_data[\"dcoord\"])\n\n    # Access the line objects to identify and remove the baseline (where all y-coords are 0)\n    mask = ~(dcoord == 0).all(axis=1)  # Create boolean mask for non-baseline segments\n    icoord = icoord[mask]\n    dcoord = dcoord[mask]\n\n    # Create line traces for each dendrogram segment\n    for i in range(len(icoord)):\n        x_coords = icoord[i]\n        y_coords = dcoord[i]\n\n        if orientation in [\"top\", \"bottom\"]:\n            # Standard orientation\n            if orientation == \"bottom\":\n                y_coords = -y_coords + max(dcoord.flatten())\n        else:\n            # Swap coordinates for left/right orientation\n            x_coords, y_coords = y_coords, x_coords\n            if orientation == \"left\":\n                x_coords = -x_coords + max(dcoord.flatten())\n                # Shift dendrogram to touch the right edge\n                x_coords = x_coords + (max(x_coords) - min(x_coords)) * 0.03\n\n        # Create scatter trace for this segment\n        trace = go.Scatter(\n            x=x_coords,\n            y=y_coords,\n            mode=\"lines\",\n            line=dict(color=color, width=line_width),\n            showlegend=False,\n            hoverinfo=\"skip\",\n        )\n        traces.append(trace)\n\n    return traces, dendro_data\n</code></pre>"},{"location":"api/cluster/dendrogram/","title":"Dendrogram","text":"<p>The <code>dendrogram</code> module performs hierarchical aggolomerative clustering.</p>"},{"location":"api/cluster/dendrogram/#the-dendrogram-class","title":"The <code>Dendrogram</code> Class","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram","title":"<code>Dendrogram</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dendrogram.</p> <p>Typical usage:</p> <pre><code>from lexos.cluster import Dendrogram\n\ndendrogram = Dendrogram(dtm=dtm)\ndendrogram.show()\n</code></pre> <p>The dtm parameter can be a a DTM instance or a pandas DataFrame with terms as columns and docs as rows (the output of <code>DTM.to_df(transpose=True)</code>). It can also be an equivalent numpy array or list of lists. But in most cases, it will be most convenient to use a DTM instance.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>dtm</code>                 (<code>Optional[ArrayLike | DTM | DataFrame]</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>metric</code>                 (<code>Optional[str]</code>)             </li> <li> <code>method</code>                 (<code>Optional[str]</code>)             </li> <li> <code>truncate_mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>color_threshold</code>                 (<code>Optional[float]</code>)             </li> <li> <code>get_leaves</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>orientation</code>                 (<code>Optional[str]</code>)             </li> <li> <code>count_sort</code>                 (<code>Optional[bool | str]</code>)             </li> <li> <code>distance_sort</code>                 (<code>Optional[bool | str]</code>)             </li> <li> <code>show_leaf_counts</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>no_plot</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>no_labels</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>leaf_rotation</code>                 (<code>Optional[int]</code>)             </li> <li> <code>leaf_font_size</code>                 (<code>Optional[int]</code>)             </li> <li> <code>leaf_label_func</code>                 (<code>Optional[Callable]</code>)             </li> <li> <code>show_contracted</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>link_color_func</code>                 (<code>Optional[Callable]</code>)             </li> <li> <code>ax</code>                 (<code>Optional[Axes]</code>)             </li> <li> <code>above_threshold_color</code>                 (<code>Optional[str]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>figsize</code>                 (<code>Optional[tuple]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> </ul> Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>class Dendrogram(BaseModel):\n    \"\"\"Dendrogram.\n\n    Typical usage:\n\n    ```python\n    from lexos.cluster import Dendrogram\n\n    dendrogram = Dendrogram(dtm=dtm)\n    dendrogram.show()\n    ```\n\n    The dtm parameter can be a a DTM instance or a pandas DataFrame with terms\n    as columns and docs as rows (the output of `DTM.to_df(transpose=True)`).\n    It can also be an equivalent numpy array or list of lists. But in most cases,\n    it will be most convenient to use a DTM instance.\n    \"\"\"\n\n    dtm: Optional[ArrayLike | DTM | pd.DataFrame] = Field(\n        None, description=\"The document-term matrix.\"\n    )\n    labels: Optional[list[str]] = Field(\n        None, description=\"The labels for the dendrogram.\"\n    )\n    metric: Optional[str] = Field(\n        \"euclidean\", description=\"The metric to use for the dendrogram.\"\n    )\n    method: Optional[str] = Field(\n        \"average\", description=\"The method to use for the dendrogram.\"\n    )\n    truncate_mode: Optional[str] = Field(\n        None, description=\"The truncate mode for the dendrogram.\"\n    )\n    color_threshold: Optional[float] = Field(\n        None, description=\"The color threshold for the dendrogram.\"\n    )\n    get_leaves: Optional[bool] = Field(\n        True, description=\"The get leaves for the dendrogram.\"\n    )\n    orientation: Optional[str] = Field(\n        \"top\", description=\"The orientation for the dendrogram.\"\n    )\n    count_sort: Optional[bool | str] = Field(\n        None, description=\"The count sort for the dendrogram.\"\n    )\n    distance_sort: Optional[bool | str] = Field(\n        None, description=\"The distance sort for the dendrogram.\"\n    )\n    show_leaf_counts: Optional[bool] = Field(\n        False, description=\"The show leaf counts for the dendrogram.\"\n    )\n    no_plot: Optional[bool] = Field(\n        False, description=\"The no plot for the dendrogram.\"\n    )\n    no_labels: Optional[bool] = Field(\n        False, description=\"The no labels for the dendrogram.\"\n    )\n    leaf_rotation: Optional[int] = Field(\n        90, description=\"The leaf rotation for the dendrogram.\"\n    )\n    leaf_font_size: Optional[int] = Field(\n        None, description=\"The leaf font size for the dendrogram.\"\n    )\n    leaf_label_func: Optional[Callable] = Field(\n        None, description=\"The leaf label function for the dendrogram.\"\n    )\n    show_contracted: Optional[bool] = Field(\n        False, description=\"The show contracted for the dendrogram.\"\n    )\n    link_color_func: Optional[Callable] = Field(\n        None, description=\"The link color function for the dendrogram.\"\n    )\n    ax: Optional[Axes] = Field(None, description=\"The ax for the dendrogram.\")\n    above_threshold_color: Optional[str] = Field(\n        \"C0\", description=\"The above threshold color for the dendrogram.\"\n    )\n    title: Optional[str] = Field(None, description=\"The title for the dendrogram.\")\n    figsize: Optional[tuple] = Field(\n        (10, 10), description=\"The figsize for the dendrogram.\"\n    )\n    fig: Optional[plt.Figure] = Field(\n        None, description=\"The figure for the dendrogram.\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Initialize the Dendrogram instance.\"\"\"\n        super().__init__(**data)\n\n        # Ensure there is a document-term matrix with more than one document\n        if self.dtm is None:\n            raise LexosException(\"You must provide a document-term matrix.\")\n\n        # Ensure there are labels\n        if not self.labels:\n            if isinstance(self.dtm, DTM):\n                self.labels = self.dtm.labels\n            elif isinstance(self.dtm, pd.DataFrame):\n                self.labels = self.dtm.columns.values.tolist()\n            else:\n                self.labels = [f\"Doc{i + 1}\" for i, _ in enumerate(self.dtm)]\n\n        # Get the matrix based on the data type\n        matrix = self._get_valid_matrix()\n\n        # Check to see if the number of labels matches the number of rows\n        # Make sure we have a matrix length for list input\n        if isinstance(matrix, list):\n            matrix_length = len(matrix)\n        else:\n            matrix_length = matrix.shape[0]\n        if len(self.labels) != matrix_length:\n            raise LexosException(\n                \"The number of labels must match the number of documents.\"\n            )\n\n        # Generate the pairwise distance and linkage matrices\n        X = pdist(matrix, metric=self.metric)\n        Z = sch.linkage(X, self.method)\n\n        # Generate the dendrogram\n        fig, ax = plt.subplots(figsize=self.figsize)\n        if self.title:\n            plt.title(self.title)\n        sch.dendrogram(\n            Z,\n            labels=self.labels,\n            truncate_mode=self.truncate_mode,\n            color_threshold=self.color_threshold,\n            get_leaves=self.get_leaves,\n            orientation=self.orientation,\n            count_sort=self.count_sort,\n            distance_sort=self.distance_sort,\n            show_leaf_counts=self.show_leaf_counts,\n            no_plot=self.no_plot,\n            no_labels=self.no_labels,\n            leaf_rotation=self.leaf_rotation,\n            leaf_font_size=self.leaf_font_size,\n            leaf_label_func=self.leaf_label_func,\n            show_contracted=self.show_contracted,\n            link_color_func=self.link_color_func,\n            ax=self.ax,\n            above_threshold_color=self.above_threshold_color,\n        )\n        self.fig = fig\n        plt.close()\n\n    def _get_valid_matrix(self):\n        \"\"\"Get a valid matrix based on the data type of the dtm.\"\"\"\n        shape_error = \"The document-term matrix must have more than one document.\"\n        type_error = \"The document-term matrix must contain only numeric values.\"\n        label_error = \"The number of labels must match the number of documents.\"\n\n        # DTM input\n        if isinstance(self.dtm, DTM):\n            if len(self.dtm.labels) &lt; 2:\n                raise LexosException(shape_error)\n            df = self.dtm.to_df()\n            df.index.name = \"terms\"\n            matrix = df.T\n\n        # DataFrame input\n        elif isinstance(self.dtm, pd.DataFrame):\n            if self.dtm.shape[0] &lt; 3:\n                raise LexosException(shape_error)\n            if not np.issubdtype(self.dtm.values.dtype, np.number):\n                raise LexosException(type_error)\n            matrix = self.dtm\n\n        # Raw array/list input\n        else:\n            matrix = self.dtm\n\n            # List input\n            if isinstance(matrix, list):\n                if len(matrix) &lt; 2:\n                    raise LexosException(shape_error)\n                if not all(isinstance(x, (int, float)) for row in matrix for x in row):\n                    raise LexosException(type_error)\n\n            # NumPy array input\n            elif isinstance(matrix, np.ndarray):\n                # Consolidated NumPy array checks\n                if matrix.shape[0] &lt; 2:\n                    raise LexosException(shape_error)\n                if not np.issubdtype(matrix.dtype, np.number):\n                    raise LexosException(type_error)\n            # You might want an 'else' here if there are other unsupported types\n            else:\n                raise LexosException(\"Unsupported document-term matrix type.\")\n\n        # Make sure we have a matrix length for list input\n        if isinstance(matrix, list):\n            matrix_length = len(matrix)\n        else:\n            matrix_length = matrix.shape[0]\n\n        # Check labels vs matrix row count\n        if self.labels and len(self.labels) != matrix_length:\n            raise LexosException(label_error)\n\n        return matrix\n\n    @validate_call\n    def save(self, path: Path | str):\n        \"\"\"Save the figure as a file.\n\n        Args:\n            path (Path | str): The path to the file to save.\n        \"\"\"\n        if not path or path == \"\":\n            raise LexosException(\"You must provide a valid path.\")\n        self.fig.savefig(path)\n\n    def show(self):\n        \"\"\"Show the figure if it is hidden.\n\n        This is a helper method. You can also reference the figure\n        using `Dendrogram.fig`. This will generally display in a\n        Jupyter notebook.\n        \"\"\"\n        return self.fig\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.above_threshold_color","title":"<code>above_threshold_color: Optional[str] = 'C0'</code>  <code>pydantic-field</code>","text":"<p>The above threshold color for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.ax","title":"<code>ax: Optional[Axes] = None</code>  <code>pydantic-field</code>","text":"<p>The ax for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.color_threshold","title":"<code>color_threshold: Optional[float] = None</code>  <code>pydantic-field</code>","text":"<p>The color threshold for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.count_sort","title":"<code>count_sort: Optional[bool | str] = None</code>  <code>pydantic-field</code>","text":"<p>The count sort for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.distance_sort","title":"<code>distance_sort: Optional[bool | str] = None</code>  <code>pydantic-field</code>","text":"<p>The distance sort for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.dtm","title":"<code>dtm: Optional[ArrayLike | DTM | pd.DataFrame] = None</code>  <code>pydantic-field</code>","text":"<p>The document-term matrix.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.figsize","title":"<code>figsize: Optional[tuple] = (10, 10)</code>  <code>pydantic-field</code>","text":"<p>The figsize for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.get_leaves","title":"<code>get_leaves: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>The get leaves for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.labels","title":"<code>labels: Optional[list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>The labels for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.leaf_font_size","title":"<code>leaf_font_size: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The leaf font size for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.leaf_label_func","title":"<code>leaf_label_func: Optional[Callable] = None</code>  <code>pydantic-field</code>","text":"<p>The leaf label function for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.leaf_rotation","title":"<code>leaf_rotation: Optional[int] = 90</code>  <code>pydantic-field</code>","text":"<p>The leaf rotation for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.link_color_func","title":"<code>link_color_func: Optional[Callable] = None</code>  <code>pydantic-field</code>","text":"<p>The link color function for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.method","title":"<code>method: Optional[str] = 'average'</code>  <code>pydantic-field</code>","text":"<p>The method to use for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.metric","title":"<code>metric: Optional[str] = 'euclidean'</code>  <code>pydantic-field</code>","text":"<p>The metric to use for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.no_labels","title":"<code>no_labels: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>The no labels for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.no_plot","title":"<code>no_plot: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>The no plot for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.orientation","title":"<code>orientation: Optional[str] = 'top'</code>  <code>pydantic-field</code>","text":"<p>The orientation for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.show_contracted","title":"<code>show_contracted: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>The show contracted for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.show_leaf_counts","title":"<code>show_leaf_counts: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>The show leaf counts for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.truncate_mode","title":"<code>truncate_mode: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The truncate mode for the dendrogram.</p>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize the Dendrogram instance.</p> Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialize the Dendrogram instance.\"\"\"\n    super().__init__(**data)\n\n    # Ensure there is a document-term matrix with more than one document\n    if self.dtm is None:\n        raise LexosException(\"You must provide a document-term matrix.\")\n\n    # Ensure there are labels\n    if not self.labels:\n        if isinstance(self.dtm, DTM):\n            self.labels = self.dtm.labels\n        elif isinstance(self.dtm, pd.DataFrame):\n            self.labels = self.dtm.columns.values.tolist()\n        else:\n            self.labels = [f\"Doc{i + 1}\" for i, _ in enumerate(self.dtm)]\n\n    # Get the matrix based on the data type\n    matrix = self._get_valid_matrix()\n\n    # Check to see if the number of labels matches the number of rows\n    # Make sure we have a matrix length for list input\n    if isinstance(matrix, list):\n        matrix_length = len(matrix)\n    else:\n        matrix_length = matrix.shape[0]\n    if len(self.labels) != matrix_length:\n        raise LexosException(\n            \"The number of labels must match the number of documents.\"\n        )\n\n    # Generate the pairwise distance and linkage matrices\n    X = pdist(matrix, metric=self.metric)\n    Z = sch.linkage(X, self.method)\n\n    # Generate the dendrogram\n    fig, ax = plt.subplots(figsize=self.figsize)\n    if self.title:\n        plt.title(self.title)\n    sch.dendrogram(\n        Z,\n        labels=self.labels,\n        truncate_mode=self.truncate_mode,\n        color_threshold=self.color_threshold,\n        get_leaves=self.get_leaves,\n        orientation=self.orientation,\n        count_sort=self.count_sort,\n        distance_sort=self.distance_sort,\n        show_leaf_counts=self.show_leaf_counts,\n        no_plot=self.no_plot,\n        no_labels=self.no_labels,\n        leaf_rotation=self.leaf_rotation,\n        leaf_font_size=self.leaf_font_size,\n        leaf_label_func=self.leaf_label_func,\n        show_contracted=self.show_contracted,\n        link_color_func=self.link_color_func,\n        ax=self.ax,\n        above_threshold_color=self.above_threshold_color,\n    )\n    self.fig = fig\n    plt.close()\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.save","title":"<code>save(path: Path | str)</code>","text":"<p>Save the figure as a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file to save.</p> required Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>@validate_call\ndef save(self, path: Path | str):\n    \"\"\"Save the figure as a file.\n\n    Args:\n        path (Path | str): The path to the file to save.\n    \"\"\"\n    if not path or path == \"\":\n        raise LexosException(\"You must provide a valid path.\")\n    self.fig.savefig(path)\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.show","title":"<code>show()</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. You can also reference the figure using <code>Dendrogram.fig</code>. This will generally display in a Jupyter notebook.</p> Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>def show(self):\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. You can also reference the figure\n    using `Dendrogram.fig`. This will generally display in a\n    Jupyter notebook.\n    \"\"\"\n    return self.fig\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize the Dendrogram instance.</p> Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialize the Dendrogram instance.\"\"\"\n    super().__init__(**data)\n\n    # Ensure there is a document-term matrix with more than one document\n    if self.dtm is None:\n        raise LexosException(\"You must provide a document-term matrix.\")\n\n    # Ensure there are labels\n    if not self.labels:\n        if isinstance(self.dtm, DTM):\n            self.labels = self.dtm.labels\n        elif isinstance(self.dtm, pd.DataFrame):\n            self.labels = self.dtm.columns.values.tolist()\n        else:\n            self.labels = [f\"Doc{i + 1}\" for i, _ in enumerate(self.dtm)]\n\n    # Get the matrix based on the data type\n    matrix = self._get_valid_matrix()\n\n    # Check to see if the number of labels matches the number of rows\n    # Make sure we have a matrix length for list input\n    if isinstance(matrix, list):\n        matrix_length = len(matrix)\n    else:\n        matrix_length = matrix.shape[0]\n    if len(self.labels) != matrix_length:\n        raise LexosException(\n            \"The number of labels must match the number of documents.\"\n        )\n\n    # Generate the pairwise distance and linkage matrices\n    X = pdist(matrix, metric=self.metric)\n    Z = sch.linkage(X, self.method)\n\n    # Generate the dendrogram\n    fig, ax = plt.subplots(figsize=self.figsize)\n    if self.title:\n        plt.title(self.title)\n    sch.dendrogram(\n        Z,\n        labels=self.labels,\n        truncate_mode=self.truncate_mode,\n        color_threshold=self.color_threshold,\n        get_leaves=self.get_leaves,\n        orientation=self.orientation,\n        count_sort=self.count_sort,\n        distance_sort=self.distance_sort,\n        show_leaf_counts=self.show_leaf_counts,\n        no_plot=self.no_plot,\n        no_labels=self.no_labels,\n        leaf_rotation=self.leaf_rotation,\n        leaf_font_size=self.leaf_font_size,\n        leaf_label_func=self.leaf_label_func,\n        show_contracted=self.show_contracted,\n        link_color_func=self.link_color_func,\n        ax=self.ax,\n        above_threshold_color=self.above_threshold_color,\n    )\n    self.fig = fig\n    plt.close()\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram._get_valid_matrix","title":"<code>_get_valid_matrix()</code>","text":"<p>Get a valid matrix based on the data type of the dtm.</p> Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>def _get_valid_matrix(self):\n    \"\"\"Get a valid matrix based on the data type of the dtm.\"\"\"\n    shape_error = \"The document-term matrix must have more than one document.\"\n    type_error = \"The document-term matrix must contain only numeric values.\"\n    label_error = \"The number of labels must match the number of documents.\"\n\n    # DTM input\n    if isinstance(self.dtm, DTM):\n        if len(self.dtm.labels) &lt; 2:\n            raise LexosException(shape_error)\n        df = self.dtm.to_df()\n        df.index.name = \"terms\"\n        matrix = df.T\n\n    # DataFrame input\n    elif isinstance(self.dtm, pd.DataFrame):\n        if self.dtm.shape[0] &lt; 3:\n            raise LexosException(shape_error)\n        if not np.issubdtype(self.dtm.values.dtype, np.number):\n            raise LexosException(type_error)\n        matrix = self.dtm\n\n    # Raw array/list input\n    else:\n        matrix = self.dtm\n\n        # List input\n        if isinstance(matrix, list):\n            if len(matrix) &lt; 2:\n                raise LexosException(shape_error)\n            if not all(isinstance(x, (int, float)) for row in matrix for x in row):\n                raise LexosException(type_error)\n\n        # NumPy array input\n        elif isinstance(matrix, np.ndarray):\n            # Consolidated NumPy array checks\n            if matrix.shape[0] &lt; 2:\n                raise LexosException(shape_error)\n            if not np.issubdtype(matrix.dtype, np.number):\n                raise LexosException(type_error)\n        # You might want an 'else' here if there are other unsupported types\n        else:\n            raise LexosException(\"Unsupported document-term matrix type.\")\n\n    # Make sure we have a matrix length for list input\n    if isinstance(matrix, list):\n        matrix_length = len(matrix)\n    else:\n        matrix_length = matrix.shape[0]\n\n    # Check labels vs matrix row count\n    if self.labels and len(self.labels) != matrix_length:\n        raise LexosException(label_error)\n\n    return matrix\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.save","title":"<code>save(path: Path | str)</code>","text":"<p>Save the figure as a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file to save.</p> required Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>@validate_call\ndef save(self, path: Path | str):\n    \"\"\"Save the figure as a file.\n\n    Args:\n        path (Path | str): The path to the file to save.\n    \"\"\"\n    if not path or path == \"\":\n        raise LexosException(\"You must provide a valid path.\")\n    self.fig.savefig(path)\n</code></pre>"},{"location":"api/cluster/dendrogram/#lexos.cluster.dendrogram.Dendrogram.show","title":"<code>show()</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. You can also reference the figure using <code>Dendrogram.fig</code>. This will generally display in a Jupyter notebook.</p> Source code in <code>lexos/cluster/dendrogram.py</code> <pre><code>def show(self):\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. You can also reference the figure\n    using `Dendrogram.fig`. This will generally display in a\n    Jupyter notebook.\n    \"\"\"\n    return self.fig\n</code></pre>"},{"location":"api/cluster/kmeans/","title":"KMeans","text":"<p>The <code>kmeans</code> module performs k-means clustering.</p>"},{"location":"api/cluster/kmeans/#the-kmeans-class","title":"The <code>KMeans</code> Class","text":""},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans","title":"<code>KMeans</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Perform and visualize KMeans clustering with optional dimensionality reduction.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>dtm</code>                 (<code>DTM | DataFrame | ndarray</code>)             </li> <li> <code>k</code>                 (<code>int</code>)             </li> <li> <code>init</code>                 (<code>Literal['k-means++', 'random']</code>)             </li> <li> <code>max_iter</code>                 (<code>int</code>)             </li> <li> <code>n_init</code>                 (<code>int</code>)             </li> <li> <code>tol</code>                 (<code>float</code>)             </li> <li> <code>random_state</code>                 (<code>Optional[int]</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>cluster_assignments</code>                 (<code>Optional[ndarray]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> </ul> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>class KMeans(BaseModel):\n    \"\"\"Perform and visualize KMeans clustering with optional dimensionality reduction.\"\"\"\n\n    # Configurable parameters for clustering\n    dtm: DTM | pd.DataFrame | np.ndarray = Field(\n        default=None, description=\"Input document-term matrix.\"\n    )\n    k: int = Field(default=2, description=\"Number of clusters to use.\")\n    init: Literal[\"k-means++\", \"random\"] = Field(\n        default=\"k-means++\", description=\"Initialization method for centroids.\"\n    )\n    max_iter: int = Field(\n        default=300, description=\"Maximum number of iterations for the algorithm.\"\n    )\n    n_init: int = Field(default=10, description=\"Number of initializations to perform.\")\n    tol: float = Field(default=1e-4, description=\"Relative tolerance for convergence.\")\n    random_state: Optional[int] = Field(\n        default=42, description=\"Random seed for reproducibility.\"\n    )\n\n    # Attributes populated after clustering\n    labels: Optional[list[str]] = None\n    cluster_assignments: Optional[np.ndarray] = None\n    fig: Optional[go.Figure] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Initialize KMeans clustering with the provided parameters.\"\"\"\n        super().__init__(**data)\n\n        # Get a valid matrix from the input DTM or DataFrame\n        matrix = self._get_valid_matrix()\n\n        if self.k is None:\n            raise LexosException(\n                \"Number of clusters 'k' must be specified for KMeans clustering.\"\n            )\n        try:\n            _kmeans = sklearn_KMeans(\n                n_clusters=self.k,\n                init=self.init,\n                max_iter=self.max_iter,\n                n_init=self.n_init,\n                tol=self.tol,\n                random_state=self.random_state,\n            )\n        except Exception as e:\n            raise LexosException(f\"KMeans clustering failed: {e}\")\n        try:\n            self.cluster_assignments = _kmeans.fit_predict(matrix)\n        except Exception as e:\n            raise LexosException(f\"KMeans clustering failed: {e}\")\n\n    def _get_valid_matrix(self) -&gt; np.ndarray:\n        \"\"\"Convert the input into a valid NumPy matrix format.\n\n        Supports DTM (Lexos), pandas DataFrame, or NumPy array.\n        Raises an error for unsupported formats or too few documents.\n        \"\"\"\n        if isinstance(self.dtm, DTM):\n            df = self.dtm.to_df().T\n        elif isinstance(self.dtm, pd.DataFrame):\n            df = self.dtm.T\n        elif isinstance(self.dtm, np.ndarray):\n            df = pd.DataFrame(self.dtm)\n        else:\n            raise LexosException(\n                \"Unsupported input: must be DTM, DataFrame, or ndarray.\"\n            )\n\n        # Must have more than 1 document to cluster\n        if df.shape[0] &lt; 2:\n            raise LexosException(\"Need at least 2 documents for clustering.\")\n\n        return df.values\n\n    @validate_call(config=model_config)\n    def elbow_plot(\n        self,\n        k_range: range = range(1, 10),\n        show: bool = True,\n        save_path: Optional[str] = None,\n        return_knee: bool = False,\n    ) -&gt; Optional[int]:\n        \"\"\"Generate an elbow plot to help determine the optimal number of clusters (k).\n\n        Args:\n            k_range (range): Range of k values to evaluate.\n            show (bool): Whether to display the plot interactively.\n            save_path (Optional[str]): Optional file path to save the elbow plot.\n            return_knee (bool): If True, return the detected elbow point (optimal k).\n\n        Returns:\n            Optional[int]: Optimal number of clusters, only if return_knee is True.\n        \"\"\"\n        # Ensure valid matrix and k range based on document count\n        matrix = self._get_valid_matrix()\n\n        min_k = min(k_range)\n        max_k = min(len(matrix), max(k_range))\n\n        if min_k &gt; max_k:\n            raise LexosException(\n                f\"Invalid k range ({min_k}\u2013{max(k_range)}) exceeds document count ({len(matrix)}).\"\n            )\n\n        adjusted_range = range(min_k, max_k + 1)\n        msg.info(\n            f\"Running elbow plot for k = {min_k} to {max_k} (limited to document count)\"\n        )\n\n        # Run KMeans for each k in the specified range and record inertia\n        inertias = []\n        for k in adjusted_range:\n            try:\n                model = sklearn_KMeans(\n                    n_clusters=k,\n                    init=self.init,\n                    max_iter=self.max_iter,\n                    n_init=self.n_init,\n                    tol=self.tol,\n                    random_state=42,\n                )\n                model.fit(matrix)\n                inertias.append(model.inertia_)\n            except Exception as e:\n                raise LexosException(f\"Error fitting KMeans for k={k}: {e}\")\n\n        # Use the \"maximum distance to line\" method to detect elbow\n        point1 = np.array([adjusted_range[0], inertias[0]])\n        point2 = np.array([adjusted_range[-1], inertias[-1]])\n\n        def distance_to_line(p):\n            return np.linalg.norm(\n                np.cross(point2 - point1, point1 - p)\n            ) / np.linalg.norm(point2 - point1)\n\n        distances = [\n            distance_to_line(np.array([k, inertia]))\n            for k, inertia in zip(adjusted_range, inertias)\n        ]\n        optimal_k = adjusted_range[np.argmax(distances)]\n\n        # Plot inertia vs. number of clusters and show elbow with vertical line\n        plt.figure(figsize=(8, 5))\n        plt.plot(list(adjusted_range), inertias, marker=\"o\", label=\"Inertia\")\n        plt.axvline(\n            optimal_k, color=\"red\", linestyle=\"--\", label=f\"Elbow at k={optimal_k}\"\n        )\n        plt.xlabel(\"Number of Clusters (k)\")\n        plt.ylabel(\"Inertia (Within-cluster Sum of Squares)\")\n        plt.title(\"Elbow Method for Optimal k\")\n        plt.grid(True)\n        plt.legend()\n\n        if save_path:\n            plt.savefig(save_path)\n\n        if show:\n            plt.show()\n        else:\n            plt.close()\n\n        if return_knee:\n            return optimal_k\n\n    @validate_call(config=model_config)\n    def save(self, path: str | Path, html: bool = False, **kwargs: Any) -&gt; None:\n        \"\"\"Save the most recent Plotly figure to an image or HTML file.\n\n        Args:\n            path (str | Path): Path to the output image file.\n            html (bool): If True, save as HTML; otherwise, save as image.\n            **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html and https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_html.html.\n        \"\"\"\n        if self.fig is None:\n            raise LexosException(\"No figure available: run a plot method first.\")\n        if html:\n            self.fig.write_html(path, **kwargs)\n        else:\n            self.fig.write_image(path, **kwargs)\n\n    @validate_call(config=model_config)\n    def scatter(\n        self,\n        dim: int = 2,\n        title: Optional[str] = None,\n        show: bool = False,\n        save_path: Optional[str | Path] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[go.Figure]:\n        \"\"\"Generate a 2D or 3D PCA scatter plot of the KMeans clusters.\n\n        Args:\n            show (bool): Whether to display the plot.\n            dim: (int): The number of dimensions.\n            title (Optional[str]): Optional title for the plot.\n            save_path (Optional[str | Path]): Optional file path to save the plot.\n            **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.\n\n        Returns:\n            go.Figure: The Plotly 3D scatter plot.\n        \"\"\"\n        if self.cluster_assignments is None:\n            raise LexosException(\"You must run clustering before plotting.\")\n\n        if dim not in [2, 3]:\n            raise LexosException(\"The number of dimensions must be either 2 or 3.\")\n\n        # Reduce dimensions for plot\n        matrix = self._get_valid_matrix()\n        try:\n            pca = PCA(n_components=dim)\n        except ValueError as e:\n            raise LexosException(f\"Failed to perform PCA: {e}\")\n        try:\n            reduced = pca.fit_transform(matrix)\n        except Exception as e:\n            raise LexosException(f\"Failed to reduce dimensions: {e}\")\n\n        # Start cluster numbering from 1 for display\n        cluster_assignments = [str(i + 1) for i in self.cluster_assignments]\n\n        # Build DataFrame for plotting\n        if dim == 2:\n            df = pd.DataFrame(\n                {\n                    \"x\": reduced[:, 0],\n                    \"y\": reduced[:, 1],\n                    \"Cluster\": cluster_assignments,\n                    \"Document\": self.labels\n                    or [f\"Doc{i + 1}\" for i in range(len(matrix))],\n                }\n            )\n        else:\n            df = pd.DataFrame(\n                {\n                    \"x\": reduced[:, 0],\n                    \"y\": reduced[:, 1],\n                    \"z\": reduced[:, 2],\n                    \"Cluster\": cluster_assignments,\n                    \"Document\": self.labels\n                    or [f\"Doc{i + 1}\" for i in range(len(matrix))],\n                }\n            )\n\n        # Create scatter plot\n        if dim == 2:\n            fig = px.scatter(\n                df,\n                x=\"x\",\n                y=\"y\",\n                color=\"Cluster\",\n                hover_name=\"Document\",\n                title=title,\n            )\n        else:\n            fig = px.scatter_3d(\n                df,\n                x=\"x\",\n                y=\"y\",\n                z=\"z\",\n                color=\"Cluster\",\n                hover_name=\"Document\",\n                title=title,\n            )\n\n        # Update the layout\n        fig.update_layout(margin=dict(l=12, r=10, t=40, b=10))\n\n        # Assign the figure to the instance attribute\n        self.fig = fig\n\n        # Save the figure if requested\n        if save_path:\n            fig.write_image(save_path, **kwargs)\n\n        # Show the figure if requested\n        if show:\n            config = dict(\n                displaylogo=False,\n                modeBarButtonsToRemove=[\"toggleSpikelines\"],\n                scrollZoom=True,\n            )\n            fig.show(config=config)\n            return None\n\n        # Otherwise, return the figure\n        else:\n            return fig\n\n    @validate_call(config=model_config)\n    def to_csv(self, path: str | Path, **kwargs: Any) -&gt; None:\n        \"\"\"Export a CSV of PCA coordinates and cluster labels.\n\n        Args:\n            path (str | Path): File path to save the CSV.\n            **kwargs (Any): Additional parameters for pandas DataFrame.to_csv().\n        \"\"\"\n        if self.cluster_assignments is None:\n            raise LexosException(\"No clustering results: run clustering first.\")\n\n        # Perform PCA to 2 components\n        matrix = self._get_valid_matrix()\n        pca = PCA(n_components=2)\n        coords = pca.fit_transform(matrix)\n\n        # Create output DataFrame\n        df = pd.DataFrame(\n            {\n                \"Document\": self.labels or [f\"Doc{i + 1}\" for i in range(len(coords))],\n                \"Cluster\": self.cluster_assignments.astype(str),\n                \"PC1\": coords[:, 0],\n                \"PC2\": coords[:, 1],\n            }\n        )\n\n        # Export to CSV\n        try:\n            df.to_csv(path, index=False)\n        except Exception as e:\n            raise LexosException(f\"Failed to export CSV: {e}\")\n\n    @validate_call(config=model_config)\n    def voronoi(\n        self,\n        title: Optional[str] = None,\n        show: bool = True,\n        save_path: Optional[str | Path] = None,\n        grid_step: Optional[float] = None,\n        max_points: int = 200_000,\n        **kwargs: Any,\n    ) -&gt; Optional[go.Figure]:\n        \"\"\"Plot Voronoi-like decision regions for KMeans clustering using 2D PCA.\n\n        Args:\n            title (Optional[str]): Optional title for the plot.\n            show (bool): Whether to display the plot interactively.\n            save_path (Optional[str | Path]): File path to save the plot.\n            grid_step (Optional[float]): Grid step size; estimated if None.\n            max_points (int): Maximum grid points for memory efficiency.\n            **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.\n        \"\"\"\n        # Reduce dimensions for 2D Voronoi visualization\n        matrix = self._get_valid_matrix()\n        try:\n            pca = PCA(n_components=2)\n        except ValueError as e:\n            raise LexosException(f\"Failed to perform PCA: {e}\")\n        try:\n            reduced = pca.fit_transform(matrix)\n        except Exception as e:\n            raise LexosException(f\"Failed to reduce dimensions: {e}\")\n\n        if self.k is None:\n            raise LexosException(\n                \"Number of clusters 'k' must be specified for KMeans clustering.\"\n            )\n\n        # Fit KMeans on reduced data for plotting\n        kmeans = sklearn_KMeans(\n            n_clusters=self.k,\n            init=self.init,\n            max_iter=self.max_iter,\n            n_init=self.n_init,\n            tol=self.tol,\n            random_state=42,\n        ).fit(reduced)\n\n        centroids = kmeans.cluster_centers_\n\n        # Define grid boundaries with buffer\n        x_min, x_max = reduced[:, 0].min() - 1, reduced[:, 0].max() + 1\n        y_min, y_max = reduced[:, 1].min() - 1, reduced[:, 1].max() + 1\n\n        # Estimate grid resolution to avoid memory overload\n        if grid_step is None:\n            range_area = (x_max - x_min) * (y_max - y_min)\n            grid_step = (range_area / max_points) ** 0.5\n            msg.info(\n                f\"Grid step auto-adjusted to {grid_step:.2f} to avoid memory overload.\"\n            )\n\n        # Create mesh grid and predict cluster for each point\n        xx, yy = np.meshgrid(\n            np.arange(x_min, x_max, grid_step), np.arange(y_min, y_max, grid_step)\n        )\n        grid = np.c_[xx.ravel(), yy.ravel()]\n        z = kmeans.predict(grid).reshape(xx.shape)\n\n        fig = go.Figure()\n\n        # Add background colored Voronoi regions\n        fig.add_trace(\n            go.Heatmap(\n                x=xx[0],\n                y=yy[:, 0],\n                z=z,\n                colorscale=\"YlGnBu\",\n                showscale=False,\n                opacity=0.4,\n            )\n        )\n\n        # Overlay documents per cluster\n        doc_labels = np.array(\n            self.labels or [f\"Doc{i + 1}\" for i in range(len(reduced))]\n        )\n        for i in range(self.k):\n            cluster_mask = self.cluster_assignments == i\n            fig.add_trace(\n                go.Scatter(\n                    x=reduced[cluster_mask, 0],\n                    y=reduced[cluster_mask, 1],\n                    mode=\"markers\",\n                    name=f\"Cluster {i + 1}\",\n                    text=doc_labels[cluster_mask],\n                    hovertemplate=\"%{text}&lt;extra&gt;&lt;/extra&gt;\",\n                    marker=dict(size=8),\n                )\n            )\n\n        # Add centroid markers\n        fig.add_trace(\n            go.Scatter(\n                x=centroids[:, 0],\n                y=centroids[:, 1],\n                mode=\"markers+text\",\n                name=\"Centroids\",\n                text=[f\"C{i + 1}\" for i in range(self.k)],\n                hoverinfo=\"text\",\n                textposition=\"top center\",\n                marker=dict(symbol=\"x\", size=14, color=\"black\"),\n            )\n        )\n\n        fig.update_layout(\n            title=title,\n            xaxis_title=\"PC1\",\n            yaxis_title=\"PC2\",\n        )\n\n        self.fig = fig\n        if save_path:\n            fig.write_image(save_path, **kwargs)\n        if show:\n            config = dict(\n                displaylogo=False,\n                modeBarButtonsToRemove=[\"toggleSpikelines\"],\n                scrollZoom=True,\n            )\n            fig.show(config=config)\n            return None\n        else:\n            return fig\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.dtm","title":"<code>dtm: DTM | pd.DataFrame | np.ndarray = None</code>  <code>pydantic-field</code>","text":"<p>Input document-term matrix.</p>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.init","title":"<code>init: Literal['k-means++', 'random'] = 'k-means++'</code>  <code>pydantic-field</code>","text":"<p>Initialization method for centroids.</p>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.k","title":"<code>k: int = 2</code>  <code>pydantic-field</code>","text":"<p>Number of clusters to use.</p>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.max_iter","title":"<code>max_iter: int = 300</code>  <code>pydantic-field</code>","text":"<p>Maximum number of iterations for the algorithm.</p>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.n_init","title":"<code>n_init: int = 10</code>  <code>pydantic-field</code>","text":"<p>Number of initializations to perform.</p>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.random_state","title":"<code>random_state: Optional[int] = 42</code>  <code>pydantic-field</code>","text":"<p>Random seed for reproducibility.</p>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.tol","title":"<code>tol: float = 0.0001</code>  <code>pydantic-field</code>","text":"<p>Relative tolerance for convergence.</p>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize KMeans clustering with the provided parameters.</p> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialize KMeans clustering with the provided parameters.\"\"\"\n    super().__init__(**data)\n\n    # Get a valid matrix from the input DTM or DataFrame\n    matrix = self._get_valid_matrix()\n\n    if self.k is None:\n        raise LexosException(\n            \"Number of clusters 'k' must be specified for KMeans clustering.\"\n        )\n    try:\n        _kmeans = sklearn_KMeans(\n            n_clusters=self.k,\n            init=self.init,\n            max_iter=self.max_iter,\n            n_init=self.n_init,\n            tol=self.tol,\n            random_state=self.random_state,\n        )\n    except Exception as e:\n        raise LexosException(f\"KMeans clustering failed: {e}\")\n    try:\n        self.cluster_assignments = _kmeans.fit_predict(matrix)\n    except Exception as e:\n        raise LexosException(f\"KMeans clustering failed: {e}\")\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.elbow_plot","title":"<code>elbow_plot(k_range: range = range(1, 10), show: bool = True, save_path: Optional[str] = None, return_knee: bool = False) -&gt; Optional[int]</code>","text":"<p>Generate an elbow plot to help determine the optimal number of clusters (k).</p> <p>Parameters:</p> Name Type Description Default <code>k_range</code> <code>range</code> <p>Range of k values to evaluate.</p> <code>range(1, 10)</code> <code>show</code> <code>bool</code> <p>Whether to display the plot interactively.</p> <code>True</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional file path to save the elbow plot.</p> <code>None</code> <code>return_knee</code> <code>bool</code> <p>If True, return the detected elbow point (optimal k).</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Optional[int]: Optimal number of clusters, only if return_knee is True.</p> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef elbow_plot(\n    self,\n    k_range: range = range(1, 10),\n    show: bool = True,\n    save_path: Optional[str] = None,\n    return_knee: bool = False,\n) -&gt; Optional[int]:\n    \"\"\"Generate an elbow plot to help determine the optimal number of clusters (k).\n\n    Args:\n        k_range (range): Range of k values to evaluate.\n        show (bool): Whether to display the plot interactively.\n        save_path (Optional[str]): Optional file path to save the elbow plot.\n        return_knee (bool): If True, return the detected elbow point (optimal k).\n\n    Returns:\n        Optional[int]: Optimal number of clusters, only if return_knee is True.\n    \"\"\"\n    # Ensure valid matrix and k range based on document count\n    matrix = self._get_valid_matrix()\n\n    min_k = min(k_range)\n    max_k = min(len(matrix), max(k_range))\n\n    if min_k &gt; max_k:\n        raise LexosException(\n            f\"Invalid k range ({min_k}\u2013{max(k_range)}) exceeds document count ({len(matrix)}).\"\n        )\n\n    adjusted_range = range(min_k, max_k + 1)\n    msg.info(\n        f\"Running elbow plot for k = {min_k} to {max_k} (limited to document count)\"\n    )\n\n    # Run KMeans for each k in the specified range and record inertia\n    inertias = []\n    for k in adjusted_range:\n        try:\n            model = sklearn_KMeans(\n                n_clusters=k,\n                init=self.init,\n                max_iter=self.max_iter,\n                n_init=self.n_init,\n                tol=self.tol,\n                random_state=42,\n            )\n            model.fit(matrix)\n            inertias.append(model.inertia_)\n        except Exception as e:\n            raise LexosException(f\"Error fitting KMeans for k={k}: {e}\")\n\n    # Use the \"maximum distance to line\" method to detect elbow\n    point1 = np.array([adjusted_range[0], inertias[0]])\n    point2 = np.array([adjusted_range[-1], inertias[-1]])\n\n    def distance_to_line(p):\n        return np.linalg.norm(\n            np.cross(point2 - point1, point1 - p)\n        ) / np.linalg.norm(point2 - point1)\n\n    distances = [\n        distance_to_line(np.array([k, inertia]))\n        for k, inertia in zip(adjusted_range, inertias)\n    ]\n    optimal_k = adjusted_range[np.argmax(distances)]\n\n    # Plot inertia vs. number of clusters and show elbow with vertical line\n    plt.figure(figsize=(8, 5))\n    plt.plot(list(adjusted_range), inertias, marker=\"o\", label=\"Inertia\")\n    plt.axvline(\n        optimal_k, color=\"red\", linestyle=\"--\", label=f\"Elbow at k={optimal_k}\"\n    )\n    plt.xlabel(\"Number of Clusters (k)\")\n    plt.ylabel(\"Inertia (Within-cluster Sum of Squares)\")\n    plt.title(\"Elbow Method for Optimal k\")\n    plt.grid(True)\n    plt.legend()\n\n    if save_path:\n        plt.savefig(save_path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n    if return_knee:\n        return optimal_k\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.save","title":"<code>save(path: str | Path, html: bool = False, **kwargs: Any) -&gt; None</code>","text":"<p>Save the most recent Plotly figure to an image or HTML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the output image file.</p> required <code>html</code> <code>bool</code> <p>If True, save as HTML; otherwise, save as image.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html and https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_html.html.</p> <code>{}</code> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: str | Path, html: bool = False, **kwargs: Any) -&gt; None:\n    \"\"\"Save the most recent Plotly figure to an image or HTML file.\n\n    Args:\n        path (str | Path): Path to the output image file.\n        html (bool): If True, save as HTML; otherwise, save as image.\n        **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html and https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_html.html.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\"No figure available: run a plot method first.\")\n    if html:\n        self.fig.write_html(path, **kwargs)\n    else:\n        self.fig.write_image(path, **kwargs)\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.scatter","title":"<code>scatter(dim: int = 2, title: Optional[str] = None, show: bool = False, save_path: Optional[str | Path] = None, **kwargs: Any) -&gt; Optional[go.Figure]</code>","text":"<p>Generate a 2D or 3D PCA scatter plot of the KMeans clusters.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>Whether to display the plot.</p> <code>False</code> <code>dim</code> <code>int</code> <p>(int): The number of dimensions.</p> <code>2</code> <code>title</code> <code>Optional[str]</code> <p>Optional title for the plot.</p> <code>None</code> <code>save_path</code> <code>Optional[str | Path]</code> <p>Optional file path to save the plot.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Figure]</code> <p>go.Figure: The Plotly 3D scatter plot.</p> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef scatter(\n    self,\n    dim: int = 2,\n    title: Optional[str] = None,\n    show: bool = False,\n    save_path: Optional[str | Path] = None,\n    **kwargs: Any,\n) -&gt; Optional[go.Figure]:\n    \"\"\"Generate a 2D or 3D PCA scatter plot of the KMeans clusters.\n\n    Args:\n        show (bool): Whether to display the plot.\n        dim: (int): The number of dimensions.\n        title (Optional[str]): Optional title for the plot.\n        save_path (Optional[str | Path]): Optional file path to save the plot.\n        **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.\n\n    Returns:\n        go.Figure: The Plotly 3D scatter plot.\n    \"\"\"\n    if self.cluster_assignments is None:\n        raise LexosException(\"You must run clustering before plotting.\")\n\n    if dim not in [2, 3]:\n        raise LexosException(\"The number of dimensions must be either 2 or 3.\")\n\n    # Reduce dimensions for plot\n    matrix = self._get_valid_matrix()\n    try:\n        pca = PCA(n_components=dim)\n    except ValueError as e:\n        raise LexosException(f\"Failed to perform PCA: {e}\")\n    try:\n        reduced = pca.fit_transform(matrix)\n    except Exception as e:\n        raise LexosException(f\"Failed to reduce dimensions: {e}\")\n\n    # Start cluster numbering from 1 for display\n    cluster_assignments = [str(i + 1) for i in self.cluster_assignments]\n\n    # Build DataFrame for plotting\n    if dim == 2:\n        df = pd.DataFrame(\n            {\n                \"x\": reduced[:, 0],\n                \"y\": reduced[:, 1],\n                \"Cluster\": cluster_assignments,\n                \"Document\": self.labels\n                or [f\"Doc{i + 1}\" for i in range(len(matrix))],\n            }\n        )\n    else:\n        df = pd.DataFrame(\n            {\n                \"x\": reduced[:, 0],\n                \"y\": reduced[:, 1],\n                \"z\": reduced[:, 2],\n                \"Cluster\": cluster_assignments,\n                \"Document\": self.labels\n                or [f\"Doc{i + 1}\" for i in range(len(matrix))],\n            }\n        )\n\n    # Create scatter plot\n    if dim == 2:\n        fig = px.scatter(\n            df,\n            x=\"x\",\n            y=\"y\",\n            color=\"Cluster\",\n            hover_name=\"Document\",\n            title=title,\n        )\n    else:\n        fig = px.scatter_3d(\n            df,\n            x=\"x\",\n            y=\"y\",\n            z=\"z\",\n            color=\"Cluster\",\n            hover_name=\"Document\",\n            title=title,\n        )\n\n    # Update the layout\n    fig.update_layout(margin=dict(l=12, r=10, t=40, b=10))\n\n    # Assign the figure to the instance attribute\n    self.fig = fig\n\n    # Save the figure if requested\n    if save_path:\n        fig.write_image(save_path, **kwargs)\n\n    # Show the figure if requested\n    if show:\n        config = dict(\n            displaylogo=False,\n            modeBarButtonsToRemove=[\"toggleSpikelines\"],\n            scrollZoom=True,\n        )\n        fig.show(config=config)\n        return None\n\n    # Otherwise, return the figure\n    else:\n        return fig\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.to_csv","title":"<code>to_csv(path: str | Path, **kwargs: Any) -&gt; None</code>","text":"<p>Export a CSV of PCA coordinates and cluster labels.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>File path to save the CSV.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters for pandas DataFrame.to_csv().</p> <code>{}</code> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef to_csv(self, path: str | Path, **kwargs: Any) -&gt; None:\n    \"\"\"Export a CSV of PCA coordinates and cluster labels.\n\n    Args:\n        path (str | Path): File path to save the CSV.\n        **kwargs (Any): Additional parameters for pandas DataFrame.to_csv().\n    \"\"\"\n    if self.cluster_assignments is None:\n        raise LexosException(\"No clustering results: run clustering first.\")\n\n    # Perform PCA to 2 components\n    matrix = self._get_valid_matrix()\n    pca = PCA(n_components=2)\n    coords = pca.fit_transform(matrix)\n\n    # Create output DataFrame\n    df = pd.DataFrame(\n        {\n            \"Document\": self.labels or [f\"Doc{i + 1}\" for i in range(len(coords))],\n            \"Cluster\": self.cluster_assignments.astype(str),\n            \"PC1\": coords[:, 0],\n            \"PC2\": coords[:, 1],\n        }\n    )\n\n    # Export to CSV\n    try:\n        df.to_csv(path, index=False)\n    except Exception as e:\n        raise LexosException(f\"Failed to export CSV: {e}\")\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.voronoi","title":"<code>voronoi(title: Optional[str] = None, show: bool = True, save_path: Optional[str | Path] = None, grid_step: Optional[float] = None, max_points: int = 200000, **kwargs: Any) -&gt; Optional[go.Figure]</code>","text":"<p>Plot Voronoi-like decision regions for KMeans clustering using 2D PCA.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>Optional[str]</code> <p>Optional title for the plot.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot interactively.</p> <code>True</code> <code>save_path</code> <code>Optional[str | Path]</code> <p>File path to save the plot.</p> <code>None</code> <code>grid_step</code> <code>Optional[float]</code> <p>Grid step size; estimated if None.</p> <code>None</code> <code>max_points</code> <code>int</code> <p>Maximum grid points for memory efficiency.</p> <code>200000</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.</p> <code>{}</code> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef voronoi(\n    self,\n    title: Optional[str] = None,\n    show: bool = True,\n    save_path: Optional[str | Path] = None,\n    grid_step: Optional[float] = None,\n    max_points: int = 200_000,\n    **kwargs: Any,\n) -&gt; Optional[go.Figure]:\n    \"\"\"Plot Voronoi-like decision regions for KMeans clustering using 2D PCA.\n\n    Args:\n        title (Optional[str]): Optional title for the plot.\n        show (bool): Whether to display the plot interactively.\n        save_path (Optional[str | Path]): File path to save the plot.\n        grid_step (Optional[float]): Grid step size; estimated if None.\n        max_points (int): Maximum grid points for memory efficiency.\n        **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.\n    \"\"\"\n    # Reduce dimensions for 2D Voronoi visualization\n    matrix = self._get_valid_matrix()\n    try:\n        pca = PCA(n_components=2)\n    except ValueError as e:\n        raise LexosException(f\"Failed to perform PCA: {e}\")\n    try:\n        reduced = pca.fit_transform(matrix)\n    except Exception as e:\n        raise LexosException(f\"Failed to reduce dimensions: {e}\")\n\n    if self.k is None:\n        raise LexosException(\n            \"Number of clusters 'k' must be specified for KMeans clustering.\"\n        )\n\n    # Fit KMeans on reduced data for plotting\n    kmeans = sklearn_KMeans(\n        n_clusters=self.k,\n        init=self.init,\n        max_iter=self.max_iter,\n        n_init=self.n_init,\n        tol=self.tol,\n        random_state=42,\n    ).fit(reduced)\n\n    centroids = kmeans.cluster_centers_\n\n    # Define grid boundaries with buffer\n    x_min, x_max = reduced[:, 0].min() - 1, reduced[:, 0].max() + 1\n    y_min, y_max = reduced[:, 1].min() - 1, reduced[:, 1].max() + 1\n\n    # Estimate grid resolution to avoid memory overload\n    if grid_step is None:\n        range_area = (x_max - x_min) * (y_max - y_min)\n        grid_step = (range_area / max_points) ** 0.5\n        msg.info(\n            f\"Grid step auto-adjusted to {grid_step:.2f} to avoid memory overload.\"\n        )\n\n    # Create mesh grid and predict cluster for each point\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, grid_step), np.arange(y_min, y_max, grid_step)\n    )\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    z = kmeans.predict(grid).reshape(xx.shape)\n\n    fig = go.Figure()\n\n    # Add background colored Voronoi regions\n    fig.add_trace(\n        go.Heatmap(\n            x=xx[0],\n            y=yy[:, 0],\n            z=z,\n            colorscale=\"YlGnBu\",\n            showscale=False,\n            opacity=0.4,\n        )\n    )\n\n    # Overlay documents per cluster\n    doc_labels = np.array(\n        self.labels or [f\"Doc{i + 1}\" for i in range(len(reduced))]\n    )\n    for i in range(self.k):\n        cluster_mask = self.cluster_assignments == i\n        fig.add_trace(\n            go.Scatter(\n                x=reduced[cluster_mask, 0],\n                y=reduced[cluster_mask, 1],\n                mode=\"markers\",\n                name=f\"Cluster {i + 1}\",\n                text=doc_labels[cluster_mask],\n                hovertemplate=\"%{text}&lt;extra&gt;&lt;/extra&gt;\",\n                marker=dict(size=8),\n            )\n        )\n\n    # Add centroid markers\n    fig.add_trace(\n        go.Scatter(\n            x=centroids[:, 0],\n            y=centroids[:, 1],\n            mode=\"markers+text\",\n            name=\"Centroids\",\n            text=[f\"C{i + 1}\" for i in range(self.k)],\n            hoverinfo=\"text\",\n            textposition=\"top center\",\n            marker=dict(symbol=\"x\", size=14, color=\"black\"),\n        )\n    )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"PC1\",\n        yaxis_title=\"PC2\",\n    )\n\n    self.fig = fig\n    if save_path:\n        fig.write_image(save_path, **kwargs)\n    if show:\n        config = dict(\n            displaylogo=False,\n            modeBarButtonsToRemove=[\"toggleSpikelines\"],\n            scrollZoom=True,\n        )\n        fig.show(config=config)\n        return None\n    else:\n        return fig\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize KMeans clustering with the provided parameters.</p> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialize KMeans clustering with the provided parameters.\"\"\"\n    super().__init__(**data)\n\n    # Get a valid matrix from the input DTM or DataFrame\n    matrix = self._get_valid_matrix()\n\n    if self.k is None:\n        raise LexosException(\n            \"Number of clusters 'k' must be specified for KMeans clustering.\"\n        )\n    try:\n        _kmeans = sklearn_KMeans(\n            n_clusters=self.k,\n            init=self.init,\n            max_iter=self.max_iter,\n            n_init=self.n_init,\n            tol=self.tol,\n            random_state=self.random_state,\n        )\n    except Exception as e:\n        raise LexosException(f\"KMeans clustering failed: {e}\")\n    try:\n        self.cluster_assignments = _kmeans.fit_predict(matrix)\n    except Exception as e:\n        raise LexosException(f\"KMeans clustering failed: {e}\")\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans._get_valid_matrix","title":"<code>_get_valid_matrix() -&gt; np.ndarray</code>","text":"<p>Convert the input into a valid NumPy matrix format.</p> <p>Supports DTM (Lexos), pandas DataFrame, or NumPy array. Raises an error for unsupported formats or too few documents.</p> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>def _get_valid_matrix(self) -&gt; np.ndarray:\n    \"\"\"Convert the input into a valid NumPy matrix format.\n\n    Supports DTM (Lexos), pandas DataFrame, or NumPy array.\n    Raises an error for unsupported formats or too few documents.\n    \"\"\"\n    if isinstance(self.dtm, DTM):\n        df = self.dtm.to_df().T\n    elif isinstance(self.dtm, pd.DataFrame):\n        df = self.dtm.T\n    elif isinstance(self.dtm, np.ndarray):\n        df = pd.DataFrame(self.dtm)\n    else:\n        raise LexosException(\n            \"Unsupported input: must be DTM, DataFrame, or ndarray.\"\n        )\n\n    # Must have more than 1 document to cluster\n    if df.shape[0] &lt; 2:\n        raise LexosException(\"Need at least 2 documents for clustering.\")\n\n    return df.values\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.elbow_plot","title":"<code>elbow_plot(k_range: range = range(1, 10), show: bool = True, save_path: Optional[str] = None, return_knee: bool = False) -&gt; Optional[int]</code>","text":"<p>Generate an elbow plot to help determine the optimal number of clusters (k).</p> <p>Parameters:</p> Name Type Description Default <code>k_range</code> <code>range</code> <p>Range of k values to evaluate.</p> <code>range(1, 10)</code> <code>show</code> <code>bool</code> <p>Whether to display the plot interactively.</p> <code>True</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional file path to save the elbow plot.</p> <code>None</code> <code>return_knee</code> <code>bool</code> <p>If True, return the detected elbow point (optimal k).</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Optional[int]: Optimal number of clusters, only if return_knee is True.</p> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef elbow_plot(\n    self,\n    k_range: range = range(1, 10),\n    show: bool = True,\n    save_path: Optional[str] = None,\n    return_knee: bool = False,\n) -&gt; Optional[int]:\n    \"\"\"Generate an elbow plot to help determine the optimal number of clusters (k).\n\n    Args:\n        k_range (range): Range of k values to evaluate.\n        show (bool): Whether to display the plot interactively.\n        save_path (Optional[str]): Optional file path to save the elbow plot.\n        return_knee (bool): If True, return the detected elbow point (optimal k).\n\n    Returns:\n        Optional[int]: Optimal number of clusters, only if return_knee is True.\n    \"\"\"\n    # Ensure valid matrix and k range based on document count\n    matrix = self._get_valid_matrix()\n\n    min_k = min(k_range)\n    max_k = min(len(matrix), max(k_range))\n\n    if min_k &gt; max_k:\n        raise LexosException(\n            f\"Invalid k range ({min_k}\u2013{max(k_range)}) exceeds document count ({len(matrix)}).\"\n        )\n\n    adjusted_range = range(min_k, max_k + 1)\n    msg.info(\n        f\"Running elbow plot for k = {min_k} to {max_k} (limited to document count)\"\n    )\n\n    # Run KMeans for each k in the specified range and record inertia\n    inertias = []\n    for k in adjusted_range:\n        try:\n            model = sklearn_KMeans(\n                n_clusters=k,\n                init=self.init,\n                max_iter=self.max_iter,\n                n_init=self.n_init,\n                tol=self.tol,\n                random_state=42,\n            )\n            model.fit(matrix)\n            inertias.append(model.inertia_)\n        except Exception as e:\n            raise LexosException(f\"Error fitting KMeans for k={k}: {e}\")\n\n    # Use the \"maximum distance to line\" method to detect elbow\n    point1 = np.array([adjusted_range[0], inertias[0]])\n    point2 = np.array([adjusted_range[-1], inertias[-1]])\n\n    def distance_to_line(p):\n        return np.linalg.norm(\n            np.cross(point2 - point1, point1 - p)\n        ) / np.linalg.norm(point2 - point1)\n\n    distances = [\n        distance_to_line(np.array([k, inertia]))\n        for k, inertia in zip(adjusted_range, inertias)\n    ]\n    optimal_k = adjusted_range[np.argmax(distances)]\n\n    # Plot inertia vs. number of clusters and show elbow with vertical line\n    plt.figure(figsize=(8, 5))\n    plt.plot(list(adjusted_range), inertias, marker=\"o\", label=\"Inertia\")\n    plt.axvline(\n        optimal_k, color=\"red\", linestyle=\"--\", label=f\"Elbow at k={optimal_k}\"\n    )\n    plt.xlabel(\"Number of Clusters (k)\")\n    plt.ylabel(\"Inertia (Within-cluster Sum of Squares)\")\n    plt.title(\"Elbow Method for Optimal k\")\n    plt.grid(True)\n    plt.legend()\n\n    if save_path:\n        plt.savefig(save_path)\n\n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n    if return_knee:\n        return optimal_k\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.scatter","title":"<code>scatter(dim: int = 2, title: Optional[str] = None, show: bool = False, save_path: Optional[str | Path] = None, **kwargs: Any) -&gt; Optional[go.Figure]</code>","text":"<p>Generate a 2D or 3D PCA scatter plot of the KMeans clusters.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>Whether to display the plot.</p> <code>False</code> <code>dim</code> <code>int</code> <p>(int): The number of dimensions.</p> <code>2</code> <code>title</code> <code>Optional[str]</code> <p>Optional title for the plot.</p> <code>None</code> <code>save_path</code> <code>Optional[str | Path]</code> <p>Optional file path to save the plot.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Figure]</code> <p>go.Figure: The Plotly 3D scatter plot.</p> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef scatter(\n    self,\n    dim: int = 2,\n    title: Optional[str] = None,\n    show: bool = False,\n    save_path: Optional[str | Path] = None,\n    **kwargs: Any,\n) -&gt; Optional[go.Figure]:\n    \"\"\"Generate a 2D or 3D PCA scatter plot of the KMeans clusters.\n\n    Args:\n        show (bool): Whether to display the plot.\n        dim: (int): The number of dimensions.\n        title (Optional[str]): Optional title for the plot.\n        save_path (Optional[str | Path]): Optional file path to save the plot.\n        **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.\n\n    Returns:\n        go.Figure: The Plotly 3D scatter plot.\n    \"\"\"\n    if self.cluster_assignments is None:\n        raise LexosException(\"You must run clustering before plotting.\")\n\n    if dim not in [2, 3]:\n        raise LexosException(\"The number of dimensions must be either 2 or 3.\")\n\n    # Reduce dimensions for plot\n    matrix = self._get_valid_matrix()\n    try:\n        pca = PCA(n_components=dim)\n    except ValueError as e:\n        raise LexosException(f\"Failed to perform PCA: {e}\")\n    try:\n        reduced = pca.fit_transform(matrix)\n    except Exception as e:\n        raise LexosException(f\"Failed to reduce dimensions: {e}\")\n\n    # Start cluster numbering from 1 for display\n    cluster_assignments = [str(i + 1) for i in self.cluster_assignments]\n\n    # Build DataFrame for plotting\n    if dim == 2:\n        df = pd.DataFrame(\n            {\n                \"x\": reduced[:, 0],\n                \"y\": reduced[:, 1],\n                \"Cluster\": cluster_assignments,\n                \"Document\": self.labels\n                or [f\"Doc{i + 1}\" for i in range(len(matrix))],\n            }\n        )\n    else:\n        df = pd.DataFrame(\n            {\n                \"x\": reduced[:, 0],\n                \"y\": reduced[:, 1],\n                \"z\": reduced[:, 2],\n                \"Cluster\": cluster_assignments,\n                \"Document\": self.labels\n                or [f\"Doc{i + 1}\" for i in range(len(matrix))],\n            }\n        )\n\n    # Create scatter plot\n    if dim == 2:\n        fig = px.scatter(\n            df,\n            x=\"x\",\n            y=\"y\",\n            color=\"Cluster\",\n            hover_name=\"Document\",\n            title=title,\n        )\n    else:\n        fig = px.scatter_3d(\n            df,\n            x=\"x\",\n            y=\"y\",\n            z=\"z\",\n            color=\"Cluster\",\n            hover_name=\"Document\",\n            title=title,\n        )\n\n    # Update the layout\n    fig.update_layout(margin=dict(l=12, r=10, t=40, b=10))\n\n    # Assign the figure to the instance attribute\n    self.fig = fig\n\n    # Save the figure if requested\n    if save_path:\n        fig.write_image(save_path, **kwargs)\n\n    # Show the figure if requested\n    if show:\n        config = dict(\n            displaylogo=False,\n            modeBarButtonsToRemove=[\"toggleSpikelines\"],\n            scrollZoom=True,\n        )\n        fig.show(config=config)\n        return None\n\n    # Otherwise, return the figure\n    else:\n        return fig\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.to_csv","title":"<code>to_csv(path: str | Path, **kwargs: Any) -&gt; None</code>","text":"<p>Export a CSV of PCA coordinates and cluster labels.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>File path to save the CSV.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters for pandas DataFrame.to_csv().</p> <code>{}</code> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef to_csv(self, path: str | Path, **kwargs: Any) -&gt; None:\n    \"\"\"Export a CSV of PCA coordinates and cluster labels.\n\n    Args:\n        path (str | Path): File path to save the CSV.\n        **kwargs (Any): Additional parameters for pandas DataFrame.to_csv().\n    \"\"\"\n    if self.cluster_assignments is None:\n        raise LexosException(\"No clustering results: run clustering first.\")\n\n    # Perform PCA to 2 components\n    matrix = self._get_valid_matrix()\n    pca = PCA(n_components=2)\n    coords = pca.fit_transform(matrix)\n\n    # Create output DataFrame\n    df = pd.DataFrame(\n        {\n            \"Document\": self.labels or [f\"Doc{i + 1}\" for i in range(len(coords))],\n            \"Cluster\": self.cluster_assignments.astype(str),\n            \"PC1\": coords[:, 0],\n            \"PC2\": coords[:, 1],\n        }\n    )\n\n    # Export to CSV\n    try:\n        df.to_csv(path, index=False)\n    except Exception as e:\n        raise LexosException(f\"Failed to export CSV: {e}\")\n</code></pre>"},{"location":"api/cluster/kmeans/#lexos.cluster.kmeans.KMeans.voronoi","title":"<code>voronoi(title: Optional[str] = None, show: bool = True, save_path: Optional[str | Path] = None, grid_step: Optional[float] = None, max_points: int = 200000, **kwargs: Any) -&gt; Optional[go.Figure]</code>","text":"<p>Plot Voronoi-like decision regions for KMeans clustering using 2D PCA.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>Optional[str]</code> <p>Optional title for the plot.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot interactively.</p> <code>True</code> <code>save_path</code> <code>Optional[str | Path]</code> <p>File path to save the plot.</p> <code>None</code> <code>grid_step</code> <code>Optional[float]</code> <p>Grid step size; estimated if None.</p> <code>None</code> <code>max_points</code> <code>int</code> <p>Maximum grid points for memory efficiency.</p> <code>200000</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.</p> <code>{}</code> Source code in <code>lexos/cluster/kmeans/kmeans.py</code> <pre><code>@validate_call(config=model_config)\ndef voronoi(\n    self,\n    title: Optional[str] = None,\n    show: bool = True,\n    save_path: Optional[str | Path] = None,\n    grid_step: Optional[float] = None,\n    max_points: int = 200_000,\n    **kwargs: Any,\n) -&gt; Optional[go.Figure]:\n    \"\"\"Plot Voronoi-like decision regions for KMeans clustering using 2D PCA.\n\n    Args:\n        title (Optional[str]): Optional title for the plot.\n        show (bool): Whether to display the plot interactively.\n        save_path (Optional[str | Path]): File path to save the plot.\n        grid_step (Optional[float]): Grid step size; estimated if None.\n        max_points (int): Maximum grid points for memory efficiency.\n        **kwargs (Any): Additional parameters for saving the figure. See https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html.\n    \"\"\"\n    # Reduce dimensions for 2D Voronoi visualization\n    matrix = self._get_valid_matrix()\n    try:\n        pca = PCA(n_components=2)\n    except ValueError as e:\n        raise LexosException(f\"Failed to perform PCA: {e}\")\n    try:\n        reduced = pca.fit_transform(matrix)\n    except Exception as e:\n        raise LexosException(f\"Failed to reduce dimensions: {e}\")\n\n    if self.k is None:\n        raise LexosException(\n            \"Number of clusters 'k' must be specified for KMeans clustering.\"\n        )\n\n    # Fit KMeans on reduced data for plotting\n    kmeans = sklearn_KMeans(\n        n_clusters=self.k,\n        init=self.init,\n        max_iter=self.max_iter,\n        n_init=self.n_init,\n        tol=self.tol,\n        random_state=42,\n    ).fit(reduced)\n\n    centroids = kmeans.cluster_centers_\n\n    # Define grid boundaries with buffer\n    x_min, x_max = reduced[:, 0].min() - 1, reduced[:, 0].max() + 1\n    y_min, y_max = reduced[:, 1].min() - 1, reduced[:, 1].max() + 1\n\n    # Estimate grid resolution to avoid memory overload\n    if grid_step is None:\n        range_area = (x_max - x_min) * (y_max - y_min)\n        grid_step = (range_area / max_points) ** 0.5\n        msg.info(\n            f\"Grid step auto-adjusted to {grid_step:.2f} to avoid memory overload.\"\n        )\n\n    # Create mesh grid and predict cluster for each point\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, grid_step), np.arange(y_min, y_max, grid_step)\n    )\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    z = kmeans.predict(grid).reshape(xx.shape)\n\n    fig = go.Figure()\n\n    # Add background colored Voronoi regions\n    fig.add_trace(\n        go.Heatmap(\n            x=xx[0],\n            y=yy[:, 0],\n            z=z,\n            colorscale=\"YlGnBu\",\n            showscale=False,\n            opacity=0.4,\n        )\n    )\n\n    # Overlay documents per cluster\n    doc_labels = np.array(\n        self.labels or [f\"Doc{i + 1}\" for i in range(len(reduced))]\n    )\n    for i in range(self.k):\n        cluster_mask = self.cluster_assignments == i\n        fig.add_trace(\n            go.Scatter(\n                x=reduced[cluster_mask, 0],\n                y=reduced[cluster_mask, 1],\n                mode=\"markers\",\n                name=f\"Cluster {i + 1}\",\n                text=doc_labels[cluster_mask],\n                hovertemplate=\"%{text}&lt;extra&gt;&lt;/extra&gt;\",\n                marker=dict(size=8),\n            )\n        )\n\n    # Add centroid markers\n    fig.add_trace(\n        go.Scatter(\n            x=centroids[:, 0],\n            y=centroids[:, 1],\n            mode=\"markers+text\",\n            name=\"Centroids\",\n            text=[f\"C{i + 1}\" for i in range(self.k)],\n            hoverinfo=\"text\",\n            textposition=\"top center\",\n            marker=dict(symbol=\"x\", size=14, color=\"black\"),\n        )\n    )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"PC1\",\n        yaxis_title=\"PC2\",\n    )\n\n    self.fig = fig\n    if save_path:\n        fig.write_image(save_path, **kwargs)\n    if show:\n        config = dict(\n            displaylogo=False,\n            modeBarButtonsToRemove=[\"toggleSpikelines\"],\n            scrollZoom=True,\n        )\n        fig.show(config=config)\n        return None\n    else:\n        return fig\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/","title":"Plotly Dendrograms","text":""},{"location":"api/cluster/plotly_dendrogram/#the-plotlydendrogram-class","title":"The <code>PlotlyDendrogram</code> Class","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram","title":"<code>PlotlyDendrogram</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>PlotlyDendrogram.</p> <p>Typical usage:</p> <pre><code>from lexos.cluster import PlotlyDendrogram\n\ndendrogram = PlotlyDendrogram(dtm)\ndendrogram.show()\n\nNeeds some work in returning the figure as a figure\nand html and html div.\n</code></pre> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>dtm</code>                 (<code>Optional[ArrayLike | DTM | DataFrame]</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>metric</code>                 (<code>Optional[str]</code>)             </li> <li> <code>method</code>                 (<code>Optional[str]</code>)             </li> <li> <code>truncate_mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>get_leaves</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>orientation</code>                 (<code>Optional[str]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>figsize</code>                 (<code>Optional[tuple]</code>)             </li> <li> <code>colorscale</code>                 (<code>Optional[list]</code>)             </li> <li> <code>hovertext</code>                 (<code>Optional[list]</code>)             </li> <li> <code>color_threshold</code>                 (<code>Optional[float]</code>)             </li> <li> <code>config</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>x_tickangle</code>                 (<code>Optional[int]</code>)             </li> <li> <code>y_tickangle</code>                 (<code>Optional[int]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> <li> <code>layout</code>                 (<code>Optional[dict]</code>)             </li> </ul> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>class PlotlyDendrogram(BaseModel):\n    \"\"\"PlotlyDendrogram.\n\n    Typical usage:\n\n    ```python\n    from lexos.cluster import PlotlyDendrogram\n\n    dendrogram = PlotlyDendrogram(dtm)\n    dendrogram.show()\n\n    Needs some work in returning the figure as a figure\n    and html and html div.\n    ```\n    \"\"\"\n\n    dtm: Optional[ArrayLike | DTM | pd.DataFrame] = Field(\n        None, json_schema_extra={\"The document-term matrix.\"}\n    )\n    labels: Optional[list[str]] = Field(\n        None, description=\"The labels for the dendrogram.\"\n    )\n    metric: Optional[str] = Field(\n        \"euclidean\", description=\"The metric to use for the dendrogram.\"\n    )\n    method: Optional[str] = Field(\n        \"average\", description=\"The method to use for the dendrogram.\"\n    )\n    truncate_mode: Optional[str] = Field(\n        None, description=\"The truncate mode for the dendrogram.\"\n    )\n    get_leaves: Optional[bool] = Field(\n        True, description=\"The get leaves for the dendrogram.\"\n    )\n    orientation: Optional[str] = Field(\n        \"bottom\", description=\"The orientation for the dendrogram.\"\n    )\n    title: Optional[str] = Field(None, description=\"The title for the dendrogram.\")\n    figsize: Optional[tuple] = Field(\n        (10, 10), description=\"The figsize for the dendrogram.\"\n    )\n    colorscale: Optional[list] = Field(\n        None, description=\"The colorscale for the dendrogram.\"\n    )\n    hovertext: Optional[list] = Field(\n        None, description=\"The hovertext for the dendrogram.\"\n    )\n    color_threshold: Optional[float] = Field(\n        None, description=\"The color threshold for the dendrogram.\"\n    )\n    config: Optional[dict] = Field(\n        dict(\n            displaylogo=False,\n            modeBarButtonsToRemove=[\"toggleSpikelines\"],\n            scrollZoom=True,\n        ),\n        description=\"The config for the dendrogram.\",\n    )\n    x_tickangle: Optional[int] = Field(\n        0, description=\"The x tickangle for the dendrogram.\"\n    )\n    y_tickangle: Optional[int] = Field(\n        0, description=\"The y tickangle for the dendrogram.\"\n    )\n    fig: Optional[Figure] = Field(None, description=\"The figure for the dendrogram.\")\n    layout: Optional[dict] = Field(\n        {},\n        description=\"The layout for the dendrogram. Keywords and values to be passed to plotly.graph_objects.Figure.update_layout().\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data):\n        \"\"\"Initialize the PlotlyDendrogram instance.\"\"\"\n        super().__init__(**data)\n\n        def distfun(x: ArrayLike) -&gt; ArrayLike:\n            \"\"\"Get the pairwise distance matrix.\n\n            Args:\n                x (ArrayLike): The distance matrix.\n\n            Returns:\n                ArrayLike: The pairwise distance matrix.\n            \"\"\"\n            return pdist(x, metric=self.metric)\n\n        def linkagefun(x: ArrayLike) -&gt; ArrayLike:\n            \"\"\"Get the hierarchical clustering encoded as a linkage matrix.\n\n            Args:\n                x (ArrayLike): The pairwise distance matrix.\n\n            Returns:\n                ArrayLike: The linkage matrix.\n            \"\"\"\n            return sch.linkage(x, self.method)\n\n        # Ensure there is a document-term matrix\n        if self.dtm is None:\n            raise LexosException(\"You must provide a document-term matrix.\")\n\n        # Get the matrix based on the data type\n        matrix = self._get_valid_matrix()\n\n        # Ensure there are labels (moved after matrix validation to get correct shape)\n        if not self.labels:\n            if isinstance(self.dtm, DTM):\n                self.labels = self.dtm.labels\n            elif isinstance(self.dtm, pd.DataFrame):\n                # Corrected to use index for labels, as rows are documents after transpose\n                self.labels = self.dtm.index.tolist()\n            else:  # If matrix is a numpy array or list (now handled by _get_valid_matrix converting to numpy)\n                self.labels = [f\"Doc{i + 1}\" for i in range(matrix.shape[0])]\n\n        # Create the figure\n        self.fig = create_dendrogram(\n            matrix,\n            labels=self.labels,\n            distfun=distfun,\n            linkagefun=linkagefun,\n            orientation=self.orientation,\n            colorscale=self.colorscale,\n            hovertext=self.hovertext,\n            color_threshold=self.color_threshold,\n        )\n\n        # Set the standard layout\n        self.fig.update_layout(\n            margin=dict(l=0, r=0, b=0, t=0, pad=0),\n            hovermode=\"x\",\n            paper_bgcolor=\"rgba(0, 0, 0, 0)\",\n            plot_bgcolor=\"rgba(0, 0, 0, 0)\",\n            xaxis=dict(\n                showline=False, ticks=\"\", tickangle=self.x_tickangle, automargin=True\n            ),\n            yaxis=dict(\n                showline=False, ticks=\"\", tickangle=self.y_tickangle, automargin=True\n            ),\n        )\n\n        # Set the title\n        if self.title is not None:\n            title = dict(\n                text=self.title, x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"\n            )\n            self.fig.update_layout(title=title, margin=dict(t=40))\n\n        # Add user-configured layout\n        self.fig.update_layout(self.layout)\n\n        # Hack to ensure that leaves on the edge of the plot are not clipped.\n        # Only works for bottom and left orientation.\n        # Add an invisible scatter point to extend the margin.\n        if self.orientation in [\"bottom\", \"left\"]:\n            x_value = max([max(data[\"x\"]) for data in self.fig.data])\n            dummy_scatter = Scatter(\n                x=[x_value], y=[0], mode=\"markers\", opacity=0, hoverinfo=\"skip\"\n            )\n            self.fig.add_trace(trace=dummy_scatter)\n\n        # Move labels for top and right orientation\n        if self.orientation == \"top\":\n            self.fig.update_xaxes(side=\"top\")\n        if self.orientation == \"right\":\n            self.fig.update_yaxes(side=\"right\")\n\n        plt.close()\n\n    def _get_valid_matrix(self):\n        \"\"\"Get a valid matrix based on the data type of the dtm.\"\"\"  # End of _get_valid_matrix docstring\n        if isinstance(self.dtm, DTM):\n            matrix = self.dtm.to_df()\n            matrix.index.name = \"terms\"\n            matrix = matrix.T\n        elif isinstance(self.dtm, list):  # Added handling for list input\n            matrix = np.array(self.dtm)  # Convert list to numpy array\n        else:\n            matrix = self.dtm\n\n        # Now, `matrix` will always be a pandas DataFrame or a numpy array when we check `shape`\n        if matrix.shape[0] &lt; 2:\n            raise LexosException(\n                \"The document-term matrix must have more than one document.\"\n            )\n        return matrix\n\n    def show(self) -&gt; None:\n        \"\"\"Show the figure.\"\"\"  # End of show docstring\n        if self.fig is None:\n            raise LexosException(\n                \"You must call the instance before showing the figure.\"\n            )\n        self.fig.show(config=self.config)\n\n    def to_html(self, **kwargs):\n        \"\"\"Create an HTML representation of the figure.\n\n        Wrapper from the Plotly Figure to_html method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n        \"\"\"  # End of to_html docstring\n        if self.fig is None:\n            raise LexosException(\"You must call the instance before generating HTML.\")\n        return self.fig.to_html(**kwargs)\n\n    def to_image(self, **kwargs):\n        \"\"\"Create a static image of the figure.\n\n        Wrapper from the Plotly Figure to_image method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n        \"\"\"  # End of to_image docstring\n        if self.fig is None:\n            raise LexosException(\n                \"You must call the instance before generating an image.\"\n            )\n        return self.fig.to_image(**kwargs)\n\n    @validate_call(config=model_config)\n    def write_html(self, path: Path | str, **kwargs):\n        \"\"\"Save an HTML representation of the figure to disk.\n\n        Wrapper from the Plotly Figure write_html method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n        \"\"\"\n        if self.fig is None:\n            raise LexosException(\"You must call the instance before saving the figure.\")\n        # Removed: if \"file\" in kwargs: kwargs[\"file\"] = path\n        self.fig.write_html(\n            str(path), **kwargs\n        )  # Convert path to string for write_html\n\n    @validate_call(config=model_config)\n    def write_image(self, path: Path | str, **kwargs):\n        \"\"\"Save a static image of the figure to disk.\n\n        Wrapper from the Plotly Figure write_image method.\n        See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n        \"\"\"\n        if self.fig is None:\n            raise LexosException(\"You must call the instance before saving the figure.\")\n        # Removed: if \"file\" in kwargs: kwargs[\"file\"] = path\n        self.fig.write_image(\n            str(path), **kwargs\n        )  # Convert path to string for write_image\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.color_threshold","title":"<code>color_threshold: Optional[float] = None</code>  <code>pydantic-field</code>","text":"<p>The color threshold for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.colorscale","title":"<code>colorscale: Optional[list] = None</code>  <code>pydantic-field</code>","text":"<p>The colorscale for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.config","title":"<code>config: Optional[dict] = dict(displaylogo=False, modeBarButtonsToRemove=['toggleSpikelines'], scrollZoom=True)</code>  <code>pydantic-field</code>","text":"<p>The config for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.figsize","title":"<code>figsize: Optional[tuple] = (10, 10)</code>  <code>pydantic-field</code>","text":"<p>The figsize for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.get_leaves","title":"<code>get_leaves: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>The get leaves for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.hovertext","title":"<code>hovertext: Optional[list] = None</code>  <code>pydantic-field</code>","text":"<p>The hovertext for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.labels","title":"<code>labels: Optional[list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>The labels for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.layout","title":"<code>layout: Optional[dict] = {}</code>  <code>pydantic-field</code>","text":"<p>The layout for the dendrogram. Keywords and values to be passed to plotly.graph_objects.Figure.update_layout().</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.method","title":"<code>method: Optional[str] = 'average'</code>  <code>pydantic-field</code>","text":"<p>The method to use for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.metric","title":"<code>metric: Optional[str] = 'euclidean'</code>  <code>pydantic-field</code>","text":"<p>The metric to use for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.orientation","title":"<code>orientation: Optional[str] = 'bottom'</code>  <code>pydantic-field</code>","text":"<p>The orientation for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.truncate_mode","title":"<code>truncate_mode: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The truncate mode for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.x_tickangle","title":"<code>x_tickangle: Optional[int] = 0</code>  <code>pydantic-field</code>","text":"<p>The x tickangle for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.y_tickangle","title":"<code>y_tickangle: Optional[int] = 0</code>  <code>pydantic-field</code>","text":"<p>The y tickangle for the dendrogram.</p>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the PlotlyDendrogram instance.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the PlotlyDendrogram instance.\"\"\"\n    super().__init__(**data)\n\n    def distfun(x: ArrayLike) -&gt; ArrayLike:\n        \"\"\"Get the pairwise distance matrix.\n\n        Args:\n            x (ArrayLike): The distance matrix.\n\n        Returns:\n            ArrayLike: The pairwise distance matrix.\n        \"\"\"\n        return pdist(x, metric=self.metric)\n\n    def linkagefun(x: ArrayLike) -&gt; ArrayLike:\n        \"\"\"Get the hierarchical clustering encoded as a linkage matrix.\n\n        Args:\n            x (ArrayLike): The pairwise distance matrix.\n\n        Returns:\n            ArrayLike: The linkage matrix.\n        \"\"\"\n        return sch.linkage(x, self.method)\n\n    # Ensure there is a document-term matrix\n    if self.dtm is None:\n        raise LexosException(\"You must provide a document-term matrix.\")\n\n    # Get the matrix based on the data type\n    matrix = self._get_valid_matrix()\n\n    # Ensure there are labels (moved after matrix validation to get correct shape)\n    if not self.labels:\n        if isinstance(self.dtm, DTM):\n            self.labels = self.dtm.labels\n        elif isinstance(self.dtm, pd.DataFrame):\n            # Corrected to use index for labels, as rows are documents after transpose\n            self.labels = self.dtm.index.tolist()\n        else:  # If matrix is a numpy array or list (now handled by _get_valid_matrix converting to numpy)\n            self.labels = [f\"Doc{i + 1}\" for i in range(matrix.shape[0])]\n\n    # Create the figure\n    self.fig = create_dendrogram(\n        matrix,\n        labels=self.labels,\n        distfun=distfun,\n        linkagefun=linkagefun,\n        orientation=self.orientation,\n        colorscale=self.colorscale,\n        hovertext=self.hovertext,\n        color_threshold=self.color_threshold,\n    )\n\n    # Set the standard layout\n    self.fig.update_layout(\n        margin=dict(l=0, r=0, b=0, t=0, pad=0),\n        hovermode=\"x\",\n        paper_bgcolor=\"rgba(0, 0, 0, 0)\",\n        plot_bgcolor=\"rgba(0, 0, 0, 0)\",\n        xaxis=dict(\n            showline=False, ticks=\"\", tickangle=self.x_tickangle, automargin=True\n        ),\n        yaxis=dict(\n            showline=False, ticks=\"\", tickangle=self.y_tickangle, automargin=True\n        ),\n    )\n\n    # Set the title\n    if self.title is not None:\n        title = dict(\n            text=self.title, x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"\n        )\n        self.fig.update_layout(title=title, margin=dict(t=40))\n\n    # Add user-configured layout\n    self.fig.update_layout(self.layout)\n\n    # Hack to ensure that leaves on the edge of the plot are not clipped.\n    # Only works for bottom and left orientation.\n    # Add an invisible scatter point to extend the margin.\n    if self.orientation in [\"bottom\", \"left\"]:\n        x_value = max([max(data[\"x\"]) for data in self.fig.data])\n        dummy_scatter = Scatter(\n            x=[x_value], y=[0], mode=\"markers\", opacity=0, hoverinfo=\"skip\"\n        )\n        self.fig.add_trace(trace=dummy_scatter)\n\n    # Move labels for top and right orientation\n    if self.orientation == \"top\":\n        self.fig.update_xaxes(side=\"top\")\n    if self.orientation == \"right\":\n        self.fig.update_yaxes(side=\"right\")\n\n    plt.close()\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.show","title":"<code>show() -&gt; None</code>","text":"<p>Show the figure.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Show the figure.\"\"\"  # End of show docstring\n    if self.fig is None:\n        raise LexosException(\n            \"You must call the instance before showing the figure.\"\n        )\n    self.fig.show(config=self.config)\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.to_html","title":"<code>to_html(**kwargs)</code>","text":"<p>Create an HTML representation of the figure.</p> <p>Wrapper from the Plotly Figure to_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def to_html(self, **kwargs):\n    \"\"\"Create an HTML representation of the figure.\n\n    Wrapper from the Plotly Figure to_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"  # End of to_html docstring\n    if self.fig is None:\n        raise LexosException(\"You must call the instance before generating HTML.\")\n    return self.fig.to_html(**kwargs)\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.to_image","title":"<code>to_image(**kwargs)</code>","text":"<p>Create a static image of the figure.</p> <p>Wrapper from the Plotly Figure to_image method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def to_image(self, **kwargs):\n    \"\"\"Create a static image of the figure.\n\n    Wrapper from the Plotly Figure to_image method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"  # End of to_image docstring\n    if self.fig is None:\n        raise LexosException(\n            \"You must call the instance before generating an image.\"\n        )\n    return self.fig.to_image(**kwargs)\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.write_html","title":"<code>write_html(path: Path | str, **kwargs)</code>","text":"<p>Save an HTML representation of the figure to disk.</p> <p>Wrapper from the Plotly Figure write_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>@validate_call(config=model_config)\ndef write_html(self, path: Path | str, **kwargs):\n    \"\"\"Save an HTML representation of the figure to disk.\n\n    Wrapper from the Plotly Figure write_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\"You must call the instance before saving the figure.\")\n    # Removed: if \"file\" in kwargs: kwargs[\"file\"] = path\n    self.fig.write_html(\n        str(path), **kwargs\n    )  # Convert path to string for write_html\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.write_image","title":"<code>write_image(path: Path | str, **kwargs)</code>","text":"<p>Save a static image of the figure to disk.</p> <p>Wrapper from the Plotly Figure write_image method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>@validate_call(config=model_config)\ndef write_image(self, path: Path | str, **kwargs):\n    \"\"\"Save a static image of the figure to disk.\n\n    Wrapper from the Plotly Figure write_image method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\"You must call the instance before saving the figure.\")\n    # Removed: if \"file\" in kwargs: kwargs[\"file\"] = path\n    self.fig.write_image(\n        str(path), **kwargs\n    )  # Convert path to string for write_image\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the PlotlyDendrogram instance.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the PlotlyDendrogram instance.\"\"\"\n    super().__init__(**data)\n\n    def distfun(x: ArrayLike) -&gt; ArrayLike:\n        \"\"\"Get the pairwise distance matrix.\n\n        Args:\n            x (ArrayLike): The distance matrix.\n\n        Returns:\n            ArrayLike: The pairwise distance matrix.\n        \"\"\"\n        return pdist(x, metric=self.metric)\n\n    def linkagefun(x: ArrayLike) -&gt; ArrayLike:\n        \"\"\"Get the hierarchical clustering encoded as a linkage matrix.\n\n        Args:\n            x (ArrayLike): The pairwise distance matrix.\n\n        Returns:\n            ArrayLike: The linkage matrix.\n        \"\"\"\n        return sch.linkage(x, self.method)\n\n    # Ensure there is a document-term matrix\n    if self.dtm is None:\n        raise LexosException(\"You must provide a document-term matrix.\")\n\n    # Get the matrix based on the data type\n    matrix = self._get_valid_matrix()\n\n    # Ensure there are labels (moved after matrix validation to get correct shape)\n    if not self.labels:\n        if isinstance(self.dtm, DTM):\n            self.labels = self.dtm.labels\n        elif isinstance(self.dtm, pd.DataFrame):\n            # Corrected to use index for labels, as rows are documents after transpose\n            self.labels = self.dtm.index.tolist()\n        else:  # If matrix is a numpy array or list (now handled by _get_valid_matrix converting to numpy)\n            self.labels = [f\"Doc{i + 1}\" for i in range(matrix.shape[0])]\n\n    # Create the figure\n    self.fig = create_dendrogram(\n        matrix,\n        labels=self.labels,\n        distfun=distfun,\n        linkagefun=linkagefun,\n        orientation=self.orientation,\n        colorscale=self.colorscale,\n        hovertext=self.hovertext,\n        color_threshold=self.color_threshold,\n    )\n\n    # Set the standard layout\n    self.fig.update_layout(\n        margin=dict(l=0, r=0, b=0, t=0, pad=0),\n        hovermode=\"x\",\n        paper_bgcolor=\"rgba(0, 0, 0, 0)\",\n        plot_bgcolor=\"rgba(0, 0, 0, 0)\",\n        xaxis=dict(\n            showline=False, ticks=\"\", tickangle=self.x_tickangle, automargin=True\n        ),\n        yaxis=dict(\n            showline=False, ticks=\"\", tickangle=self.y_tickangle, automargin=True\n        ),\n    )\n\n    # Set the title\n    if self.title is not None:\n        title = dict(\n            text=self.title, x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"\n        )\n        self.fig.update_layout(title=title, margin=dict(t=40))\n\n    # Add user-configured layout\n    self.fig.update_layout(self.layout)\n\n    # Hack to ensure that leaves on the edge of the plot are not clipped.\n    # Only works for bottom and left orientation.\n    # Add an invisible scatter point to extend the margin.\n    if self.orientation in [\"bottom\", \"left\"]:\n        x_value = max([max(data[\"x\"]) for data in self.fig.data])\n        dummy_scatter = Scatter(\n            x=[x_value], y=[0], mode=\"markers\", opacity=0, hoverinfo=\"skip\"\n        )\n        self.fig.add_trace(trace=dummy_scatter)\n\n    # Move labels for top and right orientation\n    if self.orientation == \"top\":\n        self.fig.update_xaxes(side=\"top\")\n    if self.orientation == \"right\":\n        self.fig.update_yaxes(side=\"right\")\n\n    plt.close()\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram._get_valid_matrix","title":"<code>_get_valid_matrix()</code>","text":"<p>Get a valid matrix based on the data type of the dtm.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def _get_valid_matrix(self):\n    \"\"\"Get a valid matrix based on the data type of the dtm.\"\"\"  # End of _get_valid_matrix docstring\n    if isinstance(self.dtm, DTM):\n        matrix = self.dtm.to_df()\n        matrix.index.name = \"terms\"\n        matrix = matrix.T\n    elif isinstance(self.dtm, list):  # Added handling for list input\n        matrix = np.array(self.dtm)  # Convert list to numpy array\n    else:\n        matrix = self.dtm\n\n    # Now, `matrix` will always be a pandas DataFrame or a numpy array when we check `shape`\n    if matrix.shape[0] &lt; 2:\n        raise LexosException(\n            \"The document-term matrix must have more than one document.\"\n        )\n    return matrix\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.show","title":"<code>show() -&gt; None</code>","text":"<p>Show the figure.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Show the figure.\"\"\"  # End of show docstring\n    if self.fig is None:\n        raise LexosException(\n            \"You must call the instance before showing the figure.\"\n        )\n    self.fig.show(config=self.config)\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.to_html","title":"<code>to_html(**kwargs)</code>","text":"<p>Create an HTML representation of the figure.</p> <p>Wrapper from the Plotly Figure to_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def to_html(self, **kwargs):\n    \"\"\"Create an HTML representation of the figure.\n\n    Wrapper from the Plotly Figure to_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"  # End of to_html docstring\n    if self.fig is None:\n        raise LexosException(\"You must call the instance before generating HTML.\")\n    return self.fig.to_html(**kwargs)\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.to_image","title":"<code>to_image(**kwargs)</code>","text":"<p>Create a static image of the figure.</p> <p>Wrapper from the Plotly Figure to_image method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>def to_image(self, **kwargs):\n    \"\"\"Create a static image of the figure.\n\n    Wrapper from the Plotly Figure to_image method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"  # End of to_image docstring\n    if self.fig is None:\n        raise LexosException(\n            \"You must call the instance before generating an image.\"\n        )\n    return self.fig.to_image(**kwargs)\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.write_html","title":"<code>write_html(path: Path | str, **kwargs)</code>","text":"<p>Save an HTML representation of the figure to disk.</p> <p>Wrapper from the Plotly Figure write_html method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>@validate_call(config=model_config)\ndef write_html(self, path: Path | str, **kwargs):\n    \"\"\"Save an HTML representation of the figure to disk.\n\n    Wrapper from the Plotly Figure write_html method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\"You must call the instance before saving the figure.\")\n    # Removed: if \"file\" in kwargs: kwargs[\"file\"] = path\n    self.fig.write_html(\n        str(path), **kwargs\n    )  # Convert path to string for write_html\n</code></pre>"},{"location":"api/cluster/plotly_dendrogram/#lexos.cluster.plotly_dendrogram.PlotlyDendrogram.write_image","title":"<code>write_image(path: Path | str, **kwargs)</code>","text":"<p>Save a static image of the figure to disk.</p> <p>Wrapper from the Plotly Figure write_image method. See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.</p> Source code in <code>lexos/cluster/plotly_dendrogram.py</code> <pre><code>@validate_call(config=model_config)\ndef write_image(self, path: Path | str, **kwargs):\n    \"\"\"Save a static image of the figure to disk.\n\n    Wrapper from the Plotly Figure write_image method.\n    See https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\"You must call the instance before saving the figure.\")\n    # Removed: if \"file\" in kwargs: kwargs[\"file\"] = path\n    self.fig.write_image(\n        str(path), **kwargs\n    )  # Convert path to string for write_image\n</code></pre>"},{"location":"api/cluster/sync_script/","title":"API Documentation: <code>sync_script.py</code>","text":""},{"location":"api/cluster/sync_script/#lexos.cluster.sync_script.SYNC_SCRIPT","title":"<code>SYNC_SCRIPT = \"\\n    &lt;script&gt;\\n    document.addEventListener('DOMContentLoaded', function() {\\n        var plotDiv = document.querySelector('.plotly-graph-div');\\n        if (plotDiv) {\\n            var isUpdating = false;\\n\\n            // Store original ranges and mapping information\\n            var originalRanges = {\\n                heatmapX: null,\\n                heatmapY: null,\\n                colDendX: null,\\n                rowDendY: null\\n            };\\n\\n            // Store the number of data points for proper scaling\\n            var dataInfo = {\\n                numRows: 50,  // Number of rows in heatmap\\n                numCols: 4    // Number of columns in heatmap\\n            };\\n\\n            // Initialize original ranges\\n            setTimeout(function() {\\n                var layout = plotDiv.layout;\\n                originalRanges.heatmapX = [0, dataInfo.numCols - 1];\\n                originalRanges.heatmapY = [0, dataInfo.numRows - 1];\\n                originalRanges.colDendX = layout.xaxis2.range || [0, 45];\\n                originalRanges.rowDendY = layout.yaxis3.range || [0, 500];\\n            }, 100);\\n\\n            plotDiv.on('plotly_relayout', function(eventData) {\\n                if (isUpdating) return;\\n                isUpdating = true;\\n\\n                var updates = {};\\n                var hasUpdates = false;\\n\\n                // Handle heatmap x-axis changes\\n                if (eventData['xaxis4.range[0]'] !== undefined) {\\n                    var heatmapXRange = [eventData['xaxis4.range[0]'], eventData['xaxis4.range[1]']];\\n\\n                    // Calculate proportional range for column dendrogram\\n                    var heatmapWidth = originalRanges.heatmapX[1] - originalRanges.heatmapX[0];\\n                    var dendXWidth = originalRanges.colDendX[1] - originalRanges.colDendX[0];\\n\\n                    var startProp = (heatmapXRange[0] - originalRanges.heatmapX[0]) / heatmapWidth;\\n                    var endProp = (heatmapXRange[1] - originalRanges.heatmapX[0]) / heatmapWidth;\\n\\n                    var dendXStart = originalRanges.colDendX[0] + (startProp * dendXWidth);\\n                    var dendXEnd = originalRanges.colDendX[0] + (endProp * dendXWidth);\\n\\n                    updates['xaxis2.range'] = [dendXStart, dendXEnd];\\n                    hasUpdates = true;\\n                }\\n\\n                // Handle heatmap y-axis changes\\n                if (eventData['yaxis4.range[0]'] !== undefined) {\\n                    var heatmapYRange = [eventData['yaxis4.range[0]'], eventData['yaxis4.range[1]']];\\n\\n                    // Calculate proportional range for row dendrogram\\n                    var heatmapHeight = originalRanges.heatmapY[1] - originalRanges.heatmapY[0];\\n                    var dendYHeight = originalRanges.rowDendY[1] - originalRanges.rowDendY[0];\\n\\n                    var startProp = (heatmapYRange[0] - originalRanges.heatmapY[0]) / heatmapHeight;\\n                    var endProp = (heatmapYRange[1] - originalRanges.heatmapY[0]) / heatmapHeight;\\n\\n                    var dendYStart = originalRanges.rowDendY[0] + (startProp * dendYHeight);\\n                    var dendYEnd = originalRanges.rowDendY[0] + (endProp * dendYHeight);\\n\\n                    updates['yaxis3.range'] = [dendYStart, dendYEnd];\\n                    hasUpdates = true;\\n                }\\n\\n                // Handle column dendrogram changes\\n                if (eventData['xaxis2.range[0]'] !== undefined) {\\n                    var dendXRange = [eventData['xaxis2.range[0]'], eventData['xaxis2.range[1]']];\\n\\n                    var dendXWidth = originalRanges.colDendX[1] - originalRanges.colDendX[0];\\n                    var heatmapWidth = originalRanges.heatmapX[1] - originalRanges.heatmapX[0];\\n\\n                    var startProp = (dendXRange[0] - originalRanges.colDendX[0]) / dendXWidth;\\n                    var endProp = (dendXRange[1] - originalRanges.colDendX[0]) / dendXWidth;\\n\\n                    var heatmapXStart = originalRanges.heatmapX[0] + (startProp * heatmapWidth);\\n                    var heatmapXEnd = originalRanges.heatmapX[0] + (endProp * heatmapWidth);\\n\\n                    updates['xaxis4.range'] = [heatmapXStart, heatmapXEnd];\\n                    hasUpdates = true;\\n                }\\n\\n                // Handle row dendrogram changes - FIXED VERSION\\n                if (eventData['yaxis3.range[0]'] !== undefined) {\\n                    var dendYRange = [eventData['yaxis3.range[0]'], eventData['yaxis3.range[1]']];\\n\\n                    var dendYHeight = originalRanges.rowDendY[1] - originalRanges.rowDendY[0];\\n                    var heatmapHeight = originalRanges.heatmapY[1] - originalRanges.heatmapY[0];\\n\\n                    // Calculate proportion based on the dendrogram's coordinate system\\n                    var startProp = (dendYRange[0] - originalRanges.rowDendY[0]) / dendYHeight;\\n                    var endProp = (dendYRange[1] - originalRanges.rowDendY[0]) / dendYHeight;\\n\\n                    // Map to heatmap coordinates, but consider the reversed y-axis\\n                    // The dendrogram y-axis is NOT reversed, but heatmap y-axis IS reversed\\n                    var heatmapYStart = originalRanges.heatmapY[0] + ((1 - endProp) * heatmapHeight);\\n                    var heatmapYEnd = originalRanges.heatmapY[0] + ((1 - startProp) * heatmapHeight);\\n\\n                    // Ensure we don't go outside bounds\\n                    heatmapYStart = Math.max(originalRanges.heatmapY[0], Math.min(originalRanges.heatmapY[1], heatmapYStart));\\n                    heatmapYEnd = Math.max(originalRanges.heatmapY[0], Math.min(originalRanges.heatmapY[1], heatmapYEnd));\\n\\n                    updates['yaxis4.range'] = [heatmapYStart, heatmapYEnd];\\n                    hasUpdates = true;\\n                }\\n\\n                if (hasUpdates) {\\n                    Plotly.relayout(plotDiv, updates).then(function() {\\n                        isUpdating = false;\\n                    });\\n                } else {\\n                    isUpdating = false;\\n                }\\n            });\\n\\n            // Handle selection events specifically\\n            plotDiv.on('plotly_selected', function(eventData) {\\n                if (isUpdating || !eventData || !eventData.range) return;\\n                isUpdating = true;\\n\\n                var updates = {};\\n                var range = eventData.range;\\n\\n                // Handle row dendrogram selection (yaxis3)\\n                if (range.y &amp;&amp; eventData.points &amp;&amp; eventData.points[0] &amp;&amp; eventData.points[0].yaxis === 'y3') {\\n                    var dendYRange = [range.y[0], range.y[1]];\\n\\n                    var dendYHeight = originalRanges.rowDendY[1] - originalRanges.rowDendY[0];\\n                    var heatmapHeight = originalRanges.heatmapY[1] - originalRanges.heatmapY[0];\\n\\n                    var startProp = (dendYRange[0] - originalRanges.rowDendY[0]) / dendYHeight;\\n                    var endProp = (dendYRange[1] - originalRanges.rowDendY[0]) / dendYHeight;\\n\\n                    // Handle reversed y-axis mapping\\n                    var heatmapYStart = originalRanges.heatmapY[0] + ((1 - endProp) * heatmapHeight);\\n                    var heatmapYEnd = originalRanges.heatmapY[0] + ((1 - startProp) * heatmapHeight);\\n\\n                    updates['yaxis3.range'] = dendYRange;\\n                    updates['yaxis4.range'] = [heatmapYStart, heatmapYEnd];\\n\\n                    Plotly.relayout(plotDiv, updates).then(function() {\\n                        isUpdating = false;\\n                    });\\n                } else {\\n                    isUpdating = false;\\n                }\\n            });\\n        }\\n    });\\n    &lt;/script&gt;\\n\"</code>  <code>module-attribute</code>","text":""},{"location":"api/corpus/","title":"Corpus","text":"<p>The <code>corpus</code> module provides functionality for document management and statistical analysis in the Lexos ecosystem. It provides centralized storage, metadata management, and inter-module communication capabilities that enable seamless integration with analysis modules. By default, it is entirely file-based; however, there is an option to manage a corpus database with SQLite.</p>"},{"location":"api/corpus/#core-classes","title":"Core Classes","text":""},{"location":"api/corpus/#corpus-corpuspy","title":"<code>Corpus</code> (<code>corpus.py</code>)","text":"<p>The <code>corpus</code> module main container for managing collections of documents. Provides document storage, metadata management, and inter-module communication capabilities.</p>"},{"location":"api/corpus/#record-recordpy","title":"<code>Record</code> (<code>record.py</code>)","text":"<p>The <code>record</code> module implements an individual document container with robust metadata and serialization capabilities.</p>"},{"location":"api/corpus/#corpusstats-corpus_statspy","title":"<code>CorpusStats</code> (<code>corpus_stats.py</code>)","text":"<p>The <code>CorpusStats</code> module provides methods for generating statistics about a corpus.</p>"},{"location":"api/corpus/#lexosmodelcache-and-recordsdict-utilspy","title":"<code>LexosModelCache</code> and <code>RecordsDict</code> (<code>utils.py</code>)","text":"<p>The <code>utils</code> module provides utility classes for efficient model management and type-safe record storage.</p>"},{"location":"api/corpus/#sqlite-database","title":"SQLite Database","text":"<p>Database management is implemented in two modules:</p>"},{"location":"api/corpus/#sqlitebackend-databasepy","title":"<code>SQLiteBackend</code> (database.py)","text":"<p>The <code>database</code> module provides the main database functionality.</p>"},{"location":"api/corpus/#sqlitecorpus-integrationpy","title":"<code>SQLiteCorpus</code> (integration.py)","text":"<p>The <code>integration</code> module the handler for integration with the main <code>corpus</code> API.</p>"},{"location":"api/corpus/#corpus-analysis-report","title":"Corpus Analysis Report","text":"<p>The <code>corpus_analysis_report</code> module provides a helper function for generating a comprehensive analysis of the contents of a <code>corpus</code> instance.</p>"},{"location":"api/corpus/corpus/","title":"corpus","text":""},{"location":"api/corpus/corpus/#module-description","title":"Module Description","text":"<p>The Lexos Corpus module is used to manage, search, and analyze text collections. Whilst you can easily pass documents loaded from files or assigned in memory to any Lexos tool, the Corpus module provides useful ways of managing your documents, especially for larger collections. Think of a corpus as a smart filing cabinet for your texts. Each document in your corpus is wrapped in a <code>Record</code> object \u2014 a container that holds not just the text itself, but also metadata (like author or date) and optional linguistic analysis.</p> <p>The corpus module allows you to serialize and de-serialize your records to disk, generate statistics about your documents, and activate and de-active records for analysis, and filter and search your documents.</p> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus","title":"<code>Corpus</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of Record objects.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>corpus_dir</code>                 (<code>str</code>)             </li> <li> <code>corpus_metadata_file</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>records</code>                 (<code>RecordsDict</code>)             </li> <li> <code>names</code>                 (<code>dict[str, list[str]]</code>)             </li> <li> <code>meta</code>                 (<code>dict[str, Any]</code>)             </li> <li> <code>analysis_results</code>                 (<code>dict[str, dict[str, Any]]</code>)             </li> <li> <code>model_cache</code>                 (<code>LexosModelCache</code>)             </li> <li> <code>num_active_docs</code>                 (<code>int</code>)             </li> <li> <code>num_docs</code>                 (<code>int</code>)             </li> <li> <code>num_terms</code>                 (<code>int</code>)             </li> <li> <code>num_tokens</code>                 (<code>int</code>)             </li> <li> <code>terms</code>                 (<code>set</code>)             </li> </ul> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>class Corpus(BaseModel):\n    \"\"\"A collection of Record objects.\"\"\"\n\n    corpus_dir: str = Field(\n        \"corpus\", description=\"The path to the directory where the corpus is stored.\"\n    )\n    corpus_metadata_file: str = Field(\n        \"corpus_metadata.json\",\n        description=\"The name of the corpus metadata file.\",\n    )\n    name: str = Field(None, description=\"The name of the corpus.\")\n    records: RecordsDict = Field({}, description=\"Dictionary of records in the corpus.\")\n    names: dict[str, list[str]] = Field(default_factory=dict)\n    meta: dict[str, Any] = Field(\n        {},\n        description=\"Metadata dictionary for arbitrary metadata relating to the corpus.\",\n    )\n    analysis_results: dict[str, dict[str, Any]] = Field(\n        default_factory=dict,\n        description=\"Storage for results from external analysis modules (kmeans, topwords, kwic, etc.)\",\n    )\n    model_cache: LexosModelCache = Field(\n        LexosModelCache(),\n        description=\"A cache for spaCy models used in the Corpus.\",\n        exclude=True,\n    )\n    num_active_docs: int = Field(\n        0, description=\"Number of active records in the corpus.\"\n    )\n    num_docs: int = Field(0, description=\"Total number of records in the corpus.\")\n    num_terms: int = Field(0, description=\"Total number of unique terms in the corpus.\")\n    num_tokens: int = Field(0, description=\"Total number of tokens in the corpus.\")\n    terms: set = Field(set(), description=\"Set of unique terms in the corpus.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data):\n        \"\"\"Initialise the Corpus with a data directory and a metadata file.\"\"\"\n        super().__init__(**data)\n        corpus_dir = Path(self.corpus_dir)\n        Path(corpus_dir / \"data\").mkdir(parents=True, exist_ok=True)\n\n        # Load existing metadata if it exists, otherwise create new\n        metadata_file = corpus_dir / self.corpus_metadata_file\n        if metadata_file.exists():\n            # Load existing metadata to preserve record info\n            existing_metadata = srsly.read_json(metadata_file)\n            # Preserve the 'meta' dict which contains record metadata\n            if \"meta\" in existing_metadata and existing_metadata[\"meta\"]:\n                self.meta = existing_metadata[\"meta\"]\n\n        # NOTE: We use model_dump() on the Corpus model here. The Corpus\n        # computed fields (e.g., `terms` is a set) are safe to serialize\n        # and do not raise the `LexosException`. We explicitly convert\n        # `terms` to a list to make it JSON-serializable. If future\n        # computed fields are added to Corpus that rely on external state\n        # or can raise, this call should be revised to exclude those fields\n        # (e.g., model_dump(exclude=[...])).\n        data = self.model_dump()\n        data[\"terms\"] = list(data[\"terms\"])\n        srsly.write_json(metadata_file, data)\n        msg.good(\"Corpus created.\")\n\n    def __iter__(self) -&gt; Iterable[Record]:\n        \"\"\"Make the corpus iterable.\n\n        Returns:\n            Iterator[Record]: An iterator over the Record objects in the corpus.\n        \"\"\"\n        return iter(self.records.values())\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the Corpus.\"\"\"\n        fields = {field: getattr(self, field) for field in self.model_fields_set}\n        field_list = [f\"{k}={v}\" for k, v in fields.items()]\n        rep = f\"Corpus({', '.join(sorted(field_list))})\"\n        return rep\n\n    @property\n    def active_terms(self) -&gt; set:\n        \"\"\"Return the set of active terms in the Corpus.\n\n        Returns:\n            set: A set of active term strings found in active parsed records.\n        \"\"\"\n        active_terms = set()\n        for record in self.records.values():\n            if record.is_parsed and record.is_active:\n                active_terms.update(record.terms.keys())\n        return active_terms\n\n    @property\n    def meta_df(self) -&gt; pd.DataFrame:\n        \"\"\"Return a DataFrame of the Corpus metadata.\"\"\"\n        if not self.meta:\n            raise LexosException(\"No metadata available in the Corpus.\")\n        df = pd.DataFrame([self.meta])\n        df.fillna(\"\", inplace=True)\n        return df\n\n    @property\n    def num_active_tokens(self) -&gt; int:\n        \"\"\"Return the number of active tokens in the Corpus.\n\n        Returns:\n            int: The total number of tokens in active parsed records.\n        \"\"\"\n        if len(self.active_terms) == 0:\n            return 0\n        return sum(\n            record.num_tokens()\n            for record in self.records.values()\n            if record.is_active and record.is_parsed\n        )\n\n    @property\n    def num_active_terms(self) -&gt; int:\n        \"\"\"Return the number of active terms in the Corpus.\"\"\"\n        if len(self.active_terms) == 0:\n            return 0\n        return len(self.active_terms)\n\n    def _add_to_corpus(self, record: Record, cache: Optional[bool] = False) -&gt; None:\n        \"\"\"Add a record to the Corpus.\n\n        Args:\n            record (Record): A Record doc.\n            cache (Optional[bool]): Whether to cache the record. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        # Update corpus records table\n        # We intentionally exclude computed fields here when dumping a\n        # Record for meta storage because those computed properties (e.g.,\n        # `terms`, `text`, `tokens`) may attempt to evaluate state-dependent\n        # computed values that can raise for unparsed records. By explicitly\n        # excluding them and then annotating `num_tokens`/`num_terms` using\n        # guarded access below, we avoid calling computed fields on records\n        # that are not parsed.\n        meta = record.model_dump(\n            exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n        )\n        # Ensure ID is always string for JSON serialization (redundant with mode=\"json\" but kept for clarity)\n        if \"id\" in meta:\n            meta[\"id\"] = str(meta[\"id\"])\n        num_tokens = record.num_tokens() if record.is_parsed else 0\n        num_terms = record.num_terms() if record.is_parsed else 0\n        meta[\"num_tokens\"] = num_tokens\n        meta[\"num_terms\"] = num_terms\n        # Use string ID as key to avoid UUID serialization issues\n        self.meta[str(record.id)] = meta\n\n        # Save the record to disk -- currently, this is always done\n        corpus_dir = Path(self.corpus_dir)\n        filename = f\"{record.id}.bin\"\n        filepath = corpus_dir / \"data\" / filename\n        record.meta[\"filename\"] = str(filename)\n        record.meta[\"filepath\"] = str(filepath)\n        record.to_disk(record.meta[\"filepath\"])\n\n        # Update the Corpus records dictionary\n        record_id_str = str(record.id)\n        self.records[record_id_str] = record\n\n        # Update the Corpus names\n        if record.name not in self.names:\n            self.names[record.name] = []\n        self.names[record.name].append(str(record.id))  # Explicitly convert to string\n\n        # Update the Corpus statistics\n        self._update_corpus_state()\n\n    def _ensure_unique_name(self, name: str = None) -&gt; str:\n        \"\"\"Ensure that no names are duplicated in the Corpus.\n\n        Args:\n            name (str): The record name.\n\n        Returns:\n            A string.\n        \"\"\"\n        if not name:\n            return f\"untitled_{uuid.uuid1()}\"\n        if name in self.names:\n            return f\"{name}_{uuid.uuid1()}\"\n        return name\n\n    def _generate_unique_id(self, type: str = \"uuid4\") -&gt; str:\n        \"\"\"Generate a unique ID for the record.\n\n        Args:\n            type (str): The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".\n\n        Returns:\n            str: A unique ID for the record.\n        \"\"\"\n        if type == \"integer\":\n            # Generate an integer ID\n            return max(self.records.keys(), default=0) + 1\n        elif type == \"uuid4\":\n            # Generate initial UUID\n            new_id = str(uuid.uuid4())\n\n            # Keep generating new UUIDs until one is not in the records dic\n            while new_id in self.records:\n                new_id = str(uuid.uuid4())\n            return new_id\n        else:\n            raise LexosException(\n                f\"Invalid ID type '{type}'. Must be 'integer' or 'uuid4'.\"\n            )\n\n    def _get_by_name(self, name: str) -&gt; list[str]:\n        \"\"\"Get all record IDs from the Corpus by name.\n\n        Args:\n            name (str): The name of the record(s) to get.\n\n        Returns:\n            list[str]: A list of record IDs with the given name.\n        \"\"\"\n        if name not in self.names:\n            raise LexosException(\n                f\"Record with name {name} does not exist in the Corpus.\"\n            )\n        return self.names[name]\n\n    def _update_corpus_state(self):\n        \"\"\"Update the Corpus state after adding or removing records.\n\n        Note:\n            This method recalculates the number of records, active records,\n            terms, tokens, and unique terms in the entire Corpus.\n        \"\"\"\n        self.num_docs = len(self.records)\n        self.num_active_docs = sum(\n            1 for r in self.records.values() if r and r.is_active\n        )\n        self.num_terms = sum(\n            r.num_terms() for r in self.records.values() if r and r.is_parsed\n        )\n        self.num_tokens = sum(\n            r.num_tokens() for r in self.records.values() if r and r.is_parsed\n        )\n        # We call model_dump() on Corpus to create a JSON of the corpus\n        # metadata. This excludes Record-specific computed fields and the\n        # `records` mapping so we only write top-level corpus metadata.\n        # In particular, Record-computed fields are excluded so we don't\n        # force evaluation across records which could trigger exceptions\n        # for unparsed records.\n        corpus_data = self.model_dump(\n            exclude=[\"content\", \"terms\", \"text\", \"tokens\", \"records\"]\n        )\n        # Convert any remaining UUIDs to strings\n        for key, value in corpus_data.items():\n            if hasattr(value, \"hex\"):  # UUID objects have .hex attribute\n                corpus_data[key] = str(value)\n\n        srsly.write_json(\n            Path(self.corpus_dir) / self.corpus_metadata_file,\n            corpus_data,\n        )\n\n    def _sanitize_metadata(self, metadata: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Convert non-JSON-serializable types to strings in metadata.\n\n        Args:\n            metadata: Original metadata dictionary\n\n        Returns:\n            Sanitized metadata dictionary with JSON-serializable values\n        \"\"\"\n        from datetime import date, datetime\n        from pathlib import Path\n        from uuid import UUID\n\n        sanitized = {}\n        for key, value in metadata.items():\n            if isinstance(value, UUID):\n                sanitized[key] = str(value)\n            elif isinstance(value, (datetime, date)):\n                sanitized[key] = value.isoformat()\n            elif isinstance(value, Path):\n                sanitized[key] = str(value)\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_metadata(value)  # Recursive\n            elif isinstance(value, list):\n                sanitized[key] = [\n                    self._sanitize_metadata({\"item\": item})[\"item\"]\n                    if isinstance(item, dict)\n                    else str(item)\n                    if isinstance(item, (UUID, datetime, date, Path))\n                    else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n\n        return sanitized\n\n    @validate_call(config=model_config)\n    def add(\n        self,\n        content: Doc | Record | str | list[Doc | Record | str],\n        name: Optional[str] = None,\n        is_active: Optional[bool] = True,\n        model: Optional[str] = None,\n        extensions: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        id_type: Optional[str] = \"uuid4\",\n        cache: Optional[bool] = False,\n    ):\n        \"\"\"Add a record to the Corpus.\n\n        Args:\n            content (Doc | Record | str | list[Doc | Record | str]): A text string, Record, or a spaCy Doc, or a list of any of these.\n            name (str): A name for the record.\n            is_active (bool): Whether or not the record is active.\n            model (str): The name of the language model used to parse the record (optional).\n            extensions (list[str]): A list of extension names to add to the record.\n            metadata (dict[str, Any]): A dict containing any metadata.\n            id_type (str): The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".\n            cache (bool): Whether or not to cache the record.\n        \"\"\"\n        # Sanitize metadata to ensure JSON-serializable types\n        if metadata is not None:\n            metadata = self._sanitize_metadata(metadata)\n\n        # If content is not a list, treat it as a single item\n        if isinstance(content, (Doc, Record, str)):\n            items = [content]\n        else:\n            items = list(content)\n\n        for item in items:\n            # Generate a unique ID for the record\n            new_id = self._generate_unique_id(type=id_type)\n\n            # Keep generating new UUIDs until one is not in the records dic\n            # while new_id in self.records:\n            #    new_id = str(uuid.uuid4())\n\n            if isinstance(item, Record):\n                record = item\n                if record.id and str(record.id) in self.records:\n                    raise LexosException(\n                        f\"Record with ID {record.id} already exists in the Corpus.\"\n                    )\n            else:\n                record_kwargs = dict(\n                    id=new_id,\n                    name=name,  # self._ensure_unique_name(name),\n                    is_active=is_active,\n                    content=item,\n                    model=model,\n                    data_source=None,\n                )\n                if extensions is not None:\n                    record_kwargs[\"extensions\"] = extensions\n                if metadata is not None:\n                    record_kwargs[\"meta\"] = metadata\n                record = Record(**record_kwargs)\n\n            # Add arbitrary metadata properties\n            if metadata:\n                record.meta.update(metadata)\n\n            # Add the record to the Corpus\n            self._add_to_corpus(record, cache=cache)\n\n    def _add_to_corpus_without_state_update(\n        self, record: Record, cache: Optional[bool] = False\n    ) -&gt; None:\n        \"\"\"Add a record to the Corpus without updating corpus state.\n\n        This is an internal method used for batch operations where state\n        updates are deferred until all records are added.\n\n        Args:\n            record (Record): A Record doc.\n            cache (Optional[bool]): Whether to cache the record. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        # Update corpus records table\n        meta = record.model_dump(\n            exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n        )\n        # Ensure ID is always string for JSON serialization\n        if \"id\" in meta:\n            meta[\"id\"] = str(meta[\"id\"])\n        num_tokens = record.num_tokens() if record.is_parsed else 0\n        num_terms = record.num_terms() if record.is_parsed else 0\n        meta[\"num_tokens\"] = num_tokens\n        meta[\"num_terms\"] = num_terms\n        # Use string ID as key\n        self.meta[str(record.id)] = meta\n\n        # Save the record to disk\n        corpus_dir = Path(self.corpus_dir)\n        filename = f\"{record.id}.bin\"\n        filepath = corpus_dir / \"data\" / filename\n        record.meta[\"filename\"] = str(filename)\n        record.meta[\"filepath\"] = str(filepath)\n        record.to_disk(record.meta[\"filepath\"])\n\n        # Update the Corpus records dictionary\n        record_id_str = str(record.id)\n        self.records[record_id_str] = record\n\n        # Update the Corpus names\n        if record.name not in self.names:\n            self.names[record.name] = []\n        self.names[record.name].append(str(record.id))\n\n        # Note: _update_corpus_state() is NOT called here\n\n    def add_from_files(\n        self,\n        paths: Path | str | list[Path | str],\n        max_workers: Optional[int] = None,\n        worker_strategy: str = \"auto\",\n        batch_size: int = 100,\n        show_progress: bool = True,\n        name_template: Optional[str] = None,\n        is_active: bool = True,\n        model: Optional[str] = None,\n        extensions: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        id_type: str = \"uuid4\",\n    ) -&gt; None:\n        \"\"\"Load files directly into corpus using parallel I/O.\n\n        This method streams files into the corpus without holding all\n        content in memory, making it suitable for very large datasets.\n        Files are loaded in parallel using the ParallelLoader with all\n        its optimization features (smart file ordering, auto-tuning, etc.).\n\n        State updates are deferred until all files are loaded for optimal\n        performance.\n\n        Args:\n            paths (Path | str | list[Path | str]): File paths or directories to load.\n            max_workers (Optional[int]): Maximum number of worker threads.\n                If None, auto-calculated based on worker_strategy.\n            worker_strategy (str): Worker allocation strategy. Options:\n                - \"auto\": Analyzes file types and chooses optimal strategy (default)\n                - \"io_bound\": More workers for I/O-intensive operations\n                - \"cpu_bound\": Fewer workers for CPU-intensive operations\n                - \"balanced\": Middle ground between I/O and CPU\n            batch_size (int): Number of files to process in each batch. Default 100.\n            show_progress (bool): Whether to show progress bar. Default True.\n            name_template (Optional[str]): Template for generating record names.\n                Can include {filename}, {stem}, {index}. If None, uses filename stem.\n            is_active (bool): Whether records should be marked as active. Default True.\n            model (Optional[str]): Name of language model used to parse records.\n            extensions (Optional[list[str]]): List of extension names to add to records.\n            metadata (Optional[dict[str, Any]]): Metadata to add to all records.\n            id_type (str): Type of ID to generate (\"integer\" or \"uuid4\"). Default \"uuid4\".\n\n        Example:\n            ```python\n            corpus = Corpus(\"my_corpus\")\n            # Load all text files from a directory\n            corpus.add_from_files(\"path/to/texts/\")\n            # With custom naming\n            corpus.add_from_files(\n                [\"file1.txt\", \"file2.txt\"],\n                name_template=\"{stem}_{index}\",\n                metadata={\"source\": \"collection_a\"}\n            )\n            ```\n        \"\"\"\n        from lexos.io.parallel_loader import ParallelLoader\n\n        # Sanitize metadata if provided\n        if metadata is not None:\n            metadata = self._sanitize_metadata(metadata)\n\n        # Create ParallelLoader with specified settings\n        loader = ParallelLoader(\n            max_workers=max_workers,\n            worker_strategy=worker_strategy,\n            batch_size=batch_size,\n            show_progress=show_progress,\n        )\n\n        # Track for error reporting\n        loaded_count = 0\n        error_count = 0\n        errors = []\n\n        # Stream files and add to corpus\n        for index, (path, name, mime_type, text, error) in enumerate(\n            loader.load_streaming(paths), start=1\n        ):\n            if error:\n                error_count += 1\n                errors.append((path, error))\n                continue\n\n            # Generate record name from template or use default\n            if name_template:\n                record_name = name_template.format(\n                    filename=Path(path).name, stem=name, index=index\n                )\n            else:\n                record_name = name\n\n            # Generate unique ID\n            record_id = self._generate_unique_id(type=id_type)\n\n            # Create record kwargs\n            record_kwargs = dict(\n                id=record_id,\n                name=record_name,\n                is_active=is_active,\n                content=text,\n                model=model,\n                data_source=str(path),\n            )\n\n            if extensions is not None:\n                record_kwargs[\"extensions\"] = extensions\n\n            if metadata is not None:\n                record_kwargs[\"meta\"] = metadata.copy()\n\n            # Create and add record without updating state\n            record = Record(**record_kwargs)\n            self._add_to_corpus_without_state_update(record)\n            loaded_count += 1\n\n        # Update corpus state once at the end\n        self._update_corpus_state()\n\n        # Report results\n        from wasabi import msg\n\n        msg.good(f\"Loaded {loaded_count} files into corpus. Errors: {error_count}\")\n\n        if errors and error_count &lt;= 10:  # Show first 10 errors\n            msg.warn(\"Errors encountered:\")\n            for path, error in errors[:10]:\n                msg.fail(f\"  {path}: {error}\")\n\n    @validate_call(config=model_config)\n    def filter_records(self, **metadata_filters: Any) -&gt; list[Record]:\n        \"\"\"Return records matching metadata key-value pairs.\n\n        Args:\n            **metadata_filters (Any): Arbitrary metadata fields and their required values.\n\n        Returns:\n            List of Record objects matching all metadata criteria.\n        \"\"\"\n        results = []\n        for record in self.records.values():\n            if not hasattr(record, \"meta\") or not isinstance(record.meta, dict):\n                continue\n            match = True\n            for key, value in metadata_filters.items():\n                if key not in record.meta or record.meta[key] != value:\n                    match = False\n                    break\n            if match:\n                results.append(record)\n        return results\n\n    @validate_call(config=model_config)\n    def get(\n        self,\n        id: Optional[str | list[str]] = None,\n        name: Optional[str | list[str]] = None,\n    ) -&gt; Record | list[Record]:\n        \"\"\"Get a record from the Corpus by ID.\n\n        Tries to get the record from memory; otherwise loads it from file.\n\n        Args:\n            id (str | list[str]): A record id or list of ids from the Corpus records.\n            name (str | list[str]): A record name or list of names from the Corpus records.\n\n        Returns:\n            Record | list[Record]: The record(s) with the given ID(s) or name(s).\n        \"\"\"\n        # Ensure either id or name is provided\n        if not id and not name:\n            raise LexosException(\n                \"Must provide either an ID or a name to remove a record.\"\n            )\n\n        # Ensure id is a list\n        if isinstance(id, str):\n            ids = [id]\n        elif isinstance(id, list):\n            ids = id\n        else:\n            ids = []\n\n        # If name is provided, get the IDs from the name(s)\n        if name and not id:\n            if isinstance(name, str):\n                name = [name]\n            ids = []\n            for n in name:\n                ids.extend(self._get_by_name(n))\n\n        result = []\n        for id in ids:\n            # If the id is in the Corpus cache, return the record\n            if id in self.records.keys():\n                result.append(self.records[id])\n\n            # Otherwise, load the record from file\n            else:\n                record = self.records[id]\n                result.append(\n                    record._from_disk(\n                        record.meta[\"filepath\"], record.model, self.model_cache\n                    )\n                )\n        if len(result) == 1:\n            return result[0]\n        return result\n\n    @validate_call(config=model_config)\n    def get_stats(\n        self,\n        active_only: bool = True,\n        type: str = \"tokens\",\n        min_df: int | None = None,\n        max_df: int | None = None,\n        max_n_terms: int | None = None,\n        token_list: list[tuple[str, str, list[str]]] = None,\n    ) -&gt; CorpusStats:\n        \"\"\"Get the Corpus statistics.\n\n        Args:\n            active_only (bool): If True, only include active records in the statistics. Defaults to True.\n            type (str): The type of statistics to return. Can be \"tokens\" or \"characters\". Defaults to \"tokens\".\n            min_df (int | None): Minimum record frequency for terms to be included in the statistics. Defaults to None.\n            max_df (int | None): Maximum record frequency for terms to be included in the statistics. Defaults to None.\n            max_n_terms (int | None): Maximum number of terms to include in the statistics. Defaults to None.\n            token_list (list[tuple[str, str, list[str]]]): A list of tuples containing the record ID, name, and tokens. If not provided, it will be generated from the records.\n\n        Returns:\n            CorpusStats: An object containing the Corpus statistics.\n        \"\"\"\n\n        def get_token_strings(record: Record) -&gt; list[str]:\n            \"\"\"Get the token strings from a record.\n\n            Args:\n                record (Record): The Record object to get the token strings from.\n\n            Returns:\n                list[str]: A list of token strings from the record.\n            \"\"\"\n            if record.is_parsed:\n                return [token.text for token in record.content]\n            # We could use xx_sent_ud_sm, but for now, split on whitespace\n            else:\n                return record.content.split()\n\n        if not token_list:\n            # Filter the records to only include active ones\n            if active_only:\n                records = [\n                    record for record in self.records.values() if record.is_active\n                ]\n            # Otherwise, include all records\n            else:\n                records = list(self.records.values())\n\n            # Get the token list from the records\n            if type == \"tokens\":\n                token_list = [\n                    (str(record.id), record.name, get_token_strings(record))\n                    for record in records\n                ]\n            elif type == \"characters\":\n                token_list = [\n                    (str(record.id), record.name, list(record.content.text))\n                    if record.is_parsed\n                    else (str(record.id), record.name, list(record.content))\n                    for record in records\n                ]\n\n        return CorpusStats(\n            docs=token_list, min_df=min_df, max_df=max_df, max_n_terms=max_n_terms\n        )\n\n    @validate_call(config=model_config)\n    def load(\n        self,\n        path: Path | str = None,\n        corpus_dir: Optional[Path | str] = None,\n        cache: Optional[bool] = False,\n    ) -&gt; None:\n        \"\"\"Load a Corpus from a zip archive or directory.\n\n        Args:\n            path (Path | str): The path of the zip archive or directory to load.\n            corpus_dir (Optional[Path | str]): The directory where the Corpus is to be unzipped.\n            cache (Optional[bool]): Whether to cache the records in the Corpus. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        # Ensure that a corpus_dir exists, or create one if it doesn't\n        if not corpus_dir:\n            corpus_dir = Path(self.corpus_dir)\n            corpus_dir.mkdir(parents=True, exist_ok=True)\n\n        # If the path is a file, try to unpack it as a zip archive\n        if Path(path).is_file():\n            try:\n                shutil.unpack_archive(path, corpus_dir)\n            except shutil.ReadError as e:\n                raise LexosException(\n                    f\"Failed to unpack archive: {e}. Ensure the file is a valid zip archive.\"\n                )\n\n        # Open the metadata file and load the metadata\n        metadata_path = corpus_dir / self.corpus_metadata_file\n        metadata = srsly.read_json(metadata_path)\n        for key, value in metadata.items():\n            setattr(self, key, value)\n\n        # If cache is set, load the records into the model cache\n        if cache:\n            for record in self.records.values():\n                if isinstance(record, Record):\n                    # Load the record from disk\n                    record.from_disk(\n                        corpus_dir / \"data\" / f\"{record.id}.bin\",\n                        model=record.model,\n                        model_cache=self.model_cache,\n                    )\n                else:\n                    raise LexosException(\n                        \"Records in the Corpus must be of type Record.\"\n                    )\n\n    @validate_call(config=model_config)\n    def save(self, path: Path | str = None) -&gt; None:\n        \"\"\"Save the Corpus as a zip archive.\n\n        Args:\n            path (Path | str): The path to save the Corpus to.\n\n        Returns:\n            None\n        \"\"\"\n        shutil.make_archive(path / f\"{self.name}\", \"zip\", self.corpus_dir)\n\n    @validate_call(config=model_config)\n    def remove(\n        self,\n        id: Optional[str | list[str]] = None,\n        name: Optional[str | list[str]] = None,\n    ) -&gt; None:\n        \"\"\"Remove a record from the corpus by ID.\n\n        Args:\n            id (str | list[str]): The ID of the record to remove.\n            name (str | list[str]): The name of the record to remove.\n\n        Returns:\n            None\n        \"\"\"\n        # Ensure either id or name is provided\n        if not id and not name:\n            raise LexosException(\n                \"Must provide either an ID or a name to remove a record.\"\n            )\n\n        # Ensure id is a list\n        if isinstance(id, str):\n            ids = [id]\n        elif isinstance(id, list):\n            ids = id\n        else:\n            ids = []\n\n        # If name is provided, get the IDs from the name(s)\n        if name and not id:\n            if isinstance(name, str):\n                name = [name]\n            ids = []\n            for n in name:\n                ids.extend(self._get_by_name(n))\n\n        for id in ids:\n            # Remove the entry from the records dictionary and names list\n            try:\n                entry = self.records.pop(id)\n            except KeyError:\n                raise LexosException(\n                    f\"Record with ID {id} does not exist in the Corpus.\"\n                )\n            try:\n                if entry.name in self.names:\n                    self.names[entry.name].remove(str(entry.id))\n                    if not self.names[entry.name]:  # Remove empty lists\n                        self.names.pop(entry.name)\n            except KeyError:\n                raise LexosException(\n                    f\"Record with name {entry.name} does not exist in the Corpus.\"\n                )\n\n        # Update the Corpus state after removing the record\n        self._update_corpus_state()\n\n    @validate_call(config=model_config)\n    def set(self, id: str, **props) -&gt; None:\n        \"\"\"Set a property or properties of a record in the Corpus.\n\n        Args:\n            id (str): A record id.\n            **props (dict): The dict containing any other properties to set.\n\n        Returns:\n            None\n        \"\"\"\n        # Get the record by ID\n        record = self.records[id]\n\n        # Save the record's filepath, thenupdate the specified properties\n        old_filepath = record.meta.get(\"filepath\", None)\n        record.set(**props)\n\n        # If the filepath has changed, delete the old file\n        if record.meta.get(\"filepath\", None) != old_filepath:\n            Path(old_filepath).unlink(missing_ok=True)\n\n        # If the record has a filepath, ensure the file is in the data directory\n        filepath = record.meta.get(\"filepath\")\n        if filepath and filepath not in str(Path(self.corpus_dir) / \"data\"):\n            record.to_disk(filepath, extensions=record.extensions)\n\n        # Update the record in the Corpus and update the corpus state\n        self.records[id] = record\n        self._update_corpus_state()\n\n    @validate_call(config=model_config)\n    def term_counts(\n        self, n: Optional[int] = 10, most_common: Optional[bool] = True\n    ) -&gt; Counter:\n        \"\"\"Get a Counter with the most common Corpus term counts.\n\n        Args:\n            n (Optional[int]): The number of most common terms to return. Defaults to 10.\n            most_common (Optional[bool]): If True, return the n most common terms; otherwise, return the n least common terms.\n\n        Returns:\n            A collections.Counter object containing the n most common term counts for all records in the Corpus.\n        \"\"\"\n        # Count the terms in all records\n        counter = Counter()\n        for record in self.records.values():\n            if record.is_parsed:\n                counter.update(record.terms)\n\n        # Optionally filter the results\n        if most_common and n:\n            return counter.most_common(n)\n        elif not most_common and n:\n            return counter.most_common()[: -n - 1 : -1]\n        elif most_common is False and n is None:\n            return counter.most_common()[::]\n        else:\n            return counter\n\n    @validate_call(config=model_config)\n    def to_df(\n        self, exclude: list[str] = [\"content\", \"terms\", \"tokens\"]\n    ) -&gt; pd.DataFrame:\n        \"\"\"Return a table of the Corpus records.\n\n        Args:\n            exclude (list[str]): A list of fields to exclude from the dataframe. If you wish to exclude metadata fields with the same name as model fields, you can use the prefix \"metadata_\" to avoid conflicts.\n\n        Returns:\n            pd.DataFrame: A dataframe representing the records in the Corpus.\n        \"\"\"\n        rows = []\n        for record in self.records.values():  # &lt;- Fix the duplicate\n            if record is None:  # Skip None records\n                continue\n\n            # Get model categories.\n            # NOTE: We avoid calling `model_dump()` on `Record` objects that are\n            # unparsed because Pydantic may attempt to evaluate computed fields\n            # while creating the serialized dict. Several computed properties on\n            # `Record` (e.g., `terms`, `tokens`, `num_terms`, and\n            # `num_tokens`) raise `LexosException(\"Record is not parsed.\")`\n            # when the record is not parsed. If `model_dump()` evaluates those\n            # properties for an unparsed record, it will raise and cause\n            # `to_df()` to fail. Therefore:\n            #  - For parsed records, we call `record.model_dump()` and use the\n            #    model-dump output (it includes computed fields safely).\n            #  - For unparsed records, we *do not* call `model_dump()`; we\n            #    instead build a minimal, safe `row` from stored fields and\n            #    set any computed-like values to safe defaults (empty list,\n            #    0, or empty string). This produces robust DataFrame output\n            #    for corpora that contain a mix of parsed and unparsed\n            #    records without triggering computed-field side-effects.\n            fields_that_may_raise = {\n                \"terms\",\n                \"tokens\",\n                \"num_terms\",\n                \"num_tokens\",\n                \"text\",\n            }\n            # Build a dump_exclude set to prevent model_dump from computing\n            # sensitive fields on unparsed records\n            dump_exclude = set(exclude)\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                # Parsed records: safely model_dump, excluding any user-requested fields\n                row = record.model_dump(exclude=list(dump_exclude))\n            else:\n                # Unparsed records: avoid model_dump to prevent computed property evaluation\n                base_fields = [\n                    \"id\",\n                    \"name\",\n                    \"is_active\",\n                    \"content\",\n                    \"model\",\n                    \"extensions\",\n                    \"data_source\",\n                    \"meta\",\n                ]\n                row = {}\n                for f in base_fields:\n                    if f in exclude:\n                        continue\n                    try:\n                        value = getattr(record, f, None)\n                    except Exception:\n                        # Defensive: if getattr triggers an error, skip and set None\n                        value = None\n                    # Serialize Doc-like content into text rather than bytes to keep DataFrame friendly\n                    if f == \"content\" and value is not None:\n                        try:\n                            from spacy.tokens import Doc\n\n                            if isinstance(value, Doc):\n                                value = value.text\n                        except Exception:\n                            pass\n                    # Ensure id is serialized to string to match model_dump output for parsed records\n                    if f == \"id\" and value is not None:\n                        try:\n                            value = str(value)\n                        except Exception:\n                            pass\n                    # Sanitize meta similar to model_dump\n                    if f == \"meta\" and value is not None:\n                        try:\n                            value = record._sanitize_metadata(value)\n                        except Exception:\n                            pass\n                    row[f] = value\n\n            # Patch for unparsed records: fill terms/tokens/num_terms/num_tokens/text\n            # Only if those fields are not excluded\n            if \"terms\" not in exclude:\n                if hasattr(record, \"is_parsed\") and record.is_parsed:\n                    row[\"terms\"] = list(record.terms)\n                else:\n                    row[\"terms\"] = []\n            if \"tokens\" not in exclude:\n                if hasattr(record, \"is_parsed\") and record.is_parsed:\n                    row[\"tokens\"] = record.tokens\n                else:\n                    row[\"tokens\"] = []\n            if \"num_terms\" not in exclude:\n                if hasattr(record, \"is_parsed\") and record.is_parsed:\n                    row[\"num_terms\"] = record.num_terms()\n                else:\n                    row[\"num_terms\"] = 0\n            if \"num_tokens\" not in exclude:\n                if hasattr(record, \"is_parsed\") and record.is_parsed:\n                    row[\"num_tokens\"] = record.num_tokens()\n                else:\n                    row[\"num_tokens\"] = 0\n            if \"text\" not in exclude:\n                if hasattr(record, \"is_parsed\") and record.is_parsed:\n                    row[\"text\"] = record.text\n                else:\n                    row[\"text\"] = \"\"\n\n            # Add metadata categories, respecting exclude list\n            metadata = row.pop(\"meta\", {})\n            for key, value in metadata.items():\n                # Exclude metadata fields if requested\n                if key in exclude or f\"metadata_{key}\" in exclude:\n                    continue\n                if key in row:\n                    key = f\"metadata_{key}\"\n                row[key] = value\n\n            # Append the row to the rows list\n            rows.append(row)\n\n        # Create a DataFrame from the rows\n        if rows:  # Only create DataFrame if we have data\n            df = pd.DataFrame(rows)\n            # Fill NaN with appropriate values based on column dtype\n            fill_values = {}\n            for col in df.columns:\n                if pd.api.types.is_numeric_dtype(df[col]):\n                    fill_values[col] = 0\n                elif pd.api.types.is_bool_dtype(df[col]):\n                    fill_values[col] = False\n                else:\n                    fill_values[col] = \"\"\n\n            df = df.fillna(fill_values)  # Use assignment instead of inplace\n            return df\n        else:\n            # Return empty DataFrame with basic columns if no records\n            return pd.DataFrame(columns=[\"id\", \"name\", \"is_active\"])\n\n    # =============================================================================\n    # COMMUNICATION ARCHITECTURE - Phase 1.5\n    # =============================================================================\n\n    @validate_call(config=model_config)\n    def import_analysis_results(\n        self,\n        module_name: str,\n        results_data: dict[str, Any],\n        version: str = \"1.0.0\",\n        overwrite: bool = False,\n    ) -&gt; None:\n        \"\"\"Import analysis results from external modules into corpus metadata.\n\n        Args:\n            module_name: Name of the external module (e.g., 'kmeans', 'topwords', 'kwic', 'text_classification')\n            results_data: Dictionary containing the analysis results\n            version: Version string for result versioning and compatibility\n            overwrite: Whether to overwrite existing results for this module\n\n        Note:\n            This is a framework implementation. Full functionality requires\n            peer modules to be implemented and their result schemas defined.\n\n        Returns:\n            None\n        \"\"\"\n        # TODO: Add result schema validation once peer modules are available\n        # TODO: Add proper versioning system for backward compatibility\n        # TODO: Implement result correlation capabilities across modules\n\n        if module_name in self.analysis_results and not overwrite:\n            raise ValueError(\n                f\"Results for module '{module_name}' already exist. \"\n                f\"Use overwrite=True to replace them.\"\n            )\n\n        # Basic result structure with metadata\n        self.analysis_results[module_name] = {\n            \"version\": version,\n            \"timestamp\": pd.Timestamp.now().isoformat(),\n            \"corpus_state\": {\n                \"num_docs\": self.num_docs,\n                \"num_active_docs\": self.num_active_docs,\n                \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n            },\n            \"results\": results_data,\n        }\n\n        msg.good(f\"Imported {module_name} analysis results (version {version})\")\n\n    @validate_call(config=model_config)\n    def get_analysis_results(self, module_name: str = None) -&gt; dict[str, Any]:\n        \"\"\"Retrieve analysis results from external modules.\n\n        Args:\n            module_name: Specific module name to retrieve, or None for all results\n\n        Returns:\n            Dictionary containing analysis results\n        \"\"\"\n        if module_name:\n            if module_name not in self.analysis_results:\n                raise ValueError(f\"No results found for module '{module_name}'\")\n            return self.analysis_results[module_name]\n\n        return self.analysis_results\n\n    @validate_call(config=model_config)\n    def export_statistical_fingerprint(self) -&gt; dict[str, Any]:\n        \"\"\"Export standardized statistical summary for external modules.\n\n        Returns:\n            Dictionary containing corpus statistical fingerprint for external module consumption\n\n        Note:\n            This provides the standardized API for external modules to consume corpus statistics.\n        \"\"\"\n        # TODO: Expand fingerprint based on external module requirements\n        # TODO: Add feature extraction optimized for different analysis types\n\n        try:\n            stats = self.get_stats(active_only=True)\n\n            # Core statistical fingerprint\n            fingerprint = {\n                \"corpus_metadata\": {\n                    \"name\": self.name,\n                    \"num_docs\": self.num_docs,\n                    \"num_active_docs\": self.num_active_docs,\n                    \"num_tokens\": self.num_tokens,\n                    \"num_terms\": self.num_terms,\n                    \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n                },\n                \"distribution_stats\": stats.distribution_stats,\n                \"percentiles\": stats.percentiles,\n                \"text_diversity\": stats.text_diversity_stats,\n                \"basic_stats\": {\n                    \"mean\": stats.mean,\n                    \"std\": stats.standard_deviation,\n                    \"iqr_values\": stats.iqr_values,\n                    \"iqr_bounds\": stats.iqr_bounds,\n                },\n                \"document_features\": stats.doc_stats_df.to_dict(\"records\"),\n                \"term_frequencies\": self.term_counts(\n                    n=100, most_common=True\n                ),  # Top 100 terms\n            }\n\n            return fingerprint\n\n        except Exception as e:\n            # Fallback fingerprint if CorpusStats fails\n            return {\n                \"corpus_metadata\": {\n                    \"name\": self.name,\n                    \"num_docs\": self.num_docs,\n                    \"num_active_docs\": self.num_active_docs,\n                    \"num_tokens\": self.num_tokens,\n                    \"num_terms\": self.num_terms,\n                    \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n                },\n                \"error\": f\"Statistical analysis failed: {str(e)}\",\n                \"basic_features\": {\n                    \"document_ids\": list(self.records.keys()),\n                    \"document_names\": list(self.names.keys()),\n                },\n            }\n\n    def _generate_corpus_fingerprint(self) -&gt; str:\n        \"\"\"Generate a unique fingerprint for corpus state validation.\n\n        Returns:\n            SHA256 hash representing current corpus state\n        \"\"\"\n        import hashlib\n\n        # Create fingerprint from corpus state\n        state_data = {\n            \"num_docs\": self.num_docs,\n            \"num_active_docs\": self.num_active_docs,\n            \"record_ids\": sorted(self.records.keys()),\n            \"active_record_ids\": sorted(\n                [k for k, v in self.records.items() if v and v.is_active]\n            ),\n        }\n\n        state_string = str(sorted(state_data.items()))\n        return hashlib.sha256(state_string.encode()).hexdigest()[:16]  # First 16 chars\n\n    @validate_call(config=model_config)\n    def validate_analysis_compatibility(self, module_name: str) -&gt; dict[str, Any]:\n        \"\"\"Validate if stored analysis results are compatible with current corpus state.\n\n        Args:\n            module_name: Name of the module to validate\n\n        Returns:\n            Dictionary containing validation results and recommendations\n        \"\"\"\n        if module_name not in self.analysis_results:\n            return {\n                \"compatible\": False,\n                \"reason\": f\"No analysis results found for module '{module_name}'\",\n            }\n\n        stored_results = self.analysis_results[module_name]\n        stored_state = stored_results.get(\"corpus_state\", {})\n        current_fingerprint = self._generate_corpus_fingerprint()\n        stored_fingerprint = stored_state.get(\"corpus_fingerprint\", \"\")\n\n        compatibility = {\n            \"compatible\": stored_fingerprint == current_fingerprint,\n            \"current_fingerprint\": current_fingerprint,\n            \"stored_fingerprint\": stored_fingerprint,\n            \"stored_timestamp\": stored_results.get(\"timestamp\", \"unknown\"),\n            \"stored_version\": stored_results.get(\"version\", \"unknown\"),\n        }\n\n        if not compatibility[\"compatible\"]:\n            compatibility[\"reason\"] = (\n                \"Corpus state has changed since analysis was performed\"\n            )\n            compatibility[\"recommendation\"] = (\n                f\"Re-run {module_name} analysis with current corpus state\"\n            )\n\n            # Detailed state comparison\n            compatibility[\"state_changes\"] = {\n                \"num_docs\": {\n                    \"stored\": stored_state.get(\"num_docs\", 0),\n                    \"current\": self.num_docs,\n                    \"changed\": stored_state.get(\"num_docs\", 0) != self.num_docs,\n                },\n                \"num_active_docs\": {\n                    \"stored\": stored_state.get(\"num_active_docs\", 0),\n                    \"current\": self.num_active_docs,\n                    \"changed\": stored_state.get(\"num_active_docs\", 0)\n                    != self.num_active_docs,\n                },\n            }\n\n        return compatibility\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.active_terms","title":"<code>active_terms: set</code>  <code>property</code>","text":"<p>Return the set of active terms in the Corpus.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set</code> <p>A set of active term strings found in active parsed records.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.analysis_results","title":"<code>analysis_results: dict[str, dict[str, Any]]</code>  <code>pydantic-field</code>","text":"<p>Storage for results from external analysis modules (kmeans, topwords, kwic, etc.)</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.corpus_dir","title":"<code>corpus_dir: str = 'corpus'</code>  <code>pydantic-field</code>","text":"<p>The path to the directory where the corpus is stored.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.corpus_metadata_file","title":"<code>corpus_metadata_file: str = 'corpus_metadata.json'</code>  <code>pydantic-field</code>","text":"<p>The name of the corpus metadata file.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.meta","title":"<code>meta: dict[str, Any] = {}</code>  <code>pydantic-field</code>","text":"<p>Metadata dictionary for arbitrary metadata relating to the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.meta_df","title":"<code>meta_df: pd.DataFrame</code>  <code>property</code>","text":"<p>Return a DataFrame of the Corpus metadata.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.model_cache","title":"<code>model_cache: LexosModelCache = LexosModelCache()</code>  <code>pydantic-field</code>","text":"<p>A cache for spaCy models used in the Corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.name","title":"<code>name: str = None</code>  <code>pydantic-field</code>","text":"<p>The name of the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_active_docs","title":"<code>num_active_docs: int = 0</code>  <code>pydantic-field</code>","text":"<p>Number of active records in the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_active_terms","title":"<code>num_active_terms: int</code>  <code>property</code>","text":"<p>Return the number of active terms in the Corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_active_tokens","title":"<code>num_active_tokens: int</code>  <code>property</code>","text":"<p>Return the number of active tokens in the Corpus.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total number of tokens in active parsed records.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_docs","title":"<code>num_docs: int = 0</code>  <code>pydantic-field</code>","text":"<p>Total number of records in the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_terms","title":"<code>num_terms: int = 0</code>  <code>pydantic-field</code>","text":"<p>Total number of unique terms in the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_tokens","title":"<code>num_tokens: int = 0</code>  <code>pydantic-field</code>","text":"<p>Total number of tokens in the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.records","title":"<code>records: RecordsDict = {}</code>  <code>pydantic-field</code>","text":"<p>Dictionary of records in the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.terms","title":"<code>terms: set = set()</code>  <code>pydantic-field</code>","text":"<p>Set of unique terms in the corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialise the Corpus with a data directory and a metadata file.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialise the Corpus with a data directory and a metadata file.\"\"\"\n    super().__init__(**data)\n    corpus_dir = Path(self.corpus_dir)\n    Path(corpus_dir / \"data\").mkdir(parents=True, exist_ok=True)\n\n    # Load existing metadata if it exists, otherwise create new\n    metadata_file = corpus_dir / self.corpus_metadata_file\n    if metadata_file.exists():\n        # Load existing metadata to preserve record info\n        existing_metadata = srsly.read_json(metadata_file)\n        # Preserve the 'meta' dict which contains record metadata\n        if \"meta\" in existing_metadata and existing_metadata[\"meta\"]:\n            self.meta = existing_metadata[\"meta\"]\n\n    # NOTE: We use model_dump() on the Corpus model here. The Corpus\n    # computed fields (e.g., `terms` is a set) are safe to serialize\n    # and do not raise the `LexosException`. We explicitly convert\n    # `terms` to a list to make it JSON-serializable. If future\n    # computed fields are added to Corpus that rely on external state\n    # or can raise, this call should be revised to exclude those fields\n    # (e.g., model_dump(exclude=[...])).\n    data = self.model_dump()\n    data[\"terms\"] = list(data[\"terms\"])\n    srsly.write_json(metadata_file, data)\n    msg.good(\"Corpus created.\")\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.__iter__","title":"<code>__iter__() -&gt; Iterable[Record]</code>","text":"<p>Make the corpus iterable.</p> <p>Returns:</p> Type Description <code>Iterable[Record]</code> <p>Iterator[Record]: An iterator over the Record objects in the corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __iter__(self) -&gt; Iterable[Record]:\n    \"\"\"Make the corpus iterable.\n\n    Returns:\n        Iterator[Record]: An iterator over the Record objects in the corpus.\n    \"\"\"\n    return iter(self.records.values())\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of the Corpus.\"\"\"\n    fields = {field: getattr(self, field) for field in self.model_fields_set}\n    field_list = [f\"{k}={v}\" for k, v in fields.items()]\n    rep = f\"Corpus({', '.join(sorted(field_list))})\"\n    return rep\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.add","title":"<code>add(content: Doc | Record | str | list[Doc | Record | str], name: Optional[str] = None, is_active: Optional[bool] = True, model: Optional[str] = None, extensions: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, id_type: Optional[str] = 'uuid4', cache: Optional[bool] = False)</code>","text":"<p>Add a record to the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Doc | Record | str | list[Doc | Record | str]</code> <p>A text string, Record, or a spaCy Doc, or a list of any of these.</p> required <code>name</code> <code>str</code> <p>A name for the record.</p> <code>None</code> <code>is_active</code> <code>bool</code> <p>Whether or not the record is active.</p> <code>True</code> <code>model</code> <code>str</code> <p>The name of the language model used to parse the record (optional).</p> <code>None</code> <code>extensions</code> <code>list[str]</code> <p>A list of extension names to add to the record.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any]</code> <p>A dict containing any metadata.</p> <code>None</code> <code>id_type</code> <code>str</code> <p>The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".</p> <code>'uuid4'</code> <code>cache</code> <code>bool</code> <p>Whether or not to cache the record.</p> <code>False</code> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef add(\n    self,\n    content: Doc | Record | str | list[Doc | Record | str],\n    name: Optional[str] = None,\n    is_active: Optional[bool] = True,\n    model: Optional[str] = None,\n    extensions: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    id_type: Optional[str] = \"uuid4\",\n    cache: Optional[bool] = False,\n):\n    \"\"\"Add a record to the Corpus.\n\n    Args:\n        content (Doc | Record | str | list[Doc | Record | str]): A text string, Record, or a spaCy Doc, or a list of any of these.\n        name (str): A name for the record.\n        is_active (bool): Whether or not the record is active.\n        model (str): The name of the language model used to parse the record (optional).\n        extensions (list[str]): A list of extension names to add to the record.\n        metadata (dict[str, Any]): A dict containing any metadata.\n        id_type (str): The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".\n        cache (bool): Whether or not to cache the record.\n    \"\"\"\n    # Sanitize metadata to ensure JSON-serializable types\n    if metadata is not None:\n        metadata = self._sanitize_metadata(metadata)\n\n    # If content is not a list, treat it as a single item\n    if isinstance(content, (Doc, Record, str)):\n        items = [content]\n    else:\n        items = list(content)\n\n    for item in items:\n        # Generate a unique ID for the record\n        new_id = self._generate_unique_id(type=id_type)\n\n        # Keep generating new UUIDs until one is not in the records dic\n        # while new_id in self.records:\n        #    new_id = str(uuid.uuid4())\n\n        if isinstance(item, Record):\n            record = item\n            if record.id and str(record.id) in self.records:\n                raise LexosException(\n                    f\"Record with ID {record.id} already exists in the Corpus.\"\n                )\n        else:\n            record_kwargs = dict(\n                id=new_id,\n                name=name,  # self._ensure_unique_name(name),\n                is_active=is_active,\n                content=item,\n                model=model,\n                data_source=None,\n            )\n            if extensions is not None:\n                record_kwargs[\"extensions\"] = extensions\n            if metadata is not None:\n                record_kwargs[\"meta\"] = metadata\n            record = Record(**record_kwargs)\n\n        # Add arbitrary metadata properties\n        if metadata:\n            record.meta.update(metadata)\n\n        # Add the record to the Corpus\n        self._add_to_corpus(record, cache=cache)\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.add_from_files","title":"<code>add_from_files(paths: Path | str | list[Path | str], max_workers: Optional[int] = None, worker_strategy: str = 'auto', batch_size: int = 100, show_progress: bool = True, name_template: Optional[str] = None, is_active: bool = True, model: Optional[str] = None, extensions: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, id_type: str = 'uuid4') -&gt; None</code>","text":"<p>Load files directly into corpus using parallel I/O.</p> <p>This method streams files into the corpus without holding all content in memory, making it suitable for very large datasets. Files are loaded in parallel using the ParallelLoader with all its optimization features (smart file ordering, auto-tuning, etc.).</p> <p>State updates are deferred until all files are loaded for optimal performance.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Path | str | list[Path | str]</code> <p>File paths or directories to load.</p> required <code>max_workers</code> <code>Optional[int]</code> <p>Maximum number of worker threads. If None, auto-calculated based on worker_strategy.</p> <code>None</code> <code>worker_strategy</code> <code>str</code> <p>Worker allocation strategy. Options: - \"auto\": Analyzes file types and chooses optimal strategy (default) - \"io_bound\": More workers for I/O-intensive operations - \"cpu_bound\": Fewer workers for CPU-intensive operations - \"balanced\": Middle ground between I/O and CPU</p> <code>'auto'</code> <code>batch_size</code> <code>int</code> <p>Number of files to process in each batch. Default 100.</p> <code>100</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bar. Default True.</p> <code>True</code> <code>name_template</code> <code>Optional[str]</code> <p>Template for generating record names. Can include {filename}, {stem}, {index}. If None, uses filename stem.</p> <code>None</code> <code>is_active</code> <code>bool</code> <p>Whether records should be marked as active. Default True.</p> <code>True</code> <code>model</code> <code>Optional[str]</code> <p>Name of language model used to parse records.</p> <code>None</code> <code>extensions</code> <code>Optional[list[str]]</code> <p>List of extension names to add to records.</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Metadata to add to all records.</p> <code>None</code> <code>id_type</code> <code>str</code> <p>Type of ID to generate (\"integer\" or \"uuid4\"). Default \"uuid4\".</p> <code>'uuid4'</code> Example <pre><code>corpus = Corpus(\"my_corpus\")\n# Load all text files from a directory\ncorpus.add_from_files(\"path/to/texts/\")\n# With custom naming\ncorpus.add_from_files(\n    [\"file1.txt\", \"file2.txt\"],\n    name_template=\"{stem}_{index}\",\n    metadata={\"source\": \"collection_a\"}\n)\n</code></pre> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def add_from_files(\n    self,\n    paths: Path | str | list[Path | str],\n    max_workers: Optional[int] = None,\n    worker_strategy: str = \"auto\",\n    batch_size: int = 100,\n    show_progress: bool = True,\n    name_template: Optional[str] = None,\n    is_active: bool = True,\n    model: Optional[str] = None,\n    extensions: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    id_type: str = \"uuid4\",\n) -&gt; None:\n    \"\"\"Load files directly into corpus using parallel I/O.\n\n    This method streams files into the corpus without holding all\n    content in memory, making it suitable for very large datasets.\n    Files are loaded in parallel using the ParallelLoader with all\n    its optimization features (smart file ordering, auto-tuning, etc.).\n\n    State updates are deferred until all files are loaded for optimal\n    performance.\n\n    Args:\n        paths (Path | str | list[Path | str]): File paths or directories to load.\n        max_workers (Optional[int]): Maximum number of worker threads.\n            If None, auto-calculated based on worker_strategy.\n        worker_strategy (str): Worker allocation strategy. Options:\n            - \"auto\": Analyzes file types and chooses optimal strategy (default)\n            - \"io_bound\": More workers for I/O-intensive operations\n            - \"cpu_bound\": Fewer workers for CPU-intensive operations\n            - \"balanced\": Middle ground between I/O and CPU\n        batch_size (int): Number of files to process in each batch. Default 100.\n        show_progress (bool): Whether to show progress bar. Default True.\n        name_template (Optional[str]): Template for generating record names.\n            Can include {filename}, {stem}, {index}. If None, uses filename stem.\n        is_active (bool): Whether records should be marked as active. Default True.\n        model (Optional[str]): Name of language model used to parse records.\n        extensions (Optional[list[str]]): List of extension names to add to records.\n        metadata (Optional[dict[str, Any]]): Metadata to add to all records.\n        id_type (str): Type of ID to generate (\"integer\" or \"uuid4\"). Default \"uuid4\".\n\n    Example:\n        ```python\n        corpus = Corpus(\"my_corpus\")\n        # Load all text files from a directory\n        corpus.add_from_files(\"path/to/texts/\")\n        # With custom naming\n        corpus.add_from_files(\n            [\"file1.txt\", \"file2.txt\"],\n            name_template=\"{stem}_{index}\",\n            metadata={\"source\": \"collection_a\"}\n        )\n        ```\n    \"\"\"\n    from lexos.io.parallel_loader import ParallelLoader\n\n    # Sanitize metadata if provided\n    if metadata is not None:\n        metadata = self._sanitize_metadata(metadata)\n\n    # Create ParallelLoader with specified settings\n    loader = ParallelLoader(\n        max_workers=max_workers,\n        worker_strategy=worker_strategy,\n        batch_size=batch_size,\n        show_progress=show_progress,\n    )\n\n    # Track for error reporting\n    loaded_count = 0\n    error_count = 0\n    errors = []\n\n    # Stream files and add to corpus\n    for index, (path, name, mime_type, text, error) in enumerate(\n        loader.load_streaming(paths), start=1\n    ):\n        if error:\n            error_count += 1\n            errors.append((path, error))\n            continue\n\n        # Generate record name from template or use default\n        if name_template:\n            record_name = name_template.format(\n                filename=Path(path).name, stem=name, index=index\n            )\n        else:\n            record_name = name\n\n        # Generate unique ID\n        record_id = self._generate_unique_id(type=id_type)\n\n        # Create record kwargs\n        record_kwargs = dict(\n            id=record_id,\n            name=record_name,\n            is_active=is_active,\n            content=text,\n            model=model,\n            data_source=str(path),\n        )\n\n        if extensions is not None:\n            record_kwargs[\"extensions\"] = extensions\n\n        if metadata is not None:\n            record_kwargs[\"meta\"] = metadata.copy()\n\n        # Create and add record without updating state\n        record = Record(**record_kwargs)\n        self._add_to_corpus_without_state_update(record)\n        loaded_count += 1\n\n    # Update corpus state once at the end\n    self._update_corpus_state()\n\n    # Report results\n    from wasabi import msg\n\n    msg.good(f\"Loaded {loaded_count} files into corpus. Errors: {error_count}\")\n\n    if errors and error_count &lt;= 10:  # Show first 10 errors\n        msg.warn(\"Errors encountered:\")\n        for path, error in errors[:10]:\n            msg.fail(f\"  {path}: {error}\")\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.export_statistical_fingerprint","title":"<code>export_statistical_fingerprint() -&gt; dict[str, Any]</code>","text":"<p>Export standardized statistical summary for external modules.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing corpus statistical fingerprint for external module consumption</p> Note <p>This provides the standardized API for external modules to consume corpus statistics.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef export_statistical_fingerprint(self) -&gt; dict[str, Any]:\n    \"\"\"Export standardized statistical summary for external modules.\n\n    Returns:\n        Dictionary containing corpus statistical fingerprint for external module consumption\n\n    Note:\n        This provides the standardized API for external modules to consume corpus statistics.\n    \"\"\"\n    # TODO: Expand fingerprint based on external module requirements\n    # TODO: Add feature extraction optimized for different analysis types\n\n    try:\n        stats = self.get_stats(active_only=True)\n\n        # Core statistical fingerprint\n        fingerprint = {\n            \"corpus_metadata\": {\n                \"name\": self.name,\n                \"num_docs\": self.num_docs,\n                \"num_active_docs\": self.num_active_docs,\n                \"num_tokens\": self.num_tokens,\n                \"num_terms\": self.num_terms,\n                \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n            },\n            \"distribution_stats\": stats.distribution_stats,\n            \"percentiles\": stats.percentiles,\n            \"text_diversity\": stats.text_diversity_stats,\n            \"basic_stats\": {\n                \"mean\": stats.mean,\n                \"std\": stats.standard_deviation,\n                \"iqr_values\": stats.iqr_values,\n                \"iqr_bounds\": stats.iqr_bounds,\n            },\n            \"document_features\": stats.doc_stats_df.to_dict(\"records\"),\n            \"term_frequencies\": self.term_counts(\n                n=100, most_common=True\n            ),  # Top 100 terms\n        }\n\n        return fingerprint\n\n    except Exception as e:\n        # Fallback fingerprint if CorpusStats fails\n        return {\n            \"corpus_metadata\": {\n                \"name\": self.name,\n                \"num_docs\": self.num_docs,\n                \"num_active_docs\": self.num_active_docs,\n                \"num_tokens\": self.num_tokens,\n                \"num_terms\": self.num_terms,\n                \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n            },\n            \"error\": f\"Statistical analysis failed: {str(e)}\",\n            \"basic_features\": {\n                \"document_ids\": list(self.records.keys()),\n                \"document_names\": list(self.names.keys()),\n            },\n        }\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.filter_records","title":"<code>filter_records(**metadata_filters: Any) -&gt; list[Record]</code>","text":"<p>Return records matching metadata key-value pairs.</p> <p>Parameters:</p> Name Type Description Default <code>**metadata_filters</code> <code>Any</code> <p>Arbitrary metadata fields and their required values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Record]</code> <p>List of Record objects matching all metadata criteria.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef filter_records(self, **metadata_filters: Any) -&gt; list[Record]:\n    \"\"\"Return records matching metadata key-value pairs.\n\n    Args:\n        **metadata_filters (Any): Arbitrary metadata fields and their required values.\n\n    Returns:\n        List of Record objects matching all metadata criteria.\n    \"\"\"\n    results = []\n    for record in self.records.values():\n        if not hasattr(record, \"meta\") or not isinstance(record.meta, dict):\n            continue\n        match = True\n        for key, value in metadata_filters.items():\n            if key not in record.meta or record.meta[key] != value:\n                match = False\n                break\n        if match:\n            results.append(record)\n    return results\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.get","title":"<code>get(id: Optional[str | list[str]] = None, name: Optional[str | list[str]] = None) -&gt; Record | list[Record]</code>","text":"<p>Get a record from the Corpus by ID.</p> <p>Tries to get the record from memory; otherwise loads it from file.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | list[str]</code> <p>A record id or list of ids from the Corpus records.</p> <code>None</code> <code>name</code> <code>str | list[str]</code> <p>A record name or list of names from the Corpus records.</p> <code>None</code> <p>Returns:</p> Type Description <code>Record | list[Record]</code> <p>Record | list[Record]: The record(s) with the given ID(s) or name(s).</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get(\n    self,\n    id: Optional[str | list[str]] = None,\n    name: Optional[str | list[str]] = None,\n) -&gt; Record | list[Record]:\n    \"\"\"Get a record from the Corpus by ID.\n\n    Tries to get the record from memory; otherwise loads it from file.\n\n    Args:\n        id (str | list[str]): A record id or list of ids from the Corpus records.\n        name (str | list[str]): A record name or list of names from the Corpus records.\n\n    Returns:\n        Record | list[Record]: The record(s) with the given ID(s) or name(s).\n    \"\"\"\n    # Ensure either id or name is provided\n    if not id and not name:\n        raise LexosException(\n            \"Must provide either an ID or a name to remove a record.\"\n        )\n\n    # Ensure id is a list\n    if isinstance(id, str):\n        ids = [id]\n    elif isinstance(id, list):\n        ids = id\n    else:\n        ids = []\n\n    # If name is provided, get the IDs from the name(s)\n    if name and not id:\n        if isinstance(name, str):\n            name = [name]\n        ids = []\n        for n in name:\n            ids.extend(self._get_by_name(n))\n\n    result = []\n    for id in ids:\n        # If the id is in the Corpus cache, return the record\n        if id in self.records.keys():\n            result.append(self.records[id])\n\n        # Otherwise, load the record from file\n        else:\n            record = self.records[id]\n            result.append(\n                record._from_disk(\n                    record.meta[\"filepath\"], record.model, self.model_cache\n                )\n            )\n    if len(result) == 1:\n        return result[0]\n    return result\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.get_analysis_results","title":"<code>get_analysis_results(module_name: str = None) -&gt; dict[str, Any]</code>","text":"<p>Retrieve analysis results from external modules.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Specific module name to retrieve, or None for all results</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing analysis results</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get_analysis_results(self, module_name: str = None) -&gt; dict[str, Any]:\n    \"\"\"Retrieve analysis results from external modules.\n\n    Args:\n        module_name: Specific module name to retrieve, or None for all results\n\n    Returns:\n        Dictionary containing analysis results\n    \"\"\"\n    if module_name:\n        if module_name not in self.analysis_results:\n            raise ValueError(f\"No results found for module '{module_name}'\")\n        return self.analysis_results[module_name]\n\n    return self.analysis_results\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.get_stats","title":"<code>get_stats(active_only: bool = True, type: str = 'tokens', min_df: int | None = None, max_df: int | None = None, max_n_terms: int | None = None, token_list: list[tuple[str, str, list[str]]] = None) -&gt; CorpusStats</code>","text":"<p>Get the Corpus statistics.</p> <p>Parameters:</p> Name Type Description Default <code>active_only</code> <code>bool</code> <p>If True, only include active records in the statistics. Defaults to True.</p> <code>True</code> <code>type</code> <code>str</code> <p>The type of statistics to return. Can be \"tokens\" or \"characters\". Defaults to \"tokens\".</p> <code>'tokens'</code> <code>min_df</code> <code>int | None</code> <p>Minimum record frequency for terms to be included in the statistics. Defaults to None.</p> <code>None</code> <code>max_df</code> <code>int | None</code> <p>Maximum record frequency for terms to be included in the statistics. Defaults to None.</p> <code>None</code> <code>max_n_terms</code> <code>int | None</code> <p>Maximum number of terms to include in the statistics. Defaults to None.</p> <code>None</code> <code>token_list</code> <code>list[tuple[str, str, list[str]]]</code> <p>A list of tuples containing the record ID, name, and tokens. If not provided, it will be generated from the records.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CorpusStats</code> <code>CorpusStats</code> <p>An object containing the Corpus statistics.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get_stats(\n    self,\n    active_only: bool = True,\n    type: str = \"tokens\",\n    min_df: int | None = None,\n    max_df: int | None = None,\n    max_n_terms: int | None = None,\n    token_list: list[tuple[str, str, list[str]]] = None,\n) -&gt; CorpusStats:\n    \"\"\"Get the Corpus statistics.\n\n    Args:\n        active_only (bool): If True, only include active records in the statistics. Defaults to True.\n        type (str): The type of statistics to return. Can be \"tokens\" or \"characters\". Defaults to \"tokens\".\n        min_df (int | None): Minimum record frequency for terms to be included in the statistics. Defaults to None.\n        max_df (int | None): Maximum record frequency for terms to be included in the statistics. Defaults to None.\n        max_n_terms (int | None): Maximum number of terms to include in the statistics. Defaults to None.\n        token_list (list[tuple[str, str, list[str]]]): A list of tuples containing the record ID, name, and tokens. If not provided, it will be generated from the records.\n\n    Returns:\n        CorpusStats: An object containing the Corpus statistics.\n    \"\"\"\n\n    def get_token_strings(record: Record) -&gt; list[str]:\n        \"\"\"Get the token strings from a record.\n\n        Args:\n            record (Record): The Record object to get the token strings from.\n\n        Returns:\n            list[str]: A list of token strings from the record.\n        \"\"\"\n        if record.is_parsed:\n            return [token.text for token in record.content]\n        # We could use xx_sent_ud_sm, but for now, split on whitespace\n        else:\n            return record.content.split()\n\n    if not token_list:\n        # Filter the records to only include active ones\n        if active_only:\n            records = [\n                record for record in self.records.values() if record.is_active\n            ]\n        # Otherwise, include all records\n        else:\n            records = list(self.records.values())\n\n        # Get the token list from the records\n        if type == \"tokens\":\n            token_list = [\n                (str(record.id), record.name, get_token_strings(record))\n                for record in records\n            ]\n        elif type == \"characters\":\n            token_list = [\n                (str(record.id), record.name, list(record.content.text))\n                if record.is_parsed\n                else (str(record.id), record.name, list(record.content))\n                for record in records\n            ]\n\n    return CorpusStats(\n        docs=token_list, min_df=min_df, max_df=max_df, max_n_terms=max_n_terms\n    )\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.import_analysis_results","title":"<code>import_analysis_results(module_name: str, results_data: dict[str, Any], version: str = '1.0.0', overwrite: bool = False) -&gt; None</code>","text":"<p>Import analysis results from external modules into corpus metadata.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the external module (e.g., 'kmeans', 'topwords', 'kwic', 'text_classification')</p> required <code>results_data</code> <code>dict[str, Any]</code> <p>Dictionary containing the analysis results</p> required <code>version</code> <code>str</code> <p>Version string for result versioning and compatibility</p> <code>'1.0.0'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing results for this module</p> <code>False</code> Note <p>This is a framework implementation. Full functionality requires peer modules to be implemented and their result schemas defined.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef import_analysis_results(\n    self,\n    module_name: str,\n    results_data: dict[str, Any],\n    version: str = \"1.0.0\",\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Import analysis results from external modules into corpus metadata.\n\n    Args:\n        module_name: Name of the external module (e.g., 'kmeans', 'topwords', 'kwic', 'text_classification')\n        results_data: Dictionary containing the analysis results\n        version: Version string for result versioning and compatibility\n        overwrite: Whether to overwrite existing results for this module\n\n    Note:\n        This is a framework implementation. Full functionality requires\n        peer modules to be implemented and their result schemas defined.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO: Add result schema validation once peer modules are available\n    # TODO: Add proper versioning system for backward compatibility\n    # TODO: Implement result correlation capabilities across modules\n\n    if module_name in self.analysis_results and not overwrite:\n        raise ValueError(\n            f\"Results for module '{module_name}' already exist. \"\n            f\"Use overwrite=True to replace them.\"\n        )\n\n    # Basic result structure with metadata\n    self.analysis_results[module_name] = {\n        \"version\": version,\n        \"timestamp\": pd.Timestamp.now().isoformat(),\n        \"corpus_state\": {\n            \"num_docs\": self.num_docs,\n            \"num_active_docs\": self.num_active_docs,\n            \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n        },\n        \"results\": results_data,\n    }\n\n    msg.good(f\"Imported {module_name} analysis results (version {version})\")\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.load","title":"<code>load(path: Path | str = None, corpus_dir: Optional[Path | str] = None, cache: Optional[bool] = False) -&gt; None</code>","text":"<p>Load a Corpus from a zip archive or directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path of the zip archive or directory to load.</p> <code>None</code> <code>corpus_dir</code> <code>Optional[Path | str]</code> <p>The directory where the Corpus is to be unzipped.</p> <code>None</code> <code>cache</code> <code>Optional[bool]</code> <p>Whether to cache the records in the Corpus. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef load(\n    self,\n    path: Path | str = None,\n    corpus_dir: Optional[Path | str] = None,\n    cache: Optional[bool] = False,\n) -&gt; None:\n    \"\"\"Load a Corpus from a zip archive or directory.\n\n    Args:\n        path (Path | str): The path of the zip archive or directory to load.\n        corpus_dir (Optional[Path | str]): The directory where the Corpus is to be unzipped.\n        cache (Optional[bool]): Whether to cache the records in the Corpus. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure that a corpus_dir exists, or create one if it doesn't\n    if not corpus_dir:\n        corpus_dir = Path(self.corpus_dir)\n        corpus_dir.mkdir(parents=True, exist_ok=True)\n\n    # If the path is a file, try to unpack it as a zip archive\n    if Path(path).is_file():\n        try:\n            shutil.unpack_archive(path, corpus_dir)\n        except shutil.ReadError as e:\n            raise LexosException(\n                f\"Failed to unpack archive: {e}. Ensure the file is a valid zip archive.\"\n            )\n\n    # Open the metadata file and load the metadata\n    metadata_path = corpus_dir / self.corpus_metadata_file\n    metadata = srsly.read_json(metadata_path)\n    for key, value in metadata.items():\n        setattr(self, key, value)\n\n    # If cache is set, load the records into the model cache\n    if cache:\n        for record in self.records.values():\n            if isinstance(record, Record):\n                # Load the record from disk\n                record.from_disk(\n                    corpus_dir / \"data\" / f\"{record.id}.bin\",\n                    model=record.model,\n                    model_cache=self.model_cache,\n                )\n            else:\n                raise LexosException(\n                    \"Records in the Corpus must be of type Record.\"\n                )\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.remove","title":"<code>remove(id: Optional[str | list[str]] = None, name: Optional[str | list[str]] = None) -&gt; None</code>","text":"<p>Remove a record from the corpus by ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | list[str]</code> <p>The ID of the record to remove.</p> <code>None</code> <code>name</code> <code>str | list[str]</code> <p>The name of the record to remove.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef remove(\n    self,\n    id: Optional[str | list[str]] = None,\n    name: Optional[str | list[str]] = None,\n) -&gt; None:\n    \"\"\"Remove a record from the corpus by ID.\n\n    Args:\n        id (str | list[str]): The ID of the record to remove.\n        name (str | list[str]): The name of the record to remove.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure either id or name is provided\n    if not id and not name:\n        raise LexosException(\n            \"Must provide either an ID or a name to remove a record.\"\n        )\n\n    # Ensure id is a list\n    if isinstance(id, str):\n        ids = [id]\n    elif isinstance(id, list):\n        ids = id\n    else:\n        ids = []\n\n    # If name is provided, get the IDs from the name(s)\n    if name and not id:\n        if isinstance(name, str):\n            name = [name]\n        ids = []\n        for n in name:\n            ids.extend(self._get_by_name(n))\n\n    for id in ids:\n        # Remove the entry from the records dictionary and names list\n        try:\n            entry = self.records.pop(id)\n        except KeyError:\n            raise LexosException(\n                f\"Record with ID {id} does not exist in the Corpus.\"\n            )\n        try:\n            if entry.name in self.names:\n                self.names[entry.name].remove(str(entry.id))\n                if not self.names[entry.name]:  # Remove empty lists\n                    self.names.pop(entry.name)\n        except KeyError:\n            raise LexosException(\n                f\"Record with name {entry.name} does not exist in the Corpus.\"\n            )\n\n    # Update the Corpus state after removing the record\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.save","title":"<code>save(path: Path | str = None) -&gt; None</code>","text":"<p>Save the Corpus as a zip archive.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the Corpus to.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str = None) -&gt; None:\n    \"\"\"Save the Corpus as a zip archive.\n\n    Args:\n        path (Path | str): The path to save the Corpus to.\n\n    Returns:\n        None\n    \"\"\"\n    shutil.make_archive(path / f\"{self.name}\", \"zip\", self.corpus_dir)\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.set","title":"<code>set(id: str, **props) -&gt; None</code>","text":"<p>Set a property or properties of a record in the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>A record id.</p> required <code>**props</code> <code>dict</code> <p>The dict containing any other properties to set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef set(self, id: str, **props) -&gt; None:\n    \"\"\"Set a property or properties of a record in the Corpus.\n\n    Args:\n        id (str): A record id.\n        **props (dict): The dict containing any other properties to set.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the record by ID\n    record = self.records[id]\n\n    # Save the record's filepath, thenupdate the specified properties\n    old_filepath = record.meta.get(\"filepath\", None)\n    record.set(**props)\n\n    # If the filepath has changed, delete the old file\n    if record.meta.get(\"filepath\", None) != old_filepath:\n        Path(old_filepath).unlink(missing_ok=True)\n\n    # If the record has a filepath, ensure the file is in the data directory\n    filepath = record.meta.get(\"filepath\")\n    if filepath and filepath not in str(Path(self.corpus_dir) / \"data\"):\n        record.to_disk(filepath, extensions=record.extensions)\n\n    # Update the record in the Corpus and update the corpus state\n    self.records[id] = record\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.term_counts","title":"<code>term_counts(n: Optional[int] = 10, most_common: Optional[bool] = True) -&gt; Counter</code>","text":"<p>Get a Counter with the most common Corpus term counts.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>The number of most common terms to return. Defaults to 10.</p> <code>10</code> <code>most_common</code> <code>Optional[bool]</code> <p>If True, return the n most common terms; otherwise, return the n least common terms.</p> <code>True</code> <p>Returns:</p> Type Description <code>Counter</code> <p>A collections.Counter object containing the n most common term counts for all records in the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef term_counts(\n    self, n: Optional[int] = 10, most_common: Optional[bool] = True\n) -&gt; Counter:\n    \"\"\"Get a Counter with the most common Corpus term counts.\n\n    Args:\n        n (Optional[int]): The number of most common terms to return. Defaults to 10.\n        most_common (Optional[bool]): If True, return the n most common terms; otherwise, return the n least common terms.\n\n    Returns:\n        A collections.Counter object containing the n most common term counts for all records in the Corpus.\n    \"\"\"\n    # Count the terms in all records\n    counter = Counter()\n    for record in self.records.values():\n        if record.is_parsed:\n            counter.update(record.terms)\n\n    # Optionally filter the results\n    if most_common and n:\n        return counter.most_common(n)\n    elif not most_common and n:\n        return counter.most_common()[: -n - 1 : -1]\n    elif most_common is False and n is None:\n        return counter.most_common()[::]\n    else:\n        return counter\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.to_df","title":"<code>to_df(exclude: list[str] = ['content', 'terms', 'tokens']) -&gt; pd.DataFrame</code>","text":"<p>Return a table of the Corpus records.</p> <p>Parameters:</p> Name Type Description Default <code>exclude</code> <code>list[str]</code> <p>A list of fields to exclude from the dataframe. If you wish to exclude metadata fields with the same name as model fields, you can use the prefix \"metadata_\" to avoid conflicts.</p> <code>['content', 'terms', 'tokens']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe representing the records in the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef to_df(\n    self, exclude: list[str] = [\"content\", \"terms\", \"tokens\"]\n) -&gt; pd.DataFrame:\n    \"\"\"Return a table of the Corpus records.\n\n    Args:\n        exclude (list[str]): A list of fields to exclude from the dataframe. If you wish to exclude metadata fields with the same name as model fields, you can use the prefix \"metadata_\" to avoid conflicts.\n\n    Returns:\n        pd.DataFrame: A dataframe representing the records in the Corpus.\n    \"\"\"\n    rows = []\n    for record in self.records.values():  # &lt;- Fix the duplicate\n        if record is None:  # Skip None records\n            continue\n\n        # Get model categories.\n        # NOTE: We avoid calling `model_dump()` on `Record` objects that are\n        # unparsed because Pydantic may attempt to evaluate computed fields\n        # while creating the serialized dict. Several computed properties on\n        # `Record` (e.g., `terms`, `tokens`, `num_terms`, and\n        # `num_tokens`) raise `LexosException(\"Record is not parsed.\")`\n        # when the record is not parsed. If `model_dump()` evaluates those\n        # properties for an unparsed record, it will raise and cause\n        # `to_df()` to fail. Therefore:\n        #  - For parsed records, we call `record.model_dump()` and use the\n        #    model-dump output (it includes computed fields safely).\n        #  - For unparsed records, we *do not* call `model_dump()`; we\n        #    instead build a minimal, safe `row` from stored fields and\n        #    set any computed-like values to safe defaults (empty list,\n        #    0, or empty string). This produces robust DataFrame output\n        #    for corpora that contain a mix of parsed and unparsed\n        #    records without triggering computed-field side-effects.\n        fields_that_may_raise = {\n            \"terms\",\n            \"tokens\",\n            \"num_terms\",\n            \"num_tokens\",\n            \"text\",\n        }\n        # Build a dump_exclude set to prevent model_dump from computing\n        # sensitive fields on unparsed records\n        dump_exclude = set(exclude)\n        if hasattr(record, \"is_parsed\") and record.is_parsed:\n            # Parsed records: safely model_dump, excluding any user-requested fields\n            row = record.model_dump(exclude=list(dump_exclude))\n        else:\n            # Unparsed records: avoid model_dump to prevent computed property evaluation\n            base_fields = [\n                \"id\",\n                \"name\",\n                \"is_active\",\n                \"content\",\n                \"model\",\n                \"extensions\",\n                \"data_source\",\n                \"meta\",\n            ]\n            row = {}\n            for f in base_fields:\n                if f in exclude:\n                    continue\n                try:\n                    value = getattr(record, f, None)\n                except Exception:\n                    # Defensive: if getattr triggers an error, skip and set None\n                    value = None\n                # Serialize Doc-like content into text rather than bytes to keep DataFrame friendly\n                if f == \"content\" and value is not None:\n                    try:\n                        from spacy.tokens import Doc\n\n                        if isinstance(value, Doc):\n                            value = value.text\n                    except Exception:\n                        pass\n                # Ensure id is serialized to string to match model_dump output for parsed records\n                if f == \"id\" and value is not None:\n                    try:\n                        value = str(value)\n                    except Exception:\n                        pass\n                # Sanitize meta similar to model_dump\n                if f == \"meta\" and value is not None:\n                    try:\n                        value = record._sanitize_metadata(value)\n                    except Exception:\n                        pass\n                row[f] = value\n\n        # Patch for unparsed records: fill terms/tokens/num_terms/num_tokens/text\n        # Only if those fields are not excluded\n        if \"terms\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"terms\"] = list(record.terms)\n            else:\n                row[\"terms\"] = []\n        if \"tokens\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"tokens\"] = record.tokens\n            else:\n                row[\"tokens\"] = []\n        if \"num_terms\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"num_terms\"] = record.num_terms()\n            else:\n                row[\"num_terms\"] = 0\n        if \"num_tokens\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"num_tokens\"] = record.num_tokens()\n            else:\n                row[\"num_tokens\"] = 0\n        if \"text\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"text\"] = record.text\n            else:\n                row[\"text\"] = \"\"\n\n        # Add metadata categories, respecting exclude list\n        metadata = row.pop(\"meta\", {})\n        for key, value in metadata.items():\n            # Exclude metadata fields if requested\n            if key in exclude or f\"metadata_{key}\" in exclude:\n                continue\n            if key in row:\n                key = f\"metadata_{key}\"\n            row[key] = value\n\n        # Append the row to the rows list\n        rows.append(row)\n\n    # Create a DataFrame from the rows\n    if rows:  # Only create DataFrame if we have data\n        df = pd.DataFrame(rows)\n        # Fill NaN with appropriate values based on column dtype\n        fill_values = {}\n        for col in df.columns:\n            if pd.api.types.is_numeric_dtype(df[col]):\n                fill_values[col] = 0\n            elif pd.api.types.is_bool_dtype(df[col]):\n                fill_values[col] = False\n            else:\n                fill_values[col] = \"\"\n\n        df = df.fillna(fill_values)  # Use assignment instead of inplace\n        return df\n    else:\n        # Return empty DataFrame with basic columns if no records\n        return pd.DataFrame(columns=[\"id\", \"name\", \"is_active\"])\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.validate_analysis_compatibility","title":"<code>validate_analysis_compatibility(module_name: str) -&gt; dict[str, Any]</code>","text":"<p>Validate if stored analysis results are compatible with current corpus state.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module to validate</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing validation results and recommendations</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef validate_analysis_compatibility(self, module_name: str) -&gt; dict[str, Any]:\n    \"\"\"Validate if stored analysis results are compatible with current corpus state.\n\n    Args:\n        module_name: Name of the module to validate\n\n    Returns:\n        Dictionary containing validation results and recommendations\n    \"\"\"\n    if module_name not in self.analysis_results:\n        return {\n            \"compatible\": False,\n            \"reason\": f\"No analysis results found for module '{module_name}'\",\n        }\n\n    stored_results = self.analysis_results[module_name]\n    stored_state = stored_results.get(\"corpus_state\", {})\n    current_fingerprint = self._generate_corpus_fingerprint()\n    stored_fingerprint = stored_state.get(\"corpus_fingerprint\", \"\")\n\n    compatibility = {\n        \"compatible\": stored_fingerprint == current_fingerprint,\n        \"current_fingerprint\": current_fingerprint,\n        \"stored_fingerprint\": stored_fingerprint,\n        \"stored_timestamp\": stored_results.get(\"timestamp\", \"unknown\"),\n        \"stored_version\": stored_results.get(\"version\", \"unknown\"),\n    }\n\n    if not compatibility[\"compatible\"]:\n        compatibility[\"reason\"] = (\n            \"Corpus state has changed since analysis was performed\"\n        )\n        compatibility[\"recommendation\"] = (\n            f\"Re-run {module_name} analysis with current corpus state\"\n        )\n\n        # Detailed state comparison\n        compatibility[\"state_changes\"] = {\n            \"num_docs\": {\n                \"stored\": stored_state.get(\"num_docs\", 0),\n                \"current\": self.num_docs,\n                \"changed\": stored_state.get(\"num_docs\", 0) != self.num_docs,\n            },\n            \"num_active_docs\": {\n                \"stored\": stored_state.get(\"num_active_docs\", 0),\n                \"current\": self.num_active_docs,\n                \"changed\": stored_state.get(\"num_active_docs\", 0)\n                != self.num_active_docs,\n            },\n        }\n\n    return compatibility\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialise the Corpus with a data directory and a metadata file.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialise the Corpus with a data directory and a metadata file.\"\"\"\n    super().__init__(**data)\n    corpus_dir = Path(self.corpus_dir)\n    Path(corpus_dir / \"data\").mkdir(parents=True, exist_ok=True)\n\n    # Load existing metadata if it exists, otherwise create new\n    metadata_file = corpus_dir / self.corpus_metadata_file\n    if metadata_file.exists():\n        # Load existing metadata to preserve record info\n        existing_metadata = srsly.read_json(metadata_file)\n        # Preserve the 'meta' dict which contains record metadata\n        if \"meta\" in existing_metadata and existing_metadata[\"meta\"]:\n            self.meta = existing_metadata[\"meta\"]\n\n    # NOTE: We use model_dump() on the Corpus model here. The Corpus\n    # computed fields (e.g., `terms` is a set) are safe to serialize\n    # and do not raise the `LexosException`. We explicitly convert\n    # `terms` to a list to make it JSON-serializable. If future\n    # computed fields are added to Corpus that rely on external state\n    # or can raise, this call should be revised to exclude those fields\n    # (e.g., model_dump(exclude=[...])).\n    data = self.model_dump()\n    data[\"terms\"] = list(data[\"terms\"])\n    srsly.write_json(metadata_file, data)\n    msg.good(\"Corpus created.\")\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.__iter__","title":"<code>__iter__() -&gt; Iterable[Record]</code>","text":"<p>Make the corpus iterable.</p> <p>Returns:</p> Type Description <code>Iterable[Record]</code> <p>Iterator[Record]: An iterator over the Record objects in the corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __iter__(self) -&gt; Iterable[Record]:\n    \"\"\"Make the corpus iterable.\n\n    Returns:\n        Iterator[Record]: An iterator over the Record objects in the corpus.\n    \"\"\"\n    return iter(self.records.values())\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of the Corpus.\"\"\"\n    fields = {field: getattr(self, field) for field in self.model_fields_set}\n    field_list = [f\"{k}={v}\" for k, v in fields.items()]\n    rep = f\"Corpus({', '.join(sorted(field_list))})\"\n    return rep\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.active_terms","title":"<code>active_terms: set</code>  <code>property</code>","text":"<p>Return the set of active terms in the Corpus.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set</code> <p>A set of active term strings found in active parsed records.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.meta_df","title":"<code>meta_df: pd.DataFrame</code>  <code>property</code>","text":"<p>Return a DataFrame of the Corpus metadata.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_active_tokens","title":"<code>num_active_tokens: int</code>  <code>property</code>","text":"<p>Return the number of active tokens in the Corpus.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total number of tokens in active parsed records.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.num_active_terms","title":"<code>num_active_terms: int</code>  <code>property</code>","text":"<p>Return the number of active terms in the Corpus.</p>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus._add_to_corpus","title":"<code>_add_to_corpus(record: Record, cache: Optional[bool] = False) -&gt; None</code>","text":"<p>Add a record to the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>A Record doc.</p> required <code>cache</code> <code>Optional[bool]</code> <p>Whether to cache the record. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def _add_to_corpus(self, record: Record, cache: Optional[bool] = False) -&gt; None:\n    \"\"\"Add a record to the Corpus.\n\n    Args:\n        record (Record): A Record doc.\n        cache (Optional[bool]): Whether to cache the record. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Update corpus records table\n    # We intentionally exclude computed fields here when dumping a\n    # Record for meta storage because those computed properties (e.g.,\n    # `terms`, `text`, `tokens`) may attempt to evaluate state-dependent\n    # computed values that can raise for unparsed records. By explicitly\n    # excluding them and then annotating `num_tokens`/`num_terms` using\n    # guarded access below, we avoid calling computed fields on records\n    # that are not parsed.\n    meta = record.model_dump(\n        exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n    )\n    # Ensure ID is always string for JSON serialization (redundant with mode=\"json\" but kept for clarity)\n    if \"id\" in meta:\n        meta[\"id\"] = str(meta[\"id\"])\n    num_tokens = record.num_tokens() if record.is_parsed else 0\n    num_terms = record.num_terms() if record.is_parsed else 0\n    meta[\"num_tokens\"] = num_tokens\n    meta[\"num_terms\"] = num_terms\n    # Use string ID as key to avoid UUID serialization issues\n    self.meta[str(record.id)] = meta\n\n    # Save the record to disk -- currently, this is always done\n    corpus_dir = Path(self.corpus_dir)\n    filename = f\"{record.id}.bin\"\n    filepath = corpus_dir / \"data\" / filename\n    record.meta[\"filename\"] = str(filename)\n    record.meta[\"filepath\"] = str(filepath)\n    record.to_disk(record.meta[\"filepath\"])\n\n    # Update the Corpus records dictionary\n    record_id_str = str(record.id)\n    self.records[record_id_str] = record\n\n    # Update the Corpus names\n    if record.name not in self.names:\n        self.names[record.name] = []\n    self.names[record.name].append(str(record.id))  # Explicitly convert to string\n\n    # Update the Corpus statistics\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus._ensure_unique_name","title":"<code>_ensure_unique_name(name: str = None) -&gt; str</code>","text":"<p>Ensure that no names are duplicated in the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The record name.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A string.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def _ensure_unique_name(self, name: str = None) -&gt; str:\n    \"\"\"Ensure that no names are duplicated in the Corpus.\n\n    Args:\n        name (str): The record name.\n\n    Returns:\n        A string.\n    \"\"\"\n    if not name:\n        return f\"untitled_{uuid.uuid1()}\"\n    if name in self.names:\n        return f\"{name}_{uuid.uuid1()}\"\n    return name\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus._generate_unique_id","title":"<code>_generate_unique_id(type: str = 'uuid4') -&gt; str</code>","text":"<p>Generate a unique ID for the record.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".</p> <code>'uuid4'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique ID for the record.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def _generate_unique_id(self, type: str = \"uuid4\") -&gt; str:\n    \"\"\"Generate a unique ID for the record.\n\n    Args:\n        type (str): The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".\n\n    Returns:\n        str: A unique ID for the record.\n    \"\"\"\n    if type == \"integer\":\n        # Generate an integer ID\n        return max(self.records.keys(), default=0) + 1\n    elif type == \"uuid4\":\n        # Generate initial UUID\n        new_id = str(uuid.uuid4())\n\n        # Keep generating new UUIDs until one is not in the records dic\n        while new_id in self.records:\n            new_id = str(uuid.uuid4())\n        return new_id\n    else:\n        raise LexosException(\n            f\"Invalid ID type '{type}'. Must be 'integer' or 'uuid4'.\"\n        )\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus._get_by_name","title":"<code>_get_by_name(name: str) -&gt; list[str]</code>","text":"<p>Get all record IDs from the Corpus by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the record(s) to get.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of record IDs with the given name.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def _get_by_name(self, name: str) -&gt; list[str]:\n    \"\"\"Get all record IDs from the Corpus by name.\n\n    Args:\n        name (str): The name of the record(s) to get.\n\n    Returns:\n        list[str]: A list of record IDs with the given name.\n    \"\"\"\n    if name not in self.names:\n        raise LexosException(\n            f\"Record with name {name} does not exist in the Corpus.\"\n        )\n    return self.names[name]\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus._update_corpus_state","title":"<code>_update_corpus_state()</code>","text":"<p>Update the Corpus state after adding or removing records.</p> Note <p>This method recalculates the number of records, active records, terms, tokens, and unique terms in the entire Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def _update_corpus_state(self):\n    \"\"\"Update the Corpus state after adding or removing records.\n\n    Note:\n        This method recalculates the number of records, active records,\n        terms, tokens, and unique terms in the entire Corpus.\n    \"\"\"\n    self.num_docs = len(self.records)\n    self.num_active_docs = sum(\n        1 for r in self.records.values() if r and r.is_active\n    )\n    self.num_terms = sum(\n        r.num_terms() for r in self.records.values() if r and r.is_parsed\n    )\n    self.num_tokens = sum(\n        r.num_tokens() for r in self.records.values() if r and r.is_parsed\n    )\n    # We call model_dump() on Corpus to create a JSON of the corpus\n    # metadata. This excludes Record-specific computed fields and the\n    # `records` mapping so we only write top-level corpus metadata.\n    # In particular, Record-computed fields are excluded so we don't\n    # force evaluation across records which could trigger exceptions\n    # for unparsed records.\n    corpus_data = self.model_dump(\n        exclude=[\"content\", \"terms\", \"text\", \"tokens\", \"records\"]\n    )\n    # Convert any remaining UUIDs to strings\n    for key, value in corpus_data.items():\n        if hasattr(value, \"hex\"):  # UUID objects have .hex attribute\n            corpus_data[key] = str(value)\n\n    srsly.write_json(\n        Path(self.corpus_dir) / self.corpus_metadata_file,\n        corpus_data,\n    )\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus._sanitize_metadata","title":"<code>_sanitize_metadata(metadata: dict[str, Any]) -&gt; dict[str, Any]</code>","text":"<p>Convert non-JSON-serializable types to strings in metadata.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[str, Any]</code> <p>Original metadata dictionary</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Sanitized metadata dictionary with JSON-serializable values</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def _sanitize_metadata(self, metadata: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Convert non-JSON-serializable types to strings in metadata.\n\n    Args:\n        metadata: Original metadata dictionary\n\n    Returns:\n        Sanitized metadata dictionary with JSON-serializable values\n    \"\"\"\n    from datetime import date, datetime\n    from pathlib import Path\n    from uuid import UUID\n\n    sanitized = {}\n    for key, value in metadata.items():\n        if isinstance(value, UUID):\n            sanitized[key] = str(value)\n        elif isinstance(value, (datetime, date)):\n            sanitized[key] = value.isoformat()\n        elif isinstance(value, Path):\n            sanitized[key] = str(value)\n        elif isinstance(value, dict):\n            sanitized[key] = self._sanitize_metadata(value)  # Recursive\n        elif isinstance(value, list):\n            sanitized[key] = [\n                self._sanitize_metadata({\"item\": item})[\"item\"]\n                if isinstance(item, dict)\n                else str(item)\n                if isinstance(item, (UUID, datetime, date, Path))\n                else item\n                for item in value\n            ]\n        else:\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.add","title":"<code>add(content: Doc | Record | str | list[Doc | Record | str], name: Optional[str] = None, is_active: Optional[bool] = True, model: Optional[str] = None, extensions: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, id_type: Optional[str] = 'uuid4', cache: Optional[bool] = False)</code>","text":"<p>Add a record to the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Doc | Record | str | list[Doc | Record | str]</code> <p>A text string, Record, or a spaCy Doc, or a list of any of these.</p> required <code>name</code> <code>str</code> <p>A name for the record.</p> <code>None</code> <code>is_active</code> <code>bool</code> <p>Whether or not the record is active.</p> <code>True</code> <code>model</code> <code>str</code> <p>The name of the language model used to parse the record (optional).</p> <code>None</code> <code>extensions</code> <code>list[str]</code> <p>A list of extension names to add to the record.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any]</code> <p>A dict containing any metadata.</p> <code>None</code> <code>id_type</code> <code>str</code> <p>The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".</p> <code>'uuid4'</code> <code>cache</code> <code>bool</code> <p>Whether or not to cache the record.</p> <code>False</code> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef add(\n    self,\n    content: Doc | Record | str | list[Doc | Record | str],\n    name: Optional[str] = None,\n    is_active: Optional[bool] = True,\n    model: Optional[str] = None,\n    extensions: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    id_type: Optional[str] = \"uuid4\",\n    cache: Optional[bool] = False,\n):\n    \"\"\"Add a record to the Corpus.\n\n    Args:\n        content (Doc | Record | str | list[Doc | Record | str]): A text string, Record, or a spaCy Doc, or a list of any of these.\n        name (str): A name for the record.\n        is_active (bool): Whether or not the record is active.\n        model (str): The name of the language model used to parse the record (optional).\n        extensions (list[str]): A list of extension names to add to the record.\n        metadata (dict[str, Any]): A dict containing any metadata.\n        id_type (str): The type of ID to generate. Can be \"integer\" or \"uuid4\". Defaults to \"uuid4\".\n        cache (bool): Whether or not to cache the record.\n    \"\"\"\n    # Sanitize metadata to ensure JSON-serializable types\n    if metadata is not None:\n        metadata = self._sanitize_metadata(metadata)\n\n    # If content is not a list, treat it as a single item\n    if isinstance(content, (Doc, Record, str)):\n        items = [content]\n    else:\n        items = list(content)\n\n    for item in items:\n        # Generate a unique ID for the record\n        new_id = self._generate_unique_id(type=id_type)\n\n        # Keep generating new UUIDs until one is not in the records dic\n        # while new_id in self.records:\n        #    new_id = str(uuid.uuid4())\n\n        if isinstance(item, Record):\n            record = item\n            if record.id and str(record.id) in self.records:\n                raise LexosException(\n                    f\"Record with ID {record.id} already exists in the Corpus.\"\n                )\n        else:\n            record_kwargs = dict(\n                id=new_id,\n                name=name,  # self._ensure_unique_name(name),\n                is_active=is_active,\n                content=item,\n                model=model,\n                data_source=None,\n            )\n            if extensions is not None:\n                record_kwargs[\"extensions\"] = extensions\n            if metadata is not None:\n                record_kwargs[\"meta\"] = metadata\n            record = Record(**record_kwargs)\n\n        # Add arbitrary metadata properties\n        if metadata:\n            record.meta.update(metadata)\n\n        # Add the record to the Corpus\n        self._add_to_corpus(record, cache=cache)\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.filter_records","title":"<code>filter_records(**metadata_filters: Any) -&gt; list[Record]</code>","text":"<p>Return records matching metadata key-value pairs.</p> <p>Parameters:</p> Name Type Description Default <code>**metadata_filters</code> <code>Any</code> <p>Arbitrary metadata fields and their required values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Record]</code> <p>List of Record objects matching all metadata criteria.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef filter_records(self, **metadata_filters: Any) -&gt; list[Record]:\n    \"\"\"Return records matching metadata key-value pairs.\n\n    Args:\n        **metadata_filters (Any): Arbitrary metadata fields and their required values.\n\n    Returns:\n        List of Record objects matching all metadata criteria.\n    \"\"\"\n    results = []\n    for record in self.records.values():\n        if not hasattr(record, \"meta\") or not isinstance(record.meta, dict):\n            continue\n        match = True\n        for key, value in metadata_filters.items():\n            if key not in record.meta or record.meta[key] != value:\n                match = False\n                break\n        if match:\n            results.append(record)\n    return results\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.get","title":"<code>get(id: Optional[str | list[str]] = None, name: Optional[str | list[str]] = None) -&gt; Record | list[Record]</code>","text":"<p>Get a record from the Corpus by ID.</p> <p>Tries to get the record from memory; otherwise loads it from file.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | list[str]</code> <p>A record id or list of ids from the Corpus records.</p> <code>None</code> <code>name</code> <code>str | list[str]</code> <p>A record name or list of names from the Corpus records.</p> <code>None</code> <p>Returns:</p> Type Description <code>Record | list[Record]</code> <p>Record | list[Record]: The record(s) with the given ID(s) or name(s).</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get(\n    self,\n    id: Optional[str | list[str]] = None,\n    name: Optional[str | list[str]] = None,\n) -&gt; Record | list[Record]:\n    \"\"\"Get a record from the Corpus by ID.\n\n    Tries to get the record from memory; otherwise loads it from file.\n\n    Args:\n        id (str | list[str]): A record id or list of ids from the Corpus records.\n        name (str | list[str]): A record name or list of names from the Corpus records.\n\n    Returns:\n        Record | list[Record]: The record(s) with the given ID(s) or name(s).\n    \"\"\"\n    # Ensure either id or name is provided\n    if not id and not name:\n        raise LexosException(\n            \"Must provide either an ID or a name to remove a record.\"\n        )\n\n    # Ensure id is a list\n    if isinstance(id, str):\n        ids = [id]\n    elif isinstance(id, list):\n        ids = id\n    else:\n        ids = []\n\n    # If name is provided, get the IDs from the name(s)\n    if name and not id:\n        if isinstance(name, str):\n            name = [name]\n        ids = []\n        for n in name:\n            ids.extend(self._get_by_name(n))\n\n    result = []\n    for id in ids:\n        # If the id is in the Corpus cache, return the record\n        if id in self.records.keys():\n            result.append(self.records[id])\n\n        # Otherwise, load the record from file\n        else:\n            record = self.records[id]\n            result.append(\n                record._from_disk(\n                    record.meta[\"filepath\"], record.model, self.model_cache\n                )\n            )\n    if len(result) == 1:\n        return result[0]\n    return result\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.get_stats","title":"<code>get_stats(active_only: bool = True, type: str = 'tokens', min_df: int | None = None, max_df: int | None = None, max_n_terms: int | None = None, token_list: list[tuple[str, str, list[str]]] = None) -&gt; CorpusStats</code>","text":"<p>Get the Corpus statistics.</p> <p>Parameters:</p> Name Type Description Default <code>active_only</code> <code>bool</code> <p>If True, only include active records in the statistics. Defaults to True.</p> <code>True</code> <code>type</code> <code>str</code> <p>The type of statistics to return. Can be \"tokens\" or \"characters\". Defaults to \"tokens\".</p> <code>'tokens'</code> <code>min_df</code> <code>int | None</code> <p>Minimum record frequency for terms to be included in the statistics. Defaults to None.</p> <code>None</code> <code>max_df</code> <code>int | None</code> <p>Maximum record frequency for terms to be included in the statistics. Defaults to None.</p> <code>None</code> <code>max_n_terms</code> <code>int | None</code> <p>Maximum number of terms to include in the statistics. Defaults to None.</p> <code>None</code> <code>token_list</code> <code>list[tuple[str, str, list[str]]]</code> <p>A list of tuples containing the record ID, name, and tokens. If not provided, it will be generated from the records.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CorpusStats</code> <code>CorpusStats</code> <p>An object containing the Corpus statistics.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get_stats(\n    self,\n    active_only: bool = True,\n    type: str = \"tokens\",\n    min_df: int | None = None,\n    max_df: int | None = None,\n    max_n_terms: int | None = None,\n    token_list: list[tuple[str, str, list[str]]] = None,\n) -&gt; CorpusStats:\n    \"\"\"Get the Corpus statistics.\n\n    Args:\n        active_only (bool): If True, only include active records in the statistics. Defaults to True.\n        type (str): The type of statistics to return. Can be \"tokens\" or \"characters\". Defaults to \"tokens\".\n        min_df (int | None): Minimum record frequency for terms to be included in the statistics. Defaults to None.\n        max_df (int | None): Maximum record frequency for terms to be included in the statistics. Defaults to None.\n        max_n_terms (int | None): Maximum number of terms to include in the statistics. Defaults to None.\n        token_list (list[tuple[str, str, list[str]]]): A list of tuples containing the record ID, name, and tokens. If not provided, it will be generated from the records.\n\n    Returns:\n        CorpusStats: An object containing the Corpus statistics.\n    \"\"\"\n\n    def get_token_strings(record: Record) -&gt; list[str]:\n        \"\"\"Get the token strings from a record.\n\n        Args:\n            record (Record): The Record object to get the token strings from.\n\n        Returns:\n            list[str]: A list of token strings from the record.\n        \"\"\"\n        if record.is_parsed:\n            return [token.text for token in record.content]\n        # We could use xx_sent_ud_sm, but for now, split on whitespace\n        else:\n            return record.content.split()\n\n    if not token_list:\n        # Filter the records to only include active ones\n        if active_only:\n            records = [\n                record for record in self.records.values() if record.is_active\n            ]\n        # Otherwise, include all records\n        else:\n            records = list(self.records.values())\n\n        # Get the token list from the records\n        if type == \"tokens\":\n            token_list = [\n                (str(record.id), record.name, get_token_strings(record))\n                for record in records\n            ]\n        elif type == \"characters\":\n            token_list = [\n                (str(record.id), record.name, list(record.content.text))\n                if record.is_parsed\n                else (str(record.id), record.name, list(record.content))\n                for record in records\n            ]\n\n    return CorpusStats(\n        docs=token_list, min_df=min_df, max_df=max_df, max_n_terms=max_n_terms\n    )\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.load","title":"<code>load(path: Path | str = None, corpus_dir: Optional[Path | str] = None, cache: Optional[bool] = False) -&gt; None</code>","text":"<p>Load a Corpus from a zip archive or directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path of the zip archive or directory to load.</p> <code>None</code> <code>corpus_dir</code> <code>Optional[Path | str]</code> <p>The directory where the Corpus is to be unzipped.</p> <code>None</code> <code>cache</code> <code>Optional[bool]</code> <p>Whether to cache the records in the Corpus. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef load(\n    self,\n    path: Path | str = None,\n    corpus_dir: Optional[Path | str] = None,\n    cache: Optional[bool] = False,\n) -&gt; None:\n    \"\"\"Load a Corpus from a zip archive or directory.\n\n    Args:\n        path (Path | str): The path of the zip archive or directory to load.\n        corpus_dir (Optional[Path | str]): The directory where the Corpus is to be unzipped.\n        cache (Optional[bool]): Whether to cache the records in the Corpus. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure that a corpus_dir exists, or create one if it doesn't\n    if not corpus_dir:\n        corpus_dir = Path(self.corpus_dir)\n        corpus_dir.mkdir(parents=True, exist_ok=True)\n\n    # If the path is a file, try to unpack it as a zip archive\n    if Path(path).is_file():\n        try:\n            shutil.unpack_archive(path, corpus_dir)\n        except shutil.ReadError as e:\n            raise LexosException(\n                f\"Failed to unpack archive: {e}. Ensure the file is a valid zip archive.\"\n            )\n\n    # Open the metadata file and load the metadata\n    metadata_path = corpus_dir / self.corpus_metadata_file\n    metadata = srsly.read_json(metadata_path)\n    for key, value in metadata.items():\n        setattr(self, key, value)\n\n    # If cache is set, load the records into the model cache\n    if cache:\n        for record in self.records.values():\n            if isinstance(record, Record):\n                # Load the record from disk\n                record.from_disk(\n                    corpus_dir / \"data\" / f\"{record.id}.bin\",\n                    model=record.model,\n                    model_cache=self.model_cache,\n                )\n            else:\n                raise LexosException(\n                    \"Records in the Corpus must be of type Record.\"\n                )\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.save","title":"<code>save(path: Path | str = None) -&gt; None</code>","text":"<p>Save the Corpus as a zip archive.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the Corpus to.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str = None) -&gt; None:\n    \"\"\"Save the Corpus as a zip archive.\n\n    Args:\n        path (Path | str): The path to save the Corpus to.\n\n    Returns:\n        None\n    \"\"\"\n    shutil.make_archive(path / f\"{self.name}\", \"zip\", self.corpus_dir)\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.remove","title":"<code>remove(id: Optional[str | list[str]] = None, name: Optional[str | list[str]] = None) -&gt; None</code>","text":"<p>Remove a record from the corpus by ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | list[str]</code> <p>The ID of the record to remove.</p> <code>None</code> <code>name</code> <code>str | list[str]</code> <p>The name of the record to remove.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef remove(\n    self,\n    id: Optional[str | list[str]] = None,\n    name: Optional[str | list[str]] = None,\n) -&gt; None:\n    \"\"\"Remove a record from the corpus by ID.\n\n    Args:\n        id (str | list[str]): The ID of the record to remove.\n        name (str | list[str]): The name of the record to remove.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure either id or name is provided\n    if not id and not name:\n        raise LexosException(\n            \"Must provide either an ID or a name to remove a record.\"\n        )\n\n    # Ensure id is a list\n    if isinstance(id, str):\n        ids = [id]\n    elif isinstance(id, list):\n        ids = id\n    else:\n        ids = []\n\n    # If name is provided, get the IDs from the name(s)\n    if name and not id:\n        if isinstance(name, str):\n            name = [name]\n        ids = []\n        for n in name:\n            ids.extend(self._get_by_name(n))\n\n    for id in ids:\n        # Remove the entry from the records dictionary and names list\n        try:\n            entry = self.records.pop(id)\n        except KeyError:\n            raise LexosException(\n                f\"Record with ID {id} does not exist in the Corpus.\"\n            )\n        try:\n            if entry.name in self.names:\n                self.names[entry.name].remove(str(entry.id))\n                if not self.names[entry.name]:  # Remove empty lists\n                    self.names.pop(entry.name)\n        except KeyError:\n            raise LexosException(\n                f\"Record with name {entry.name} does not exist in the Corpus.\"\n            )\n\n    # Update the Corpus state after removing the record\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.set","title":"<code>set(id: str, **props) -&gt; None</code>","text":"<p>Set a property or properties of a record in the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>A record id.</p> required <code>**props</code> <code>dict</code> <p>The dict containing any other properties to set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef set(self, id: str, **props) -&gt; None:\n    \"\"\"Set a property or properties of a record in the Corpus.\n\n    Args:\n        id (str): A record id.\n        **props (dict): The dict containing any other properties to set.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the record by ID\n    record = self.records[id]\n\n    # Save the record's filepath, thenupdate the specified properties\n    old_filepath = record.meta.get(\"filepath\", None)\n    record.set(**props)\n\n    # If the filepath has changed, delete the old file\n    if record.meta.get(\"filepath\", None) != old_filepath:\n        Path(old_filepath).unlink(missing_ok=True)\n\n    # If the record has a filepath, ensure the file is in the data directory\n    filepath = record.meta.get(\"filepath\")\n    if filepath and filepath not in str(Path(self.corpus_dir) / \"data\"):\n        record.to_disk(filepath, extensions=record.extensions)\n\n    # Update the record in the Corpus and update the corpus state\n    self.records[id] = record\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.term_counts","title":"<code>term_counts(n: Optional[int] = 10, most_common: Optional[bool] = True) -&gt; Counter</code>","text":"<p>Get a Counter with the most common Corpus term counts.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>The number of most common terms to return. Defaults to 10.</p> <code>10</code> <code>most_common</code> <code>Optional[bool]</code> <p>If True, return the n most common terms; otherwise, return the n least common terms.</p> <code>True</code> <p>Returns:</p> Type Description <code>Counter</code> <p>A collections.Counter object containing the n most common term counts for all records in the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef term_counts(\n    self, n: Optional[int] = 10, most_common: Optional[bool] = True\n) -&gt; Counter:\n    \"\"\"Get a Counter with the most common Corpus term counts.\n\n    Args:\n        n (Optional[int]): The number of most common terms to return. Defaults to 10.\n        most_common (Optional[bool]): If True, return the n most common terms; otherwise, return the n least common terms.\n\n    Returns:\n        A collections.Counter object containing the n most common term counts for all records in the Corpus.\n    \"\"\"\n    # Count the terms in all records\n    counter = Counter()\n    for record in self.records.values():\n        if record.is_parsed:\n            counter.update(record.terms)\n\n    # Optionally filter the results\n    if most_common and n:\n        return counter.most_common(n)\n    elif not most_common and n:\n        return counter.most_common()[: -n - 1 : -1]\n    elif most_common is False and n is None:\n        return counter.most_common()[::]\n    else:\n        return counter\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.to_df","title":"<code>to_df(exclude: list[str] = ['content', 'terms', 'tokens']) -&gt; pd.DataFrame</code>","text":"<p>Return a table of the Corpus records.</p> <p>Parameters:</p> Name Type Description Default <code>exclude</code> <code>list[str]</code> <p>A list of fields to exclude from the dataframe. If you wish to exclude metadata fields with the same name as model fields, you can use the prefix \"metadata_\" to avoid conflicts.</p> <code>['content', 'terms', 'tokens']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe representing the records in the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef to_df(\n    self, exclude: list[str] = [\"content\", \"terms\", \"tokens\"]\n) -&gt; pd.DataFrame:\n    \"\"\"Return a table of the Corpus records.\n\n    Args:\n        exclude (list[str]): A list of fields to exclude from the dataframe. If you wish to exclude metadata fields with the same name as model fields, you can use the prefix \"metadata_\" to avoid conflicts.\n\n    Returns:\n        pd.DataFrame: A dataframe representing the records in the Corpus.\n    \"\"\"\n    rows = []\n    for record in self.records.values():  # &lt;- Fix the duplicate\n        if record is None:  # Skip None records\n            continue\n\n        # Get model categories.\n        # NOTE: We avoid calling `model_dump()` on `Record` objects that are\n        # unparsed because Pydantic may attempt to evaluate computed fields\n        # while creating the serialized dict. Several computed properties on\n        # `Record` (e.g., `terms`, `tokens`, `num_terms`, and\n        # `num_tokens`) raise `LexosException(\"Record is not parsed.\")`\n        # when the record is not parsed. If `model_dump()` evaluates those\n        # properties for an unparsed record, it will raise and cause\n        # `to_df()` to fail. Therefore:\n        #  - For parsed records, we call `record.model_dump()` and use the\n        #    model-dump output (it includes computed fields safely).\n        #  - For unparsed records, we *do not* call `model_dump()`; we\n        #    instead build a minimal, safe `row` from stored fields and\n        #    set any computed-like values to safe defaults (empty list,\n        #    0, or empty string). This produces robust DataFrame output\n        #    for corpora that contain a mix of parsed and unparsed\n        #    records without triggering computed-field side-effects.\n        fields_that_may_raise = {\n            \"terms\",\n            \"tokens\",\n            \"num_terms\",\n            \"num_tokens\",\n            \"text\",\n        }\n        # Build a dump_exclude set to prevent model_dump from computing\n        # sensitive fields on unparsed records\n        dump_exclude = set(exclude)\n        if hasattr(record, \"is_parsed\") and record.is_parsed:\n            # Parsed records: safely model_dump, excluding any user-requested fields\n            row = record.model_dump(exclude=list(dump_exclude))\n        else:\n            # Unparsed records: avoid model_dump to prevent computed property evaluation\n            base_fields = [\n                \"id\",\n                \"name\",\n                \"is_active\",\n                \"content\",\n                \"model\",\n                \"extensions\",\n                \"data_source\",\n                \"meta\",\n            ]\n            row = {}\n            for f in base_fields:\n                if f in exclude:\n                    continue\n                try:\n                    value = getattr(record, f, None)\n                except Exception:\n                    # Defensive: if getattr triggers an error, skip and set None\n                    value = None\n                # Serialize Doc-like content into text rather than bytes to keep DataFrame friendly\n                if f == \"content\" and value is not None:\n                    try:\n                        from spacy.tokens import Doc\n\n                        if isinstance(value, Doc):\n                            value = value.text\n                    except Exception:\n                        pass\n                # Ensure id is serialized to string to match model_dump output for parsed records\n                if f == \"id\" and value is not None:\n                    try:\n                        value = str(value)\n                    except Exception:\n                        pass\n                # Sanitize meta similar to model_dump\n                if f == \"meta\" and value is not None:\n                    try:\n                        value = record._sanitize_metadata(value)\n                    except Exception:\n                        pass\n                row[f] = value\n\n        # Patch for unparsed records: fill terms/tokens/num_terms/num_tokens/text\n        # Only if those fields are not excluded\n        if \"terms\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"terms\"] = list(record.terms)\n            else:\n                row[\"terms\"] = []\n        if \"tokens\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"tokens\"] = record.tokens\n            else:\n                row[\"tokens\"] = []\n        if \"num_terms\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"num_terms\"] = record.num_terms()\n            else:\n                row[\"num_terms\"] = 0\n        if \"num_tokens\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"num_tokens\"] = record.num_tokens()\n            else:\n                row[\"num_tokens\"] = 0\n        if \"text\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"text\"] = record.text\n            else:\n                row[\"text\"] = \"\"\n\n        # Add metadata categories, respecting exclude list\n        metadata = row.pop(\"meta\", {})\n        for key, value in metadata.items():\n            # Exclude metadata fields if requested\n            if key in exclude or f\"metadata_{key}\" in exclude:\n                continue\n            if key in row:\n                key = f\"metadata_{key}\"\n            row[key] = value\n\n        # Append the row to the rows list\n        rows.append(row)\n\n    # Create a DataFrame from the rows\n    if rows:  # Only create DataFrame if we have data\n        df = pd.DataFrame(rows)\n        # Fill NaN with appropriate values based on column dtype\n        fill_values = {}\n        for col in df.columns:\n            if pd.api.types.is_numeric_dtype(df[col]):\n                fill_values[col] = 0\n            elif pd.api.types.is_bool_dtype(df[col]):\n                fill_values[col] = False\n            else:\n                fill_values[col] = \"\"\n\n        df = df.fillna(fill_values)  # Use assignment instead of inplace\n        return df\n    else:\n        # Return empty DataFrame with basic columns if no records\n        return pd.DataFrame(columns=[\"id\", \"name\", \"is_active\"])\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.import_analysis_results","title":"<code>import_analysis_results(module_name: str, results_data: dict[str, Any], version: str = '1.0.0', overwrite: bool = False) -&gt; None</code>","text":"<p>Import analysis results from external modules into corpus metadata.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the external module (e.g., 'kmeans', 'topwords', 'kwic', 'text_classification')</p> required <code>results_data</code> <code>dict[str, Any]</code> <p>Dictionary containing the analysis results</p> required <code>version</code> <code>str</code> <p>Version string for result versioning and compatibility</p> <code>'1.0.0'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing results for this module</p> <code>False</code> Note <p>This is a framework implementation. Full functionality requires peer modules to be implemented and their result schemas defined.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef import_analysis_results(\n    self,\n    module_name: str,\n    results_data: dict[str, Any],\n    version: str = \"1.0.0\",\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Import analysis results from external modules into corpus metadata.\n\n    Args:\n        module_name: Name of the external module (e.g., 'kmeans', 'topwords', 'kwic', 'text_classification')\n        results_data: Dictionary containing the analysis results\n        version: Version string for result versioning and compatibility\n        overwrite: Whether to overwrite existing results for this module\n\n    Note:\n        This is a framework implementation. Full functionality requires\n        peer modules to be implemented and their result schemas defined.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO: Add result schema validation once peer modules are available\n    # TODO: Add proper versioning system for backward compatibility\n    # TODO: Implement result correlation capabilities across modules\n\n    if module_name in self.analysis_results and not overwrite:\n        raise ValueError(\n            f\"Results for module '{module_name}' already exist. \"\n            f\"Use overwrite=True to replace them.\"\n        )\n\n    # Basic result structure with metadata\n    self.analysis_results[module_name] = {\n        \"version\": version,\n        \"timestamp\": pd.Timestamp.now().isoformat(),\n        \"corpus_state\": {\n            \"num_docs\": self.num_docs,\n            \"num_active_docs\": self.num_active_docs,\n            \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n        },\n        \"results\": results_data,\n    }\n\n    msg.good(f\"Imported {module_name} analysis results (version {version})\")\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.get_analysis_results","title":"<code>get_analysis_results(module_name: str = None) -&gt; dict[str, Any]</code>","text":"<p>Retrieve analysis results from external modules.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Specific module name to retrieve, or None for all results</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing analysis results</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get_analysis_results(self, module_name: str = None) -&gt; dict[str, Any]:\n    \"\"\"Retrieve analysis results from external modules.\n\n    Args:\n        module_name: Specific module name to retrieve, or None for all results\n\n    Returns:\n        Dictionary containing analysis results\n    \"\"\"\n    if module_name:\n        if module_name not in self.analysis_results:\n            raise ValueError(f\"No results found for module '{module_name}'\")\n        return self.analysis_results[module_name]\n\n    return self.analysis_results\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.export_statistical_fingerprint","title":"<code>export_statistical_fingerprint() -&gt; dict[str, Any]</code>","text":"<p>Export standardized statistical summary for external modules.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing corpus statistical fingerprint for external module consumption</p> Note <p>This provides the standardized API for external modules to consume corpus statistics.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef export_statistical_fingerprint(self) -&gt; dict[str, Any]:\n    \"\"\"Export standardized statistical summary for external modules.\n\n    Returns:\n        Dictionary containing corpus statistical fingerprint for external module consumption\n\n    Note:\n        This provides the standardized API for external modules to consume corpus statistics.\n    \"\"\"\n    # TODO: Expand fingerprint based on external module requirements\n    # TODO: Add feature extraction optimized for different analysis types\n\n    try:\n        stats = self.get_stats(active_only=True)\n\n        # Core statistical fingerprint\n        fingerprint = {\n            \"corpus_metadata\": {\n                \"name\": self.name,\n                \"num_docs\": self.num_docs,\n                \"num_active_docs\": self.num_active_docs,\n                \"num_tokens\": self.num_tokens,\n                \"num_terms\": self.num_terms,\n                \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n            },\n            \"distribution_stats\": stats.distribution_stats,\n            \"percentiles\": stats.percentiles,\n            \"text_diversity\": stats.text_diversity_stats,\n            \"basic_stats\": {\n                \"mean\": stats.mean,\n                \"std\": stats.standard_deviation,\n                \"iqr_values\": stats.iqr_values,\n                \"iqr_bounds\": stats.iqr_bounds,\n            },\n            \"document_features\": stats.doc_stats_df.to_dict(\"records\"),\n            \"term_frequencies\": self.term_counts(\n                n=100, most_common=True\n            ),  # Top 100 terms\n        }\n\n        return fingerprint\n\n    except Exception as e:\n        # Fallback fingerprint if CorpusStats fails\n        return {\n            \"corpus_metadata\": {\n                \"name\": self.name,\n                \"num_docs\": self.num_docs,\n                \"num_active_docs\": self.num_active_docs,\n                \"num_tokens\": self.num_tokens,\n                \"num_terms\": self.num_terms,\n                \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n            },\n            \"error\": f\"Statistical analysis failed: {str(e)}\",\n            \"basic_features\": {\n                \"document_ids\": list(self.records.keys()),\n                \"document_names\": list(self.names.keys()),\n            },\n        }\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus._generate_corpus_fingerprint","title":"<code>_generate_corpus_fingerprint() -&gt; str</code>","text":"<p>Generate a unique fingerprint for corpus state validation.</p> <p>Returns:</p> Type Description <code>str</code> <p>SHA256 hash representing current corpus state</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def _generate_corpus_fingerprint(self) -&gt; str:\n    \"\"\"Generate a unique fingerprint for corpus state validation.\n\n    Returns:\n        SHA256 hash representing current corpus state\n    \"\"\"\n    import hashlib\n\n    # Create fingerprint from corpus state\n    state_data = {\n        \"num_docs\": self.num_docs,\n        \"num_active_docs\": self.num_active_docs,\n        \"record_ids\": sorted(self.records.keys()),\n        \"active_record_ids\": sorted(\n            [k for k, v in self.records.items() if v and v.is_active]\n        ),\n    }\n\n    state_string = str(sorted(state_data.items()))\n    return hashlib.sha256(state_string.encode()).hexdigest()[:16]  # First 16 chars\n</code></pre>"},{"location":"api/corpus/corpus/#lexos.corpus.corpus.Corpus.validate_analysis_compatibility","title":"<code>validate_analysis_compatibility(module_name: str) -&gt; dict[str, Any]</code>","text":"<p>Validate if stored analysis results are compatible with current corpus state.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module to validate</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing validation results and recommendations</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef validate_analysis_compatibility(self, module_name: str) -&gt; dict[str, Any]:\n    \"\"\"Validate if stored analysis results are compatible with current corpus state.\n\n    Args:\n        module_name: Name of the module to validate\n\n    Returns:\n        Dictionary containing validation results and recommendations\n    \"\"\"\n    if module_name not in self.analysis_results:\n        return {\n            \"compatible\": False,\n            \"reason\": f\"No analysis results found for module '{module_name}'\",\n        }\n\n    stored_results = self.analysis_results[module_name]\n    stored_state = stored_results.get(\"corpus_state\", {})\n    current_fingerprint = self._generate_corpus_fingerprint()\n    stored_fingerprint = stored_state.get(\"corpus_fingerprint\", \"\")\n\n    compatibility = {\n        \"compatible\": stored_fingerprint == current_fingerprint,\n        \"current_fingerprint\": current_fingerprint,\n        \"stored_fingerprint\": stored_fingerprint,\n        \"stored_timestamp\": stored_results.get(\"timestamp\", \"unknown\"),\n        \"stored_version\": stored_results.get(\"version\", \"unknown\"),\n    }\n\n    if not compatibility[\"compatible\"]:\n        compatibility[\"reason\"] = (\n            \"Corpus state has changed since analysis was performed\"\n        )\n        compatibility[\"recommendation\"] = (\n            f\"Re-run {module_name} analysis with current corpus state\"\n        )\n\n        # Detailed state comparison\n        compatibility[\"state_changes\"] = {\n            \"num_docs\": {\n                \"stored\": stored_state.get(\"num_docs\", 0),\n                \"current\": self.num_docs,\n                \"changed\": stored_state.get(\"num_docs\", 0) != self.num_docs,\n            },\n            \"num_active_docs\": {\n                \"stored\": stored_state.get(\"num_active_docs\", 0),\n                \"current\": self.num_active_docs,\n                \"changed\": stored_state.get(\"num_active_docs\", 0)\n                != self.num_active_docs,\n            },\n        }\n\n    return compatibility\n</code></pre>"},{"location":"api/corpus/corpus_analysis_report/","title":"corpus_analysis_report","text":""},{"location":"api/corpus/corpus_analysis_report/#module-description","title":"Module Description","text":"<p>The <code>corpus_analysis_report</code> provides a single helper function for generating a comprehensive analysis of corpus contents.</p> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/corpus/corpus_analysis_report/#lexos.corpus.corpus_analysis_report.create_corpus_analysis_report","title":"<code>create_corpus_analysis_report(corpus: Corpus, output_dir: str = None, console_output=True, html=False) -&gt; str</code>","text":"<p>Create a comprehensive report of the corpus analysis results.</p> <p>This function exports various statistics and summaries of the corpus analysis to CSV files and generates a text report.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>Corpus</code> <p>The corpus object containing documents and metadata.</p> required <code>output_dir</code> <code>str</code> <p>The directory path where the report files will be saved.</p> <code>None</code> <code>console_output</code> <code>bool</code> <p>Whether to print progress messages to the console.</p> <code>True</code> <code>html</code> <code>bool</code> <p>Whether to generate an HTML report (not implemented yet).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated report as a string in either HTML or Markdown format.</p> Source code in <code>lexos/corpus/corpus_analysis_report.py</code> <pre><code>def create_corpus_analysis_report(\n    corpus: Corpus,\n    output_dir: str = None,\n    console_output=True,\n    html=False,\n) -&gt; str:\n    \"\"\"Create a comprehensive report of the corpus analysis results.\n\n    This function exports various statistics and summaries of the corpus\n    analysis to CSV files and generates a text report.\n\n    Args:\n        corpus (Corpus): The corpus object containing documents and metadata.\n        output_dir (str): The directory path where the report files will be saved.\n        console_output (bool): Whether to print progress messages to the console.\n        html (bool): Whether to generate an HTML report (not implemented yet).\n\n    Returns:\n        str: The generated report as a string in either HTML or Markdown format.\n    \"\"\"\n    # Get stats object once to avoid multiple calls\n    stats = corpus.get_stats(active_only=True)\n\n    # Only create output directory and save files if output_dir is provided\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        output_dir.mkdir(exist_ok=True)\n\n        # 1. Export corpus data to CSV\n        corpus_df = corpus.to_df()\n        csv_path = output_dir / \"corpus_overview.csv\"\n        corpus_df.to_csv(csv_path, index=False)\n        if console_output:\n            msg.good(f\"\u2713 Exported corpus overview to {csv_path}\")\n            msg.info(f\"   - {len(corpus_df)} documents\")\n            msg.info(f\"   - {len(corpus_df.columns)} data columns\")\n\n        # 2. Export detailed statistics\n        stats_df = stats.doc_stats_df\n        stats_path = output_dir / \"document_statistics.csv\"\n        stats_df.to_csv(stats_path)\n        if console_output:\n            msg.good(f\"\u2713 Exported detailed statistics to {stats_path}\")\n\n        # 3. Export analysis results summary\n        summary_data = {\n            \"corpus_name\": corpus.name,\n            \"total_documents\": corpus.num_docs,\n            \"active_documents\": corpus.num_active_docs,\n            \"total_tokens\": corpus.num_tokens,\n            \"mean_document_length\": stats.mean,\n            \"std_document_length\": stats.standard_deviation,\n            \"iqr_outliers_count\": len(stats.iqr_outliers),\n            \"corpus_fingerprint\": corpus._generate_corpus_fingerprint(),\n        }\n\n        # Add quality metrics\n        quality = stats.corpus_quality_metrics\n        summary_data.update(\n            {\n                \"length_balance_classification\": quality[\"document_length_balance\"][\n                    \"classification\"\n                ],\n                \"sampling_adequacy\": quality[\"vocabulary_richness\"][\n                    \"sampling_adequacy\"\n                ],\n                \"size_adequacy\": quality[\"corpus_size_metrics\"][\"size_adequacy\"],\n            }\n        )\n\n        # Add Zipf analysis\n        zipf = stats.zipf_analysis\n        summary_data.update(\n            {\n                \"zipf_follows_law\": zipf[\"follows_zipf\"],\n                \"zipf_goodness_of_fit\": zipf[\"zipf_goodness_of_fit\"],\n                \"zipf_r_squared\": zipf[\"r_squared\"],\n            }\n        )\n\n        summary_df = pd.DataFrame([summary_data])\n        summary_path = output_dir / \"corpus_summary.csv\"\n        summary_df.to_csv(summary_path, index=False)\n        if console_output:\n            msg.good(f\"\u2713 Exported corpus summary to {summary_path}\")\n\n        # 4. Export analysis results from modules\n        if corpus.analysis_results:\n            results_path = output_dir / \"module_analysis_results.json\"\n            with open(results_path, \"w\") as f:\n                json.dump(corpus.analysis_results, f, indent=2, default=str)\n            if console_output:\n                msg.good(f\"\u2713 Exported module results to {results_path}\")\n\n    # Get quality and zipf metrics from stats object\n    quality = stats.corpus_quality_metrics\n    zipf = stats.zipf_analysis\n\n    # Always generate HTML first\n    html_report = \"&lt;html&gt;&lt;head&gt;&lt;title&gt;Corpus Analysis Report&lt;/title&gt;&lt;/head&gt;&lt;body&gt;\"\n    html_report += \"&lt;h1&gt;Corpus Analysis Report&lt;/h1&gt;\"\n    html_report += f\"&lt;p&gt;Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}&lt;/p&gt;\"\n    html_report += \"&lt;hr&gt;\"\n    html_report += f\"&lt;h2&gt;Corpus Overview&lt;/h2&gt;&lt;ul&gt;\"\n    html_report += f\"&lt;li&gt;Name: {corpus.name}&lt;/li&gt;\"\n    html_report += f\"&lt;li&gt;Total Documents: {corpus.num_docs}&lt;/li&gt;\"\n    html_report += f\"&lt;li&gt;Active Documents: {corpus.num_active_docs}&lt;/li&gt;\"\n    html_report += f\"&lt;li&gt;Total Tokens: {corpus.num_tokens}&lt;/li&gt;\"\n    html_report += \"&lt;/ul&gt;\"\n    html_report += f\"&lt;h2&gt;Statistical Summary&lt;/h2&gt;&lt;ul&gt;\"\n    html_report += f\"&lt;li&gt;Mean Length: {stats.mean:.1f} tokens&lt;/li&gt;\"\n    html_report += f\"&lt;li&gt;Standard Deviation: {stats.standard_deviation:.1f} tokens&lt;/li&gt;\"\n    html_report += (\n        f\"&lt;li&gt;Shortest Document: {stats.doc_stats_df['total_tokens'].min()} tokens&lt;/li&gt;\"\n    )\n    html_report += (\n        f\"&lt;li&gt;Longest Document: {stats.doc_stats_df['total_tokens'].max()} tokens&lt;/li&gt;\"\n    )\n    html_report += f\"&lt;li&gt;IQR Outliers: {len(stats.iqr_outliers)}&lt;/li&gt;\"\n    html_report += \"&lt;/ul&gt;\"\n    html_report += f\"&lt;h2&gt;Quality Assessment&lt;/h2&gt;&lt;ul&gt;\"\n    html_report += f\"&lt;li&gt;Length Balance: {quality['document_length_balance']['classification']}&lt;/li&gt;\"\n    html_report += f\"&lt;li&gt;Sampling Adequacy: {quality['vocabulary_richness']['sampling_adequacy']}&lt;/li&gt;\"\n    html_report += (\n        f\"&lt;li&gt;Size Adequacy: {quality['corpus_size_metrics']['size_adequacy']}&lt;/li&gt;\"\n    )\n    html_report += f\"&lt;li&gt;Follows Zipf's Law: {zipf['follows_zipf']}&lt;/li&gt;\"\n    html_report += \"&lt;/ul&gt;\"\n    if corpus.analysis_results:\n        html_report += \"&lt;h2&gt;Module Analyses&lt;/h2&gt;&lt;ul&gt;\"\n        for module_name, data in corpus.analysis_results.items():\n            html_report += f\"&lt;li&gt;{module_name}: Version {data['version']} ({data['timestamp']})&lt;/li&gt;\"\n        html_report += \"&lt;/ul&gt;\"\n    html_report += \"&lt;/body&gt;&lt;/html&gt;\"\n\n    # Determine output format\n    if html:\n        report = html_report\n        ext = \".html\"\n    else:\n        # Convert HTML to Markdown\n        report = convert(html_report)\n        ext = \".md\"\n\n    # Save report if output_dir is provided\n    if output_dir is not None:\n        report_path = output_dir / f\"analysis_report{ext}\"\n        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(report)\n        if console_output:\n            msg.good(f\"\u2713 Created comprehensive report at {report_path}\")\n\n            msg.good(f\"\\n\ud83d\udcc2 All files saved to: {output_dir.absolute()}\")\n            msg.good(f\"\\n\ud83d\udccb Files created:\")\n            for file_path in output_dir.glob(\"*\"):\n                file_size = file_path.stat().st_size\n                msg.info(f\"   \ud83d\udcc4 {file_path.name}: {file_size:,} bytes\")\n\n            msg.good(f\"\\n\ud83d\udca1 Sharing Tips:\")\n            msg.info(f\"   \ud83d\udcca Use CSV files for data analysis in Excel/R/Python\")\n            msg.info(f\"   \ud83d\udccb Share the report file for quick overview\")\n            msg.info(f\"   \ud83d\udd17 Use JSON files for integration with other tools\")\n            msg.info(\n                f\"   \ud83d\udcbe The corpus directory contains all original documents and metadata\"\n            )\n\n    return report\n</code></pre>"},{"location":"api/corpus/corpus_stats/","title":"corpus_stats","text":""},{"location":"api/corpus/corpus_stats/#module-description","title":"Module Description","text":"<p>The <code>corpus_stats</code> module provides a handler class for corpus statistics.</p> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats","title":"<code>CorpusStats</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class to hold statistics about a Corpus.</p> <p>The input should be a list of tuples, where each tuple contains:     - id: A unique identifier for the record.     - label: A label for the record.     - token list: A list of tokens in the record. Tokens can be words, n-grams, or any other token unit.     - Settings to pass to the DTM vectorizer, such as min_df, max_df, and max_n_terms.</p> <p>To reproduce the webapp:</p> <ul> <li>stats = CorpusStats(docs=docs)</li> <li>stats.doc_stats_df # The DataFrame containing record statistics.</li> <li>stats.mean # The mean count for the entire corpus.</li> <li>stats.standard_deviation # The standard deviation for the entire corpus.</li> <li>stats.get_iqr_outliers() # Get outliers based on interquartile range (IQR).</li> <li>stats.get_std_outliers() # Get outliers based on standard deviation.</li> <li>stats.plot(column=\"total_tokens\", type=\"plotly_boxplot\" title=\"Corpus Boxplot\") # Plot the boxplot of total tokens with Plotly.</li> </ul> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>docs</code>                 (<code>list[tuple[str, str, list[str]]]</code>)             </li> <li> <code>min_df</code>                 (<code>int | None</code>)             </li> <li> <code>max_df</code>                 (<code>int | None</code>)             </li> <li> <code>max_n_terms</code>                 (<code>int | None</code>)             </li> <li> <code>dtm</code>                 (<code>DTM</code>)             </li> </ul> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>class CorpusStats(BaseModel):\n    \"\"\"A class to hold statistics about a Corpus.\n\n    The input should be a list of tuples, where each tuple contains:\n        - id: A unique identifier for the record.\n        - label: A label for the record.\n        - token list: A list of tokens in the record. Tokens can be words, n-grams, or any other token unit.\n        - Settings to pass to the DTM vectorizer, such as min_df, max_df, and max_n_terms.\n\n    To reproduce the webapp:\n\n      - stats = CorpusStats(docs=docs)\n      - stats.doc_stats_df # The DataFrame containing record statistics.\n      - stats.mean # The mean count for the entire corpus.\n      - stats.standard_deviation # The standard deviation for the entire corpus.\n      - stats.get_iqr_outliers() # Get outliers based on interquartile range (IQR).\n      - stats.get_std_outliers() # Get outliers based on standard deviation.\n      - stats.plot(column=\"total_tokens\", type=\"plotly_boxplot\" title=\"Corpus Boxplot\") # Plot the boxplot of total tokens with Plotly.\n    \"\"\"\n\n    docs: list[tuple[str, str, list[str]]]\n    min_df: int | None = None\n    max_df: int | None = None\n    max_n_terms: int | None = None\n    dtm: DTM = Field(\n        default=None, description=\"Document-Term Matrix (DTM) for the Corpus.\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data):\n        \"\"\"Initialize the CorpusStats and create the DTM.\"\"\"\n        super().__init__(**data)\n        # Separate the ids and labels from the docs\n        object.__setattr__(self, \"ids\", [doc[0] for doc in self.docs])\n        object.__setattr__(self, \"labels\", [doc[1] for doc in self.docs])\n\n        # Configure the DTM vectorizer with the provided settings\n        vectorizer_kwargs = {}\n        if self.min_df is not None:\n            vectorizer_kwargs[\"min_df\"] = self.min_df\n        if self.max_df is not None:\n            vectorizer_kwargs[\"max_df\"] = self.max_df\n        if self.max_n_terms is not None:\n            vectorizer_kwargs[\"max_n_terms\"] = self.max_n_terms\n\n        # Create and initialize the Document-Term Matrix (DTM) using the provided token lists\n        object.__setattr__(self, \"dtm\", DTM())\n        # Pass vectorizer kwargs during the call rather than initialization\n        # NB. DTM.to_df() will not work unless columns are unique labels\n        self.dtm(\n            docs=[doc[2] for doc in self.docs],\n            labels=make_labels_unique(self.labels),\n            **vectorizer_kwargs,\n        )\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"Get the Document-Term Matrix (DTM) in sparse format.\"\"\"\n        return self.dtm.to_df()\n\n    @cached_property\n    def doc_stats_df(self) -&gt; pd.DataFrame:\n        \"\"\"Get a Pandas dataframe containing the statistics of each record.\n\n        Returns:\n            A Pandas dataframe containing statistics of each record.\n        \"\"\"\n        return self._get_doc_stats_df()\n\n    @cached_property\n    def mean_and_spread(self) -&gt; tuple[float, float]:\n        \"\"\"Get the mean and standard deviation of the total tokens in the Corpus.\"\"\"\n        df = self.df.sparse.to_dense().T\n        df[\"Total\"] = df.sum(axis=1)\n        return df[\"Total\"].mean(), df[\"Total\"].std()\n\n    @property\n    def mean(self) -&gt; float:\n        \"\"\"Get the mean of the total tokens in the Corpus.\"\"\"\n        return self.mean_and_spread[0]\n\n    @property\n    def standard_deviation(self) -&gt; float:\n        \"\"\"Get the standard deviation of the total tokens in the Corpus.\"\"\"\n        return self.mean_and_spread[1]\n\n    @cached_property\n    def iqr_values(self) -&gt; tuple[float, float, float]:\n        \"\"\"Get the Q1, Q3, and IQR values for total tokens.\n\n        Returns:\n            tuple[float, float, float]: A tuple containing (q1, q3, iqr)\n        \"\"\"\n        doc_lengths = self.doc_stats_df[\"total_tokens\"].values\n        q1 = np.quantile(doc_lengths, 0.25)\n        q3 = np.quantile(doc_lengths, 0.75)\n        iqr = q3 - q1\n        return q1, q3, iqr\n\n    @cached_property\n    def iqr_bounds(self) -&gt; tuple[float, float]:\n        \"\"\"Get the IQR outlier bounds for total tokens.\n\n        Returns:\n            tuple[float, float]: A tuple containing (lower_bound, upper_bound)\n        \"\"\"\n        q1, q3, iqr = self.iqr_values\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n        return lower_bound, upper_bound\n\n    @cached_property\n    def iqr_outliers(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Get the IQR outliers for total tokens.\n\n        Returns:\n            list[tuple[str, str]]: A list of tuples containing the record ID\n            and record name for each outlier.\n        \"\"\"\n        doc_lengths = self.doc_stats_df[\"total_tokens\"].values\n        lower_bound, upper_bound = self.iqr_bounds\n\n        return [\n            (str(self.ids[i]), str(self.labels[i]))\n            for i, length in enumerate(doc_lengths)\n            if length &lt; lower_bound or length &gt; upper_bound\n        ]\n\n    @cached_property\n    def distribution_stats(self) -&gt; dict[str, float]:\n        \"\"\"Get comprehensive distribution statistics for record lengths.\n\n        Returns:\n            dict[str, float]: Dictionary containing skewness, kurtosis, and normality test results.\n        \"\"\"\n        doc_lengths = self.doc_stats_df[\"total_tokens\"].values\n\n        # Calculate skewness and kurtosis\n        skewness = stats.skew(doc_lengths)\n        kurt = stats.kurtosis(doc_lengths)  # Excess kurtosis (normal dist = 0)\n\n        # Shapiro-Wilk normality test\n        shapiro_stat, shapiro_p = stats.shapiro(doc_lengths)\n\n        # Coefficient of variation (relative variability)\n        cv = self.standard_deviation / self.mean if self.mean != 0 else 0\n\n        return {\n            \"skewness\": skewness,\n            \"kurtosis\": kurt,\n            \"coefficient_of_variation\": cv,\n            \"shapiro_statistic\": shapiro_stat,\n            \"shapiro_p_value\": shapiro_p,\n            \"is_normal\": shapiro_p &gt; 0.05,  # Conservative threshold\n        }\n\n    @cached_property\n    def percentiles(self) -&gt; dict[str, float]:\n        \"\"\"Get comprehensive percentile analysis for record lengths.\n\n        Returns:\n            dict[str, float]: Dictionary containing various percentiles.\n        \"\"\"\n        doc_lengths = self.doc_stats_df[\"total_tokens\"].values\n\n        return {\n            \"percentile_5\": np.percentile(doc_lengths, 5),\n            \"percentile_10\": np.percentile(doc_lengths, 10),\n            \"percentile_25\": np.percentile(doc_lengths, 25),  # Q1\n            \"percentile_50\": np.percentile(doc_lengths, 50),  # Median\n            \"percentile_75\": np.percentile(doc_lengths, 75),  # Q3\n            \"percentile_90\": np.percentile(doc_lengths, 90),\n            \"percentile_95\": np.percentile(doc_lengths, 95),\n            \"min\": np.min(doc_lengths),\n            \"max\": np.max(doc_lengths),\n            \"range\": np.max(doc_lengths) - np.min(doc_lengths),\n        }\n\n    @cached_property\n    def text_diversity_stats(self) -&gt; dict[str, float]:\n        \"\"\"Get text-specific diversity and complexity statistics.\n\n        Returns:\n            dict[str, float]: Dictionary containing lexical diversity measures.\n        \"\"\"\n        doc_stats = self.doc_stats_df\n\n        # Type-Token Ratio statistics across corpus\n        ttr_values = (\n            doc_stats[\"vocabulary_density\"].values / 100\n        )  # Convert from percentage\n\n        # Hapax legomena statistics\n        hapax_values = doc_stats[\"hapax_legomena\"].values\n        hapax_ratio = hapax_values / doc_stats[\"total_tokens\"].values\n\n        # Calculate corpus-level diversity metrics\n        total_tokens = doc_stats[\"total_tokens\"].sum()\n        total_terms = (\n            len(self.dtm.sorted_terms_list)\n            if hasattr(self.dtm, \"sorted_terms_list\")\n            else doc_stats[\"total_terms\"].sum()\n        )\n\n        # Hapax dislegomena statistics\n        dislegomena_values = doc_stats[\"hapax_dislegomena\"].values\n        dislegomena_ratio = dislegomena_values / doc_stats[\"total_tokens\"].values\n\n        return {\n            \"mean_ttr\": np.mean(ttr_values),\n            \"median_ttr\": np.median(ttr_values),\n            \"std_ttr\": np.std(ttr_values),\n            \"corpus_ttr\": total_terms / total_tokens if total_tokens &gt; 0 else 0,\n            \"mean_hapax_ratio\": np.mean(hapax_ratio),\n            \"median_hapax_ratio\": np.median(hapax_ratio),\n            \"std_hapax_ratio\": np.std(hapax_ratio),\n            \"total_hapax\": hapax_values.sum(),\n            \"corpus_hapax_ratio\": hapax_values.sum() / total_tokens\n            if total_tokens &gt; 0\n            else 0,\n            \"mean_dislegomena_ratio\": np.mean(dislegomena_ratio),\n            \"median_dislegomena_ratio\": np.median(dislegomena_ratio),\n            \"total_dislegomena\": dislegomena_values.sum(),\n            \"corpus_dislegomena_ratio\": dislegomena_values.sum() / total_tokens\n            if total_tokens &gt; 0\n            else 0,\n        }\n\n    def _get_doc_stats_df(self) -&gt; pd.DataFrame:\n        \"\"\"Get a Pandas dataframe containing the statistics of each record.\n\n        Returns:\n            pd.DataFrame: A Pandas dataframe containing statistics of each record.\n        \"\"\"\n        # Check if empty corpus is given.\n        if self.df.empty:\n            raise ValueError(\n                \"The DataFrame is empty. Please provide a valid DataFrame.\"\n            )\n\n        # Convert the DataFrame to dense format\n        df = self.dtm.to_df().sparse.to_dense()\n\n        # Replace the unique columns with the original columns (labels)\n        df.columns = self.labels\n\n        # Transpose the DataFrame so that documents are rows\n        df = df.T\n\n        # Create file_stats DataFrame\n        file_stats = pd.DataFrame(self.labels, columns=[\"Documents\"])\n        file_stats.set_index(\"Documents\", inplace=True)\n\n        # Count terms appearing exactly once in each document\n        file_stats[f\"hapax_legomena\"] = df.eq(1).sum(axis=1)\n\n        # Calculate total tokens in each document\n        file_stats[\"total_tokens\"] = df.sum(axis=1)\n\n        # Number of distinct terms in each document\n        file_stats[\"total_terms\"] = df.ne(0).sum(axis=1)\n\n        # Calculate vocabulary density\n        file_stats[\"vocabulary_density\"] = (\n            file_stats[\"total_terms\"] / file_stats[\"total_tokens\"] * 100\n        ).round(2)\n\n        # Add hapax dislegomena (words appearing exactly twice)\n        file_stats[\"hapax_dislegomena\"] = df.eq(2).sum(axis=1)\n\n        return file_stats\n\n    def get_iqr_outliers(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Get the interquartile range (IQR) outliers in the Corpus.\n\n        Returns:\n            list[tuple[str, str]]: A list of tuples containing the record ID\n            and record name for each outlier.\n        \"\"\"\n        return self.iqr_outliers\n\n    def get_std_outliers(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Get the standard deviation outliers in the Corpus.\n\n        Returns:\n            list[tuple[str, str]]: A list of tuples containing the record ID\n            and record name for each outlier.\n        \"\"\"\n        # Get doc lengths from the doc_stats_df\n        doc_lengths = self.doc_stats_df[\"total_tokens\"].values\n\n        # Calculate mean and std\n        mean = doc_lengths.mean()\n        std_dev = doc_lengths.std()\n\n        return [\n            (str(self.ids[i]), str(self.labels[i]))\n            for i, length in enumerate(doc_lengths)\n            if abs(length - mean) &gt; 2 * std_dev\n        ]\n\n    def compare_groups(\n        self,\n        group1_labels: list[str],\n        group2_labels: list[str],\n        metric: str = \"total_tokens\",\n        test_type: str = \"mann_whitney\",\n    ) -&gt; dict:\n        \"\"\"Compare two groups of records using statistical tests.\n\n        Args:\n            group1_labels: List of record labels for group 1\n            group2_labels: List of record labels for group 2\n            metric: Column name to compare (default: \"total_tokens\")\n            test_type: Statistical test to use (\"mann_whitney\", \"t_test\", \"welch_t\")\n\n        Returns:\n            dict: Test results including statistic, p-value, and effect size\n        \"\"\"\n        doc_stats = self.doc_stats_df\n\n        # Get values for each group\n        group1_values = doc_stats.loc[group1_labels, metric].values\n        group2_values = doc_stats.loc[group2_labels, metric].values\n\n        results = {\n            \"group1_size\": len(group1_values),\n            \"group2_size\": len(group2_values),\n            \"group1_mean\": np.mean(group1_values),\n            \"group2_mean\": np.mean(group2_values),\n            \"metric\": metric,\n            \"test_type\": test_type,\n        }\n\n        if test_type == \"mann_whitney\":\n            # Mann-Whitney U test: Non-parametric test comparing distributions\n            # Tests null hypothesis that distributions are identical\n            statistic, p_value = stats.mannwhitneyu(\n                group1_values, group2_values, alternative=\"two-sided\"\n            )\n\n            # Calculate effect size using rank biserial correlation:\n            # Formula: r = (2U)/(n1*n2) - 1\n            # Where U is the Mann-Whitney U statistic\n            # Range: -1 to +1 (like Pearson correlation)\n            # Interpretation: proportion of pairs where group1 &gt; group2, adjusted\n            n1, n2 = len(group1_values), len(group2_values)\n            effect_size = (2 * statistic) / (n1 * n2) - 1\n            results.update(\n                {\n                    \"statistic\": statistic,\n                    \"p_value\": p_value,\n                    \"effect_size\": effect_size,\n                    \"effect_size_interpretation\": self._interpret_effect_size(\n                        abs(effect_size)\n                    ),\n                }\n            )\n\n        elif test_type == \"t_test\":\n            # Independent samples t-test: Parametric test assuming equal variances\n            # Tests null hypothesis that population means are equal\n            statistic, p_value = stats.ttest_ind(\n                group1_values, group2_values, equal_var=True\n            )\n\n            # Calculate Cohen's d effect size:\n            # First, compute pooled standard deviation using formula:\n            # s_pooled = sqrt[((n1-1)*s1\u00b2 + (n2-1)*s2\u00b2) / (n1+n2-2)]\n            # This weights each group's variance by its degrees of freedom\n            pooled_std = np.sqrt(\n                (\n                    (len(group1_values) - 1) * np.var(group1_values, ddof=1)\n                    + (len(group2_values) - 1) * np.var(group2_values, ddof=1)\n                )\n                / (len(group1_values) + len(group2_values) - 2)\n            )\n\n            # Cohen's d = (mean1 - mean2) / pooled_std\n            # Standardized mean difference in pooled standard deviation units\n            # Interpretation: 0.2=small, 0.5=medium, 0.8=large effect\n            cohens_d = (np.mean(group1_values) - np.mean(group2_values)) / pooled_std\n            results.update(\n                {\n                    \"statistic\": statistic,\n                    \"p_value\": p_value,\n                    \"effect_size\": cohens_d,\n                    \"effect_size_interpretation\": self._interpret_cohens_d(\n                        abs(cohens_d)\n                    ),\n                }\n            )\n\n        elif test_type == \"welch_t\":\n            # Welch's t-test: Parametric test NOT assuming equal variances\n            # Uses Satterthwaite approximation for degrees of freedom\n            # More robust when group variances differ substantially\n            statistic, p_value = stats.ttest_ind(\n                group1_values, group2_values, equal_var=False\n            )\n\n            # Calculate Cohen's d for unequal variances:\n            # Even though we use Welch's t-test, we still use pooled std for Cohen's d\n            # as it provides a standardized effect size comparable across studies\n            s1, s2 = np.std(group1_values, ddof=1), np.std(group2_values, ddof=1)\n            n1, n2 = len(group1_values), len(group2_values)\n\n            # Pooled standard deviation (same formula as regular t-test)\n            # This maintains comparability of effect sizes across different test types\n            pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n\n            # Cohen's d = standardized mean difference\n            cohens_d = (np.mean(group1_values) - np.mean(group2_values)) / pooled_std\n            results.update(\n                {\n                    \"statistic\": statistic,\n                    \"p_value\": p_value,\n                    \"effect_size\": cohens_d,\n                    \"effect_size_interpretation\": self._interpret_cohens_d(\n                        abs(cohens_d)\n                    ),\n                }\n            )\n\n        # Add significance interpretation\n        results[\"is_significant\"] = p_value &lt; 0.05\n        results[\"significance_level\"] = (\n            \"p &lt; 0.001\"\n            if p_value &lt; 0.001\n            else \"p &lt; 0.01\"\n            if p_value &lt; 0.01\n            else \"p &lt; 0.05\"\n            if p_value &lt; 0.05\n            else \"ns\"\n        )\n\n        return results\n\n    def bootstrap_confidence_interval(\n        self,\n        metric: str = \"total_tokens\",\n        confidence_level: float = 0.95,\n        n_bootstrap: int = 1000,\n    ) -&gt; dict:\n        \"\"\"Calculate bootstrap confidence intervals for a given metric.\n\n        Args:\n            metric: Column name to analyze\n            confidence_level: Confidence level (default: 0.95 for 95% CI)\n            n_bootstrap: Number of bootstrap samples\n\n        Returns:\n            dict: Bootstrap statistics including confidence intervals\n        \"\"\"\n        values = self.doc_stats_df[metric].values\n\n        # Bootstrap sampling\n        bootstrap_means = []\n        for _ in range(n_bootstrap):\n            bootstrap_sample = np.random.choice(values, size=len(values), replace=True)\n            bootstrap_means.append(np.mean(bootstrap_sample))\n\n        bootstrap_means = np.array(bootstrap_means)\n\n        # Calculate confidence intervals\n        alpha = 1 - confidence_level\n        lower_percentile = (alpha / 2) * 100\n        upper_percentile = (1 - alpha / 2) * 100\n\n        ci_lower = np.percentile(bootstrap_means, lower_percentile)\n        ci_upper = np.percentile(bootstrap_means, upper_percentile)\n\n        return {\n            \"metric\": metric,\n            \"confidence_level\": confidence_level,\n            \"n_bootstrap\": n_bootstrap,\n            \"original_mean\": np.mean(values),\n            \"bootstrap_mean\": np.mean(bootstrap_means),\n            \"bootstrap_std\": np.std(bootstrap_means),\n            \"ci_lower\": ci_lower,\n            \"ci_upper\": ci_upper,\n            \"margin_of_error\": (ci_upper - ci_lower) / 2,\n        }\n\n    def _interpret_effect_size(self, effect_size: float) -&gt; str:\n        \"\"\"Interpret effect size magnitude.\"\"\"\n        if effect_size &lt; 0.1:\n            return \"negligible\"\n        elif effect_size &lt; 0.3:\n            return \"small\"\n        elif effect_size &lt; 0.5:\n            return \"medium\"\n        else:\n            return \"large\"\n\n    def _interpret_cohens_d(self, cohens_d: float) -&gt; str:\n        \"\"\"Interpret Cohen's d effect size.\"\"\"\n        if cohens_d &lt; 0.2:\n            return \"negligible\"\n        elif cohens_d &lt; 0.5:\n            return \"small\"\n        elif cohens_d &lt; 0.8:\n            return \"medium\"\n        else:\n            return \"large\"\n\n    @cached_property\n    def advanced_lexical_diversity(self) -&gt; dict[str, float]:\n        \"\"\"Calculate advanced lexical diversity measures beyond simple TTR.\n\n        Returns:\n            dict: Advanced diversity measures including MTLD, HD-D, and more\n        \"\"\"\n        doc_stats = self.doc_stats_df\n\n        # Moving Average Type-Token Ratio (MATTR) simulation\n        # Calculate TTR for overlapping windows to reduce text length sensitivity\n        def calculate_mattr(tokens_list: list[str], window_size: int = 50) -&gt; float:\n            if len(tokens_list) &lt; window_size:\n                return len(set(tokens_list)) / len(tokens_list) if tokens_list else 0\n\n            ttrs = []\n            for i in range(len(tokens_list) - window_size + 1):\n                window = tokens_list[i : i + window_size]\n                ttr = len(set(window)) / len(window)\n                ttrs.append(ttr)\n            return np.mean(ttrs) if ttrs else 0\n\n        # Corrected TTR (CTTR) - TTR divided by square root of tokens\n        def calculate_cttr(types: int, tokens: int) -&gt; float:\n            return types / np.sqrt(2 * tokens) if tokens &gt; 0 else 0\n\n        # Root TTR (RTTR) - Types divided by square root of tokens\n        def calculate_rttr(types: int, tokens: int) -&gt; float:\n            return types / np.sqrt(tokens) if tokens &gt; 0 else 0\n\n        # Log TTR (LogTTR) - Log of types divided by log of tokens\n        def calculate_log_ttr(types: int, tokens: int) -&gt; float:\n            if types &gt; 0 and tokens &gt; 0:\n                return np.log(types) / np.log(tokens)\n            return 0\n\n        # Calculate for each document\n        doc_diversity = []\n        for _, row in doc_stats.iterrows():\n            tokens = row[\"total_tokens\"]\n            types = row[\"total_terms\"]\n            if isinstance(tokens, float) and math.isnan(tokens):\n                tokens = 0\n            else:\n                tokens = int(tokens)\n            if isinstance(types, float) and math.isnan(types):\n                types = 0\n            else:\n                types = int(types)\n\n            diversity = {\n                \"ttr\": types / tokens if tokens &gt; 0 else 0,\n                \"cttr\": calculate_cttr(types, tokens),\n                \"rttr\": calculate_rttr(types, tokens),\n                \"log_ttr\": calculate_log_ttr(types, tokens),\n            }\n            doc_diversity.append(diversity)\n\n        # Aggregate statistics\n        diversity_df = pd.DataFrame(doc_diversity)\n\n        return {\n            \"mean_cttr\": diversity_df[\"cttr\"].mean(),\n            \"median_cttr\": diversity_df[\"cttr\"].median(),\n            \"std_cttr\": diversity_df[\"cttr\"].std(),\n            \"mean_rttr\": diversity_df[\"rttr\"].mean(),\n            \"median_rttr\": diversity_df[\"rttr\"].median(),\n            \"std_rttr\": diversity_df[\"rttr\"].std(),\n            \"mean_log_ttr\": diversity_df[\"log_ttr\"].mean(),\n            \"median_log_ttr\": diversity_df[\"log_ttr\"].median(),\n            \"std_log_ttr\": diversity_df[\"log_ttr\"].std(),\n            \"diversity_range\": diversity_df[\"ttr\"].max() - diversity_df[\"ttr\"].min(),\n            \"diversity_coefficient_variation\": diversity_df[\"ttr\"].std()\n            / diversity_df[\"ttr\"].mean()\n            if diversity_df[\"ttr\"].mean() &gt; 0\n            else 0,\n        }\n\n    @cached_property\n    def zipf_analysis(self) -&gt; dict[str, float | bool | str]:\n        \"\"\"Analyze corpus term frequency distribution using Zipf's law.\n\n        Returns:\n            dict: Zipf distribution analysis including slope, R-squared, and goodness of fit\n        \"\"\"\n        try:\n            # Get term frequencies from DTM\n            if hasattr(self.dtm, \"sorted_term_counts\"):\n                term_counts = list(self.dtm.sorted_term_counts.values())\n            else:\n                # Fallback: aggregate from document stats\n                df = self.dtm.to_df().sparse.to_dense()\n                term_counts = df.sum(axis=0).sort_values(ascending=False).values\n\n            if len(term_counts) &lt; 10:  # Need sufficient data for meaningful analysis\n                return {\n                    \"zipf_slope\": 0.0,\n                    \"zipf_intercept\": 0.0,\n                    \"r_squared\": 0.0,\n                    \"zipf_goodness_of_fit\": \"insufficient_data\",\n                    \"follows_zipf\": False,\n                    \"num_terms\": len(term_counts),\n                }\n\n            # Filter out zero counts and prepare for log-log analysis\n            term_counts = [count for count in term_counts if count &gt; 0]\n            ranks = np.arange(1, len(term_counts) + 1)\n\n            # Log-log transformation for Zipf analysis\n            log_ranks = np.log10(ranks)\n            log_freqs = np.log10(term_counts)\n\n            # Linear regression on log-log data\n            slope, intercept, r_value, p_value, std_err = stats.linregress(\n                log_ranks, log_freqs\n            )\n            r_squared = r_value**2\n\n            # Zipf's law predicts slope around -1\n            zipf_deviation = abs(slope + 1.0)  # How far from ideal Zipf slope of -1\n\n            # Classify goodness of fit\n            if r_squared &gt; 0.9 and zipf_deviation &lt; 0.3:\n                fit_quality = \"excellent\"\n                follows_zipf = True\n            elif r_squared &gt; 0.8 and zipf_deviation &lt; 0.5:\n                fit_quality = \"good\"\n                follows_zipf = True\n            elif r_squared &gt; 0.6 and zipf_deviation &lt; 0.7:\n                fit_quality = \"moderate\"\n                follows_zipf = True\n            else:\n                fit_quality = \"poor\"\n                follows_zipf = False\n\n            return {\n                \"zipf_slope\": slope,\n                \"zipf_intercept\": intercept,\n                \"r_squared\": r_squared,\n                \"p_value\": p_value,\n                \"std_error\": std_err,\n                \"zipf_deviation\": zipf_deviation,\n                \"zipf_goodness_of_fit\": fit_quality,\n                \"follows_zipf\": follows_zipf,\n                \"num_terms\": len(term_counts),\n                \"frequency_range\": {\n                    \"min\": int(min(term_counts)),\n                    \"max\": int(max(term_counts)),\n                    \"ratio\": max(term_counts) / min(term_counts)\n                    if min(term_counts) &gt; 0\n                    else 0,\n                },\n            }\n\n        except Exception as e:\n            return {\n                \"zipf_slope\": 0.0,\n                \"zipf_intercept\": 0.0,\n                \"r_squared\": 0.0,\n                \"zipf_goodness_of_fit\": f\"error: {str(e)}\",\n                \"follows_zipf\": False,\n                \"num_terms\": 0,\n            }\n\n    @cached_property\n    def corpus_quality_metrics(self) -&gt; dict[str, float | int | str]:\n        \"\"\"Calculate corpus quality and balance metrics for research validity.\n\n        Returns:\n            dict: Quality metrics including balance, coverage, and sampling adequacy\n        \"\"\"\n        doc_stats = self.doc_stats_df\n\n        # Document length balance analysis\n        doc_lengths = doc_stats[\"total_tokens\"].values\n        length_cv = (\n            np.std(doc_lengths) / np.mean(doc_lengths)\n            if np.mean(doc_lengths) &gt; 0\n            else 0\n        )\n\n        # Vocabulary density balance\n        vocab_densities = doc_stats[\"vocabulary_density\"].values / 100\n        density_cv = (\n            np.std(vocab_densities) / np.mean(vocab_densities)\n            if np.mean(vocab_densities) &gt; 0\n            else 0\n        )\n\n        # Term coverage analysis\n        total_tokens = doc_stats[\"total_tokens\"].sum()\n        total_unique_terms = (\n            len(self.dtm.sorted_terms_list)\n            if hasattr(self.dtm, \"sorted_terms_list\")\n            else doc_stats[\"total_terms\"].sum()\n        )\n\n        # Hapax analysis for vocabulary richness\n        total_hapax = doc_stats[\"hapax_legomena\"].sum()\n        total_dislegomena = doc_stats[\"hapax_dislegomena\"].sum()\n\n        # Calculate vocabulary growth rate (simplified)\n        # This approximates how much new vocabulary each document contributes\n        vocab_growth = total_unique_terms / len(doc_stats) if len(doc_stats) &gt; 0 else 0\n\n        # Corpus balance classification\n        def classify_balance(cv: float) -&gt; str:\n            if cv &lt; 0.2:\n                return \"very_balanced\"\n            elif cv &lt; 0.4:\n                return \"balanced\"\n            elif cv &lt; 0.6:\n                return \"moderately_unbalanced\"\n            else:\n                return \"highly_unbalanced\"\n\n        # Sampling adequacy (based on vocabulary saturation)\n        vocab_saturation = (\n            total_hapax / total_unique_terms if total_unique_terms &gt; 0 else 0\n        )\n\n        def assess_sampling_adequacy(saturation: float) -&gt; str:\n            if saturation &lt; 0.1:\n                return \"excellent\"  # Very few hapax, good coverage\n            elif saturation &lt; 0.3:\n                return \"good\"\n            elif saturation &lt; 0.5:\n                return \"adequate\"\n            else:\n                return \"insufficient\"  # Too many hapax, need more data\n\n        return {\n            \"document_length_balance\": {\n                \"coefficient_variation\": length_cv,\n                \"classification\": classify_balance(length_cv),\n                \"range_ratio\": np.max(doc_lengths) / np.min(doc_lengths)\n                if np.min(doc_lengths) &gt; 0\n                else 0,\n            },\n            \"vocabulary_density_balance\": {\n                \"coefficient_variation\": density_cv,\n                \"classification\": classify_balance(density_cv),\n            },\n            \"corpus_coverage\": {\n                \"total_tokens\": int(total_tokens),\n                \"unique_terms\": int(total_unique_terms),\n                \"coverage_ratio\": total_unique_terms / total_tokens\n                if total_tokens &gt; 0\n                else 0,\n                \"vocab_growth_per_doc\": vocab_growth,\n            },\n            \"vocabulary_richness\": {\n                \"hapax_ratio\": total_hapax / total_tokens if total_tokens &gt; 0 else 0,\n                \"dislegomena_ratio\": total_dislegomena / total_tokens\n                if total_tokens &gt; 0\n                else 0,\n                \"vocabulary_saturation\": vocab_saturation,\n                \"sampling_adequacy\": assess_sampling_adequacy(vocab_saturation),\n            },\n            \"corpus_size_metrics\": {\n                \"num_documents\": len(doc_stats),\n                \"mean_doc_length\": np.mean(doc_lengths),\n                \"median_doc_length\": np.median(doc_lengths),\n                \"recommended_min_docs\": max(\n                    30, int(total_unique_terms * 0.1)\n                ),  # Rule of thumb\n                \"size_adequacy\": \"adequate\" if len(doc_stats) &gt;= 30 else \"small\",\n            },\n        }\n\n    @validate_call(config=model_config)\n    def plot(\n        self,\n        column: str = \"token_lengths\",\n        type: str = \"seaborn_boxplot\",\n        title: str = None,\n    ) -&gt; None:\n        \"\"\"Generate a plot of the Corpus.\n\n        Args:\n            column: The column to plot from the doc_stats_df.\n            type: The type of plot to generate. Currently only \"seaborn_boxplot\" and \"plotly_boxplot\" are supported.\n            title: The title of the plot. If None, the plotting function's default is used.\n        \"\"\"\n        supported_types = [\"seaborn_boxplot\", \"plotly_boxplot\"]\n        if type not in supported_types:\n            raise ValueError(\n                f\"Unsupported plot type: {type}. The following types are supported: {', '.join(supported_types)}.\"\n            )\n\n        # Use default title if None is provided\n        plot_title = title if title is not None else \"Corpus Boxplot\"\n\n        if type == \"seaborn_boxplot\":\n            get_seaborn_boxplot(self.doc_stats_df, column=column, title=plot_title)\n        elif type == \"plotly_boxplot\":\n            get_plotly_boxplot(self.doc_stats_df, column=column, title=plot_title)\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.advanced_lexical_diversity","title":"<code>advanced_lexical_diversity: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Calculate advanced lexical diversity measures beyond simple TTR.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, float]</code> <p>Advanced diversity measures including MTLD, HD-D, and more</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.corpus_quality_metrics","title":"<code>corpus_quality_metrics: dict[str, float | int | str]</code>  <code>cached</code> <code>property</code>","text":"<p>Calculate corpus quality and balance metrics for research validity.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, float | int | str]</code> <p>Quality metrics including balance, coverage, and sampling adequacy</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the Document-Term Matrix (DTM) in sparse format.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.distribution_stats","title":"<code>distribution_stats: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get comprehensive distribution statistics for record lengths.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary containing skewness, kurtosis, and normality test results.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.doc_stats_df","title":"<code>doc_stats_df: pd.DataFrame</code>  <code>cached</code> <code>property</code>","text":"<p>Get a Pandas dataframe containing the statistics of each record.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas dataframe containing statistics of each record.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.dtm","title":"<code>dtm: DTM = None</code>  <code>pydantic-field</code>","text":"<p>Document-Term Matrix (DTM) for the Corpus.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.iqr_bounds","title":"<code>iqr_bounds: tuple[float, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the IQR outlier bounds for total tokens.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: A tuple containing (lower_bound, upper_bound)</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.iqr_outliers","title":"<code>iqr_outliers: list[tuple[str, str]]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the IQR outliers for total tokens.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples containing the record ID</p> <code>list[tuple[str, str]]</code> <p>and record name for each outlier.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.iqr_values","title":"<code>iqr_values: tuple[float, float, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the Q1, Q3, and IQR values for total tokens.</p> <p>Returns:</p> Type Description <code>tuple[float, float, float]</code> <p>tuple[float, float, float]: A tuple containing (q1, q3, iqr)</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.mean","title":"<code>mean: float</code>  <code>property</code>","text":"<p>Get the mean of the total tokens in the Corpus.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.mean_and_spread","title":"<code>mean_and_spread: tuple[float, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the mean and standard deviation of the total tokens in the Corpus.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.percentiles","title":"<code>percentiles: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get comprehensive percentile analysis for record lengths.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary containing various percentiles.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.standard_deviation","title":"<code>standard_deviation: float</code>  <code>property</code>","text":"<p>Get the standard deviation of the total tokens in the Corpus.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.text_diversity_stats","title":"<code>text_diversity_stats: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get text-specific diversity and complexity statistics.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary containing lexical diversity measures.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.zipf_analysis","title":"<code>zipf_analysis: dict[str, float | bool | str]</code>  <code>cached</code> <code>property</code>","text":"<p>Analyze corpus term frequency distribution using Zipf's law.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, float | bool | str]</code> <p>Zipf distribution analysis including slope, R-squared, and goodness of fit</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the CorpusStats and create the DTM.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the CorpusStats and create the DTM.\"\"\"\n    super().__init__(**data)\n    # Separate the ids and labels from the docs\n    object.__setattr__(self, \"ids\", [doc[0] for doc in self.docs])\n    object.__setattr__(self, \"labels\", [doc[1] for doc in self.docs])\n\n    # Configure the DTM vectorizer with the provided settings\n    vectorizer_kwargs = {}\n    if self.min_df is not None:\n        vectorizer_kwargs[\"min_df\"] = self.min_df\n    if self.max_df is not None:\n        vectorizer_kwargs[\"max_df\"] = self.max_df\n    if self.max_n_terms is not None:\n        vectorizer_kwargs[\"max_n_terms\"] = self.max_n_terms\n\n    # Create and initialize the Document-Term Matrix (DTM) using the provided token lists\n    object.__setattr__(self, \"dtm\", DTM())\n    # Pass vectorizer kwargs during the call rather than initialization\n    # NB. DTM.to_df() will not work unless columns are unique labels\n    self.dtm(\n        docs=[doc[2] for doc in self.docs],\n        labels=make_labels_unique(self.labels),\n        **vectorizer_kwargs,\n    )\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.bootstrap_confidence_interval","title":"<code>bootstrap_confidence_interval(metric: str = 'total_tokens', confidence_level: float = 0.95, n_bootstrap: int = 1000) -&gt; dict</code>","text":"<p>Calculate bootstrap confidence intervals for a given metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>Column name to analyze</p> <code>'total_tokens'</code> <code>confidence_level</code> <code>float</code> <p>Confidence level (default: 0.95 for 95% CI)</p> <code>0.95</code> <code>n_bootstrap</code> <code>int</code> <p>Number of bootstrap samples</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Bootstrap statistics including confidence intervals</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def bootstrap_confidence_interval(\n    self,\n    metric: str = \"total_tokens\",\n    confidence_level: float = 0.95,\n    n_bootstrap: int = 1000,\n) -&gt; dict:\n    \"\"\"Calculate bootstrap confidence intervals for a given metric.\n\n    Args:\n        metric: Column name to analyze\n        confidence_level: Confidence level (default: 0.95 for 95% CI)\n        n_bootstrap: Number of bootstrap samples\n\n    Returns:\n        dict: Bootstrap statistics including confidence intervals\n    \"\"\"\n    values = self.doc_stats_df[metric].values\n\n    # Bootstrap sampling\n    bootstrap_means = []\n    for _ in range(n_bootstrap):\n        bootstrap_sample = np.random.choice(values, size=len(values), replace=True)\n        bootstrap_means.append(np.mean(bootstrap_sample))\n\n    bootstrap_means = np.array(bootstrap_means)\n\n    # Calculate confidence intervals\n    alpha = 1 - confidence_level\n    lower_percentile = (alpha / 2) * 100\n    upper_percentile = (1 - alpha / 2) * 100\n\n    ci_lower = np.percentile(bootstrap_means, lower_percentile)\n    ci_upper = np.percentile(bootstrap_means, upper_percentile)\n\n    return {\n        \"metric\": metric,\n        \"confidence_level\": confidence_level,\n        \"n_bootstrap\": n_bootstrap,\n        \"original_mean\": np.mean(values),\n        \"bootstrap_mean\": np.mean(bootstrap_means),\n        \"bootstrap_std\": np.std(bootstrap_means),\n        \"ci_lower\": ci_lower,\n        \"ci_upper\": ci_upper,\n        \"margin_of_error\": (ci_upper - ci_lower) / 2,\n    }\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.compare_groups","title":"<code>compare_groups(group1_labels: list[str], group2_labels: list[str], metric: str = 'total_tokens', test_type: str = 'mann_whitney') -&gt; dict</code>","text":"<p>Compare two groups of records using statistical tests.</p> <p>Parameters:</p> Name Type Description Default <code>group1_labels</code> <code>list[str]</code> <p>List of record labels for group 1</p> required <code>group2_labels</code> <code>list[str]</code> <p>List of record labels for group 2</p> required <code>metric</code> <code>str</code> <p>Column name to compare (default: \"total_tokens\")</p> <code>'total_tokens'</code> <code>test_type</code> <code>str</code> <p>Statistical test to use (\"mann_whitney\", \"t_test\", \"welch_t\")</p> <code>'mann_whitney'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Test results including statistic, p-value, and effect size</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def compare_groups(\n    self,\n    group1_labels: list[str],\n    group2_labels: list[str],\n    metric: str = \"total_tokens\",\n    test_type: str = \"mann_whitney\",\n) -&gt; dict:\n    \"\"\"Compare two groups of records using statistical tests.\n\n    Args:\n        group1_labels: List of record labels for group 1\n        group2_labels: List of record labels for group 2\n        metric: Column name to compare (default: \"total_tokens\")\n        test_type: Statistical test to use (\"mann_whitney\", \"t_test\", \"welch_t\")\n\n    Returns:\n        dict: Test results including statistic, p-value, and effect size\n    \"\"\"\n    doc_stats = self.doc_stats_df\n\n    # Get values for each group\n    group1_values = doc_stats.loc[group1_labels, metric].values\n    group2_values = doc_stats.loc[group2_labels, metric].values\n\n    results = {\n        \"group1_size\": len(group1_values),\n        \"group2_size\": len(group2_values),\n        \"group1_mean\": np.mean(group1_values),\n        \"group2_mean\": np.mean(group2_values),\n        \"metric\": metric,\n        \"test_type\": test_type,\n    }\n\n    if test_type == \"mann_whitney\":\n        # Mann-Whitney U test: Non-parametric test comparing distributions\n        # Tests null hypothesis that distributions are identical\n        statistic, p_value = stats.mannwhitneyu(\n            group1_values, group2_values, alternative=\"two-sided\"\n        )\n\n        # Calculate effect size using rank biserial correlation:\n        # Formula: r = (2U)/(n1*n2) - 1\n        # Where U is the Mann-Whitney U statistic\n        # Range: -1 to +1 (like Pearson correlation)\n        # Interpretation: proportion of pairs where group1 &gt; group2, adjusted\n        n1, n2 = len(group1_values), len(group2_values)\n        effect_size = (2 * statistic) / (n1 * n2) - 1\n        results.update(\n            {\n                \"statistic\": statistic,\n                \"p_value\": p_value,\n                \"effect_size\": effect_size,\n                \"effect_size_interpretation\": self._interpret_effect_size(\n                    abs(effect_size)\n                ),\n            }\n        )\n\n    elif test_type == \"t_test\":\n        # Independent samples t-test: Parametric test assuming equal variances\n        # Tests null hypothesis that population means are equal\n        statistic, p_value = stats.ttest_ind(\n            group1_values, group2_values, equal_var=True\n        )\n\n        # Calculate Cohen's d effect size:\n        # First, compute pooled standard deviation using formula:\n        # s_pooled = sqrt[((n1-1)*s1\u00b2 + (n2-1)*s2\u00b2) / (n1+n2-2)]\n        # This weights each group's variance by its degrees of freedom\n        pooled_std = np.sqrt(\n            (\n                (len(group1_values) - 1) * np.var(group1_values, ddof=1)\n                + (len(group2_values) - 1) * np.var(group2_values, ddof=1)\n            )\n            / (len(group1_values) + len(group2_values) - 2)\n        )\n\n        # Cohen's d = (mean1 - mean2) / pooled_std\n        # Standardized mean difference in pooled standard deviation units\n        # Interpretation: 0.2=small, 0.5=medium, 0.8=large effect\n        cohens_d = (np.mean(group1_values) - np.mean(group2_values)) / pooled_std\n        results.update(\n            {\n                \"statistic\": statistic,\n                \"p_value\": p_value,\n                \"effect_size\": cohens_d,\n                \"effect_size_interpretation\": self._interpret_cohens_d(\n                    abs(cohens_d)\n                ),\n            }\n        )\n\n    elif test_type == \"welch_t\":\n        # Welch's t-test: Parametric test NOT assuming equal variances\n        # Uses Satterthwaite approximation for degrees of freedom\n        # More robust when group variances differ substantially\n        statistic, p_value = stats.ttest_ind(\n            group1_values, group2_values, equal_var=False\n        )\n\n        # Calculate Cohen's d for unequal variances:\n        # Even though we use Welch's t-test, we still use pooled std for Cohen's d\n        # as it provides a standardized effect size comparable across studies\n        s1, s2 = np.std(group1_values, ddof=1), np.std(group2_values, ddof=1)\n        n1, n2 = len(group1_values), len(group2_values)\n\n        # Pooled standard deviation (same formula as regular t-test)\n        # This maintains comparability of effect sizes across different test types\n        pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n\n        # Cohen's d = standardized mean difference\n        cohens_d = (np.mean(group1_values) - np.mean(group2_values)) / pooled_std\n        results.update(\n            {\n                \"statistic\": statistic,\n                \"p_value\": p_value,\n                \"effect_size\": cohens_d,\n                \"effect_size_interpretation\": self._interpret_cohens_d(\n                    abs(cohens_d)\n                ),\n            }\n        )\n\n    # Add significance interpretation\n    results[\"is_significant\"] = p_value &lt; 0.05\n    results[\"significance_level\"] = (\n        \"p &lt; 0.001\"\n        if p_value &lt; 0.001\n        else \"p &lt; 0.01\"\n        if p_value &lt; 0.01\n        else \"p &lt; 0.05\"\n        if p_value &lt; 0.05\n        else \"ns\"\n    )\n\n    return results\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.get_iqr_outliers","title":"<code>get_iqr_outliers() -&gt; list[tuple[str, str]]</code>","text":"<p>Get the interquartile range (IQR) outliers in the Corpus.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples containing the record ID</p> <code>list[tuple[str, str]]</code> <p>and record name for each outlier.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def get_iqr_outliers(self) -&gt; list[tuple[str, str]]:\n    \"\"\"Get the interquartile range (IQR) outliers in the Corpus.\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples containing the record ID\n        and record name for each outlier.\n    \"\"\"\n    return self.iqr_outliers\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.get_std_outliers","title":"<code>get_std_outliers() -&gt; list[tuple[str, str]]</code>","text":"<p>Get the standard deviation outliers in the Corpus.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples containing the record ID</p> <code>list[tuple[str, str]]</code> <p>and record name for each outlier.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def get_std_outliers(self) -&gt; list[tuple[str, str]]:\n    \"\"\"Get the standard deviation outliers in the Corpus.\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples containing the record ID\n        and record name for each outlier.\n    \"\"\"\n    # Get doc lengths from the doc_stats_df\n    doc_lengths = self.doc_stats_df[\"total_tokens\"].values\n\n    # Calculate mean and std\n    mean = doc_lengths.mean()\n    std_dev = doc_lengths.std()\n\n    return [\n        (str(self.ids[i]), str(self.labels[i]))\n        for i, length in enumerate(doc_lengths)\n        if abs(length - mean) &gt; 2 * std_dev\n    ]\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.plot","title":"<code>plot(column: str = 'token_lengths', type: str = 'seaborn_boxplot', title: str = None) -&gt; None</code>","text":"<p>Generate a plot of the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The column to plot from the doc_stats_df.</p> <code>'token_lengths'</code> <code>type</code> <code>str</code> <p>The type of plot to generate. Currently only \"seaborn_boxplot\" and \"plotly_boxplot\" are supported.</p> <code>'seaborn_boxplot'</code> <code>title</code> <code>str</code> <p>The title of the plot. If None, the plotting function's default is used.</p> <code>None</code> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>@validate_call(config=model_config)\ndef plot(\n    self,\n    column: str = \"token_lengths\",\n    type: str = \"seaborn_boxplot\",\n    title: str = None,\n) -&gt; None:\n    \"\"\"Generate a plot of the Corpus.\n\n    Args:\n        column: The column to plot from the doc_stats_df.\n        type: The type of plot to generate. Currently only \"seaborn_boxplot\" and \"plotly_boxplot\" are supported.\n        title: The title of the plot. If None, the plotting function's default is used.\n    \"\"\"\n    supported_types = [\"seaborn_boxplot\", \"plotly_boxplot\"]\n    if type not in supported_types:\n        raise ValueError(\n            f\"Unsupported plot type: {type}. The following types are supported: {', '.join(supported_types)}.\"\n        )\n\n    # Use default title if None is provided\n    plot_title = title if title is not None else \"Corpus Boxplot\"\n\n    if type == \"seaborn_boxplot\":\n        get_seaborn_boxplot(self.doc_stats_df, column=column, title=plot_title)\n    elif type == \"plotly_boxplot\":\n        get_plotly_boxplot(self.doc_stats_df, column=column, title=plot_title)\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the CorpusStats and create the DTM.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the CorpusStats and create the DTM.\"\"\"\n    super().__init__(**data)\n    # Separate the ids and labels from the docs\n    object.__setattr__(self, \"ids\", [doc[0] for doc in self.docs])\n    object.__setattr__(self, \"labels\", [doc[1] for doc in self.docs])\n\n    # Configure the DTM vectorizer with the provided settings\n    vectorizer_kwargs = {}\n    if self.min_df is not None:\n        vectorizer_kwargs[\"min_df\"] = self.min_df\n    if self.max_df is not None:\n        vectorizer_kwargs[\"max_df\"] = self.max_df\n    if self.max_n_terms is not None:\n        vectorizer_kwargs[\"max_n_terms\"] = self.max_n_terms\n\n    # Create and initialize the Document-Term Matrix (DTM) using the provided token lists\n    object.__setattr__(self, \"dtm\", DTM())\n    # Pass vectorizer kwargs during the call rather than initialization\n    # NB. DTM.to_df() will not work unless columns are unique labels\n    self.dtm(\n        docs=[doc[2] for doc in self.docs],\n        labels=make_labels_unique(self.labels),\n        **vectorizer_kwargs,\n    )\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the Document-Term Matrix (DTM) in sparse format.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.doc_stats_df","title":"<code>doc_stats_df: pd.DataFrame</code>  <code>cached</code> <code>property</code>","text":"<p>Get a Pandas dataframe containing the statistics of each record.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas dataframe containing statistics of each record.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.mean_and_spread","title":"<code>mean_and_spread: tuple[float, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the mean and standard deviation of the total tokens in the Corpus.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.mean","title":"<code>mean: float</code>  <code>property</code>","text":"<p>Get the mean of the total tokens in the Corpus.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.standard_deviation","title":"<code>standard_deviation: float</code>  <code>property</code>","text":"<p>Get the standard deviation of the total tokens in the Corpus.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.iqr_values","title":"<code>iqr_values: tuple[float, float, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the Q1, Q3, and IQR values for total tokens.</p> <p>Returns:</p> Type Description <code>tuple[float, float, float]</code> <p>tuple[float, float, float]: A tuple containing (q1, q3, iqr)</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.iqr_bounds","title":"<code>iqr_bounds: tuple[float, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the IQR outlier bounds for total tokens.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: A tuple containing (lower_bound, upper_bound)</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.iqr_outliers","title":"<code>iqr_outliers: list[tuple[str, str]]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the IQR outliers for total tokens.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples containing the record ID</p> <code>list[tuple[str, str]]</code> <p>and record name for each outlier.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.distribution_stats","title":"<code>distribution_stats: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get comprehensive distribution statistics for record lengths.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary containing skewness, kurtosis, and normality test results.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.percentiles","title":"<code>percentiles: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get comprehensive percentile analysis for record lengths.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary containing various percentiles.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.text_diversity_stats","title":"<code>text_diversity_stats: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Get text-specific diversity and complexity statistics.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: Dictionary containing lexical diversity measures.</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats._get_doc_stats_df","title":"<code>_get_doc_stats_df() -&gt; pd.DataFrame</code>","text":"<p>Get a Pandas dataframe containing the statistics of each record.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A Pandas dataframe containing statistics of each record.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def _get_doc_stats_df(self) -&gt; pd.DataFrame:\n    \"\"\"Get a Pandas dataframe containing the statistics of each record.\n\n    Returns:\n        pd.DataFrame: A Pandas dataframe containing statistics of each record.\n    \"\"\"\n    # Check if empty corpus is given.\n    if self.df.empty:\n        raise ValueError(\n            \"The DataFrame is empty. Please provide a valid DataFrame.\"\n        )\n\n    # Convert the DataFrame to dense format\n    df = self.dtm.to_df().sparse.to_dense()\n\n    # Replace the unique columns with the original columns (labels)\n    df.columns = self.labels\n\n    # Transpose the DataFrame so that documents are rows\n    df = df.T\n\n    # Create file_stats DataFrame\n    file_stats = pd.DataFrame(self.labels, columns=[\"Documents\"])\n    file_stats.set_index(\"Documents\", inplace=True)\n\n    # Count terms appearing exactly once in each document\n    file_stats[f\"hapax_legomena\"] = df.eq(1).sum(axis=1)\n\n    # Calculate total tokens in each document\n    file_stats[\"total_tokens\"] = df.sum(axis=1)\n\n    # Number of distinct terms in each document\n    file_stats[\"total_terms\"] = df.ne(0).sum(axis=1)\n\n    # Calculate vocabulary density\n    file_stats[\"vocabulary_density\"] = (\n        file_stats[\"total_terms\"] / file_stats[\"total_tokens\"] * 100\n    ).round(2)\n\n    # Add hapax dislegomena (words appearing exactly twice)\n    file_stats[\"hapax_dislegomena\"] = df.eq(2).sum(axis=1)\n\n    return file_stats\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.get_iqr_outliers","title":"<code>get_iqr_outliers() -&gt; list[tuple[str, str]]</code>","text":"<p>Get the interquartile range (IQR) outliers in the Corpus.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples containing the record ID</p> <code>list[tuple[str, str]]</code> <p>and record name for each outlier.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def get_iqr_outliers(self) -&gt; list[tuple[str, str]]:\n    \"\"\"Get the interquartile range (IQR) outliers in the Corpus.\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples containing the record ID\n        and record name for each outlier.\n    \"\"\"\n    return self.iqr_outliers\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.get_std_outliers","title":"<code>get_std_outliers() -&gt; list[tuple[str, str]]</code>","text":"<p>Get the standard deviation outliers in the Corpus.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: A list of tuples containing the record ID</p> <code>list[tuple[str, str]]</code> <p>and record name for each outlier.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def get_std_outliers(self) -&gt; list[tuple[str, str]]:\n    \"\"\"Get the standard deviation outliers in the Corpus.\n\n    Returns:\n        list[tuple[str, str]]: A list of tuples containing the record ID\n        and record name for each outlier.\n    \"\"\"\n    # Get doc lengths from the doc_stats_df\n    doc_lengths = self.doc_stats_df[\"total_tokens\"].values\n\n    # Calculate mean and std\n    mean = doc_lengths.mean()\n    std_dev = doc_lengths.std()\n\n    return [\n        (str(self.ids[i]), str(self.labels[i]))\n        for i, length in enumerate(doc_lengths)\n        if abs(length - mean) &gt; 2 * std_dev\n    ]\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.compare_groups","title":"<code>compare_groups(group1_labels: list[str], group2_labels: list[str], metric: str = 'total_tokens', test_type: str = 'mann_whitney') -&gt; dict</code>","text":"<p>Compare two groups of records using statistical tests.</p> <p>Parameters:</p> Name Type Description Default <code>group1_labels</code> <code>list[str]</code> <p>List of record labels for group 1</p> required <code>group2_labels</code> <code>list[str]</code> <p>List of record labels for group 2</p> required <code>metric</code> <code>str</code> <p>Column name to compare (default: \"total_tokens\")</p> <code>'total_tokens'</code> <code>test_type</code> <code>str</code> <p>Statistical test to use (\"mann_whitney\", \"t_test\", \"welch_t\")</p> <code>'mann_whitney'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Test results including statistic, p-value, and effect size</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def compare_groups(\n    self,\n    group1_labels: list[str],\n    group2_labels: list[str],\n    metric: str = \"total_tokens\",\n    test_type: str = \"mann_whitney\",\n) -&gt; dict:\n    \"\"\"Compare two groups of records using statistical tests.\n\n    Args:\n        group1_labels: List of record labels for group 1\n        group2_labels: List of record labels for group 2\n        metric: Column name to compare (default: \"total_tokens\")\n        test_type: Statistical test to use (\"mann_whitney\", \"t_test\", \"welch_t\")\n\n    Returns:\n        dict: Test results including statistic, p-value, and effect size\n    \"\"\"\n    doc_stats = self.doc_stats_df\n\n    # Get values for each group\n    group1_values = doc_stats.loc[group1_labels, metric].values\n    group2_values = doc_stats.loc[group2_labels, metric].values\n\n    results = {\n        \"group1_size\": len(group1_values),\n        \"group2_size\": len(group2_values),\n        \"group1_mean\": np.mean(group1_values),\n        \"group2_mean\": np.mean(group2_values),\n        \"metric\": metric,\n        \"test_type\": test_type,\n    }\n\n    if test_type == \"mann_whitney\":\n        # Mann-Whitney U test: Non-parametric test comparing distributions\n        # Tests null hypothesis that distributions are identical\n        statistic, p_value = stats.mannwhitneyu(\n            group1_values, group2_values, alternative=\"two-sided\"\n        )\n\n        # Calculate effect size using rank biserial correlation:\n        # Formula: r = (2U)/(n1*n2) - 1\n        # Where U is the Mann-Whitney U statistic\n        # Range: -1 to +1 (like Pearson correlation)\n        # Interpretation: proportion of pairs where group1 &gt; group2, adjusted\n        n1, n2 = len(group1_values), len(group2_values)\n        effect_size = (2 * statistic) / (n1 * n2) - 1\n        results.update(\n            {\n                \"statistic\": statistic,\n                \"p_value\": p_value,\n                \"effect_size\": effect_size,\n                \"effect_size_interpretation\": self._interpret_effect_size(\n                    abs(effect_size)\n                ),\n            }\n        )\n\n    elif test_type == \"t_test\":\n        # Independent samples t-test: Parametric test assuming equal variances\n        # Tests null hypothesis that population means are equal\n        statistic, p_value = stats.ttest_ind(\n            group1_values, group2_values, equal_var=True\n        )\n\n        # Calculate Cohen's d effect size:\n        # First, compute pooled standard deviation using formula:\n        # s_pooled = sqrt[((n1-1)*s1\u00b2 + (n2-1)*s2\u00b2) / (n1+n2-2)]\n        # This weights each group's variance by its degrees of freedom\n        pooled_std = np.sqrt(\n            (\n                (len(group1_values) - 1) * np.var(group1_values, ddof=1)\n                + (len(group2_values) - 1) * np.var(group2_values, ddof=1)\n            )\n            / (len(group1_values) + len(group2_values) - 2)\n        )\n\n        # Cohen's d = (mean1 - mean2) / pooled_std\n        # Standardized mean difference in pooled standard deviation units\n        # Interpretation: 0.2=small, 0.5=medium, 0.8=large effect\n        cohens_d = (np.mean(group1_values) - np.mean(group2_values)) / pooled_std\n        results.update(\n            {\n                \"statistic\": statistic,\n                \"p_value\": p_value,\n                \"effect_size\": cohens_d,\n                \"effect_size_interpretation\": self._interpret_cohens_d(\n                    abs(cohens_d)\n                ),\n            }\n        )\n\n    elif test_type == \"welch_t\":\n        # Welch's t-test: Parametric test NOT assuming equal variances\n        # Uses Satterthwaite approximation for degrees of freedom\n        # More robust when group variances differ substantially\n        statistic, p_value = stats.ttest_ind(\n            group1_values, group2_values, equal_var=False\n        )\n\n        # Calculate Cohen's d for unequal variances:\n        # Even though we use Welch's t-test, we still use pooled std for Cohen's d\n        # as it provides a standardized effect size comparable across studies\n        s1, s2 = np.std(group1_values, ddof=1), np.std(group2_values, ddof=1)\n        n1, n2 = len(group1_values), len(group2_values)\n\n        # Pooled standard deviation (same formula as regular t-test)\n        # This maintains comparability of effect sizes across different test types\n        pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n\n        # Cohen's d = standardized mean difference\n        cohens_d = (np.mean(group1_values) - np.mean(group2_values)) / pooled_std\n        results.update(\n            {\n                \"statistic\": statistic,\n                \"p_value\": p_value,\n                \"effect_size\": cohens_d,\n                \"effect_size_interpretation\": self._interpret_cohens_d(\n                    abs(cohens_d)\n                ),\n            }\n        )\n\n    # Add significance interpretation\n    results[\"is_significant\"] = p_value &lt; 0.05\n    results[\"significance_level\"] = (\n        \"p &lt; 0.001\"\n        if p_value &lt; 0.001\n        else \"p &lt; 0.01\"\n        if p_value &lt; 0.01\n        else \"p &lt; 0.05\"\n        if p_value &lt; 0.05\n        else \"ns\"\n    )\n\n    return results\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.bootstrap_confidence_interval","title":"<code>bootstrap_confidence_interval(metric: str = 'total_tokens', confidence_level: float = 0.95, n_bootstrap: int = 1000) -&gt; dict</code>","text":"<p>Calculate bootstrap confidence intervals for a given metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>Column name to analyze</p> <code>'total_tokens'</code> <code>confidence_level</code> <code>float</code> <p>Confidence level (default: 0.95 for 95% CI)</p> <code>0.95</code> <code>n_bootstrap</code> <code>int</code> <p>Number of bootstrap samples</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Bootstrap statistics including confidence intervals</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def bootstrap_confidence_interval(\n    self,\n    metric: str = \"total_tokens\",\n    confidence_level: float = 0.95,\n    n_bootstrap: int = 1000,\n) -&gt; dict:\n    \"\"\"Calculate bootstrap confidence intervals for a given metric.\n\n    Args:\n        metric: Column name to analyze\n        confidence_level: Confidence level (default: 0.95 for 95% CI)\n        n_bootstrap: Number of bootstrap samples\n\n    Returns:\n        dict: Bootstrap statistics including confidence intervals\n    \"\"\"\n    values = self.doc_stats_df[metric].values\n\n    # Bootstrap sampling\n    bootstrap_means = []\n    for _ in range(n_bootstrap):\n        bootstrap_sample = np.random.choice(values, size=len(values), replace=True)\n        bootstrap_means.append(np.mean(bootstrap_sample))\n\n    bootstrap_means = np.array(bootstrap_means)\n\n    # Calculate confidence intervals\n    alpha = 1 - confidence_level\n    lower_percentile = (alpha / 2) * 100\n    upper_percentile = (1 - alpha / 2) * 100\n\n    ci_lower = np.percentile(bootstrap_means, lower_percentile)\n    ci_upper = np.percentile(bootstrap_means, upper_percentile)\n\n    return {\n        \"metric\": metric,\n        \"confidence_level\": confidence_level,\n        \"n_bootstrap\": n_bootstrap,\n        \"original_mean\": np.mean(values),\n        \"bootstrap_mean\": np.mean(bootstrap_means),\n        \"bootstrap_std\": np.std(bootstrap_means),\n        \"ci_lower\": ci_lower,\n        \"ci_upper\": ci_upper,\n        \"margin_of_error\": (ci_upper - ci_lower) / 2,\n    }\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats._interpret_effect_size","title":"<code>_interpret_effect_size(effect_size: float) -&gt; str</code>","text":"<p>Interpret effect size magnitude.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def _interpret_effect_size(self, effect_size: float) -&gt; str:\n    \"\"\"Interpret effect size magnitude.\"\"\"\n    if effect_size &lt; 0.1:\n        return \"negligible\"\n    elif effect_size &lt; 0.3:\n        return \"small\"\n    elif effect_size &lt; 0.5:\n        return \"medium\"\n    else:\n        return \"large\"\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats._interpret_cohens_d","title":"<code>_interpret_cohens_d(cohens_d: float) -&gt; str</code>","text":"<p>Interpret Cohen's d effect size.</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def _interpret_cohens_d(self, cohens_d: float) -&gt; str:\n    \"\"\"Interpret Cohen's d effect size.\"\"\"\n    if cohens_d &lt; 0.2:\n        return \"negligible\"\n    elif cohens_d &lt; 0.5:\n        return \"small\"\n    elif cohens_d &lt; 0.8:\n        return \"medium\"\n    else:\n        return \"large\"\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.advanced_lexical_diversity","title":"<code>advanced_lexical_diversity: dict[str, float]</code>  <code>cached</code> <code>property</code>","text":"<p>Calculate advanced lexical diversity measures beyond simple TTR.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, float]</code> <p>Advanced diversity measures including MTLD, HD-D, and more</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.zipf_analysis","title":"<code>zipf_analysis: dict[str, float | bool | str]</code>  <code>cached</code> <code>property</code>","text":"<p>Analyze corpus term frequency distribution using Zipf's law.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, float | bool | str]</code> <p>Zipf distribution analysis including slope, R-squared, and goodness of fit</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.corpus_quality_metrics","title":"<code>corpus_quality_metrics: dict[str, float | int | str]</code>  <code>cached</code> <code>property</code>","text":"<p>Calculate corpus quality and balance metrics for research validity.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, float | int | str]</code> <p>Quality metrics including balance, coverage, and sampling adequacy</p>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.CorpusStats.plot","title":"<code>plot(column: str = 'token_lengths', type: str = 'seaborn_boxplot', title: str = None) -&gt; None</code>","text":"<p>Generate a plot of the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The column to plot from the doc_stats_df.</p> <code>'token_lengths'</code> <code>type</code> <code>str</code> <p>The type of plot to generate. Currently only \"seaborn_boxplot\" and \"plotly_boxplot\" are supported.</p> <code>'seaborn_boxplot'</code> <code>title</code> <code>str</code> <p>The title of the plot. If None, the plotting function's default is used.</p> <code>None</code> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>@validate_call(config=model_config)\ndef plot(\n    self,\n    column: str = \"token_lengths\",\n    type: str = \"seaborn_boxplot\",\n    title: str = None,\n) -&gt; None:\n    \"\"\"Generate a plot of the Corpus.\n\n    Args:\n        column: The column to plot from the doc_stats_df.\n        type: The type of plot to generate. Currently only \"seaborn_boxplot\" and \"plotly_boxplot\" are supported.\n        title: The title of the plot. If None, the plotting function's default is used.\n    \"\"\"\n    supported_types = [\"seaborn_boxplot\", \"plotly_boxplot\"]\n    if type not in supported_types:\n        raise ValueError(\n            f\"Unsupported plot type: {type}. The following types are supported: {', '.join(supported_types)}.\"\n        )\n\n    # Use default title if None is provided\n    plot_title = title if title is not None else \"Corpus Boxplot\"\n\n    if type == \"seaborn_boxplot\":\n        get_seaborn_boxplot(self.doc_stats_df, column=column, title=plot_title)\n    elif type == \"plotly_boxplot\":\n        get_plotly_boxplot(self.doc_stats_df, column=column, title=plot_title)\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.make_labels_unique","title":"<code>make_labels_unique(labels: list[str]) -&gt; list[str]</code>","text":"<p>Make labels unique by adding suffixes recursively.</p> <p>For duplicate labels, append \"-001\", \"-002\", etc. This function handles the case where a label already has a suffix that would conflict with the generated suffixes by recursively renaming.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>list[str]</code> <p>A list of labels that may contain duplicates.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A new list with unique labels. Duplicates get suffixes \"-001\", \"-002\", etc.</p> Example <p>make_labels_unique([\"doc\", \"doc\", \"report\"]) [\"doc-001\", \"doc-002\", \"report\"]</p> <p>make_labels_unique([\"doc\", \"doc-001\", \"doc\"]) [\"doc-001-001\", \"doc-001-002\", \"doc-002\"]</p> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>def make_labels_unique(labels: list[str]) -&gt; list[str]:\n    \"\"\"Make labels unique by adding suffixes recursively.\n\n    For duplicate labels, append \"-001\", \"-002\", etc. This function handles\n    the case where a label already has a suffix that would conflict with\n    the generated suffixes by recursively renaming.\n\n    Args:\n        labels: A list of labels that may contain duplicates.\n\n    Returns:\n        A new list with unique labels. Duplicates get suffixes \"-001\", \"-002\", etc.\n\n    Example:\n        &gt;&gt;&gt; make_labels_unique([\"doc\", \"doc\", \"report\"])\n        [\"doc-001\", \"doc-002\", \"report\"]\n\n        &gt;&gt;&gt; make_labels_unique([\"doc\", \"doc-001\", \"doc\"])\n        [\"doc-001-001\", \"doc-001-002\", \"doc-002\"]\n    \"\"\"\n    if not labels:\n        return labels\n\n    label_counts = {}\n    result = []\n\n    for label in labels:\n        label_counts[label] = label_counts.get(label, 0) + 1\n\n    # Find all labels that appear more than once\n    duplicates = {label for label, count in label_counts.items() if count &gt; 1}\n\n    if not duplicates:\n        return labels\n\n    # Track which labels need suffixes\n    label_indices = {label: 0 for label in duplicates}\n\n    for label in labels:\n        if label in duplicates:\n            label_indices[label] += 1\n            new_label = f\"{label}-{label_indices[label]:03d}\"\n\n            # Recursively handle conflicts: if the new label is also a duplicate,\n            # rename all instances including the newly generated one\n            if new_label in label_counts or new_label in duplicates:\n                # Re-run make_labels_unique with the conflicting label treated as duplicate\n                temp_labels = result + [new_label] + labels[len(result) + 1 :]\n                return make_labels_unique(temp_labels)\n\n            result.append(new_label)\n        else:\n            result.append(label)\n\n    return result\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.get_seaborn_boxplot","title":"<code>get_seaborn_boxplot(df: pd.DataFrame, column: str, title: str = 'Corpus Boxplot') -&gt; None</code>","text":"<p>Get a boxplot of the specified column in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Pandas DataFrame.</p> required <code>column</code> <code>str</code> <p>The column to plot.</p> required <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>'Corpus Boxplot'</code> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef get_seaborn_boxplot(\n    df: pd.DataFrame, column: str, title: str = \"Corpus Boxplot\"\n) -&gt; None:\n    \"\"\"Get a boxplot of the specified column in the DataFrame.\n\n    Args:\n        df: A Pandas DataFrame.\n        column: The column to plot.\n        title: The title of the plot.\n    \"\"\"\n    sns.set_theme(style=\"darkgrid\")\n    ax = sns.boxplot(y=df[column], width=0.25)\n    sns.swarmplot(y=column, data=df, color=\"black\", ax=ax)\n    ax.set_title(title)\n    plt.show()\n</code></pre>"},{"location":"api/corpus/corpus_stats/#lexos.corpus.corpus_stats.get_plotly_boxplot","title":"<code>get_plotly_boxplot(df: pd.DataFrame, column: str, title: str = 'Corpus Boxplot') -&gt; None</code>","text":"<p>Get a boxplot of the specified column in the DataFrame using Plotly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Pandas DataFrame.</p> required <code>column</code> <code>str</code> <p>The column to plot.</p> required <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>'Corpus Boxplot'</code> Source code in <code>lexos/corpus/corpus_stats.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef get_plotly_boxplot(\n    df: pd.DataFrame, column: str, title: str = \"Corpus Boxplot\"\n) -&gt; None:\n    \"\"\"Get a boxplot of the specified column in the DataFrame using Plotly.\n\n    Args:\n        df: A Pandas DataFrame.\n        column: The column to plot.\n        title: The title of the plot.\n    \"\"\"\n    # Get file names.\n    labels = df.index.tolist()\n\n    # Set up the points.\n    scatter_plot = go.Scatter(\n        x=labels,\n        y=df[column].values,\n        hoverinfo=\"text\",\n        mode=\"markers\",\n        marker=dict(color=\"green\"),\n        text=labels,\n    )\n\n    # Set up the box plot.\n    box_plot = go.Box(\n        x0=0,  # Initial position of the box plot\n        y=df[column].values,\n        hoverinfo=\"y\",\n        marker=dict(color=\"green\"),\n        jitter=0.15,\n    )\n\n    # Create a figure with two subplots and fill the figure.\n    figure = make_subplots(rows=1, cols=2, shared_yaxes=False)\n    figure.append_trace(trace=scatter_plot, row=1, col=1)\n    figure.append_trace(trace=box_plot, row=1, col=2)\n\n    # Hide useless information on x-axis and set up title.\n    figure.layout.update(\n        title={\n            \"text\": title,\n            \"x\": 0.5,  # x position (0-1)\n            \"xanchor\": \"center\",  # Horizontal alignment\n            \"y\": 0.99,  # y position (0-1)\n            \"yanchor\": \"top\",  # Vertical alignment\n        },\n        height=300,\n        width=500,\n        dragmode=\"pan\",\n        showlegend=False,\n        margin=dict(r=0, b=30, t=15, pad=4),\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis=dict(\n            showline=False,\n            zeroline=False,\n            gridcolor=\"black\",\n            title=\"Total Tokens\",\n        ),\n        xaxis2=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis2=dict(\n            showline=False,\n            zeroline=False,\n            gridcolor=\"black\",\n        ),\n        hovermode=\"closest\",\n        paper_bgcolor=\"rgba(0, 0, 0, 0)\",\n        plot_bgcolor=\"rgba(0, 0, 0, 0)\",\n        font=dict(color=\"black\", size=14),\n    )\n\n    # Show the Plotly figure.\n    config = {\n        \"displaylogo\": False,\n        \"modeBarButtonsToRemove\": [\"toImage\", \"toggleSpikelines\"],\n        \"scrollZoom\": True,\n    }\n    figure.show(showlink=False, config=config)\n</code></pre>"},{"location":"api/corpus/record/","title":"record","text":""},{"location":"api/corpus/record/#module-description","title":"Module Description","text":"<p>The <code>record</code> module provides the <code>Record</code> class, which is the building block for every document in your corpus. Each <code>Record</code> wraps your text (or a parsed spaCy Doc) and metadata and offers a suite of methods for serialization, statistics, and manipulation.</p> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record","title":"<code>Record</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The main Record model.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>validate_assignment</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>id</code>                 (<code>int | UUID4</code>)             </li> <li> <code>name</code>                 (<code>Optional[str]</code>)             </li> <li> <code>is_active</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>content</code>                 (<code>Optional[Doc | str]</code>)             </li> <li> <code>model</code>                 (<code>Optional[str]</code>)             </li> <li> <code>extensions</code>                 (<code>list[str]</code>)             </li> <li> <code>data_source</code>                 (<code>Optional[str]</code>)             </li> <li> <code>meta</code>                 (<code>dict[str, Any]</code>)             </li> </ul> Source code in <code>lexos/corpus/record.py</code> <pre><code>class Record(BaseModel):\n    \"\"\"The main Record model.\"\"\"\n\n    id: int | UUID4 = uuid.uuid4()\n    name: Optional[str] = None\n    is_active: Optional[bool] = True\n    content: Optional[Doc | str] = None\n    model: Optional[str] = None\n    extensions: list[str] = Field(default_factory=list)\n    data_source: Optional[str] = None\n    meta: dict[str, Any] = Field(default_factory=dict)\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        validate_assignment=True,\n        json_schema_extra=DocJSONSchema.schema(),\n    )\n\n    @field_serializer(\"content\")\n    def serialize_content(self, content: Doc | str) -&gt; bytes | str:\n        \"\"\"Serialize the content to bytes if it is a Doc object.\n\n        Args:\n            content (Doc | str): The content to serialize.\n\n        Returns:\n            bytes | str: The serialized content as bytes if it is a Doc, otherwise the original string.\n        \"\"\"\n        if isinstance(content, Doc):\n            content.user_data[\"extensions\"] = {}\n            for ext in self.extensions:\n                content.user_data[\"extensions\"][ext] = [\n                    token._.get(ext) for token in content\n                ]\n            return content.to_bytes()\n        return content\n\n    @field_serializer(\"id\")\n    def serialize_id(self, id, _info) -&gt; str:\n        \"\"\"Always serialize ID as string for JSON compatibility.\n\n        Args:\n            id (UUID|int|str): The ID value being serialized.\n            _info (Any): Encoder info (pydantic serializer internals).\n\n        Returns:\n            str: The serialized ID as a string.\n        \"\"\"\n        return str(id)\n\n    @field_serializer(\"meta\")\n    def serialize_meta(self, meta: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Ensure metadata is JSON-serializable by converting special types to strings.\"\"\"\n        return self._sanitize_metadata(meta)\n\n    def _sanitize_metadata(self, metadata: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Convert non-JSON-serializable types to strings.\n\n        Args:\n            metadata: Original metadata dictionary\n\n        Returns:\n            Sanitized metadata dictionary with JSON-serializable values\n        \"\"\"\n        sanitized = {}\n        for key, value in metadata.items():\n            if isinstance(value, UUID):\n                sanitized[key] = str(value)\n            elif isinstance(value, (datetime, date)):\n                sanitized[key] = value.isoformat()\n            elif isinstance(value, Path):\n                sanitized[key] = str(value)\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_metadata(value)  # Recursive\n            elif isinstance(value, list):\n                sanitized[key] = [\n                    self._sanitize_metadata({\"item\": item})[\"item\"]\n                    if isinstance(item, dict)\n                    else str(item)\n                    if isinstance(item, (UUID, datetime, date, Path))\n                    else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n\n        return sanitized\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the record.\"\"\"\n        # We exclude `terms`, `text`, and `tokens` here because these are\n        # computed / cached fields that can rely on the record being parsed.\n        # For unparsed records, evaluating these computed properties will\n        # raise a LexosException. `__repr__` should be lightweight and safe\n        # to call in debugging contexts, so we exclude these computed fields\n        # intentionally.\n        fields = self.model_dump(exclude=[\"terms\", \"text\", \"tokens\"])\n        fields[\"is_parsed\"] = str(self.is_parsed)\n        if self.content and self.is_parsed:\n            fields[\"content\"] = f\"{self.content.text[:25]}...\"\n        elif self.content and not self.is_parsed:\n            fields[\"content\"] = f\"{self.content[:25]}...\"\n        else:\n            fields[\"content\"] = \"None\"\n        field_list = [f\"{k}={v}\" if v else f\"{k}=None\" for k, v in fields.items()]\n        return f\"Record({', '.join(field_list)})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a user-friendly string representation of the record for printing.\"\"\"\n        active = \"True\" if self.is_active else \"False\"\n        parsed = \"True\" if self.is_parsed else \"False\"\n\n        # Get a preview of content\n        if self.content is None:\n            content_preview = \"None\"\n        elif self.is_parsed:\n            content_preview = f\"'{self.content.text[:40]}...'\"\n        else:\n            content_preview = f\"'{self.content[:40]}...'\"\n\n        return f\"Record(id={self.id}, name={self.name!r}, active={active}, parsed={parsed}, content={content_preview})\"\n\n    @computed_field\n    @cached_property\n    def is_parsed(self) -&gt; bool:\n        \"\"\"Return whether the record is parsed.\n\n        Returns:\n            bool: True if the record content is a spaCy Doc, False otherwise.\n        \"\"\"\n        if isinstance(self.content, Doc):\n            return True\n        return False\n\n    @computed_field\n    @cached_property\n    def preview(self) -&gt; str:\n        \"\"\"Return a preview of the record text.\n\n        Returns:\n            str | None: A shortened preview of the record content, or None if content is None.\n        \"\"\"\n        if self.content is None:\n            return None\n\n        if self.is_parsed:\n            return f\"{self.content.text[0:50]}...\"\n        return f\"{self.content[0:500]}...\"\n\n    @computed_field\n    @cached_property\n    def terms(self) -&gt; Counter:\n        \"\"\"Return the terms in the record.\n\n        Returns:\n            Counter: Collection mapping term -&gt; count for the record.\n        \"\"\"\n        if self.is_parsed:\n            return Counter([t.text for t in self.content])\n        else:\n            raise LexosException(\"Record is not parsed.\")\n\n    @property\n    def text(self) -&gt; str:\n        \"\"\"Return the text of the record.\n\n        Returns:\n            str | None: The record text as string or None if no content is present.\n        \"\"\"\n        if self.is_parsed:\n            return self.content.text\n        return self.content\n\n    @cached_property\n    def tokens(self) -&gt; list[str]:\n        \"\"\"Return the tokens in the record.\n\n        Returns:\n            list[str]: A list of token strings extracted from the parsed content.\n        \"\"\"\n        if self.is_parsed:\n            return [t.text for t in self.content]\n        else:\n            raise LexosException(\"Record is not parsed.\")\n\n    def _doc_from_bytes(\n        self,\n        content: bytes,\n        model: Optional[str] = None,\n        model_cache: Optional[LexosModelCache] = None,\n    ) -&gt; Doc:\n        \"\"\"Convert bytes to a Doc object.\n\n        Args:\n            content (bytes): The bytes to convert.\n            model (Optional[str]): The spaCy model to use for loading the Doc.\n            model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n\n        Returns:\n            Doc: The content as a Doc object.\n        \"\"\"\n        # Create a Doc from the bytes\n        vocab = self._get_vocab(model, model_cache)\n        doc = Doc(vocab).from_bytes(content)\n\n        # Restore extension values\n        for ext, values in doc.user_data[\"extensions\"].items():\n            Token.set_extension(ext, default=None, force=True)\n            for i in range(len(doc)):\n                doc[i]._.set(ext, values[i])\n\n        # Clean up user_data\n        doc.user_data[\"extensions\"] = list(doc.user_data[\"extensions\"].keys())\n\n        return doc\n\n    # WARNING: This method is deprecated in favour of field serializer.\n    def _doc_to_bytes(self) -&gt; bytes:\n        \"\"\"Convert the content to bytes if it is a Doc object.\n\n        Returns:\n            bytes: The content as bytes.\n        \"\"\"\n        if not isinstance(self.content, Doc):\n            raise LexosException(\"Content is not a Doc object.\")\n\n        doc = self.content\n\n        doc.user_data[\"extensions\"] = {}\n        for ext in self.extensions:\n            doc.user_data[\"extensions\"][ext] = [token._.get(ext) for token in doc]\n\n        return doc.to_bytes()\n\n    def _get_vocab(\n        self, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None\n    ) -&gt; Vocab:\n        \"\"\"Get the vocabulary from the model or model cache.\n\n        Args:\n            model (Optional[str]): The spaCy model to use for loading the Doc.\n            model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n\n        Returns:\n            Vocab: The vocabulary of the model.\n        \"\"\"\n        if model_cache and not model:\n            raise LexosException(\"Model cache provided but no model specified.\")\n\n        if model_cache:\n            return model_cache.get_model(model).vocab\n        elif model:\n            return spacy.load(model).vocab\n        elif self.model:\n            return spacy.load(self.model).vocab\n        else:\n            raise LexosException(\n                \"No model specified for loading the Doc. Please provide a model name or a model cache.\"\n            )\n\n    @validate_call(config=model_config)\n    def from_bytes(\n        self,\n        bytestring: bytes,\n        model: Optional[str] = None,\n        model_cache: Optional[LexosModelCache] = None,\n        verify_hash: bool = True,\n    ) -&gt; None:\n        \"\"\"Deserialise the record from bytes.\n\n        Args:\n            bytestring (bytes): The bytes to load the record from.\n            model (Optional[str]): The spaCy model to use for loading the Doc.\n            model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n            verify_hash (bool): Whether to verify data integrity hash. Defaults to True.\n        \"\"\"\n        try:\n            data = msgpack.unpackb(bytestring)\n        except Exception as e:\n            raise LexosException(\n                f\"Failed to deserialize record: Invalid or corrupted data format. \"\n                f\"Suggestion: Check if the file was completely written and not corrupted.\"\n            ) from e\n\n        # Verify data integrity if hash is present\n        if verify_hash and \"data_integrity_hash\" in data:\n            stored_hash = data[\"data_integrity_hash\"]\n            # Recreate hash from core data (excluding the hash itself)\n            core_data = {k: v for k, v in data.items() if k != \"data_integrity_hash\"}\n            core_bytes = msgpack.dumps(core_data)\n            computed_hash = hashlib.sha256(core_bytes).hexdigest()\n\n            if stored_hash != computed_hash:\n                raise LexosException(\n                    f\"Data integrity check failed: Hash mismatch detected. \"\n                    f\"Expected: {stored_hash[:16]}..., Got: {computed_hash[:16]}... \"\n                    f\"Suggestion: The data may be corrupted during storage or transmission. \"\n                    f\"Try re-serializing the original document.\"\n                )\n\n        # Update the record with the loaded data\n        for k, v in data.items():\n            if k in self.model_fields:\n                if k != \"content\":\n                    setattr(self, k, v)\n\n        # If content is bytes, convert it back to a Doc object\n        if data[\"is_parsed\"] and isinstance(data[\"content\"], bytes):\n            if not model:\n                model = data.get(\"model\")\n            try:\n                self.content = self._doc_from_bytes(data[\"content\"], model, model_cache)\n            except OSError as e:\n                raise LexosException(\n                    f\"Failed to load spaCy model '{model}': {str(e)}. \"\n                    f\"Suggestion: Install the model with 'python -m spacy download {model}' \"\n                    f\"or use a different model available in your environment.\"\n                ) from e\n            except Exception as e:\n                raise LexosException(\n                    f\"Failed to deserialize spaCy document with model '{model}': {str(e)}. \"\n                    f\"Suggestion: Check model compatibility - document may have been \"\n                    f\"serialized with a different spaCy or model version.\"\n                ) from e\n\n    @validate_call(config=model_config)\n    def from_disk(\n        self,\n        path: Path | str,\n        model: Optional[str] = None,\n        model_cache: Optional[LexosModelCache] = None,\n    ) -&gt; None:\n        \"\"\"Load the record from disk.\n\n        Args:\n            path (Path | str): The path to load the record from.\n            model (Optional[str]): The spaCy model to use for loading the Doc.\n            model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n        \"\"\"\n        if not path:\n            raise LexosException(\"No path specified for loading the record.\")\n\n        # Load the data from disk\n        try:\n            with open(path, \"rb\") as f:\n                data = f.read()\n        except FileNotFoundError as e:\n            raise LexosException(\n                f\"Record file not found: {path}. \"\n                f\"Suggestion: Check if the file path is correct and the file exists.\"\n            ) from e\n        except PermissionError as e:\n            raise LexosException(\n                f\"Permission denied accessing record file: {path}. \"\n                f\"Suggestion: Check file permissions or run with appropriate privileges.\"\n            ) from e\n        except IOError as e:\n            raise LexosException(\n                f\"Failed to read record file: {path}. Error: {str(e)}. \"\n                f\"Suggestion: Check disk space, file system health, or network connectivity.\"\n            ) from e\n\n        # Get the record content from the bytestring\n        self.from_bytes(data, model=model, model_cache=model_cache)\n\n    def least_common_terms(self, n: Optional[int] = None) -&gt; list[tuple[str, int]]:\n        \"\"\"Return the least common terms.\n\n        Args:\n            n (Optional[int]): The number of least common terms to return. If None, return all terms.\n\n        Returns:\n            list[tuple[str, int]]: A list of (term, count) pairs sorted by least frequent.\n        \"\"\"\n        if self.is_parsed:\n            return (\n                sorted(self.terms.items(), key=lambda x: x[1])[:n]\n                if n\n                else sorted(self.terms.items(), key=lambda x: x[1])\n            )\n        else:\n            raise LexosException(\"Record is not parsed.\")\n\n    def most_common_terms(self, n: Optional[int] = None) -&gt; list[tuple[str, int]]:\n        \"\"\"Return the most common terms.\n\n        Args:\n            n (Optional[int]): The number of most common terms to return. If None, return all terms.\n\n        Returns:\n            list[tuple[str, int]]: A list of (term, count) pairs sorted by most frequent.\n        \"\"\"\n        if self.is_parsed:\n            return self.terms.most_common(n)\n        else:\n            raise LexosException(\"Record is not parsed.\")\n\n    def num_terms(self) -&gt; int:\n        \"\"\"Return the number of terms.\n\n        Returns:\n            int: The count of unique terms in this record.\n        \"\"\"\n        if self.is_parsed:\n            return len(self.terms)\n        else:\n            raise LexosException(\"Record is not parsed.\")\n\n    def num_tokens(self) -&gt; int:\n        \"\"\"Return the number of tokens.\n\n        Returns:\n            int: The count of token elements in this record.\n        \"\"\"\n        if self.is_parsed:\n            return len(self.tokens)\n        else:\n            raise LexosException(\"Record is not parsed.\")\n\n    @validate_call(config=model_config)\n    def set(self, **props: Any) -&gt; None:\n        \"\"\"Set a record property.\n\n        Args:\n            **props (Any): A dict containing the properties to set on the record.\n\n        Returns:\n            None\n        \"\"\"\n        for k, v in props.items():\n            setattr(self, k, v)\n\n    @validate_call(config=model_config)\n    def to_bytes(\n        self, extensions: Optional[list[str]] = [], include_hash: bool = True\n    ) -&gt; bytes:\n        \"\"\"Serialize the record to a dictionary.\n\n        Args:\n            extensions (list[str]): A list of extension names to include in the serialization.\n            include_hash (bool): Whether to include data integrity hash. Defaults to True.\n\n        Returns:\n            bytes: The serialized record.\n        \"\"\"\n        # Handle extensions\n        if extensions:\n            self.extensions = list(set(self.extensions + extensions))\n\n        # Convert record to a dictionary\n        # model_dump is used to create a serializable dict representation.\n        # We exclude the computed fields (`terms`, `text`, `tokens`) because\n        # they might trigger evaluation and raise `LexosException` for\n        # unparsed `Record` objects. The saved content is handled below,\n        # and `id` is stringified to ensure JSON compatibility.\n        data = self.model_dump(exclude=[\"terms\", \"text\", \"tokens\"])\n\n        # Make UUID serialisable\n        data[\"id\"] = str(data[\"id\"])\n\n        # WARNING: This code is deprecated in favour of field serializer.\n        # Convert the content to bytes if it is a Doc object\n        if self.is_parsed:\n            data[\"content\"] = self._doc_to_bytes()\n\n        # Add data integrity hash if requested\n        if include_hash:\n            # Create hash of the core data (excluding the hash itself)\n            core_data = {k: v for k, v in data.items() if k != \"data_integrity_hash\"}\n            core_bytes = msgpack.dumps(core_data)\n            data[\"data_integrity_hash\"] = hashlib.sha256(core_bytes).hexdigest()\n\n        return msgpack.dumps(data)\n\n    @validate_call(config=model_config)\n    def to_disk(self, path: Path | str, extensions: Optional[list[str]] = None) -&gt; None:\n        \"\"\"Save the record to disk.\n\n        Args:\n            path (Path | str): The path to save the record to.\n            extensions (list[str]): A list of extension names to include in the serialization.\n        \"\"\"\n        if not path:\n            raise LexosException(\"No path specified for saving the record.\")\n\n        if not extensions:\n            extensions = self.extensions\n\n        # Serialize and save the record\n        data = self.to_bytes(extensions)\n\n        try:\n            with open(path, \"wb\") as f:\n                f.write(data)\n        except PermissionError as e:\n            raise LexosException(\n                f\"Permission denied writing to: {path}. \"\n                f\"Suggestion: Check file/directory permissions or run with appropriate privileges.\"\n            ) from e\n        except OSError as e:\n            if \"No space left on device\" in str(e):\n                raise LexosException(\n                    f\"Insufficient disk space to save record: {path}. \"\n                    f\"Suggestion: Free up disk space or choose a different location.\"\n                ) from e\n            else:\n                raise LexosException(\n                    f\"Failed to write record to disk: {path}. Error: {str(e)}. \"\n                    f\"Suggestion: Check disk space, file system health, or network connectivity.\"\n                ) from e\n\n    def vocab_density(self) -&gt; float:\n        \"\"\"Return the vocabulary density.\n\n        Returns:\n            float: The vocabulary density of the record.\n        \"\"\"\n        if self.is_parsed:\n            return self.num_terms() / self.num_tokens()\n        else:\n            raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.is_parsed","title":"<code>is_parsed: bool</code>  <code>cached</code> <code>property</code>","text":"<p>Return whether the record is parsed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the record content is a spaCy Doc, False otherwise.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.preview","title":"<code>preview: str</code>  <code>cached</code> <code>property</code>","text":"<p>Return a preview of the record text.</p> <p>Returns:</p> Type Description <code>str</code> <p>str | None: A shortened preview of the record content, or None if content is None.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.terms","title":"<code>terms: Counter</code>  <code>cached</code> <code>property</code>","text":"<p>Return the terms in the record.</p> <p>Returns:</p> Name Type Description <code>Counter</code> <code>Counter</code> <p>Collection mapping term -&gt; count for the record.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.text","title":"<code>text: str</code>  <code>property</code>","text":"<p>Return the text of the record.</p> <p>Returns:</p> Type Description <code>str</code> <p>str | None: The record text as string or None if no content is present.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.tokens","title":"<code>tokens: list[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Return the tokens in the record.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of token strings extracted from the parsed content.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of the record.\"\"\"\n    # We exclude `terms`, `text`, and `tokens` here because these are\n    # computed / cached fields that can rely on the record being parsed.\n    # For unparsed records, evaluating these computed properties will\n    # raise a LexosException. `__repr__` should be lightweight and safe\n    # to call in debugging contexts, so we exclude these computed fields\n    # intentionally.\n    fields = self.model_dump(exclude=[\"terms\", \"text\", \"tokens\"])\n    fields[\"is_parsed\"] = str(self.is_parsed)\n    if self.content and self.is_parsed:\n        fields[\"content\"] = f\"{self.content.text[:25]}...\"\n    elif self.content and not self.is_parsed:\n        fields[\"content\"] = f\"{self.content[:25]}...\"\n    else:\n        fields[\"content\"] = \"None\"\n    field_list = [f\"{k}={v}\" if v else f\"{k}=None\" for k, v in fields.items()]\n    return f\"Record({', '.join(field_list)})\"\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Return a user-friendly string representation of the record for printing.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a user-friendly string representation of the record for printing.\"\"\"\n    active = \"True\" if self.is_active else \"False\"\n    parsed = \"True\" if self.is_parsed else \"False\"\n\n    # Get a preview of content\n    if self.content is None:\n        content_preview = \"None\"\n    elif self.is_parsed:\n        content_preview = f\"'{self.content.text[:40]}...'\"\n    else:\n        content_preview = f\"'{self.content[:40]}...'\"\n\n    return f\"Record(id={self.id}, name={self.name!r}, active={active}, parsed={parsed}, content={content_preview})\"\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.from_bytes","title":"<code>from_bytes(bytestring: bytes, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None, verify_hash: bool = True) -&gt; None</code>","text":"<p>Deserialise the record from bytes.</p> <p>Parameters:</p> Name Type Description Default <code>bytestring</code> <code>bytes</code> <p>The bytes to load the record from.</p> required <code>model</code> <code>Optional[str]</code> <p>The spaCy model to use for loading the Doc.</p> <code>None</code> <code>model_cache</code> <code>Optional[LexosModelCache]</code> <p>An optional cache for spaCy models.</p> <code>None</code> <code>verify_hash</code> <code>bool</code> <p>Whether to verify data integrity hash. Defaults to True.</p> <code>True</code> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef from_bytes(\n    self,\n    bytestring: bytes,\n    model: Optional[str] = None,\n    model_cache: Optional[LexosModelCache] = None,\n    verify_hash: bool = True,\n) -&gt; None:\n    \"\"\"Deserialise the record from bytes.\n\n    Args:\n        bytestring (bytes): The bytes to load the record from.\n        model (Optional[str]): The spaCy model to use for loading the Doc.\n        model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n        verify_hash (bool): Whether to verify data integrity hash. Defaults to True.\n    \"\"\"\n    try:\n        data = msgpack.unpackb(bytestring)\n    except Exception as e:\n        raise LexosException(\n            f\"Failed to deserialize record: Invalid or corrupted data format. \"\n            f\"Suggestion: Check if the file was completely written and not corrupted.\"\n        ) from e\n\n    # Verify data integrity if hash is present\n    if verify_hash and \"data_integrity_hash\" in data:\n        stored_hash = data[\"data_integrity_hash\"]\n        # Recreate hash from core data (excluding the hash itself)\n        core_data = {k: v for k, v in data.items() if k != \"data_integrity_hash\"}\n        core_bytes = msgpack.dumps(core_data)\n        computed_hash = hashlib.sha256(core_bytes).hexdigest()\n\n        if stored_hash != computed_hash:\n            raise LexosException(\n                f\"Data integrity check failed: Hash mismatch detected. \"\n                f\"Expected: {stored_hash[:16]}..., Got: {computed_hash[:16]}... \"\n                f\"Suggestion: The data may be corrupted during storage or transmission. \"\n                f\"Try re-serializing the original document.\"\n            )\n\n    # Update the record with the loaded data\n    for k, v in data.items():\n        if k in self.model_fields:\n            if k != \"content\":\n                setattr(self, k, v)\n\n    # If content is bytes, convert it back to a Doc object\n    if data[\"is_parsed\"] and isinstance(data[\"content\"], bytes):\n        if not model:\n            model = data.get(\"model\")\n        try:\n            self.content = self._doc_from_bytes(data[\"content\"], model, model_cache)\n        except OSError as e:\n            raise LexosException(\n                f\"Failed to load spaCy model '{model}': {str(e)}. \"\n                f\"Suggestion: Install the model with 'python -m spacy download {model}' \"\n                f\"or use a different model available in your environment.\"\n            ) from e\n        except Exception as e:\n            raise LexosException(\n                f\"Failed to deserialize spaCy document with model '{model}': {str(e)}. \"\n                f\"Suggestion: Check model compatibility - document may have been \"\n                f\"serialized with a different spaCy or model version.\"\n            ) from e\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.from_disk","title":"<code>from_disk(path: Path | str, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None) -&gt; None</code>","text":"<p>Load the record from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to load the record from.</p> required <code>model</code> <code>Optional[str]</code> <p>The spaCy model to use for loading the Doc.</p> <code>None</code> <code>model_cache</code> <code>Optional[LexosModelCache]</code> <p>An optional cache for spaCy models.</p> <code>None</code> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef from_disk(\n    self,\n    path: Path | str,\n    model: Optional[str] = None,\n    model_cache: Optional[LexosModelCache] = None,\n) -&gt; None:\n    \"\"\"Load the record from disk.\n\n    Args:\n        path (Path | str): The path to load the record from.\n        model (Optional[str]): The spaCy model to use for loading the Doc.\n        model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n    \"\"\"\n    if not path:\n        raise LexosException(\"No path specified for loading the record.\")\n\n    # Load the data from disk\n    try:\n        with open(path, \"rb\") as f:\n            data = f.read()\n    except FileNotFoundError as e:\n        raise LexosException(\n            f\"Record file not found: {path}. \"\n            f\"Suggestion: Check if the file path is correct and the file exists.\"\n        ) from e\n    except PermissionError as e:\n        raise LexosException(\n            f\"Permission denied accessing record file: {path}. \"\n            f\"Suggestion: Check file permissions or run with appropriate privileges.\"\n        ) from e\n    except IOError as e:\n        raise LexosException(\n            f\"Failed to read record file: {path}. Error: {str(e)}. \"\n            f\"Suggestion: Check disk space, file system health, or network connectivity.\"\n        ) from e\n\n    # Get the record content from the bytestring\n    self.from_bytes(data, model=model, model_cache=model_cache)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.least_common_terms","title":"<code>least_common_terms(n: Optional[int] = None) -&gt; list[tuple[str, int]]</code>","text":"<p>Return the least common terms.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>The number of least common terms to return. If None, return all terms.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, int]]</code> <p>list[tuple[str, int]]: A list of (term, count) pairs sorted by least frequent.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def least_common_terms(self, n: Optional[int] = None) -&gt; list[tuple[str, int]]:\n    \"\"\"Return the least common terms.\n\n    Args:\n        n (Optional[int]): The number of least common terms to return. If None, return all terms.\n\n    Returns:\n        list[tuple[str, int]]: A list of (term, count) pairs sorted by least frequent.\n    \"\"\"\n    if self.is_parsed:\n        return (\n            sorted(self.terms.items(), key=lambda x: x[1])[:n]\n            if n\n            else sorted(self.terms.items(), key=lambda x: x[1])\n        )\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.most_common_terms","title":"<code>most_common_terms(n: Optional[int] = None) -&gt; list[tuple[str, int]]</code>","text":"<p>Return the most common terms.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>The number of most common terms to return. If None, return all terms.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, int]]</code> <p>list[tuple[str, int]]: A list of (term, count) pairs sorted by most frequent.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def most_common_terms(self, n: Optional[int] = None) -&gt; list[tuple[str, int]]:\n    \"\"\"Return the most common terms.\n\n    Args:\n        n (Optional[int]): The number of most common terms to return. If None, return all terms.\n\n    Returns:\n        list[tuple[str, int]]: A list of (term, count) pairs sorted by most frequent.\n    \"\"\"\n    if self.is_parsed:\n        return self.terms.most_common(n)\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.num_terms","title":"<code>num_terms() -&gt; int</code>","text":"<p>Return the number of terms.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of unique terms in this record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def num_terms(self) -&gt; int:\n    \"\"\"Return the number of terms.\n\n    Returns:\n        int: The count of unique terms in this record.\n    \"\"\"\n    if self.is_parsed:\n        return len(self.terms)\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.num_tokens","title":"<code>num_tokens() -&gt; int</code>","text":"<p>Return the number of tokens.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of token elements in this record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def num_tokens(self) -&gt; int:\n    \"\"\"Return the number of tokens.\n\n    Returns:\n        int: The count of token elements in this record.\n    \"\"\"\n    if self.is_parsed:\n        return len(self.tokens)\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.serialize_content","title":"<code>serialize_content(content: Doc | str) -&gt; bytes | str</code>","text":"<p>Serialize the content to bytes if it is a Doc object.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Doc | str</code> <p>The content to serialize.</p> required <p>Returns:</p> Type Description <code>bytes | str</code> <p>bytes | str: The serialized content as bytes if it is a Doc, otherwise the original string.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@field_serializer(\"content\")\ndef serialize_content(self, content: Doc | str) -&gt; bytes | str:\n    \"\"\"Serialize the content to bytes if it is a Doc object.\n\n    Args:\n        content (Doc | str): The content to serialize.\n\n    Returns:\n        bytes | str: The serialized content as bytes if it is a Doc, otherwise the original string.\n    \"\"\"\n    if isinstance(content, Doc):\n        content.user_data[\"extensions\"] = {}\n        for ext in self.extensions:\n            content.user_data[\"extensions\"][ext] = [\n                token._.get(ext) for token in content\n            ]\n        return content.to_bytes()\n    return content\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.serialize_id","title":"<code>serialize_id(id, _info) -&gt; str</code>","text":"<p>Always serialize ID as string for JSON compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>UUID | int | str</code> <p>The ID value being serialized.</p> required <code>_info</code> <code>Any</code> <p>Encoder info (pydantic serializer internals).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized ID as a string.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@field_serializer(\"id\")\ndef serialize_id(self, id, _info) -&gt; str:\n    \"\"\"Always serialize ID as string for JSON compatibility.\n\n    Args:\n        id (UUID|int|str): The ID value being serialized.\n        _info (Any): Encoder info (pydantic serializer internals).\n\n    Returns:\n        str: The serialized ID as a string.\n    \"\"\"\n    return str(id)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.serialize_meta","title":"<code>serialize_meta(meta: dict[str, Any]) -&gt; dict[str, Any]</code>","text":"<p>Ensure metadata is JSON-serializable by converting special types to strings.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@field_serializer(\"meta\")\ndef serialize_meta(self, meta: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Ensure metadata is JSON-serializable by converting special types to strings.\"\"\"\n    return self._sanitize_metadata(meta)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.set","title":"<code>set(**props: Any) -&gt; None</code>","text":"<p>Set a record property.</p> <p>Parameters:</p> Name Type Description Default <code>**props</code> <code>Any</code> <p>A dict containing the properties to set on the record.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef set(self, **props: Any) -&gt; None:\n    \"\"\"Set a record property.\n\n    Args:\n        **props (Any): A dict containing the properties to set on the record.\n\n    Returns:\n        None\n    \"\"\"\n    for k, v in props.items():\n        setattr(self, k, v)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.to_bytes","title":"<code>to_bytes(extensions: Optional[list[str]] = [], include_hash: bool = True) -&gt; bytes</code>","text":"<p>Serialize the record to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>extensions</code> <code>list[str]</code> <p>A list of extension names to include in the serialization.</p> <code>[]</code> <code>include_hash</code> <code>bool</code> <p>Whether to include data integrity hash. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The serialized record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef to_bytes(\n    self, extensions: Optional[list[str]] = [], include_hash: bool = True\n) -&gt; bytes:\n    \"\"\"Serialize the record to a dictionary.\n\n    Args:\n        extensions (list[str]): A list of extension names to include in the serialization.\n        include_hash (bool): Whether to include data integrity hash. Defaults to True.\n\n    Returns:\n        bytes: The serialized record.\n    \"\"\"\n    # Handle extensions\n    if extensions:\n        self.extensions = list(set(self.extensions + extensions))\n\n    # Convert record to a dictionary\n    # model_dump is used to create a serializable dict representation.\n    # We exclude the computed fields (`terms`, `text`, `tokens`) because\n    # they might trigger evaluation and raise `LexosException` for\n    # unparsed `Record` objects. The saved content is handled below,\n    # and `id` is stringified to ensure JSON compatibility.\n    data = self.model_dump(exclude=[\"terms\", \"text\", \"tokens\"])\n\n    # Make UUID serialisable\n    data[\"id\"] = str(data[\"id\"])\n\n    # WARNING: This code is deprecated in favour of field serializer.\n    # Convert the content to bytes if it is a Doc object\n    if self.is_parsed:\n        data[\"content\"] = self._doc_to_bytes()\n\n    # Add data integrity hash if requested\n    if include_hash:\n        # Create hash of the core data (excluding the hash itself)\n        core_data = {k: v for k, v in data.items() if k != \"data_integrity_hash\"}\n        core_bytes = msgpack.dumps(core_data)\n        data[\"data_integrity_hash\"] = hashlib.sha256(core_bytes).hexdigest()\n\n    return msgpack.dumps(data)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.to_disk","title":"<code>to_disk(path: Path | str, extensions: Optional[list[str]] = None) -&gt; None</code>","text":"<p>Save the record to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the record to.</p> required <code>extensions</code> <code>list[str]</code> <p>A list of extension names to include in the serialization.</p> <code>None</code> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef to_disk(self, path: Path | str, extensions: Optional[list[str]] = None) -&gt; None:\n    \"\"\"Save the record to disk.\n\n    Args:\n        path (Path | str): The path to save the record to.\n        extensions (list[str]): A list of extension names to include in the serialization.\n    \"\"\"\n    if not path:\n        raise LexosException(\"No path specified for saving the record.\")\n\n    if not extensions:\n        extensions = self.extensions\n\n    # Serialize and save the record\n    data = self.to_bytes(extensions)\n\n    try:\n        with open(path, \"wb\") as f:\n            f.write(data)\n    except PermissionError as e:\n        raise LexosException(\n            f\"Permission denied writing to: {path}. \"\n            f\"Suggestion: Check file/directory permissions or run with appropriate privileges.\"\n        ) from e\n    except OSError as e:\n        if \"No space left on device\" in str(e):\n            raise LexosException(\n                f\"Insufficient disk space to save record: {path}. \"\n                f\"Suggestion: Free up disk space or choose a different location.\"\n            ) from e\n        else:\n            raise LexosException(\n                f\"Failed to write record to disk: {path}. Error: {str(e)}. \"\n                f\"Suggestion: Check disk space, file system health, or network connectivity.\"\n            ) from e\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.vocab_density","title":"<code>vocab_density() -&gt; float</code>","text":"<p>Return the vocabulary density.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The vocabulary density of the record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def vocab_density(self) -&gt; float:\n    \"\"\"Return the vocabulary density.\n\n    Returns:\n        float: The vocabulary density of the record.\n    \"\"\"\n    if self.is_parsed:\n        return self.num_terms() / self.num_tokens()\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.serialize_content","title":"<code>serialize_content(content: Doc | str) -&gt; bytes | str</code>","text":"<p>Serialize the content to bytes if it is a Doc object.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Doc | str</code> <p>The content to serialize.</p> required <p>Returns:</p> Type Description <code>bytes | str</code> <p>bytes | str: The serialized content as bytes if it is a Doc, otherwise the original string.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@field_serializer(\"content\")\ndef serialize_content(self, content: Doc | str) -&gt; bytes | str:\n    \"\"\"Serialize the content to bytes if it is a Doc object.\n\n    Args:\n        content (Doc | str): The content to serialize.\n\n    Returns:\n        bytes | str: The serialized content as bytes if it is a Doc, otherwise the original string.\n    \"\"\"\n    if isinstance(content, Doc):\n        content.user_data[\"extensions\"] = {}\n        for ext in self.extensions:\n            content.user_data[\"extensions\"][ext] = [\n                token._.get(ext) for token in content\n            ]\n        return content.to_bytes()\n    return content\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.serialize_id","title":"<code>serialize_id(id, _info) -&gt; str</code>","text":"<p>Always serialize ID as string for JSON compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>UUID | int | str</code> <p>The ID value being serialized.</p> required <code>_info</code> <code>Any</code> <p>Encoder info (pydantic serializer internals).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The serialized ID as a string.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@field_serializer(\"id\")\ndef serialize_id(self, id, _info) -&gt; str:\n    \"\"\"Always serialize ID as string for JSON compatibility.\n\n    Args:\n        id (UUID|int|str): The ID value being serialized.\n        _info (Any): Encoder info (pydantic serializer internals).\n\n    Returns:\n        str: The serialized ID as a string.\n    \"\"\"\n    return str(id)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.serialize_meta","title":"<code>serialize_meta(meta: dict[str, Any]) -&gt; dict[str, Any]</code>","text":"<p>Ensure metadata is JSON-serializable by converting special types to strings.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@field_serializer(\"meta\")\ndef serialize_meta(self, meta: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Ensure metadata is JSON-serializable by converting special types to strings.\"\"\"\n    return self._sanitize_metadata(meta)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record._sanitize_metadata","title":"<code>_sanitize_metadata(metadata: dict[str, Any]) -&gt; dict[str, Any]</code>","text":"<p>Convert non-JSON-serializable types to strings.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[str, Any]</code> <p>Original metadata dictionary</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Sanitized metadata dictionary with JSON-serializable values</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def _sanitize_metadata(self, metadata: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Convert non-JSON-serializable types to strings.\n\n    Args:\n        metadata: Original metadata dictionary\n\n    Returns:\n        Sanitized metadata dictionary with JSON-serializable values\n    \"\"\"\n    sanitized = {}\n    for key, value in metadata.items():\n        if isinstance(value, UUID):\n            sanitized[key] = str(value)\n        elif isinstance(value, (datetime, date)):\n            sanitized[key] = value.isoformat()\n        elif isinstance(value, Path):\n            sanitized[key] = str(value)\n        elif isinstance(value, dict):\n            sanitized[key] = self._sanitize_metadata(value)  # Recursive\n        elif isinstance(value, list):\n            sanitized[key] = [\n                self._sanitize_metadata({\"item\": item})[\"item\"]\n                if isinstance(item, dict)\n                else str(item)\n                if isinstance(item, (UUID, datetime, date, Path))\n                else item\n                for item in value\n            ]\n        else:\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of the record.\"\"\"\n    # We exclude `terms`, `text`, and `tokens` here because these are\n    # computed / cached fields that can rely on the record being parsed.\n    # For unparsed records, evaluating these computed properties will\n    # raise a LexosException. `__repr__` should be lightweight and safe\n    # to call in debugging contexts, so we exclude these computed fields\n    # intentionally.\n    fields = self.model_dump(exclude=[\"terms\", \"text\", \"tokens\"])\n    fields[\"is_parsed\"] = str(self.is_parsed)\n    if self.content and self.is_parsed:\n        fields[\"content\"] = f\"{self.content.text[:25]}...\"\n    elif self.content and not self.is_parsed:\n        fields[\"content\"] = f\"{self.content[:25]}...\"\n    else:\n        fields[\"content\"] = \"None\"\n    field_list = [f\"{k}={v}\" if v else f\"{k}=None\" for k, v in fields.items()]\n    return f\"Record({', '.join(field_list)})\"\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Return a user-friendly string representation of the record for printing.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a user-friendly string representation of the record for printing.\"\"\"\n    active = \"True\" if self.is_active else \"False\"\n    parsed = \"True\" if self.is_parsed else \"False\"\n\n    # Get a preview of content\n    if self.content is None:\n        content_preview = \"None\"\n    elif self.is_parsed:\n        content_preview = f\"'{self.content.text[:40]}...'\"\n    else:\n        content_preview = f\"'{self.content[:40]}...'\"\n\n    return f\"Record(id={self.id}, name={self.name!r}, active={active}, parsed={parsed}, content={content_preview})\"\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.is_parsed","title":"<code>is_parsed: bool</code>  <code>cached</code> <code>property</code>","text":"<p>Return whether the record is parsed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the record content is a spaCy Doc, False otherwise.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.preview","title":"<code>preview: str</code>  <code>cached</code> <code>property</code>","text":"<p>Return a preview of the record text.</p> <p>Returns:</p> Type Description <code>str</code> <p>str | None: A shortened preview of the record content, or None if content is None.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.terms","title":"<code>terms: Counter</code>  <code>cached</code> <code>property</code>","text":"<p>Return the terms in the record.</p> <p>Returns:</p> Name Type Description <code>Counter</code> <code>Counter</code> <p>Collection mapping term -&gt; count for the record.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.text","title":"<code>text: str</code>  <code>property</code>","text":"<p>Return the text of the record.</p> <p>Returns:</p> Type Description <code>str</code> <p>str | None: The record text as string or None if no content is present.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.tokens","title":"<code>tokens: list[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Return the tokens in the record.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of token strings extracted from the parsed content.</p>"},{"location":"api/corpus/record/#lexos.corpus.record.Record._doc_from_bytes","title":"<code>_doc_from_bytes(content: bytes, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None) -&gt; Doc</code>","text":"<p>Convert bytes to a Doc object.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>bytes</code> <p>The bytes to convert.</p> required <code>model</code> <code>Optional[str]</code> <p>The spaCy model to use for loading the Doc.</p> <code>None</code> <code>model_cache</code> <code>Optional[LexosModelCache]</code> <p>An optional cache for spaCy models.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The content as a Doc object.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def _doc_from_bytes(\n    self,\n    content: bytes,\n    model: Optional[str] = None,\n    model_cache: Optional[LexosModelCache] = None,\n) -&gt; Doc:\n    \"\"\"Convert bytes to a Doc object.\n\n    Args:\n        content (bytes): The bytes to convert.\n        model (Optional[str]): The spaCy model to use for loading the Doc.\n        model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n\n    Returns:\n        Doc: The content as a Doc object.\n    \"\"\"\n    # Create a Doc from the bytes\n    vocab = self._get_vocab(model, model_cache)\n    doc = Doc(vocab).from_bytes(content)\n\n    # Restore extension values\n    for ext, values in doc.user_data[\"extensions\"].items():\n        Token.set_extension(ext, default=None, force=True)\n        for i in range(len(doc)):\n            doc[i]._.set(ext, values[i])\n\n    # Clean up user_data\n    doc.user_data[\"extensions\"] = list(doc.user_data[\"extensions\"].keys())\n\n    return doc\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record._doc_to_bytes","title":"<code>_doc_to_bytes() -&gt; bytes</code>","text":"<p>Convert the content to bytes if it is a Doc object.</p> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The content as bytes.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def _doc_to_bytes(self) -&gt; bytes:\n    \"\"\"Convert the content to bytes if it is a Doc object.\n\n    Returns:\n        bytes: The content as bytes.\n    \"\"\"\n    if not isinstance(self.content, Doc):\n        raise LexosException(\"Content is not a Doc object.\")\n\n    doc = self.content\n\n    doc.user_data[\"extensions\"] = {}\n    for ext in self.extensions:\n        doc.user_data[\"extensions\"][ext] = [token._.get(ext) for token in doc]\n\n    return doc.to_bytes()\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record._get_vocab","title":"<code>_get_vocab(model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None) -&gt; Vocab</code>","text":"<p>Get the vocabulary from the model or model cache.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>The spaCy model to use for loading the Doc.</p> <code>None</code> <code>model_cache</code> <code>Optional[LexosModelCache]</code> <p>An optional cache for spaCy models.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Vocab</code> <code>Vocab</code> <p>The vocabulary of the model.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def _get_vocab(\n    self, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None\n) -&gt; Vocab:\n    \"\"\"Get the vocabulary from the model or model cache.\n\n    Args:\n        model (Optional[str]): The spaCy model to use for loading the Doc.\n        model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n\n    Returns:\n        Vocab: The vocabulary of the model.\n    \"\"\"\n    if model_cache and not model:\n        raise LexosException(\"Model cache provided but no model specified.\")\n\n    if model_cache:\n        return model_cache.get_model(model).vocab\n    elif model:\n        return spacy.load(model).vocab\n    elif self.model:\n        return spacy.load(self.model).vocab\n    else:\n        raise LexosException(\n            \"No model specified for loading the Doc. Please provide a model name or a model cache.\"\n        )\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.from_bytes","title":"<code>from_bytes(bytestring: bytes, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None, verify_hash: bool = True) -&gt; None</code>","text":"<p>Deserialise the record from bytes.</p> <p>Parameters:</p> Name Type Description Default <code>bytestring</code> <code>bytes</code> <p>The bytes to load the record from.</p> required <code>model</code> <code>Optional[str]</code> <p>The spaCy model to use for loading the Doc.</p> <code>None</code> <code>model_cache</code> <code>Optional[LexosModelCache]</code> <p>An optional cache for spaCy models.</p> <code>None</code> <code>verify_hash</code> <code>bool</code> <p>Whether to verify data integrity hash. Defaults to True.</p> <code>True</code> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef from_bytes(\n    self,\n    bytestring: bytes,\n    model: Optional[str] = None,\n    model_cache: Optional[LexosModelCache] = None,\n    verify_hash: bool = True,\n) -&gt; None:\n    \"\"\"Deserialise the record from bytes.\n\n    Args:\n        bytestring (bytes): The bytes to load the record from.\n        model (Optional[str]): The spaCy model to use for loading the Doc.\n        model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n        verify_hash (bool): Whether to verify data integrity hash. Defaults to True.\n    \"\"\"\n    try:\n        data = msgpack.unpackb(bytestring)\n    except Exception as e:\n        raise LexosException(\n            f\"Failed to deserialize record: Invalid or corrupted data format. \"\n            f\"Suggestion: Check if the file was completely written and not corrupted.\"\n        ) from e\n\n    # Verify data integrity if hash is present\n    if verify_hash and \"data_integrity_hash\" in data:\n        stored_hash = data[\"data_integrity_hash\"]\n        # Recreate hash from core data (excluding the hash itself)\n        core_data = {k: v for k, v in data.items() if k != \"data_integrity_hash\"}\n        core_bytes = msgpack.dumps(core_data)\n        computed_hash = hashlib.sha256(core_bytes).hexdigest()\n\n        if stored_hash != computed_hash:\n            raise LexosException(\n                f\"Data integrity check failed: Hash mismatch detected. \"\n                f\"Expected: {stored_hash[:16]}..., Got: {computed_hash[:16]}... \"\n                f\"Suggestion: The data may be corrupted during storage or transmission. \"\n                f\"Try re-serializing the original document.\"\n            )\n\n    # Update the record with the loaded data\n    for k, v in data.items():\n        if k in self.model_fields:\n            if k != \"content\":\n                setattr(self, k, v)\n\n    # If content is bytes, convert it back to a Doc object\n    if data[\"is_parsed\"] and isinstance(data[\"content\"], bytes):\n        if not model:\n            model = data.get(\"model\")\n        try:\n            self.content = self._doc_from_bytes(data[\"content\"], model, model_cache)\n        except OSError as e:\n            raise LexosException(\n                f\"Failed to load spaCy model '{model}': {str(e)}. \"\n                f\"Suggestion: Install the model with 'python -m spacy download {model}' \"\n                f\"or use a different model available in your environment.\"\n            ) from e\n        except Exception as e:\n            raise LexosException(\n                f\"Failed to deserialize spaCy document with model '{model}': {str(e)}. \"\n                f\"Suggestion: Check model compatibility - document may have been \"\n                f\"serialized with a different spaCy or model version.\"\n            ) from e\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.from_disk","title":"<code>from_disk(path: Path | str, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None) -&gt; None</code>","text":"<p>Load the record from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to load the record from.</p> required <code>model</code> <code>Optional[str]</code> <p>The spaCy model to use for loading the Doc.</p> <code>None</code> <code>model_cache</code> <code>Optional[LexosModelCache]</code> <p>An optional cache for spaCy models.</p> <code>None</code> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef from_disk(\n    self,\n    path: Path | str,\n    model: Optional[str] = None,\n    model_cache: Optional[LexosModelCache] = None,\n) -&gt; None:\n    \"\"\"Load the record from disk.\n\n    Args:\n        path (Path | str): The path to load the record from.\n        model (Optional[str]): The spaCy model to use for loading the Doc.\n        model_cache (Optional[LexosModelCache]): An optional cache for spaCy models.\n    \"\"\"\n    if not path:\n        raise LexosException(\"No path specified for loading the record.\")\n\n    # Load the data from disk\n    try:\n        with open(path, \"rb\") as f:\n            data = f.read()\n    except FileNotFoundError as e:\n        raise LexosException(\n            f\"Record file not found: {path}. \"\n            f\"Suggestion: Check if the file path is correct and the file exists.\"\n        ) from e\n    except PermissionError as e:\n        raise LexosException(\n            f\"Permission denied accessing record file: {path}. \"\n            f\"Suggestion: Check file permissions or run with appropriate privileges.\"\n        ) from e\n    except IOError as e:\n        raise LexosException(\n            f\"Failed to read record file: {path}. Error: {str(e)}. \"\n            f\"Suggestion: Check disk space, file system health, or network connectivity.\"\n        ) from e\n\n    # Get the record content from the bytestring\n    self.from_bytes(data, model=model, model_cache=model_cache)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.least_common_terms","title":"<code>least_common_terms(n: Optional[int] = None) -&gt; list[tuple[str, int]]</code>","text":"<p>Return the least common terms.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>The number of least common terms to return. If None, return all terms.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, int]]</code> <p>list[tuple[str, int]]: A list of (term, count) pairs sorted by least frequent.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def least_common_terms(self, n: Optional[int] = None) -&gt; list[tuple[str, int]]:\n    \"\"\"Return the least common terms.\n\n    Args:\n        n (Optional[int]): The number of least common terms to return. If None, return all terms.\n\n    Returns:\n        list[tuple[str, int]]: A list of (term, count) pairs sorted by least frequent.\n    \"\"\"\n    if self.is_parsed:\n        return (\n            sorted(self.terms.items(), key=lambda x: x[1])[:n]\n            if n\n            else sorted(self.terms.items(), key=lambda x: x[1])\n        )\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.most_common_terms","title":"<code>most_common_terms(n: Optional[int] = None) -&gt; list[tuple[str, int]]</code>","text":"<p>Return the most common terms.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>The number of most common terms to return. If None, return all terms.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, int]]</code> <p>list[tuple[str, int]]: A list of (term, count) pairs sorted by most frequent.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def most_common_terms(self, n: Optional[int] = None) -&gt; list[tuple[str, int]]:\n    \"\"\"Return the most common terms.\n\n    Args:\n        n (Optional[int]): The number of most common terms to return. If None, return all terms.\n\n    Returns:\n        list[tuple[str, int]]: A list of (term, count) pairs sorted by most frequent.\n    \"\"\"\n    if self.is_parsed:\n        return self.terms.most_common(n)\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.num_terms","title":"<code>num_terms() -&gt; int</code>","text":"<p>Return the number of terms.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of unique terms in this record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def num_terms(self) -&gt; int:\n    \"\"\"Return the number of terms.\n\n    Returns:\n        int: The count of unique terms in this record.\n    \"\"\"\n    if self.is_parsed:\n        return len(self.terms)\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.num_tokens","title":"<code>num_tokens() -&gt; int</code>","text":"<p>Return the number of tokens.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of token elements in this record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def num_tokens(self) -&gt; int:\n    \"\"\"Return the number of tokens.\n\n    Returns:\n        int: The count of token elements in this record.\n    \"\"\"\n    if self.is_parsed:\n        return len(self.tokens)\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.set","title":"<code>set(**props: Any) -&gt; None</code>","text":"<p>Set a record property.</p> <p>Parameters:</p> Name Type Description Default <code>**props</code> <code>Any</code> <p>A dict containing the properties to set on the record.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef set(self, **props: Any) -&gt; None:\n    \"\"\"Set a record property.\n\n    Args:\n        **props (Any): A dict containing the properties to set on the record.\n\n    Returns:\n        None\n    \"\"\"\n    for k, v in props.items():\n        setattr(self, k, v)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.to_bytes","title":"<code>to_bytes(extensions: Optional[list[str]] = [], include_hash: bool = True) -&gt; bytes</code>","text":"<p>Serialize the record to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>extensions</code> <code>list[str]</code> <p>A list of extension names to include in the serialization.</p> <code>[]</code> <code>include_hash</code> <code>bool</code> <p>Whether to include data integrity hash. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The serialized record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef to_bytes(\n    self, extensions: Optional[list[str]] = [], include_hash: bool = True\n) -&gt; bytes:\n    \"\"\"Serialize the record to a dictionary.\n\n    Args:\n        extensions (list[str]): A list of extension names to include in the serialization.\n        include_hash (bool): Whether to include data integrity hash. Defaults to True.\n\n    Returns:\n        bytes: The serialized record.\n    \"\"\"\n    # Handle extensions\n    if extensions:\n        self.extensions = list(set(self.extensions + extensions))\n\n    # Convert record to a dictionary\n    # model_dump is used to create a serializable dict representation.\n    # We exclude the computed fields (`terms`, `text`, `tokens`) because\n    # they might trigger evaluation and raise `LexosException` for\n    # unparsed `Record` objects. The saved content is handled below,\n    # and `id` is stringified to ensure JSON compatibility.\n    data = self.model_dump(exclude=[\"terms\", \"text\", \"tokens\"])\n\n    # Make UUID serialisable\n    data[\"id\"] = str(data[\"id\"])\n\n    # WARNING: This code is deprecated in favour of field serializer.\n    # Convert the content to bytes if it is a Doc object\n    if self.is_parsed:\n        data[\"content\"] = self._doc_to_bytes()\n\n    # Add data integrity hash if requested\n    if include_hash:\n        # Create hash of the core data (excluding the hash itself)\n        core_data = {k: v for k, v in data.items() if k != \"data_integrity_hash\"}\n        core_bytes = msgpack.dumps(core_data)\n        data[\"data_integrity_hash\"] = hashlib.sha256(core_bytes).hexdigest()\n\n    return msgpack.dumps(data)\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.to_disk","title":"<code>to_disk(path: Path | str, extensions: Optional[list[str]] = None) -&gt; None</code>","text":"<p>Save the record to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the record to.</p> required <code>extensions</code> <code>list[str]</code> <p>A list of extension names to include in the serialization.</p> <code>None</code> Source code in <code>lexos/corpus/record.py</code> <pre><code>@validate_call(config=model_config)\ndef to_disk(self, path: Path | str, extensions: Optional[list[str]] = None) -&gt; None:\n    \"\"\"Save the record to disk.\n\n    Args:\n        path (Path | str): The path to save the record to.\n        extensions (list[str]): A list of extension names to include in the serialization.\n    \"\"\"\n    if not path:\n        raise LexosException(\"No path specified for saving the record.\")\n\n    if not extensions:\n        extensions = self.extensions\n\n    # Serialize and save the record\n    data = self.to_bytes(extensions)\n\n    try:\n        with open(path, \"wb\") as f:\n            f.write(data)\n    except PermissionError as e:\n        raise LexosException(\n            f\"Permission denied writing to: {path}. \"\n            f\"Suggestion: Check file/directory permissions or run with appropriate privileges.\"\n        ) from e\n    except OSError as e:\n        if \"No space left on device\" in str(e):\n            raise LexosException(\n                f\"Insufficient disk space to save record: {path}. \"\n                f\"Suggestion: Free up disk space or choose a different location.\"\n            ) from e\n        else:\n            raise LexosException(\n                f\"Failed to write record to disk: {path}. Error: {str(e)}. \"\n                f\"Suggestion: Check disk space, file system health, or network connectivity.\"\n            ) from e\n</code></pre>"},{"location":"api/corpus/record/#lexos.corpus.record.Record.vocab_density","title":"<code>vocab_density() -&gt; float</code>","text":"<p>Return the vocabulary density.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The vocabulary density of the record.</p> Source code in <code>lexos/corpus/record.py</code> <pre><code>def vocab_density(self) -&gt; float:\n    \"\"\"Return the vocabulary density.\n\n    Returns:\n        float: The vocabulary density of the record.\n    \"\"\"\n    if self.is_parsed:\n        return self.num_terms() / self.num_tokens()\n    else:\n        raise LexosException(\"Record is not parsed.\")\n</code></pre>"},{"location":"api/corpus/utils/","title":"utils","text":""},{"location":"api/corpus/utils/#module-description","title":"Module Description","text":"<p>Utility functions and classes for the Lexos corpus module.</p> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.LexosModelCache","title":"<code>LexosModelCache</code>","text":"<p>A simple cache for spaCy models.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the cache.</p> <code>get_model</code> <p>Get a model from the cache or load it if not cached.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>class LexosModelCache:\n    \"\"\"A simple cache for spaCy models.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the cache.\"\"\"\n        self._cache = {}\n\n    def get_model(self, model_name: str) -&gt; Language:\n        \"\"\"Get a model from the cache or load it if not cached.\n\n        Args:\n            model_name (str): The spaCy model name to load (e.g., 'en_core_web_sm').\n\n        Returns:\n            Language: The loaded spaCy language model.\n        \"\"\"\n        if model_name not in self._cache:\n            self._cache[model_name] = load_spacy_model(model_name)\n        return self._cache[model_name]\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.LexosModelCache.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the cache.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the cache.\"\"\"\n    self._cache = {}\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.LexosModelCache.get_model","title":"<code>get_model(model_name: str) -&gt; Language</code>","text":"<p>Get a model from the cache or load it if not cached.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The spaCy model name to load (e.g., 'en_core_web_sm').</p> required <p>Returns:</p> Name Type Description <code>Language</code> <code>Language</code> <p>The loaded spaCy language model.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def get_model(self, model_name: str) -&gt; Language:\n    \"\"\"Get a model from the cache or load it if not cached.\n\n    Args:\n        model_name (str): The spaCy model name to load (e.g., 'en_core_web_sm').\n\n    Returns:\n        Language: The loaded spaCy language model.\n    \"\"\"\n    if model_name not in self._cache:\n        self._cache[model_name] = load_spacy_model(model_name)\n    return self._cache[model_name]\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.LexosModelCache.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the cache.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the cache.\"\"\"\n    self._cache = {}\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.LexosModelCache.get_model","title":"<code>get_model(model_name: str) -&gt; Language</code>","text":"<p>Get a model from the cache or load it if not cached.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The spaCy model name to load (e.g., 'en_core_web_sm').</p> required <p>Returns:</p> Name Type Description <code>Language</code> <code>Language</code> <p>The loaded spaCy language model.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def get_model(self, model_name: str) -&gt; Language:\n    \"\"\"Get a model from the cache or load it if not cached.\n\n    Args:\n        model_name (str): The spaCy model name to load (e.g., 'en_core_web_sm').\n\n    Returns:\n        Language: The loaded spaCy language model.\n    \"\"\"\n    if model_name not in self._cache:\n        self._cache[model_name] = load_spacy_model(model_name)\n    return self._cache[model_name]\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.RecordsDict","title":"<code>RecordsDict</code>","text":"<p>               Bases: <code>dict</code></p> <p>A dictionary-like class for storing Record objects.</p> <p>This class ensures that no ids can be overwritten, and it raises an error if an attempt is made to do so.</p> <p>Methods:</p> Name Description <code>__setitem__</code> <p>Set an item in the Records dictionary.</p> <code>update</code> <p>Update the Records dictionary with a non-prexisting mapping or keyword arguments.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>class RecordsDict(dict):\n    \"\"\"A dictionary-like class for storing Record objects.\n\n    This class ensures that no ids can be overwritten, and it raises an error if an attempt is made to do so.\n    \"\"\"\n\n    def __setitem__(self, key, value):\n        \"\"\"Set an item in the Records dictionary.\n\n        Args:\n            key (str): The ID of the Record.\n            value (Record): The Record object to set.\n\n        Raises:\n            KeyError: If the key already exists in the Records dictionary.\n        \"\"\"\n        if not key in self:\n            super(RecordsDict, self).__setitem__(key, value)\n        else:\n            raise Exception(f\"ID '{key}' already exists. Cannot overwrite.\")\n\n    def update(self, other=None, **kwargs: Any) -&gt; None:\n        \"\"\"Update the Records dictionary with a non-prexisting mapping or keyword arguments.\n\n        Args:\n            other (Mapping or iterable): An optional mapping or iterable of key-value pairs to update the Records dictionary.\n            **kwargs (Any): Additional keyword arguments to update the Records dictionary.\n        \"\"\"\n        if other is not None:\n            for k, v in other.items() if isinstance(other, Mapping) else other:\n                self[k] = v\n        for k, v in kwargs.items():\n            self[k] = v\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.RecordsDict.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set an item in the Records dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The ID of the Record.</p> required <code>value</code> <code>Record</code> <p>The Record object to set.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If the key already exists in the Records dictionary.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def __setitem__(self, key, value):\n    \"\"\"Set an item in the Records dictionary.\n\n    Args:\n        key (str): The ID of the Record.\n        value (Record): The Record object to set.\n\n    Raises:\n        KeyError: If the key already exists in the Records dictionary.\n    \"\"\"\n    if not key in self:\n        super(RecordsDict, self).__setitem__(key, value)\n    else:\n        raise Exception(f\"ID '{key}' already exists. Cannot overwrite.\")\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.RecordsDict.update","title":"<code>update(other=None, **kwargs: Any) -&gt; None</code>","text":"<p>Update the Records dictionary with a non-prexisting mapping or keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Mapping or iterable</code> <p>An optional mapping or iterable of key-value pairs to update the Records dictionary.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to update the Records dictionary.</p> <code>{}</code> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def update(self, other=None, **kwargs: Any) -&gt; None:\n    \"\"\"Update the Records dictionary with a non-prexisting mapping or keyword arguments.\n\n    Args:\n        other (Mapping or iterable): An optional mapping or iterable of key-value pairs to update the Records dictionary.\n        **kwargs (Any): Additional keyword arguments to update the Records dictionary.\n    \"\"\"\n    if other is not None:\n        for k, v in other.items() if isinstance(other, Mapping) else other:\n            self[k] = v\n    for k, v in kwargs.items():\n        self[k] = v\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.RecordsDict.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set an item in the Records dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The ID of the Record.</p> required <code>value</code> <code>Record</code> <p>The Record object to set.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If the key already exists in the Records dictionary.</p> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def __setitem__(self, key, value):\n    \"\"\"Set an item in the Records dictionary.\n\n    Args:\n        key (str): The ID of the Record.\n        value (Record): The Record object to set.\n\n    Raises:\n        KeyError: If the key already exists in the Records dictionary.\n    \"\"\"\n    if not key in self:\n        super(RecordsDict, self).__setitem__(key, value)\n    else:\n        raise Exception(f\"ID '{key}' already exists. Cannot overwrite.\")\n</code></pre>"},{"location":"api/corpus/utils/#lexos.corpus.utils.RecordsDict.update","title":"<code>update(other=None, **kwargs: Any) -&gt; None</code>","text":"<p>Update the Records dictionary with a non-prexisting mapping or keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Mapping or iterable</code> <p>An optional mapping or iterable of key-value pairs to update the Records dictionary.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to update the Records dictionary.</p> <code>{}</code> Source code in <code>lexos/corpus/utils.py</code> <pre><code>def update(self, other=None, **kwargs: Any) -&gt; None:\n    \"\"\"Update the Records dictionary with a non-prexisting mapping or keyword arguments.\n\n    Args:\n        other (Mapping or iterable): An optional mapping or iterable of key-value pairs to update the Records dictionary.\n        **kwargs (Any): Additional keyword arguments to update the Records dictionary.\n    \"\"\"\n    if other is not None:\n        for k, v in other.items() if isinstance(other, Mapping) else other:\n            self[k] = v\n    for k, v in kwargs.items():\n        self[k] = v\n</code></pre>"},{"location":"api/corpus/sqlite/database/","title":"database","text":""},{"location":"api/corpus/sqlite/database/#module-description","title":"Module Description","text":"<p>SQLite database backend for SQLAlchemy to integrate with the Lexos <code>Corpus</code> class.</p> <p>This is a compatibility layer for SQLModel 0.0.24 that works around primary key issues.</p> <p>Last Updated: November 20, 2025 Last Tested: November 20, 2025</p> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteRecord","title":"<code>SQLiteRecord</code>","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy table for record storage.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>class SQLiteRecord(Base):\n    \"\"\"SQLAlchemy table for record storage.\"\"\"\n\n    __tablename__ = \"records\"\n\n    # Primary identification\n    id = Column(String, primary_key=True)\n    name = Column(String)\n\n    # Content storage\n    content_text = Column(Text, nullable=False)\n    content_doc_bytes = Column(LargeBinary)\n\n    # Status and metadata\n    is_active = Column(Boolean, default=True)\n    is_parsed = Column(Boolean, default=False)\n    model = Column(String)\n\n    # Content statistics (denormalized for query performance)\n    num_tokens = Column(Integer, default=0)\n    num_terms = Column(Integer, default=0)\n    vocab_density = Column(Float, default=0.0)\n\n    # Serialized metadata as JSON string\n    metadata_json = Column(Text, default=\"{}\")\n    extensions_list = Column(Text, default=\"[]\")\n\n    # Data integrity and versioning\n    data_source = Column(String)\n    content_hash = Column(String, nullable=False)\n    created_at = Column(String, nullable=False)\n    updated_at = Column(String, nullable=False)\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteMetadata","title":"<code>SQLiteMetadata</code>","text":"<p>               Bases: <code>Base</code></p> <p>SQLAlchemy table for corpus metadata.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>class SQLiteMetadata(Base):\n    \"\"\"SQLAlchemy table for corpus metadata.\"\"\"\n\n    __tablename__ = \"corpus_metadata\"\n\n    # Corpus identification\n    corpus_id = Column(String, primary_key=True)\n    name = Column(String)\n\n    # Aggregate statistics\n    num_docs = Column(Integer, default=0)\n    num_active_docs = Column(Integer, default=0)\n    num_tokens = Column(Integer, default=0)\n    num_terms = Column(Integer, default=0)\n\n    # Configuration\n    corpus_dir = Column(String, nullable=False)\n\n    # Serialized metadata\n    metadata_json = Column(Text, default=\"{}\")\n    analysis_results_json = Column(Text, default=\"{}\")\n\n    # Versioning and integrity\n    corpus_fingerprint = Column(String, nullable=False)\n    created_at = Column(String, nullable=False)\n    updated_at = Column(String, nullable=False)\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend","title":"<code>SQLiteBackend</code>","text":"<p>Database interface for corpus operations using SQLite with full-text search.</p> <p>Methods:</p> Name Description <code>__del__</code> <p>Destructor to ensure database connections are closed.</p> <code>__init__</code> <p>Initialize the corpus database.</p> <code>add_record</code> <p>Add a Record to the database.</p> <code>close</code> <p>Close the database connection and clean up resources.</p> <code>delete_record</code> <p>Delete a record from the database.</p> <code>filter_records</code> <p>Filter records by various criteria.</p> <code>get_record</code> <p>Retrieve a Record from the database.</p> <code>get_stats</code> <p>Get aggregate corpus statistics from the database.</p> <code>search_records</code> <p>Perform full-text search on records.</p> <code>update_record</code> <p>Update an existing record in the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>class SQLiteBackend:\n    \"\"\"Database interface for corpus operations using SQLite with full-text search.\"\"\"\n\n    def __del__(self):\n        \"\"\"Destructor to ensure database connections are closed.\"\"\"\n        try:\n            self.close()\n        except:\n            pass  # Ignore errors during cleanup\n\n    def __init__(self, database_path: Union[str, Path] = \":memory:\", **kwargs: Any):\n        \"\"\"Initialize the corpus database.\n\n        Args:\n            database_path: Path to SQLite database file, or \":memory:\" for in-memory database\n            **kwargs: Additional keyword arguments for SQLAlchemy engine creation\n        \"\"\"\n        self.database_path = str(database_path)\n        self.engine = create_engine(f\"sqlite:///{self.database_path}\", **kwargs)\n        self.SessionLocal = sessionmaker(bind=self.engine)\n        self._initialize_database()\n\n    def _db_record_to_record(\n        self,\n        db_record: SQLiteRecord,\n        include_doc: bool = True,\n        model_cache: Optional[LexosModelCache] = None,\n    ) -&gt; Record:\n        \"\"\"Convert a SQLiteRecord back to a Record object.\"\"\"\n        # Deserialize metadata\n        metadata = json.loads(db_record.metadata_json)\n        extensions = json.loads(db_record.extensions_list)\n\n        # Handle content deserialization\n        if include_doc and db_record.content_doc_bytes and db_record.is_parsed:\n            # Deserialize spaCy Doc\n            content = self._deserialize_doc_content(\n                db_record.content_doc_bytes, db_record.model, model_cache\n            )\n        else:\n            # Use text content\n            content = db_record.content_text\n\n        # Create Record object\n        record = Record(\n            id=db_record.id,\n            name=db_record.name,\n            is_active=db_record.is_active,\n            content=content,\n            model=db_record.model,\n            extensions=extensions,\n            data_source=db_record.data_source,\n            meta=metadata,\n        )\n\n        return record\n\n    def _deserialize_doc_content(\n        self,\n        doc_bytes: bytes,\n        model: Optional[str] = None,\n        model_cache: Optional[LexosModelCache] = None,\n    ) -&gt; Doc:\n        \"\"\"Deserialize spaCy Doc from bytes.\"\"\"\n        try:\n            # Use Record's deserialization method\n            temp_record = Record(id=str(uuid4()), content=\"\")\n            return temp_record._doc_from_bytes(doc_bytes, model, model_cache)\n        except Exception as e:\n            raise LexosException(f\"Failed to deserialize spaCy Doc: {str(e)}\")\n\n    def _initialize_database(self):\n        \"\"\"Initialize database schema and enable full-text search.\"\"\"\n        # Create all tables\n        Base.metadata.create_all(self.engine)\n\n        # Enable FTS5 full-text search\n        with self.SessionLocal() as session:\n            # Create FTS5 virtual table for full-text search\n            session.execute(\n                text(\"\"\"\n                CREATE VIRTUAL TABLE IF NOT EXISTS records_fts USING fts5(\n                    record_id,\n                    name,\n                    content_text,\n                    metadata_text\n                )\n            \"\"\")\n            )\n\n            # Create triggers to keep FTS table synchronized\n            session.execute(\n                text(\"\"\"\n                CREATE TRIGGER IF NOT EXISTS records_fts_insert AFTER INSERT ON records\n                BEGIN\n                    INSERT INTO records_fts(record_id, name, content_text, metadata_text)\n                    VALUES (new.id, new.name, new.content_text, new.metadata_json);\n                END\n            \"\"\")\n            )\n\n            session.execute(\n                text(\"\"\"\n                CREATE TRIGGER IF NOT EXISTS records_fts_delete AFTER DELETE ON records\n                BEGIN\n                    DELETE FROM records_fts WHERE record_id = old.id;\n                END\n            \"\"\")\n            )\n\n            session.execute(\n                text(\"\"\"\n                CREATE TRIGGER IF NOT EXISTS records_fts_update AFTER UPDATE ON records\n                BEGIN\n                    UPDATE records_fts\n                    SET name = new.name,\n                        content_text = new.content_text,\n                        metadata_text = new.metadata_json\n                    WHERE record_id = new.id;\n                END\n            \"\"\")\n            )\n\n            session.commit()\n\n    def _record_to_db_record(self, record: Record) -&gt; SQLiteRecord:\n        \"\"\"Convert a Record object to SQLiteRecord for database storage.\"\"\"\n        # Extract text content\n        if record.is_parsed and isinstance(record.content, Doc):\n            content_text = record.content.text\n            # Serialize spaCy Doc if parsed\n            content_doc_bytes = record._doc_to_bytes()\n        else:\n            content_text = str(record.content) if record.content else \"\"\n            content_doc_bytes = None\n\n        # Calculate content hash\n        content_hash = hashlib.sha256(content_text.encode()).hexdigest()\n\n        # Calculate statistics\n        num_tokens = record.num_tokens() if record.is_parsed else 0\n        num_terms = record.num_terms() if record.is_parsed else 0\n        vocab_density = record.vocab_density() if record.is_parsed else 0.0\n\n        # Serialize metadata\n        metadata_json = json.dumps(record.meta, default=str)\n        extensions_list = json.dumps(record.extensions)\n\n        timestamp = datetime.now().isoformat()\n\n        db_record = SQLiteRecord()\n        db_record.id = str(record.id)\n        db_record.name = record.name\n        db_record.content_text = content_text\n        db_record.content_doc_bytes = content_doc_bytes\n        db_record.is_active = record.is_active\n        db_record.is_parsed = record.is_parsed\n        db_record.model = record.model\n        db_record.num_tokens = num_tokens\n        db_record.num_terms = num_terms\n        db_record.vocab_density = vocab_density\n        db_record.metadata_json = metadata_json\n        db_record.extensions_list = extensions_list\n        db_record.data_source = record.data_source\n        db_record.content_hash = content_hash\n        db_record.created_at = timestamp\n        db_record.updated_at = timestamp\n\n        return db_record\n\n    def add_record(self, record: Record) -&gt; None:\n        \"\"\"Add a Record to the database.\"\"\"\n        with self.SessionLocal() as session:\n            # Check if record already exists\n            existing = (\n                session.query(SQLiteRecord)\n                .filter(SQLiteRecord.id == str(record.id))\n                .first()\n            )\n            if existing:\n                raise LexosException(\n                    f\"Record with ID {record.id} already exists in database\"\n                )\n\n            # Convert Record to SQLiteRecord\n            db_record = self._record_to_db_record(record)\n\n            session.add(db_record)\n            session.commit()\n\n    def close(self):\n        \"\"\"Close the database connection and clean up resources.\"\"\"\n        if hasattr(self, \"engine\") and self.engine:\n            self.engine.dispose()\n\n    def delete_record(self, record_id: str) -&gt; bool:\n        \"\"\"Delete a record from the database.\"\"\"\n        with self.SessionLocal() as session:\n            record = (\n                session.query(SQLiteRecord).filter(SQLiteRecord.id == record_id).first()\n            )\n            if record:\n                session.delete(record)\n                session.commit()\n                return True\n            return False\n\n    def filter_records(\n        self,\n        is_active: Optional[bool] = None,\n        is_parsed: Optional[bool] = None,\n        model: Optional[str] = None,\n        min_tokens: Optional[int] = None,\n        max_tokens: Optional[int] = None,\n        limit: Optional[int] = None,\n    ) -&gt; list[Record]:\n        \"\"\"Filter records by various criteria.\"\"\"\n        with self.SessionLocal() as session:\n            query = session.query(SQLiteRecord)\n\n            if is_active is not None:\n                query = query.filter(SQLiteRecord.is_active == is_active)\n            if is_parsed is not None:\n                query = query.filter(SQLiteRecord.is_parsed == is_parsed)\n            if model is not None:\n                query = query.filter(SQLiteRecord.model == model)\n            if min_tokens is not None:\n                query = query.filter(SQLiteRecord.num_tokens &gt;= min_tokens)\n            if max_tokens is not None:\n                query = query.filter(SQLiteRecord.num_tokens &lt;= max_tokens)\n\n            if limit is not None:\n                query = query.limit(limit)\n\n            results = query.all()\n\n            return [\n                self._db_record_to_record(db_record, include_doc=False)\n                for db_record in results\n            ]\n\n    def get_stats(self) -&gt; dict[str, Any]:\n        \"\"\"Get aggregate corpus statistics from the database.\"\"\"\n        with self.SessionLocal() as session:\n            # Basic counts\n            total_records = session.execute(\n                text(\"SELECT COUNT(*) FROM records\")\n            ).scalar()\n            active_records = session.execute(\n                text(\"SELECT COUNT(*) FROM records WHERE is_active = 1\")\n            ).scalar()\n            parsed_records = session.execute(\n                text(\"SELECT COUNT(*) FROM records WHERE is_parsed = 1\")\n            ).scalar()\n\n            # Token statistics\n            total_tokens = (\n                session.execute(\n                    text(\"SELECT SUM(num_tokens) FROM records WHERE is_active = 1\")\n                ).scalar()\n                or 0\n            )\n            total_terms = (\n                session.execute(\n                    text(\"SELECT SUM(num_terms) FROM records WHERE is_active = 1\")\n                ).scalar()\n                or 0\n            )\n\n            # Vocabulary density statistics\n            avg_vocab_density = (\n                session.execute(\n                    text(\n                        \"SELECT AVG(vocab_density) FROM records WHERE is_active = 1 AND num_tokens &gt; 0\"\n                    )\n                ).scalar()\n                or 0\n            )\n\n            return {\n                \"total_records\": total_records,\n                \"active_records\": active_records,\n                \"parsed_records\": parsed_records,\n                \"total_tokens\": total_tokens,\n                \"total_terms\": total_terms,\n                \"average_vocab_density\": avg_vocab_density,\n            }\n\n    # Note: `get_stats()` is the canonical method name. Older code that used\n    # `get_corpus_stats()` should call `get_stats()` instead. This wrapper was\n    # removed to keep the sqlite submodule's API consistent with the\n    # `Corpus` public API. If you need backward compatibility across the\n    # deprecated database modules, see `src/lexos/database/database_simple.py`.\n\n    def get_record(\n        self,\n        record_id: str,\n        include_doc: bool = True,\n        model_cache: Optional[LexosModelCache] = None,\n    ) -&gt; Optional[Record]:\n        \"\"\"Retrieve a Record from the database.\"\"\"\n        with self.SessionLocal() as session:\n            db_record = (\n                session.query(SQLiteRecord).filter(SQLiteRecord.id == record_id).first()\n            )\n            if not db_record:\n                return None\n\n            return self._db_record_to_record(db_record, include_doc, model_cache)\n\n    def search_records(\n        self,\n        query: str,\n        limit: int = 100,\n        include_inactive: bool = False,\n        model_filter: Optional[str] = None,\n    ) -&gt; list[Record]:\n        \"\"\"Perform full-text search on records.\"\"\"\n        with self.SessionLocal() as session:\n            # Build FTS query - Fix: Use proper FTS5 syntax and avoid duplicate joins\n            fts_query = text(\"\"\"\n                SELECT DISTINCT r.* FROM records r\n                WHERE r.id IN (\n                    SELECT record_id FROM records_fts\n                    WHERE records_fts MATCH :query\n                )\n                AND (:include_inactive OR r.is_active = 1)\n                AND (:model_filter IS NULL OR r.model = :model_filter)\n                ORDER BY r.created_at DESC\n                LIMIT :limit\n            \"\"\")\n\n            result = session.execute(\n                fts_query,\n                {\n                    \"query\": query,\n                    \"include_inactive\": include_inactive,\n                    \"model_filter\": model_filter,\n                    \"limit\": limit,\n                },\n            )\n\n            records = []\n            for row in result:\n                # Convert row to SQLiteRecord manually\n                db_record = SQLiteRecord()\n                for i, col in enumerate(SQLiteRecord.__table__.columns):\n                    setattr(db_record, col.name, row[i])\n\n                record = self._db_record_to_record(db_record, include_doc=False)\n                records.append(record)\n\n            return records\n\n    def update_record(self, record: Record) -&gt; None:\n        \"\"\"Update an existing record in the database.\"\"\"\n        with self.SessionLocal() as session:\n            existing = (\n                session.query(SQLiteRecord)\n                .filter(SQLiteRecord.id == str(record.id))\n                .first()\n            )\n            if not existing:\n                raise LexosException(\n                    f\"Record with ID {record.id} not found in database\"\n                )\n\n            # Update the existing record\n            updated_record = self._record_to_db_record(record)\n            for key, value in updated_record.__dict__.items():\n                if (\n                    key != \"_sa_instance_state\" and key != \"id\"\n                ):  # Skip SQLAlchemy metadata and primary key\n                    setattr(existing, key, value)\n\n            session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to ensure database connections are closed.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor to ensure database connections are closed.\"\"\"\n    try:\n        self.close()\n    except:\n        pass  # Ignore errors during cleanup\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.__init__","title":"<code>__init__(database_path: Union[str, Path] = ':memory:', **kwargs: Any)</code>","text":"<p>Initialize the corpus database.</p> <p>Parameters:</p> Name Type Description Default <code>database_path</code> <code>Union[str, Path]</code> <p>Path to SQLite database file, or \":memory:\" for in-memory database</p> <code>':memory:'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for SQLAlchemy engine creation</p> <code>{}</code> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def __init__(self, database_path: Union[str, Path] = \":memory:\", **kwargs: Any):\n    \"\"\"Initialize the corpus database.\n\n    Args:\n        database_path: Path to SQLite database file, or \":memory:\" for in-memory database\n        **kwargs: Additional keyword arguments for SQLAlchemy engine creation\n    \"\"\"\n    self.database_path = str(database_path)\n    self.engine = create_engine(f\"sqlite:///{self.database_path}\", **kwargs)\n    self.SessionLocal = sessionmaker(bind=self.engine)\n    self._initialize_database()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.add_record","title":"<code>add_record(record: Record) -&gt; None</code>","text":"<p>Add a Record to the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def add_record(self, record: Record) -&gt; None:\n    \"\"\"Add a Record to the database.\"\"\"\n    with self.SessionLocal() as session:\n        # Check if record already exists\n        existing = (\n            session.query(SQLiteRecord)\n            .filter(SQLiteRecord.id == str(record.id))\n            .first()\n        )\n        if existing:\n            raise LexosException(\n                f\"Record with ID {record.id} already exists in database\"\n            )\n\n        # Convert Record to SQLiteRecord\n        db_record = self._record_to_db_record(record)\n\n        session.add(db_record)\n        session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.close","title":"<code>close()</code>","text":"<p>Close the database connection and clean up resources.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def close(self):\n    \"\"\"Close the database connection and clean up resources.\"\"\"\n    if hasattr(self, \"engine\") and self.engine:\n        self.engine.dispose()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.delete_record","title":"<code>delete_record(record_id: str) -&gt; bool</code>","text":"<p>Delete a record from the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def delete_record(self, record_id: str) -&gt; bool:\n    \"\"\"Delete a record from the database.\"\"\"\n    with self.SessionLocal() as session:\n        record = (\n            session.query(SQLiteRecord).filter(SQLiteRecord.id == record_id).first()\n        )\n        if record:\n            session.delete(record)\n            session.commit()\n            return True\n        return False\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.filter_records","title":"<code>filter_records(is_active: Optional[bool] = None, is_parsed: Optional[bool] = None, model: Optional[str] = None, min_tokens: Optional[int] = None, max_tokens: Optional[int] = None, limit: Optional[int] = None) -&gt; list[Record]</code>","text":"<p>Filter records by various criteria.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def filter_records(\n    self,\n    is_active: Optional[bool] = None,\n    is_parsed: Optional[bool] = None,\n    model: Optional[str] = None,\n    min_tokens: Optional[int] = None,\n    max_tokens: Optional[int] = None,\n    limit: Optional[int] = None,\n) -&gt; list[Record]:\n    \"\"\"Filter records by various criteria.\"\"\"\n    with self.SessionLocal() as session:\n        query = session.query(SQLiteRecord)\n\n        if is_active is not None:\n            query = query.filter(SQLiteRecord.is_active == is_active)\n        if is_parsed is not None:\n            query = query.filter(SQLiteRecord.is_parsed == is_parsed)\n        if model is not None:\n            query = query.filter(SQLiteRecord.model == model)\n        if min_tokens is not None:\n            query = query.filter(SQLiteRecord.num_tokens &gt;= min_tokens)\n        if max_tokens is not None:\n            query = query.filter(SQLiteRecord.num_tokens &lt;= max_tokens)\n\n        if limit is not None:\n            query = query.limit(limit)\n\n        results = query.all()\n\n        return [\n            self._db_record_to_record(db_record, include_doc=False)\n            for db_record in results\n        ]\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.get_record","title":"<code>get_record(record_id: str, include_doc: bool = True, model_cache: Optional[LexosModelCache] = None) -&gt; Optional[Record]</code>","text":"<p>Retrieve a Record from the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def get_record(\n    self,\n    record_id: str,\n    include_doc: bool = True,\n    model_cache: Optional[LexosModelCache] = None,\n) -&gt; Optional[Record]:\n    \"\"\"Retrieve a Record from the database.\"\"\"\n    with self.SessionLocal() as session:\n        db_record = (\n            session.query(SQLiteRecord).filter(SQLiteRecord.id == record_id).first()\n        )\n        if not db_record:\n            return None\n\n        return self._db_record_to_record(db_record, include_doc, model_cache)\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.get_stats","title":"<code>get_stats() -&gt; dict[str, Any]</code>","text":"<p>Get aggregate corpus statistics from the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get aggregate corpus statistics from the database.\"\"\"\n    with self.SessionLocal() as session:\n        # Basic counts\n        total_records = session.execute(\n            text(\"SELECT COUNT(*) FROM records\")\n        ).scalar()\n        active_records = session.execute(\n            text(\"SELECT COUNT(*) FROM records WHERE is_active = 1\")\n        ).scalar()\n        parsed_records = session.execute(\n            text(\"SELECT COUNT(*) FROM records WHERE is_parsed = 1\")\n        ).scalar()\n\n        # Token statistics\n        total_tokens = (\n            session.execute(\n                text(\"SELECT SUM(num_tokens) FROM records WHERE is_active = 1\")\n            ).scalar()\n            or 0\n        )\n        total_terms = (\n            session.execute(\n                text(\"SELECT SUM(num_terms) FROM records WHERE is_active = 1\")\n            ).scalar()\n            or 0\n        )\n\n        # Vocabulary density statistics\n        avg_vocab_density = (\n            session.execute(\n                text(\n                    \"SELECT AVG(vocab_density) FROM records WHERE is_active = 1 AND num_tokens &gt; 0\"\n                )\n            ).scalar()\n            or 0\n        )\n\n        return {\n            \"total_records\": total_records,\n            \"active_records\": active_records,\n            \"parsed_records\": parsed_records,\n            \"total_tokens\": total_tokens,\n            \"total_terms\": total_terms,\n            \"average_vocab_density\": avg_vocab_density,\n        }\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.search_records","title":"<code>search_records(query: str, limit: int = 100, include_inactive: bool = False, model_filter: Optional[str] = None) -&gt; list[Record]</code>","text":"<p>Perform full-text search on records.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def search_records(\n    self,\n    query: str,\n    limit: int = 100,\n    include_inactive: bool = False,\n    model_filter: Optional[str] = None,\n) -&gt; list[Record]:\n    \"\"\"Perform full-text search on records.\"\"\"\n    with self.SessionLocal() as session:\n        # Build FTS query - Fix: Use proper FTS5 syntax and avoid duplicate joins\n        fts_query = text(\"\"\"\n            SELECT DISTINCT r.* FROM records r\n            WHERE r.id IN (\n                SELECT record_id FROM records_fts\n                WHERE records_fts MATCH :query\n            )\n            AND (:include_inactive OR r.is_active = 1)\n            AND (:model_filter IS NULL OR r.model = :model_filter)\n            ORDER BY r.created_at DESC\n            LIMIT :limit\n        \"\"\")\n\n        result = session.execute(\n            fts_query,\n            {\n                \"query\": query,\n                \"include_inactive\": include_inactive,\n                \"model_filter\": model_filter,\n                \"limit\": limit,\n            },\n        )\n\n        records = []\n        for row in result:\n            # Convert row to SQLiteRecord manually\n            db_record = SQLiteRecord()\n            for i, col in enumerate(SQLiteRecord.__table__.columns):\n                setattr(db_record, col.name, row[i])\n\n            record = self._db_record_to_record(db_record, include_doc=False)\n            records.append(record)\n\n        return records\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.update_record","title":"<code>update_record(record: Record) -&gt; None</code>","text":"<p>Update an existing record in the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def update_record(self, record: Record) -&gt; None:\n    \"\"\"Update an existing record in the database.\"\"\"\n    with self.SessionLocal() as session:\n        existing = (\n            session.query(SQLiteRecord)\n            .filter(SQLiteRecord.id == str(record.id))\n            .first()\n        )\n        if not existing:\n            raise LexosException(\n                f\"Record with ID {record.id} not found in database\"\n            )\n\n        # Update the existing record\n        updated_record = self._record_to_db_record(record)\n        for key, value in updated_record.__dict__.items():\n            if (\n                key != \"_sa_instance_state\" and key != \"id\"\n            ):  # Skip SQLAlchemy metadata and primary key\n                setattr(existing, key, value)\n\n        session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to ensure database connections are closed.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor to ensure database connections are closed.\"\"\"\n    try:\n        self.close()\n    except:\n        pass  # Ignore errors during cleanup\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.__init__","title":"<code>__init__(database_path: Union[str, Path] = ':memory:', **kwargs: Any)</code>","text":"<p>Initialize the corpus database.</p> <p>Parameters:</p> Name Type Description Default <code>database_path</code> <code>Union[str, Path]</code> <p>Path to SQLite database file, or \":memory:\" for in-memory database</p> <code>':memory:'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for SQLAlchemy engine creation</p> <code>{}</code> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def __init__(self, database_path: Union[str, Path] = \":memory:\", **kwargs: Any):\n    \"\"\"Initialize the corpus database.\n\n    Args:\n        database_path: Path to SQLite database file, or \":memory:\" for in-memory database\n        **kwargs: Additional keyword arguments for SQLAlchemy engine creation\n    \"\"\"\n    self.database_path = str(database_path)\n    self.engine = create_engine(f\"sqlite:///{self.database_path}\", **kwargs)\n    self.SessionLocal = sessionmaker(bind=self.engine)\n    self._initialize_database()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend._db_record_to_record","title":"<code>_db_record_to_record(db_record: SQLiteRecord, include_doc: bool = True, model_cache: Optional[LexosModelCache] = None) -&gt; Record</code>","text":"<p>Convert a SQLiteRecord back to a Record object.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def _db_record_to_record(\n    self,\n    db_record: SQLiteRecord,\n    include_doc: bool = True,\n    model_cache: Optional[LexosModelCache] = None,\n) -&gt; Record:\n    \"\"\"Convert a SQLiteRecord back to a Record object.\"\"\"\n    # Deserialize metadata\n    metadata = json.loads(db_record.metadata_json)\n    extensions = json.loads(db_record.extensions_list)\n\n    # Handle content deserialization\n    if include_doc and db_record.content_doc_bytes and db_record.is_parsed:\n        # Deserialize spaCy Doc\n        content = self._deserialize_doc_content(\n            db_record.content_doc_bytes, db_record.model, model_cache\n        )\n    else:\n        # Use text content\n        content = db_record.content_text\n\n    # Create Record object\n    record = Record(\n        id=db_record.id,\n        name=db_record.name,\n        is_active=db_record.is_active,\n        content=content,\n        model=db_record.model,\n        extensions=extensions,\n        data_source=db_record.data_source,\n        meta=metadata,\n    )\n\n    return record\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend._deserialize_doc_content","title":"<code>_deserialize_doc_content(doc_bytes: bytes, model: Optional[str] = None, model_cache: Optional[LexosModelCache] = None) -&gt; Doc</code>","text":"<p>Deserialize spaCy Doc from bytes.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def _deserialize_doc_content(\n    self,\n    doc_bytes: bytes,\n    model: Optional[str] = None,\n    model_cache: Optional[LexosModelCache] = None,\n) -&gt; Doc:\n    \"\"\"Deserialize spaCy Doc from bytes.\"\"\"\n    try:\n        # Use Record's deserialization method\n        temp_record = Record(id=str(uuid4()), content=\"\")\n        return temp_record._doc_from_bytes(doc_bytes, model, model_cache)\n    except Exception as e:\n        raise LexosException(f\"Failed to deserialize spaCy Doc: {str(e)}\")\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend._initialize_database","title":"<code>_initialize_database()</code>","text":"<p>Initialize database schema and enable full-text search.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def _initialize_database(self):\n    \"\"\"Initialize database schema and enable full-text search.\"\"\"\n    # Create all tables\n    Base.metadata.create_all(self.engine)\n\n    # Enable FTS5 full-text search\n    with self.SessionLocal() as session:\n        # Create FTS5 virtual table for full-text search\n        session.execute(\n            text(\"\"\"\n            CREATE VIRTUAL TABLE IF NOT EXISTS records_fts USING fts5(\n                record_id,\n                name,\n                content_text,\n                metadata_text\n            )\n        \"\"\")\n        )\n\n        # Create triggers to keep FTS table synchronized\n        session.execute(\n            text(\"\"\"\n            CREATE TRIGGER IF NOT EXISTS records_fts_insert AFTER INSERT ON records\n            BEGIN\n                INSERT INTO records_fts(record_id, name, content_text, metadata_text)\n                VALUES (new.id, new.name, new.content_text, new.metadata_json);\n            END\n        \"\"\")\n        )\n\n        session.execute(\n            text(\"\"\"\n            CREATE TRIGGER IF NOT EXISTS records_fts_delete AFTER DELETE ON records\n            BEGIN\n                DELETE FROM records_fts WHERE record_id = old.id;\n            END\n        \"\"\")\n        )\n\n        session.execute(\n            text(\"\"\"\n            CREATE TRIGGER IF NOT EXISTS records_fts_update AFTER UPDATE ON records\n            BEGIN\n                UPDATE records_fts\n                SET name = new.name,\n                    content_text = new.content_text,\n                    metadata_text = new.metadata_json\n                WHERE record_id = new.id;\n            END\n        \"\"\")\n        )\n\n        session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend._record_to_db_record","title":"<code>_record_to_db_record(record: Record) -&gt; SQLiteRecord</code>","text":"<p>Convert a Record object to SQLiteRecord for database storage.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def _record_to_db_record(self, record: Record) -&gt; SQLiteRecord:\n    \"\"\"Convert a Record object to SQLiteRecord for database storage.\"\"\"\n    # Extract text content\n    if record.is_parsed and isinstance(record.content, Doc):\n        content_text = record.content.text\n        # Serialize spaCy Doc if parsed\n        content_doc_bytes = record._doc_to_bytes()\n    else:\n        content_text = str(record.content) if record.content else \"\"\n        content_doc_bytes = None\n\n    # Calculate content hash\n    content_hash = hashlib.sha256(content_text.encode()).hexdigest()\n\n    # Calculate statistics\n    num_tokens = record.num_tokens() if record.is_parsed else 0\n    num_terms = record.num_terms() if record.is_parsed else 0\n    vocab_density = record.vocab_density() if record.is_parsed else 0.0\n\n    # Serialize metadata\n    metadata_json = json.dumps(record.meta, default=str)\n    extensions_list = json.dumps(record.extensions)\n\n    timestamp = datetime.now().isoformat()\n\n    db_record = SQLiteRecord()\n    db_record.id = str(record.id)\n    db_record.name = record.name\n    db_record.content_text = content_text\n    db_record.content_doc_bytes = content_doc_bytes\n    db_record.is_active = record.is_active\n    db_record.is_parsed = record.is_parsed\n    db_record.model = record.model\n    db_record.num_tokens = num_tokens\n    db_record.num_terms = num_terms\n    db_record.vocab_density = vocab_density\n    db_record.metadata_json = metadata_json\n    db_record.extensions_list = extensions_list\n    db_record.data_source = record.data_source\n    db_record.content_hash = content_hash\n    db_record.created_at = timestamp\n    db_record.updated_at = timestamp\n\n    return db_record\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.add_record","title":"<code>add_record(record: Record) -&gt; None</code>","text":"<p>Add a Record to the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def add_record(self, record: Record) -&gt; None:\n    \"\"\"Add a Record to the database.\"\"\"\n    with self.SessionLocal() as session:\n        # Check if record already exists\n        existing = (\n            session.query(SQLiteRecord)\n            .filter(SQLiteRecord.id == str(record.id))\n            .first()\n        )\n        if existing:\n            raise LexosException(\n                f\"Record with ID {record.id} already exists in database\"\n            )\n\n        # Convert Record to SQLiteRecord\n        db_record = self._record_to_db_record(record)\n\n        session.add(db_record)\n        session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.close","title":"<code>close()</code>","text":"<p>Close the database connection and clean up resources.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def close(self):\n    \"\"\"Close the database connection and clean up resources.\"\"\"\n    if hasattr(self, \"engine\") and self.engine:\n        self.engine.dispose()\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.delete_record","title":"<code>delete_record(record_id: str) -&gt; bool</code>","text":"<p>Delete a record from the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def delete_record(self, record_id: str) -&gt; bool:\n    \"\"\"Delete a record from the database.\"\"\"\n    with self.SessionLocal() as session:\n        record = (\n            session.query(SQLiteRecord).filter(SQLiteRecord.id == record_id).first()\n        )\n        if record:\n            session.delete(record)\n            session.commit()\n            return True\n        return False\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.filter_records","title":"<code>filter_records(is_active: Optional[bool] = None, is_parsed: Optional[bool] = None, model: Optional[str] = None, min_tokens: Optional[int] = None, max_tokens: Optional[int] = None, limit: Optional[int] = None) -&gt; list[Record]</code>","text":"<p>Filter records by various criteria.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def filter_records(\n    self,\n    is_active: Optional[bool] = None,\n    is_parsed: Optional[bool] = None,\n    model: Optional[str] = None,\n    min_tokens: Optional[int] = None,\n    max_tokens: Optional[int] = None,\n    limit: Optional[int] = None,\n) -&gt; list[Record]:\n    \"\"\"Filter records by various criteria.\"\"\"\n    with self.SessionLocal() as session:\n        query = session.query(SQLiteRecord)\n\n        if is_active is not None:\n            query = query.filter(SQLiteRecord.is_active == is_active)\n        if is_parsed is not None:\n            query = query.filter(SQLiteRecord.is_parsed == is_parsed)\n        if model is not None:\n            query = query.filter(SQLiteRecord.model == model)\n        if min_tokens is not None:\n            query = query.filter(SQLiteRecord.num_tokens &gt;= min_tokens)\n        if max_tokens is not None:\n            query = query.filter(SQLiteRecord.num_tokens &lt;= max_tokens)\n\n        if limit is not None:\n            query = query.limit(limit)\n\n        results = query.all()\n\n        return [\n            self._db_record_to_record(db_record, include_doc=False)\n            for db_record in results\n        ]\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.get_stats","title":"<code>get_stats() -&gt; dict[str, Any]</code>","text":"<p>Get aggregate corpus statistics from the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get aggregate corpus statistics from the database.\"\"\"\n    with self.SessionLocal() as session:\n        # Basic counts\n        total_records = session.execute(\n            text(\"SELECT COUNT(*) FROM records\")\n        ).scalar()\n        active_records = session.execute(\n            text(\"SELECT COUNT(*) FROM records WHERE is_active = 1\")\n        ).scalar()\n        parsed_records = session.execute(\n            text(\"SELECT COUNT(*) FROM records WHERE is_parsed = 1\")\n        ).scalar()\n\n        # Token statistics\n        total_tokens = (\n            session.execute(\n                text(\"SELECT SUM(num_tokens) FROM records WHERE is_active = 1\")\n            ).scalar()\n            or 0\n        )\n        total_terms = (\n            session.execute(\n                text(\"SELECT SUM(num_terms) FROM records WHERE is_active = 1\")\n            ).scalar()\n            or 0\n        )\n\n        # Vocabulary density statistics\n        avg_vocab_density = (\n            session.execute(\n                text(\n                    \"SELECT AVG(vocab_density) FROM records WHERE is_active = 1 AND num_tokens &gt; 0\"\n                )\n            ).scalar()\n            or 0\n        )\n\n        return {\n            \"total_records\": total_records,\n            \"active_records\": active_records,\n            \"parsed_records\": parsed_records,\n            \"total_tokens\": total_tokens,\n            \"total_terms\": total_terms,\n            \"average_vocab_density\": avg_vocab_density,\n        }\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.get_record","title":"<code>get_record(record_id: str, include_doc: bool = True, model_cache: Optional[LexosModelCache] = None) -&gt; Optional[Record]</code>","text":"<p>Retrieve a Record from the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def get_record(\n    self,\n    record_id: str,\n    include_doc: bool = True,\n    model_cache: Optional[LexosModelCache] = None,\n) -&gt; Optional[Record]:\n    \"\"\"Retrieve a Record from the database.\"\"\"\n    with self.SessionLocal() as session:\n        db_record = (\n            session.query(SQLiteRecord).filter(SQLiteRecord.id == record_id).first()\n        )\n        if not db_record:\n            return None\n\n        return self._db_record_to_record(db_record, include_doc, model_cache)\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.search_records","title":"<code>search_records(query: str, limit: int = 100, include_inactive: bool = False, model_filter: Optional[str] = None) -&gt; list[Record]</code>","text":"<p>Perform full-text search on records.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def search_records(\n    self,\n    query: str,\n    limit: int = 100,\n    include_inactive: bool = False,\n    model_filter: Optional[str] = None,\n) -&gt; list[Record]:\n    \"\"\"Perform full-text search on records.\"\"\"\n    with self.SessionLocal() as session:\n        # Build FTS query - Fix: Use proper FTS5 syntax and avoid duplicate joins\n        fts_query = text(\"\"\"\n            SELECT DISTINCT r.* FROM records r\n            WHERE r.id IN (\n                SELECT record_id FROM records_fts\n                WHERE records_fts MATCH :query\n            )\n            AND (:include_inactive OR r.is_active = 1)\n            AND (:model_filter IS NULL OR r.model = :model_filter)\n            ORDER BY r.created_at DESC\n            LIMIT :limit\n        \"\"\")\n\n        result = session.execute(\n            fts_query,\n            {\n                \"query\": query,\n                \"include_inactive\": include_inactive,\n                \"model_filter\": model_filter,\n                \"limit\": limit,\n            },\n        )\n\n        records = []\n        for row in result:\n            # Convert row to SQLiteRecord manually\n            db_record = SQLiteRecord()\n            for i, col in enumerate(SQLiteRecord.__table__.columns):\n                setattr(db_record, col.name, row[i])\n\n            record = self._db_record_to_record(db_record, include_doc=False)\n            records.append(record)\n\n        return records\n</code></pre>"},{"location":"api/corpus/sqlite/database/#lexos.corpus.sqlite.database.SQLiteBackend.update_record","title":"<code>update_record(record: Record) -&gt; None</code>","text":"<p>Update an existing record in the database.</p> Source code in <code>lexos/corpus/sqlite/database.py</code> <pre><code>def update_record(self, record: Record) -&gt; None:\n    \"\"\"Update an existing record in the database.\"\"\"\n    with self.SessionLocal() as session:\n        existing = (\n            session.query(SQLiteRecord)\n            .filter(SQLiteRecord.id == str(record.id))\n            .first()\n        )\n        if not existing:\n            raise LexosException(\n                f\"Record with ID {record.id} not found in database\"\n            )\n\n        # Update the existing record\n        updated_record = self._record_to_db_record(record)\n        for key, value in updated_record.__dict__.items():\n            if (\n                key != \"_sa_instance_state\" and key != \"id\"\n            ):  # Skip SQLAlchemy metadata and primary key\n                setattr(existing, key, value)\n\n        session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/","title":"integration","text":""},{"location":"api/corpus/sqlite/integration/#module-description","title":"Module Description","text":"<p>Database integration layer for the Lexos <code>Corpus</code> class.</p> <p>This module extends the existing Corpus class with optional SQLite database capabilities while maintaining full compatibility with the file-based system.</p> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus","title":"<code>SQLiteCorpus</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>Corpus</code></p> <p>Corpus with SQLite database backend support.</p> <p>Extends the standard Corpus with optional database storage: - Dual storage: files + database - Full-text search across records - Efficient metadata queries - Optional database-only mode</p> <p>The database integration is completely optional and does not break existing file-based workflows.</p> <p>Fields:</p> <ul> <li> <code>corpus_dir</code>                 (<code>str</code>)             </li> <li> <code>corpus_metadata_file</code>                 (<code>str</code>)             </li> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>records</code>                 (<code>RecordsDict</code>)             </li> <li> <code>names</code>                 (<code>dict[str, list[str]]</code>)             </li> <li> <code>meta</code>                 (<code>dict[str, Any]</code>)             </li> <li> <code>analysis_results</code>                 (<code>dict[str, dict[str, Any]]</code>)             </li> <li> <code>model_cache</code>                 (<code>LexosModelCache</code>)             </li> <li> <code>num_active_docs</code>                 (<code>int</code>)             </li> <li> <code>num_docs</code>                 (<code>int</code>)             </li> <li> <code>num_terms</code>                 (<code>int</code>)             </li> <li> <code>num_tokens</code>                 (<code>int</code>)             </li> <li> <code>terms</code>                 (<code>set</code>)             </li> <li> <code>use_sqlite</code>                 (<code>bool</code>)             </li> <li> <code>sqlite_only</code>                 (<code>bool</code>)             </li> <li> <code>sqlite_path</code>                 (<code>Optional[str]</code>)             </li> <li> <code>db</code>                 (<code>Optional[SQLiteBackend]</code>)             </li> </ul> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>class SQLiteCorpus(Corpus):\n    \"\"\"Corpus with SQLite database backend support.\n\n    Extends the standard Corpus with optional database storage:\n    - Dual storage: files + database\n    - Full-text search across records\n    - Efficient metadata queries\n    - Optional database-only mode\n\n    The database integration is completely optional and does not break\n    existing file-based workflows.\n    \"\"\"\n\n    # Add database-related fields to the Pydantic model\n    use_sqlite: bool = Field(\n        default=False, description=\"Whether to enable database storage\"\n    )\n    sqlite_only: bool = Field(\n        default=False, description=\"Whether to use database-only mode\"\n    )\n    sqlite_path: Optional[str] = Field(\n        default=None, description=\"Path to SQLite database file\"\n    )\n    db: Optional[SQLiteBackend] = Field(\n        default=None, description=\"Database connection object\", exclude=True\n    )\n\n    def __init__(self, **data: Any):\n        \"\"\"Initialize corpus with optional database integration.\n\n        Args:\n            **data (Any): Standard Corpus initialization parameters\n        \"\"\"\n        # Extract database-specific parameters\n        sqlite_path = data.pop(\"sqlite_path\", None)\n        use_sqlite = data.pop(\"use_sqlite\", False)\n        sqlite_only = data.pop(\"sqlite_only\", False)\n\n        # Set the database fields\n        data[\"use_sqlite\"] = use_sqlite\n        data[\"sqlite_only\"] = sqlite_only\n        data[\"sqlite_path\"] = sqlite_path\n\n        # Initialize parent class\n        super().__init__(**data)\n\n        # Initialize database if enabled\n        if self.use_sqlite or self.sqlite_only:\n            db_path = sqlite_path or f\"{self.corpus_dir}/corpus.db\"\n            self.db = SQLiteBackend(database_path=db_path)\n            self._initialize_metadata()\n        else:\n            self.db = None\n\n    def _add_to_backend(\n        self,\n        content,\n        name: Optional[str] = None,\n        is_active: Optional[bool] = True,\n        model: Optional[str] = None,\n        extensions: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        id_type: Optional[str] = \"uuid4\",\n    ):\n        \"\"\"Add records in database-only mode without file storage.\"\"\"\n        from spacy.tokens import Doc\n\n        # Sanitize metadata to ensure JSON-serializable types (defensive)\n        if metadata is not None:\n            metadata = self._sanitize_metadata(metadata)\n\n        # Handle single or multiple content items\n        if isinstance(content, (Doc, Record, str)):\n            items = [content]\n        else:\n            items = list(content)\n\n        for item in items:\n            # Generate unique ID\n            new_id = self._generate_unique_id(type=id_type)\n\n            if isinstance(item, Record):\n                record = item\n            else:\n                record_kwargs = dict(\n                    id=new_id,\n                    name=self._ensure_unique_name(name),\n                    is_active=is_active,\n                    content=item,\n                    model=model,\n                    data_source=None,\n                )\n                if extensions is not None:\n                    record_kwargs[\"extensions\"] = extensions\n                if metadata is not None:\n                    record_kwargs[\"meta\"] = metadata\n                record = Record(**record_kwargs)\n\n                # Note: Records are created with string content and can be parsed later if needed\n                # The database stores both parsed and unparsed content efficiently\n\n            # Add to in-memory records\n            record_id_str = str(record.id)\n            self.records[record_id_str] = record\n            if record.name not in self.names:\n                self.names[record.name] = []\n            self.names[record.name].append(record_id_str)\n            # Add a meta entry similar to file-based add to keep Corpus metadata consistent\n            try:\n                meta_entry = record.model_dump(\n                    exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n                )\n                # Ensure id is a string and annotate token/term counts\n                meta_entry[\"id\"] = str(meta_entry.get(\"id\", record_id_str))\n                meta_entry[\"num_tokens\"] = (\n                    record.num_tokens() if record.is_parsed else 0\n                )\n                meta_entry[\"num_terms\"] = record.num_terms() if record.is_parsed else 0\n                self.meta[record_id_str] = meta_entry\n            except Exception:\n                # Fallback minimal meta if model_dump fails\n                self.meta[record_id_str] = {\n                    \"id\": record_id_str,\n                    \"name\": record.name,\n                    \"is_active\": record.is_active,\n                    \"num_tokens\": record.num_tokens() if record.is_parsed else 0,\n                    \"num_terms\": record.num_terms() if record.is_parsed else 0,\n                }\n\n            # Store in database\n            if self.db:\n                self.db.add_record(record)\n\n        # Update corpus state\n        self._update_corpus_state()\n\n    def __del__(self):\n        \"\"\"Destructor to ensure database connections are closed.\"\"\"\n        try:\n            self.close()\n        except:\n            pass  # Ignore errors during cleanup\n\n    def _get_timestamp(self) -&gt; str:\n        \"\"\"Get current timestamp as ISO string.\"\"\"\n        from datetime import datetime\n\n        return datetime.now().isoformat()\n\n    def _load_records_from_disk(self):\n        \"\"\"Load records from the corpus directory into memory.\n\n        This is a helper method for sync() to load file-based records\n        from disk before syncing them to the database.\n        \"\"\"\n        corpus_dir = Path(self.corpus_dir)\n        metadata_path = corpus_dir / self.corpus_metadata_file\n\n        # Check if corpus directory and metadata exist\n        if not corpus_dir.exists():\n            return\n\n        if not metadata_path.exists():\n            return\n\n        # Load metadata\n        try:\n            import srsly\n\n            metadata = srsly.read_json(metadata_path)\n\n            # Load record metadata\n            if \"meta\" in metadata and metadata[\"meta\"]:\n                for record_id, record_meta in metadata[\"meta\"].items():\n                    # Load the record from disk\n                    data_dir = corpus_dir / \"data\"\n                    record_file = data_dir / f\"{record_id}.bin\"\n\n                    if record_file.exists():\n                        # Create a Record object and load from disk\n                        record = Record(id=record_id, name=record_meta.get(\"name\", \"\"))\n                        record.from_disk(\n                            str(record_file),\n                            model=record_meta.get(\"model\"),\n                            model_cache=self.model_cache,\n                        )\n\n                        # Add to in-memory structures\n                        self.records[record_id] = record\n                        if record.name not in self.names:\n                            self.names[record.name] = []\n                        self.names[record.name].append(record_id)\n\n        except Exception as e:\n            # If loading fails, just continue with empty records\n            print(f\"Warning: Failed to load records from disk: {str(e)}\")\n\n    def _initialize_metadata(self):\n        \"\"\"Initialize corpus metadata in the database.\"\"\"\n        if not self.db:\n            return\n\n        with self.db.SessionLocal() as session:\n            # Check if corpus metadata exists\n            corpus_id = self.name or \"default\"\n            existing = (\n                session.query(SQLiteMetadata)\n                .filter(SQLiteMetadata.corpus_id == corpus_id)\n                .first()\n            )\n\n            if not existing:\n                # Create new corpus metadata\n                corpus_metadata = SQLiteMetadata()\n                corpus_metadata.corpus_id = corpus_id\n                corpus_metadata.name = self.name\n                corpus_metadata.num_docs = self.num_docs\n                corpus_metadata.num_active_docs = self.num_active_docs\n                corpus_metadata.num_tokens = self.num_tokens\n                corpus_metadata.num_terms = self.num_terms\n                corpus_metadata.corpus_dir = self.corpus_dir\n                corpus_metadata.metadata_json = json.dumps(self.meta, default=str)\n                corpus_metadata.analysis_results_json = json.dumps(\n                    self.analysis_results, default=str\n                )\n                corpus_metadata.corpus_fingerprint = self._generate_corpus_fingerprint()\n                corpus_metadata.created_at = self._get_timestamp()\n                corpus_metadata.updated_at = self._get_timestamp()\n                session.add(corpus_metadata)\n                session.commit()\n\n    def _sanitize_metadata(self, metadata: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Convert non-JSON-serializable types to strings.\n\n        Args:\n            metadata: Original metadata dictionary\n\n        Returns:\n            Sanitized metadata dictionary with JSON-serializable values\n        \"\"\"\n        from datetime import date, datetime\n        from pathlib import Path\n        from uuid import UUID\n\n        sanitized = {}\n        for key, value in metadata.items():\n            if isinstance(value, UUID):\n                sanitized[key] = str(value)\n            elif isinstance(value, (datetime, date)):\n                sanitized[key] = value.isoformat()\n            elif isinstance(value, Path):\n                sanitized[key] = str(value)\n            elif isinstance(value, dict):\n                sanitized[key] = self._sanitize_metadata(value)  # Recursive\n            elif isinstance(value, list):\n                sanitized[key] = [\n                    self._sanitize_metadata({\"item\": item})[\"item\"]\n                    if isinstance(item, dict)\n                    else str(item)\n                    if isinstance(item, (UUID, datetime, date, Path))\n                    else item\n                    for item in value\n                ]\n            else:\n                sanitized[key] = value\n\n        return sanitized\n\n    def _update_corpus_state(self):\n        \"\"\"Update corpus state in both memory and database.\"\"\"\n        # Update in-memory state\n        super()._update_corpus_state()\n\n        # Update database metadata if enabled\n        if self.db:\n            with self.db.SessionLocal() as session:\n                corpus_id = self.name or \"default\"\n                corpus_metadata = (\n                    session.query(SQLiteMetadata)\n                    .filter(SQLiteMetadata.corpus_id == corpus_id)\n                    .first()\n                )\n\n                if corpus_metadata:\n                    corpus_metadata.num_docs = self.num_docs\n                    corpus_metadata.num_active_docs = self.num_active_docs\n                    corpus_metadata.num_tokens = self.num_tokens\n                    corpus_metadata.num_terms = self.num_terms\n                    corpus_metadata.metadata_json = json.dumps(self.meta, default=str)\n                    corpus_metadata.analysis_results_json = json.dumps(\n                        self.analysis_results, default=str\n                    )\n                    corpus_metadata.corpus_fingerprint = (\n                        self._generate_corpus_fingerprint()\n                    )\n                    corpus_metadata.updated_at = self._get_timestamp()\n\n                    session.commit()\n\n    @validate_call\n    def add(\n        self,\n        content,\n        name: Optional[str] = None,\n        is_active: Optional[bool] = True,\n        model: Optional[str] = None,\n        extensions: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        id_type: Optional[str] = \"uuid4\",\n        cache: Optional[bool] = False,\n        store_in_db: Optional[bool] = None,\n    ):\n        \"\"\"Add a record to the corpus with optional database storage.\n\n        Args:\n            content (str | Doc | Record): The content of the record\n            name (Optional[str]): Optional name for the record\n            is_active (Optional[bool]): Whether the record is active\n            model (Optional[str]): spaCy model name for parsing\n            extensions (Optional[list[str]]): List of spaCy extensions to add\n            metadata (Optional[dict[str, Any]]): Optional metadata dictionary\n            id_type (Optional[str]): Type of ID to generate ('uuid4' or 'int')\n            cache (Optional[bool]): Whether to cache the record in memory\n            store_in_db (Optional[bool]): Whether to store the record in the database\n        \"\"\"\n        # Sanitize metadata to ensure JSON-serializable types\n        if metadata is not None:\n            metadata = self._sanitize_metadata(metadata)\n\n        # Determine storage strategy\n        use_db = (\n            store_in_db\n            if store_in_db is not None\n            else self.use_sqlite or self.sqlite_only\n        )\n        use_files = not self.sqlite_only\n\n        # Get current record count to track new additions\n        initial_record_count = len(self.records)\n\n        # Add using parent implementation if using files\n        if use_files:\n            super().add(\n                content=content,\n                name=name,\n                is_active=is_active,\n                model=model,\n                extensions=extensions,\n                metadata=metadata,\n                id_type=id_type,\n                cache=cache,\n            )\n        else:\n            # Database-only mode - implement add logic without file storage\n            self._add_to_backend(\n                content=content,\n                name=name,\n                is_active=is_active,\n                model=model,\n                extensions=extensions,\n                metadata=metadata,\n                id_type=id_type,\n            )\n\n        # Also store in database if enabled and we're using file storage\n        if use_db and self.db and use_files:\n            # Get the newly added records\n            current_records = list(self.records.values())\n            new_records = current_records[initial_record_count:]\n\n            for record in new_records:\n                try:\n                    # Note: Records can be parsed later if needed\n                    # The database efficiently stores both parsed and unparsed content\n\n                    self.db.add_record(record)\n                except Exception as e:\n                    # Log error but don't fail the entire operation\n                    print(f\"Warning: Failed to add record {record.id} to database: {e}\")\n\n    @validate_call\n    def filter_records(\n        self,\n        is_active: Optional[bool] = None,\n        is_parsed: Optional[bool] = None,\n        model: Optional[str] = None,\n        min_tokens: Optional[int] = None,\n        max_tokens: Optional[int] = None,\n        limit: Optional[int] = None,\n        use_database: bool = True,\n    ) -&gt; list[Record]:\n        \"\"\"Filter records by various criteria.\n\n        Args:\n            is_active: Filter by active status\n            is_parsed: Filter by parsed status\n            model: Filter by spaCy model name\n            min_tokens: Minimum number of tokens\n            max_tokens: Maximum number of tokens\n            limit: Maximum number of results\n            use_database: Whether to use database filtering (vs in-memory)\n\n        Returns:\n            List of matching Record objects\n        \"\"\"\n        if use_database and self.db:\n            return self.db.filter_records(\n                is_active=is_active,\n                is_parsed=is_parsed,\n                model=model,\n                min_tokens=min_tokens,\n                max_tokens=max_tokens,\n                limit=limit,\n            )\n        else:\n            # Fallback to in-memory filtering\n            filtered_records = []\n            for record in self.records.values():\n                if is_active is not None and record.is_active != is_active:\n                    continue\n                if is_parsed is not None and record.is_parsed != is_parsed:\n                    continue\n                if model is not None and record.model != model:\n                    continue\n                if min_tokens is not None:\n                    try:\n                        if record.num_tokens() &lt; min_tokens:\n                            continue\n                    except:\n                        continue\n                if max_tokens is not None:\n                    try:\n                        if record.num_tokens() &gt; max_tokens:\n                            continue\n                    except:\n                        continue\n\n                filtered_records.append(record)\n\n                if limit and len(filtered_records) &gt;= limit:\n                    break\n\n            return filtered_records\n\n    @validate_call\n    def get_stats(self) -&gt; dict[str, Any]:\n        \"\"\"Get corpus statistics from the database.\n\n        Returns:\n            Dictionary containing database-derived statistics\n\n        Raises:\n            LexosException: If database is not enabled\n        \"\"\"\n        if not self.db:\n            raise LexosException(\n                \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n            )\n\n        return self.db.get_stats()\n\n    @validate_call\n    def search(\n        self,\n        query: str,\n        limit: int = 100,\n        include_inactive: bool = False,\n        model_filter: Optional[str] = None,\n        load_from_db: bool = True,\n    ) -&gt; list[Record]:\n        \"\"\"Perform full-text search on corpus records.\n\n        Args:\n            query: FTS5 search query string\n            limit: Maximum number of results to return\n            include_inactive: Whether to include inactive records\n            model_filter: Optional filter by spaCy model name\n            load_from_db: Whether to load results from database (vs memory)\n\n        Returns:\n            List of matching Record objects\n\n        Raises:\n            LexosException: If database is not enabled\n        \"\"\"\n        if not self.db:\n            raise LexosException(\n                \"Database is not enabled. Initialize corpus with use_sqlite=True to use search.\"\n            )\n\n        return self.db.search_records(\n            query=query,\n            limit=limit,\n            include_inactive=include_inactive,\n            model_filter=model_filter,\n        )\n\n    @validate_call\n    def sync(self, overwrite: bool = False) -&gt; int:\n        \"\"\"Synchronize existing file-based records to the database.\n\n        This method loads records from the corpus directory on disk and adds them\n        to the database. If records are already in memory, they will be used instead.\n\n        Args:\n            overwrite: Whether to overwrite existing database records\n\n        Returns:\n            Number of records synchronized\n\n        Raises:\n            LexosException: If database is not enabled\n        \"\"\"\n        if not self.db:\n            raise LexosException(\n                \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n            )\n\n        # Load records from disk if not already in memory\n        if not self.records:\n            self._load_records_from_disk()\n\n        synced_count = 0\n\n        for record in self.records.values():\n            try:\n                if overwrite:\n                    # Check if exists and update\n                    existing = self.db.get_record(str(record.id), include_doc=False)\n                    if existing:\n                        self.db.update_record(record)\n                    else:\n                        self.db.add_record(record)\n                else:\n                    # Only add if doesn't exist\n                    existing = self.db.get_record(str(record.id), include_doc=False)\n                    if not existing:\n                        self.db.add_record(record)\n\n                synced_count += 1\n\n            except Exception as e:\n                # Log error but continue with other records\n                print(f\"Warning: Failed to sync record {record.id}: {str(e)}\")\n\n        return synced_count\n\n    @validate_call\n    def load(self, include_docs: bool = False, active_only: bool = True) -&gt; int:\n        \"\"\"Load records from database into memory.\n\n        Args:\n            include_docs: Whether to deserialize spaCy Doc content\n            active_only: Whether to load only active records\n\n        Returns:\n            Number of records loaded\n\n        Raises:\n            LexosException: If database is not enabled\n        \"\"\"\n        if not self.db:\n            raise LexosException(\n                \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n            )\n\n        # Clear existing records if loading from database\n        self.records.clear()\n        self.names.clear()\n\n        # Load records from database\n        filters = {\"is_active\": True} if active_only else {}\n        db_records = self.db.filter_records(**filters)\n\n        loaded_count = 0\n        for record in db_records:\n            # Add to in-memory structures\n            record_id_str = str(record.id)\n            self.records[record_id_str] = record\n            if record.name not in self.names:\n                self.names[record.name] = []\n            self.names[record.name].append(record_id_str)\n            # Populate meta for loaded record so Corpus metadata is consistent\n            try:\n                meta_entry = record.model_dump(\n                    exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n                )\n                if \"id\" in meta_entry:\n                    meta_entry[\"id\"] = str(meta_entry[\"id\"])\n                meta_entry[\"num_tokens\"] = (\n                    record.num_tokens() if record.is_parsed else 0\n                )\n                meta_entry[\"num_terms\"] = record.num_terms() if record.is_parsed else 0\n                self.meta[record_id_str] = meta_entry\n            except Exception:\n                self.meta[record_id_str] = {\n                    \"id\": record_id_str,\n                    \"name\": record.name,\n                    \"is_active\": record.is_active,\n                    \"num_tokens\": record.num_tokens() if record.is_parsed else 0,\n                    \"num_terms\": record.num_terms() if record.is_parsed else 0,\n                }\n            loaded_count += 1\n\n        # Update corpus state\n        self._update_corpus_state()\n\n        return loaded_count\n\n    def close(self):\n        \"\"\"Close database connections and clean up resources.\"\"\"\n        if self.db:\n            self.db.close()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.active_terms","title":"<code>active_terms: set</code>  <code>property</code>","text":"<p>Return the set of active terms in the Corpus.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set</code> <p>A set of active term strings found in active parsed records.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.analysis_results","title":"<code>analysis_results: dict[str, dict[str, Any]]</code>  <code>pydantic-field</code>","text":"<p>Storage for results from external analysis modules (kmeans, topwords, kwic, etc.)</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.corpus_dir","title":"<code>corpus_dir: str = 'corpus'</code>  <code>pydantic-field</code>","text":"<p>The path to the directory where the corpus is stored.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.corpus_metadata_file","title":"<code>corpus_metadata_file: str = 'corpus_metadata.json'</code>  <code>pydantic-field</code>","text":"<p>The name of the corpus metadata file.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.db","title":"<code>db: Optional[SQLiteBackend] = None</code>  <code>pydantic-field</code>","text":"<p>Database connection object</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.meta","title":"<code>meta: dict[str, Any] = {}</code>  <code>pydantic-field</code>","text":"<p>Metadata dictionary for arbitrary metadata relating to the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.meta_df","title":"<code>meta_df: pd.DataFrame</code>  <code>property</code>","text":"<p>Return a DataFrame of the Corpus metadata.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.model_cache","title":"<code>model_cache: LexosModelCache = LexosModelCache()</code>  <code>pydantic-field</code>","text":"<p>A cache for spaCy models used in the Corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.name","title":"<code>name: str = None</code>  <code>pydantic-field</code>","text":"<p>The name of the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.num_active_docs","title":"<code>num_active_docs: int = 0</code>  <code>pydantic-field</code>","text":"<p>Number of active records in the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.num_active_terms","title":"<code>num_active_terms: int</code>  <code>property</code>","text":"<p>Return the number of active terms in the Corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.num_active_tokens","title":"<code>num_active_tokens: int</code>  <code>property</code>","text":"<p>Return the number of active tokens in the Corpus.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total number of tokens in active parsed records.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.num_docs","title":"<code>num_docs: int = 0</code>  <code>pydantic-field</code>","text":"<p>Total number of records in the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.num_terms","title":"<code>num_terms: int = 0</code>  <code>pydantic-field</code>","text":"<p>Total number of unique terms in the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.num_tokens","title":"<code>num_tokens: int = 0</code>  <code>pydantic-field</code>","text":"<p>Total number of tokens in the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.records","title":"<code>records: RecordsDict = {}</code>  <code>pydantic-field</code>","text":"<p>Dictionary of records in the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.sqlite_only","title":"<code>sqlite_only: bool = False</code>  <code>pydantic-field</code>","text":"<p>Whether to use database-only mode</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.sqlite_path","title":"<code>sqlite_path: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>Path to SQLite database file</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.terms","title":"<code>terms: set = set()</code>  <code>pydantic-field</code>","text":"<p>Set of unique terms in the corpus.</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.use_sqlite","title":"<code>use_sqlite: bool = False</code>  <code>pydantic-field</code>","text":"<p>Whether to enable database storage</p>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to ensure database connections are closed.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor to ensure database connections are closed.\"\"\"\n    try:\n        self.close()\n    except:\n        pass  # Ignore errors during cleanup\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.__init__","title":"<code>__init__(**data: Any)</code>","text":"<p>Initialize corpus with optional database integration.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Standard Corpus initialization parameters</p> <code>{}</code> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialize corpus with optional database integration.\n\n    Args:\n        **data (Any): Standard Corpus initialization parameters\n    \"\"\"\n    # Extract database-specific parameters\n    sqlite_path = data.pop(\"sqlite_path\", None)\n    use_sqlite = data.pop(\"use_sqlite\", False)\n    sqlite_only = data.pop(\"sqlite_only\", False)\n\n    # Set the database fields\n    data[\"use_sqlite\"] = use_sqlite\n    data[\"sqlite_only\"] = sqlite_only\n    data[\"sqlite_path\"] = sqlite_path\n\n    # Initialize parent class\n    super().__init__(**data)\n\n    # Initialize database if enabled\n    if self.use_sqlite or self.sqlite_only:\n        db_path = sqlite_path or f\"{self.corpus_dir}/corpus.db\"\n        self.db = SQLiteBackend(database_path=db_path)\n        self._initialize_metadata()\n    else:\n        self.db = None\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.__iter__","title":"<code>__iter__() -&gt; Iterable[Record]</code>","text":"<p>Make the corpus iterable.</p> <p>Returns:</p> Type Description <code>Iterable[Record]</code> <p>Iterator[Record]: An iterator over the Record objects in the corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __iter__(self) -&gt; Iterable[Record]:\n    \"\"\"Make the corpus iterable.\n\n    Returns:\n        Iterator[Record]: An iterator over the Record objects in the corpus.\n    \"\"\"\n    return iter(self.records.values())\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a string representation of the Corpus.\"\"\"\n    fields = {field: getattr(self, field) for field in self.model_fields_set}\n    field_list = [f\"{k}={v}\" for k, v in fields.items()]\n    rep = f\"Corpus({', '.join(sorted(field_list))})\"\n    return rep\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.add","title":"<code>add(content, name: Optional[str] = None, is_active: Optional[bool] = True, model: Optional[str] = None, extensions: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, id_type: Optional[str] = 'uuid4', cache: Optional[bool] = False, store_in_db: Optional[bool] = None)</code>","text":"<p>Add a record to the corpus with optional database storage.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | Doc | Record</code> <p>The content of the record</p> required <code>name</code> <code>Optional[str]</code> <p>Optional name for the record</p> <code>None</code> <code>is_active</code> <code>Optional[bool]</code> <p>Whether the record is active</p> <code>True</code> <code>model</code> <code>Optional[str]</code> <p>spaCy model name for parsing</p> <code>None</code> <code>extensions</code> <code>Optional[list[str]]</code> <p>List of spaCy extensions to add</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Optional metadata dictionary</p> <code>None</code> <code>id_type</code> <code>Optional[str]</code> <p>Type of ID to generate ('uuid4' or 'int')</p> <code>'uuid4'</code> <code>cache</code> <code>Optional[bool]</code> <p>Whether to cache the record in memory</p> <code>False</code> <code>store_in_db</code> <code>Optional[bool]</code> <p>Whether to store the record in the database</p> <code>None</code> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef add(\n    self,\n    content,\n    name: Optional[str] = None,\n    is_active: Optional[bool] = True,\n    model: Optional[str] = None,\n    extensions: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    id_type: Optional[str] = \"uuid4\",\n    cache: Optional[bool] = False,\n    store_in_db: Optional[bool] = None,\n):\n    \"\"\"Add a record to the corpus with optional database storage.\n\n    Args:\n        content (str | Doc | Record): The content of the record\n        name (Optional[str]): Optional name for the record\n        is_active (Optional[bool]): Whether the record is active\n        model (Optional[str]): spaCy model name for parsing\n        extensions (Optional[list[str]]): List of spaCy extensions to add\n        metadata (Optional[dict[str, Any]]): Optional metadata dictionary\n        id_type (Optional[str]): Type of ID to generate ('uuid4' or 'int')\n        cache (Optional[bool]): Whether to cache the record in memory\n        store_in_db (Optional[bool]): Whether to store the record in the database\n    \"\"\"\n    # Sanitize metadata to ensure JSON-serializable types\n    if metadata is not None:\n        metadata = self._sanitize_metadata(metadata)\n\n    # Determine storage strategy\n    use_db = (\n        store_in_db\n        if store_in_db is not None\n        else self.use_sqlite or self.sqlite_only\n    )\n    use_files = not self.sqlite_only\n\n    # Get current record count to track new additions\n    initial_record_count = len(self.records)\n\n    # Add using parent implementation if using files\n    if use_files:\n        super().add(\n            content=content,\n            name=name,\n            is_active=is_active,\n            model=model,\n            extensions=extensions,\n            metadata=metadata,\n            id_type=id_type,\n            cache=cache,\n        )\n    else:\n        # Database-only mode - implement add logic without file storage\n        self._add_to_backend(\n            content=content,\n            name=name,\n            is_active=is_active,\n            model=model,\n            extensions=extensions,\n            metadata=metadata,\n            id_type=id_type,\n        )\n\n    # Also store in database if enabled and we're using file storage\n    if use_db and self.db and use_files:\n        # Get the newly added records\n        current_records = list(self.records.values())\n        new_records = current_records[initial_record_count:]\n\n        for record in new_records:\n            try:\n                # Note: Records can be parsed later if needed\n                # The database efficiently stores both parsed and unparsed content\n\n                self.db.add_record(record)\n            except Exception as e:\n                # Log error but don't fail the entire operation\n                print(f\"Warning: Failed to add record {record.id} to database: {e}\")\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.add_from_files","title":"<code>add_from_files(paths: Path | str | list[Path | str], max_workers: Optional[int] = None, worker_strategy: str = 'auto', batch_size: int = 100, show_progress: bool = True, name_template: Optional[str] = None, is_active: bool = True, model: Optional[str] = None, extensions: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, id_type: str = 'uuid4') -&gt; None</code>","text":"<p>Load files directly into corpus using parallel I/O.</p> <p>This method streams files into the corpus without holding all content in memory, making it suitable for very large datasets. Files are loaded in parallel using the ParallelLoader with all its optimization features (smart file ordering, auto-tuning, etc.).</p> <p>State updates are deferred until all files are loaded for optimal performance.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Path | str | list[Path | str]</code> <p>File paths or directories to load.</p> required <code>max_workers</code> <code>Optional[int]</code> <p>Maximum number of worker threads. If None, auto-calculated based on worker_strategy.</p> <code>None</code> <code>worker_strategy</code> <code>str</code> <p>Worker allocation strategy. Options: - \"auto\": Analyzes file types and chooses optimal strategy (default) - \"io_bound\": More workers for I/O-intensive operations - \"cpu_bound\": Fewer workers for CPU-intensive operations - \"balanced\": Middle ground between I/O and CPU</p> <code>'auto'</code> <code>batch_size</code> <code>int</code> <p>Number of files to process in each batch. Default 100.</p> <code>100</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bar. Default True.</p> <code>True</code> <code>name_template</code> <code>Optional[str]</code> <p>Template for generating record names. Can include {filename}, {stem}, {index}. If None, uses filename stem.</p> <code>None</code> <code>is_active</code> <code>bool</code> <p>Whether records should be marked as active. Default True.</p> <code>True</code> <code>model</code> <code>Optional[str]</code> <p>Name of language model used to parse records.</p> <code>None</code> <code>extensions</code> <code>Optional[list[str]]</code> <p>List of extension names to add to records.</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Metadata to add to all records.</p> <code>None</code> <code>id_type</code> <code>str</code> <p>Type of ID to generate (\"integer\" or \"uuid4\"). Default \"uuid4\".</p> <code>'uuid4'</code> Example <pre><code>corpus = Corpus(\"my_corpus\")\n# Load all text files from a directory\ncorpus.add_from_files(\"path/to/texts/\")\n# With custom naming\ncorpus.add_from_files(\n    [\"file1.txt\", \"file2.txt\"],\n    name_template=\"{stem}_{index}\",\n    metadata={\"source\": \"collection_a\"}\n)\n</code></pre> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>def add_from_files(\n    self,\n    paths: Path | str | list[Path | str],\n    max_workers: Optional[int] = None,\n    worker_strategy: str = \"auto\",\n    batch_size: int = 100,\n    show_progress: bool = True,\n    name_template: Optional[str] = None,\n    is_active: bool = True,\n    model: Optional[str] = None,\n    extensions: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    id_type: str = \"uuid4\",\n) -&gt; None:\n    \"\"\"Load files directly into corpus using parallel I/O.\n\n    This method streams files into the corpus without holding all\n    content in memory, making it suitable for very large datasets.\n    Files are loaded in parallel using the ParallelLoader with all\n    its optimization features (smart file ordering, auto-tuning, etc.).\n\n    State updates are deferred until all files are loaded for optimal\n    performance.\n\n    Args:\n        paths (Path | str | list[Path | str]): File paths or directories to load.\n        max_workers (Optional[int]): Maximum number of worker threads.\n            If None, auto-calculated based on worker_strategy.\n        worker_strategy (str): Worker allocation strategy. Options:\n            - \"auto\": Analyzes file types and chooses optimal strategy (default)\n            - \"io_bound\": More workers for I/O-intensive operations\n            - \"cpu_bound\": Fewer workers for CPU-intensive operations\n            - \"balanced\": Middle ground between I/O and CPU\n        batch_size (int): Number of files to process in each batch. Default 100.\n        show_progress (bool): Whether to show progress bar. Default True.\n        name_template (Optional[str]): Template for generating record names.\n            Can include {filename}, {stem}, {index}. If None, uses filename stem.\n        is_active (bool): Whether records should be marked as active. Default True.\n        model (Optional[str]): Name of language model used to parse records.\n        extensions (Optional[list[str]]): List of extension names to add to records.\n        metadata (Optional[dict[str, Any]]): Metadata to add to all records.\n        id_type (str): Type of ID to generate (\"integer\" or \"uuid4\"). Default \"uuid4\".\n\n    Example:\n        ```python\n        corpus = Corpus(\"my_corpus\")\n        # Load all text files from a directory\n        corpus.add_from_files(\"path/to/texts/\")\n        # With custom naming\n        corpus.add_from_files(\n            [\"file1.txt\", \"file2.txt\"],\n            name_template=\"{stem}_{index}\",\n            metadata={\"source\": \"collection_a\"}\n        )\n        ```\n    \"\"\"\n    from lexos.io.parallel_loader import ParallelLoader\n\n    # Sanitize metadata if provided\n    if metadata is not None:\n        metadata = self._sanitize_metadata(metadata)\n\n    # Create ParallelLoader with specified settings\n    loader = ParallelLoader(\n        max_workers=max_workers,\n        worker_strategy=worker_strategy,\n        batch_size=batch_size,\n        show_progress=show_progress,\n    )\n\n    # Track for error reporting\n    loaded_count = 0\n    error_count = 0\n    errors = []\n\n    # Stream files and add to corpus\n    for index, (path, name, mime_type, text, error) in enumerate(\n        loader.load_streaming(paths), start=1\n    ):\n        if error:\n            error_count += 1\n            errors.append((path, error))\n            continue\n\n        # Generate record name from template or use default\n        if name_template:\n            record_name = name_template.format(\n                filename=Path(path).name, stem=name, index=index\n            )\n        else:\n            record_name = name\n\n        # Generate unique ID\n        record_id = self._generate_unique_id(type=id_type)\n\n        # Create record kwargs\n        record_kwargs = dict(\n            id=record_id,\n            name=record_name,\n            is_active=is_active,\n            content=text,\n            model=model,\n            data_source=str(path),\n        )\n\n        if extensions is not None:\n            record_kwargs[\"extensions\"] = extensions\n\n        if metadata is not None:\n            record_kwargs[\"meta\"] = metadata.copy()\n\n        # Create and add record without updating state\n        record = Record(**record_kwargs)\n        self._add_to_corpus_without_state_update(record)\n        loaded_count += 1\n\n    # Update corpus state once at the end\n    self._update_corpus_state()\n\n    # Report results\n    from wasabi import msg\n\n    msg.good(f\"Loaded {loaded_count} files into corpus. Errors: {error_count}\")\n\n    if errors and error_count &lt;= 10:  # Show first 10 errors\n        msg.warn(\"Errors encountered:\")\n        for path, error in errors[:10]:\n            msg.fail(f\"  {path}: {error}\")\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.close","title":"<code>close()</code>","text":"<p>Close database connections and clean up resources.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def close(self):\n    \"\"\"Close database connections and clean up resources.\"\"\"\n    if self.db:\n        self.db.close()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.export_statistical_fingerprint","title":"<code>export_statistical_fingerprint() -&gt; dict[str, Any]</code>","text":"<p>Export standardized statistical summary for external modules.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing corpus statistical fingerprint for external module consumption</p> Note <p>This provides the standardized API for external modules to consume corpus statistics.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef export_statistical_fingerprint(self) -&gt; dict[str, Any]:\n    \"\"\"Export standardized statistical summary for external modules.\n\n    Returns:\n        Dictionary containing corpus statistical fingerprint for external module consumption\n\n    Note:\n        This provides the standardized API for external modules to consume corpus statistics.\n    \"\"\"\n    # TODO: Expand fingerprint based on external module requirements\n    # TODO: Add feature extraction optimized for different analysis types\n\n    try:\n        stats = self.get_stats(active_only=True)\n\n        # Core statistical fingerprint\n        fingerprint = {\n            \"corpus_metadata\": {\n                \"name\": self.name,\n                \"num_docs\": self.num_docs,\n                \"num_active_docs\": self.num_active_docs,\n                \"num_tokens\": self.num_tokens,\n                \"num_terms\": self.num_terms,\n                \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n            },\n            \"distribution_stats\": stats.distribution_stats,\n            \"percentiles\": stats.percentiles,\n            \"text_diversity\": stats.text_diversity_stats,\n            \"basic_stats\": {\n                \"mean\": stats.mean,\n                \"std\": stats.standard_deviation,\n                \"iqr_values\": stats.iqr_values,\n                \"iqr_bounds\": stats.iqr_bounds,\n            },\n            \"document_features\": stats.doc_stats_df.to_dict(\"records\"),\n            \"term_frequencies\": self.term_counts(\n                n=100, most_common=True\n            ),  # Top 100 terms\n        }\n\n        return fingerprint\n\n    except Exception as e:\n        # Fallback fingerprint if CorpusStats fails\n        return {\n            \"corpus_metadata\": {\n                \"name\": self.name,\n                \"num_docs\": self.num_docs,\n                \"num_active_docs\": self.num_active_docs,\n                \"num_tokens\": self.num_tokens,\n                \"num_terms\": self.num_terms,\n                \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n            },\n            \"error\": f\"Statistical analysis failed: {str(e)}\",\n            \"basic_features\": {\n                \"document_ids\": list(self.records.keys()),\n                \"document_names\": list(self.names.keys()),\n            },\n        }\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.filter_records","title":"<code>filter_records(is_active: Optional[bool] = None, is_parsed: Optional[bool] = None, model: Optional[str] = None, min_tokens: Optional[int] = None, max_tokens: Optional[int] = None, limit: Optional[int] = None, use_database: bool = True) -&gt; list[Record]</code>","text":"<p>Filter records by various criteria.</p> <p>Parameters:</p> Name Type Description Default <code>is_active</code> <code>Optional[bool]</code> <p>Filter by active status</p> <code>None</code> <code>is_parsed</code> <code>Optional[bool]</code> <p>Filter by parsed status</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by spaCy model name</p> <code>None</code> <code>min_tokens</code> <code>Optional[int]</code> <p>Minimum number of tokens</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Maximum number of results</p> <code>None</code> <code>use_database</code> <code>bool</code> <p>Whether to use database filtering (vs in-memory)</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Record]</code> <p>List of matching Record objects</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef filter_records(\n    self,\n    is_active: Optional[bool] = None,\n    is_parsed: Optional[bool] = None,\n    model: Optional[str] = None,\n    min_tokens: Optional[int] = None,\n    max_tokens: Optional[int] = None,\n    limit: Optional[int] = None,\n    use_database: bool = True,\n) -&gt; list[Record]:\n    \"\"\"Filter records by various criteria.\n\n    Args:\n        is_active: Filter by active status\n        is_parsed: Filter by parsed status\n        model: Filter by spaCy model name\n        min_tokens: Minimum number of tokens\n        max_tokens: Maximum number of tokens\n        limit: Maximum number of results\n        use_database: Whether to use database filtering (vs in-memory)\n\n    Returns:\n        List of matching Record objects\n    \"\"\"\n    if use_database and self.db:\n        return self.db.filter_records(\n            is_active=is_active,\n            is_parsed=is_parsed,\n            model=model,\n            min_tokens=min_tokens,\n            max_tokens=max_tokens,\n            limit=limit,\n        )\n    else:\n        # Fallback to in-memory filtering\n        filtered_records = []\n        for record in self.records.values():\n            if is_active is not None and record.is_active != is_active:\n                continue\n            if is_parsed is not None and record.is_parsed != is_parsed:\n                continue\n            if model is not None and record.model != model:\n                continue\n            if min_tokens is not None:\n                try:\n                    if record.num_tokens() &lt; min_tokens:\n                        continue\n                except:\n                    continue\n            if max_tokens is not None:\n                try:\n                    if record.num_tokens() &gt; max_tokens:\n                        continue\n                except:\n                    continue\n\n            filtered_records.append(record)\n\n            if limit and len(filtered_records) &gt;= limit:\n                break\n\n        return filtered_records\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.get","title":"<code>get(id: Optional[str | list[str]] = None, name: Optional[str | list[str]] = None) -&gt; Record | list[Record]</code>","text":"<p>Get a record from the Corpus by ID.</p> <p>Tries to get the record from memory; otherwise loads it from file.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | list[str]</code> <p>A record id or list of ids from the Corpus records.</p> <code>None</code> <code>name</code> <code>str | list[str]</code> <p>A record name or list of names from the Corpus records.</p> <code>None</code> <p>Returns:</p> Type Description <code>Record | list[Record]</code> <p>Record | list[Record]: The record(s) with the given ID(s) or name(s).</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get(\n    self,\n    id: Optional[str | list[str]] = None,\n    name: Optional[str | list[str]] = None,\n) -&gt; Record | list[Record]:\n    \"\"\"Get a record from the Corpus by ID.\n\n    Tries to get the record from memory; otherwise loads it from file.\n\n    Args:\n        id (str | list[str]): A record id or list of ids from the Corpus records.\n        name (str | list[str]): A record name or list of names from the Corpus records.\n\n    Returns:\n        Record | list[Record]: The record(s) with the given ID(s) or name(s).\n    \"\"\"\n    # Ensure either id or name is provided\n    if not id and not name:\n        raise LexosException(\n            \"Must provide either an ID or a name to remove a record.\"\n        )\n\n    # Ensure id is a list\n    if isinstance(id, str):\n        ids = [id]\n    elif isinstance(id, list):\n        ids = id\n    else:\n        ids = []\n\n    # If name is provided, get the IDs from the name(s)\n    if name and not id:\n        if isinstance(name, str):\n            name = [name]\n        ids = []\n        for n in name:\n            ids.extend(self._get_by_name(n))\n\n    result = []\n    for id in ids:\n        # If the id is in the Corpus cache, return the record\n        if id in self.records.keys():\n            result.append(self.records[id])\n\n        # Otherwise, load the record from file\n        else:\n            record = self.records[id]\n            result.append(\n                record._from_disk(\n                    record.meta[\"filepath\"], record.model, self.model_cache\n                )\n            )\n    if len(result) == 1:\n        return result[0]\n    return result\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.get_analysis_results","title":"<code>get_analysis_results(module_name: str = None) -&gt; dict[str, Any]</code>","text":"<p>Retrieve analysis results from external modules.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Specific module name to retrieve, or None for all results</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing analysis results</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef get_analysis_results(self, module_name: str = None) -&gt; dict[str, Any]:\n    \"\"\"Retrieve analysis results from external modules.\n\n    Args:\n        module_name: Specific module name to retrieve, or None for all results\n\n    Returns:\n        Dictionary containing analysis results\n    \"\"\"\n    if module_name:\n        if module_name not in self.analysis_results:\n            raise ValueError(f\"No results found for module '{module_name}'\")\n        return self.analysis_results[module_name]\n\n    return self.analysis_results\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.get_stats","title":"<code>get_stats() -&gt; dict[str, Any]</code>","text":"<p>Get corpus statistics from the database.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing database-derived statistics</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get corpus statistics from the database.\n\n    Returns:\n        Dictionary containing database-derived statistics\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n        )\n\n    return self.db.get_stats()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.import_analysis_results","title":"<code>import_analysis_results(module_name: str, results_data: dict[str, Any], version: str = '1.0.0', overwrite: bool = False) -&gt; None</code>","text":"<p>Import analysis results from external modules into corpus metadata.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the external module (e.g., 'kmeans', 'topwords', 'kwic', 'text_classification')</p> required <code>results_data</code> <code>dict[str, Any]</code> <p>Dictionary containing the analysis results</p> required <code>version</code> <code>str</code> <p>Version string for result versioning and compatibility</p> <code>'1.0.0'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing results for this module</p> <code>False</code> Note <p>This is a framework implementation. Full functionality requires peer modules to be implemented and their result schemas defined.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef import_analysis_results(\n    self,\n    module_name: str,\n    results_data: dict[str, Any],\n    version: str = \"1.0.0\",\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Import analysis results from external modules into corpus metadata.\n\n    Args:\n        module_name: Name of the external module (e.g., 'kmeans', 'topwords', 'kwic', 'text_classification')\n        results_data: Dictionary containing the analysis results\n        version: Version string for result versioning and compatibility\n        overwrite: Whether to overwrite existing results for this module\n\n    Note:\n        This is a framework implementation. Full functionality requires\n        peer modules to be implemented and their result schemas defined.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO: Add result schema validation once peer modules are available\n    # TODO: Add proper versioning system for backward compatibility\n    # TODO: Implement result correlation capabilities across modules\n\n    if module_name in self.analysis_results and not overwrite:\n        raise ValueError(\n            f\"Results for module '{module_name}' already exist. \"\n            f\"Use overwrite=True to replace them.\"\n        )\n\n    # Basic result structure with metadata\n    self.analysis_results[module_name] = {\n        \"version\": version,\n        \"timestamp\": pd.Timestamp.now().isoformat(),\n        \"corpus_state\": {\n            \"num_docs\": self.num_docs,\n            \"num_active_docs\": self.num_active_docs,\n            \"corpus_fingerprint\": self._generate_corpus_fingerprint(),\n        },\n        \"results\": results_data,\n    }\n\n    msg.good(f\"Imported {module_name} analysis results (version {version})\")\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.load","title":"<code>load(include_docs: bool = False, active_only: bool = True) -&gt; int</code>","text":"<p>Load records from database into memory.</p> <p>Parameters:</p> Name Type Description Default <code>include_docs</code> <code>bool</code> <p>Whether to deserialize spaCy Doc content</p> <code>False</code> <code>active_only</code> <code>bool</code> <p>Whether to load only active records</p> <code>True</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of records loaded</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef load(self, include_docs: bool = False, active_only: bool = True) -&gt; int:\n    \"\"\"Load records from database into memory.\n\n    Args:\n        include_docs: Whether to deserialize spaCy Doc content\n        active_only: Whether to load only active records\n\n    Returns:\n        Number of records loaded\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n        )\n\n    # Clear existing records if loading from database\n    self.records.clear()\n    self.names.clear()\n\n    # Load records from database\n    filters = {\"is_active\": True} if active_only else {}\n    db_records = self.db.filter_records(**filters)\n\n    loaded_count = 0\n    for record in db_records:\n        # Add to in-memory structures\n        record_id_str = str(record.id)\n        self.records[record_id_str] = record\n        if record.name not in self.names:\n            self.names[record.name] = []\n        self.names[record.name].append(record_id_str)\n        # Populate meta for loaded record so Corpus metadata is consistent\n        try:\n            meta_entry = record.model_dump(\n                exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n            )\n            if \"id\" in meta_entry:\n                meta_entry[\"id\"] = str(meta_entry[\"id\"])\n            meta_entry[\"num_tokens\"] = (\n                record.num_tokens() if record.is_parsed else 0\n            )\n            meta_entry[\"num_terms\"] = record.num_terms() if record.is_parsed else 0\n            self.meta[record_id_str] = meta_entry\n        except Exception:\n            self.meta[record_id_str] = {\n                \"id\": record_id_str,\n                \"name\": record.name,\n                \"is_active\": record.is_active,\n                \"num_tokens\": record.num_tokens() if record.is_parsed else 0,\n                \"num_terms\": record.num_terms() if record.is_parsed else 0,\n            }\n        loaded_count += 1\n\n    # Update corpus state\n    self._update_corpus_state()\n\n    return loaded_count\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.remove","title":"<code>remove(id: Optional[str | list[str]] = None, name: Optional[str | list[str]] = None) -&gt; None</code>","text":"<p>Remove a record from the corpus by ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str | list[str]</code> <p>The ID of the record to remove.</p> <code>None</code> <code>name</code> <code>str | list[str]</code> <p>The name of the record to remove.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef remove(\n    self,\n    id: Optional[str | list[str]] = None,\n    name: Optional[str | list[str]] = None,\n) -&gt; None:\n    \"\"\"Remove a record from the corpus by ID.\n\n    Args:\n        id (str | list[str]): The ID of the record to remove.\n        name (str | list[str]): The name of the record to remove.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure either id or name is provided\n    if not id and not name:\n        raise LexosException(\n            \"Must provide either an ID or a name to remove a record.\"\n        )\n\n    # Ensure id is a list\n    if isinstance(id, str):\n        ids = [id]\n    elif isinstance(id, list):\n        ids = id\n    else:\n        ids = []\n\n    # If name is provided, get the IDs from the name(s)\n    if name and not id:\n        if isinstance(name, str):\n            name = [name]\n        ids = []\n        for n in name:\n            ids.extend(self._get_by_name(n))\n\n    for id in ids:\n        # Remove the entry from the records dictionary and names list\n        try:\n            entry = self.records.pop(id)\n        except KeyError:\n            raise LexosException(\n                f\"Record with ID {id} does not exist in the Corpus.\"\n            )\n        try:\n            if entry.name in self.names:\n                self.names[entry.name].remove(str(entry.id))\n                if not self.names[entry.name]:  # Remove empty lists\n                    self.names.pop(entry.name)\n        except KeyError:\n            raise LexosException(\n                f\"Record with name {entry.name} does not exist in the Corpus.\"\n            )\n\n    # Update the Corpus state after removing the record\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.save","title":"<code>save(path: Path | str = None) -&gt; None</code>","text":"<p>Save the Corpus as a zip archive.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the Corpus to.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str = None) -&gt; None:\n    \"\"\"Save the Corpus as a zip archive.\n\n    Args:\n        path (Path | str): The path to save the Corpus to.\n\n    Returns:\n        None\n    \"\"\"\n    shutil.make_archive(path / f\"{self.name}\", \"zip\", self.corpus_dir)\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.search","title":"<code>search(query: str, limit: int = 100, include_inactive: bool = False, model_filter: Optional[str] = None, load_from_db: bool = True) -&gt; list[Record]</code>","text":"<p>Perform full-text search on corpus records.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>FTS5 search query string</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return</p> <code>100</code> <code>include_inactive</code> <code>bool</code> <p>Whether to include inactive records</p> <code>False</code> <code>model_filter</code> <code>Optional[str]</code> <p>Optional filter by spaCy model name</p> <code>None</code> <code>load_from_db</code> <code>bool</code> <p>Whether to load results from database (vs memory)</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Record]</code> <p>List of matching Record objects</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef search(\n    self,\n    query: str,\n    limit: int = 100,\n    include_inactive: bool = False,\n    model_filter: Optional[str] = None,\n    load_from_db: bool = True,\n) -&gt; list[Record]:\n    \"\"\"Perform full-text search on corpus records.\n\n    Args:\n        query: FTS5 search query string\n        limit: Maximum number of results to return\n        include_inactive: Whether to include inactive records\n        model_filter: Optional filter by spaCy model name\n        load_from_db: Whether to load results from database (vs memory)\n\n    Returns:\n        List of matching Record objects\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True to use search.\"\n        )\n\n    return self.db.search_records(\n        query=query,\n        limit=limit,\n        include_inactive=include_inactive,\n        model_filter=model_filter,\n    )\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.set","title":"<code>set(id: str, **props) -&gt; None</code>","text":"<p>Set a property or properties of a record in the Corpus.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>A record id.</p> required <code>**props</code> <code>dict</code> <p>The dict containing any other properties to set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef set(self, id: str, **props) -&gt; None:\n    \"\"\"Set a property or properties of a record in the Corpus.\n\n    Args:\n        id (str): A record id.\n        **props (dict): The dict containing any other properties to set.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the record by ID\n    record = self.records[id]\n\n    # Save the record's filepath, thenupdate the specified properties\n    old_filepath = record.meta.get(\"filepath\", None)\n    record.set(**props)\n\n    # If the filepath has changed, delete the old file\n    if record.meta.get(\"filepath\", None) != old_filepath:\n        Path(old_filepath).unlink(missing_ok=True)\n\n    # If the record has a filepath, ensure the file is in the data directory\n    filepath = record.meta.get(\"filepath\")\n    if filepath and filepath not in str(Path(self.corpus_dir) / \"data\"):\n        record.to_disk(filepath, extensions=record.extensions)\n\n    # Update the record in the Corpus and update the corpus state\n    self.records[id] = record\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.sync","title":"<code>sync(overwrite: bool = False) -&gt; int</code>","text":"<p>Synchronize existing file-based records to the database.</p> <p>This method loads records from the corpus directory on disk and adds them to the database. If records are already in memory, they will be used instead.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing database records</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of records synchronized</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef sync(self, overwrite: bool = False) -&gt; int:\n    \"\"\"Synchronize existing file-based records to the database.\n\n    This method loads records from the corpus directory on disk and adds them\n    to the database. If records are already in memory, they will be used instead.\n\n    Args:\n        overwrite: Whether to overwrite existing database records\n\n    Returns:\n        Number of records synchronized\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n        )\n\n    # Load records from disk if not already in memory\n    if not self.records:\n        self._load_records_from_disk()\n\n    synced_count = 0\n\n    for record in self.records.values():\n        try:\n            if overwrite:\n                # Check if exists and update\n                existing = self.db.get_record(str(record.id), include_doc=False)\n                if existing:\n                    self.db.update_record(record)\n                else:\n                    self.db.add_record(record)\n            else:\n                # Only add if doesn't exist\n                existing = self.db.get_record(str(record.id), include_doc=False)\n                if not existing:\n                    self.db.add_record(record)\n\n            synced_count += 1\n\n        except Exception as e:\n            # Log error but continue with other records\n            print(f\"Warning: Failed to sync record {record.id}: {str(e)}\")\n\n    return synced_count\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.term_counts","title":"<code>term_counts(n: Optional[int] = 10, most_common: Optional[bool] = True) -&gt; Counter</code>","text":"<p>Get a Counter with the most common Corpus term counts.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>The number of most common terms to return. Defaults to 10.</p> <code>10</code> <code>most_common</code> <code>Optional[bool]</code> <p>If True, return the n most common terms; otherwise, return the n least common terms.</p> <code>True</code> <p>Returns:</p> Type Description <code>Counter</code> <p>A collections.Counter object containing the n most common term counts for all records in the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef term_counts(\n    self, n: Optional[int] = 10, most_common: Optional[bool] = True\n) -&gt; Counter:\n    \"\"\"Get a Counter with the most common Corpus term counts.\n\n    Args:\n        n (Optional[int]): The number of most common terms to return. Defaults to 10.\n        most_common (Optional[bool]): If True, return the n most common terms; otherwise, return the n least common terms.\n\n    Returns:\n        A collections.Counter object containing the n most common term counts for all records in the Corpus.\n    \"\"\"\n    # Count the terms in all records\n    counter = Counter()\n    for record in self.records.values():\n        if record.is_parsed:\n            counter.update(record.terms)\n\n    # Optionally filter the results\n    if most_common and n:\n        return counter.most_common(n)\n    elif not most_common and n:\n        return counter.most_common()[: -n - 1 : -1]\n    elif most_common is False and n is None:\n        return counter.most_common()[::]\n    else:\n        return counter\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.to_df","title":"<code>to_df(exclude: list[str] = ['content', 'terms', 'tokens']) -&gt; pd.DataFrame</code>","text":"<p>Return a table of the Corpus records.</p> <p>Parameters:</p> Name Type Description Default <code>exclude</code> <code>list[str]</code> <p>A list of fields to exclude from the dataframe. If you wish to exclude metadata fields with the same name as model fields, you can use the prefix \"metadata_\" to avoid conflicts.</p> <code>['content', 'terms', 'tokens']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe representing the records in the Corpus.</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef to_df(\n    self, exclude: list[str] = [\"content\", \"terms\", \"tokens\"]\n) -&gt; pd.DataFrame:\n    \"\"\"Return a table of the Corpus records.\n\n    Args:\n        exclude (list[str]): A list of fields to exclude from the dataframe. If you wish to exclude metadata fields with the same name as model fields, you can use the prefix \"metadata_\" to avoid conflicts.\n\n    Returns:\n        pd.DataFrame: A dataframe representing the records in the Corpus.\n    \"\"\"\n    rows = []\n    for record in self.records.values():  # &lt;- Fix the duplicate\n        if record is None:  # Skip None records\n            continue\n\n        # Get model categories.\n        # NOTE: We avoid calling `model_dump()` on `Record` objects that are\n        # unparsed because Pydantic may attempt to evaluate computed fields\n        # while creating the serialized dict. Several computed properties on\n        # `Record` (e.g., `terms`, `tokens`, `num_terms`, and\n        # `num_tokens`) raise `LexosException(\"Record is not parsed.\")`\n        # when the record is not parsed. If `model_dump()` evaluates those\n        # properties for an unparsed record, it will raise and cause\n        # `to_df()` to fail. Therefore:\n        #  - For parsed records, we call `record.model_dump()` and use the\n        #    model-dump output (it includes computed fields safely).\n        #  - For unparsed records, we *do not* call `model_dump()`; we\n        #    instead build a minimal, safe `row` from stored fields and\n        #    set any computed-like values to safe defaults (empty list,\n        #    0, or empty string). This produces robust DataFrame output\n        #    for corpora that contain a mix of parsed and unparsed\n        #    records without triggering computed-field side-effects.\n        fields_that_may_raise = {\n            \"terms\",\n            \"tokens\",\n            \"num_terms\",\n            \"num_tokens\",\n            \"text\",\n        }\n        # Build a dump_exclude set to prevent model_dump from computing\n        # sensitive fields on unparsed records\n        dump_exclude = set(exclude)\n        if hasattr(record, \"is_parsed\") and record.is_parsed:\n            # Parsed records: safely model_dump, excluding any user-requested fields\n            row = record.model_dump(exclude=list(dump_exclude))\n        else:\n            # Unparsed records: avoid model_dump to prevent computed property evaluation\n            base_fields = [\n                \"id\",\n                \"name\",\n                \"is_active\",\n                \"content\",\n                \"model\",\n                \"extensions\",\n                \"data_source\",\n                \"meta\",\n            ]\n            row = {}\n            for f in base_fields:\n                if f in exclude:\n                    continue\n                try:\n                    value = getattr(record, f, None)\n                except Exception:\n                    # Defensive: if getattr triggers an error, skip and set None\n                    value = None\n                # Serialize Doc-like content into text rather than bytes to keep DataFrame friendly\n                if f == \"content\" and value is not None:\n                    try:\n                        from spacy.tokens import Doc\n\n                        if isinstance(value, Doc):\n                            value = value.text\n                    except Exception:\n                        pass\n                # Ensure id is serialized to string to match model_dump output for parsed records\n                if f == \"id\" and value is not None:\n                    try:\n                        value = str(value)\n                    except Exception:\n                        pass\n                # Sanitize meta similar to model_dump\n                if f == \"meta\" and value is not None:\n                    try:\n                        value = record._sanitize_metadata(value)\n                    except Exception:\n                        pass\n                row[f] = value\n\n        # Patch for unparsed records: fill terms/tokens/num_terms/num_tokens/text\n        # Only if those fields are not excluded\n        if \"terms\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"terms\"] = list(record.terms)\n            else:\n                row[\"terms\"] = []\n        if \"tokens\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"tokens\"] = record.tokens\n            else:\n                row[\"tokens\"] = []\n        if \"num_terms\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"num_terms\"] = record.num_terms()\n            else:\n                row[\"num_terms\"] = 0\n        if \"num_tokens\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"num_tokens\"] = record.num_tokens()\n            else:\n                row[\"num_tokens\"] = 0\n        if \"text\" not in exclude:\n            if hasattr(record, \"is_parsed\") and record.is_parsed:\n                row[\"text\"] = record.text\n            else:\n                row[\"text\"] = \"\"\n\n        # Add metadata categories, respecting exclude list\n        metadata = row.pop(\"meta\", {})\n        for key, value in metadata.items():\n            # Exclude metadata fields if requested\n            if key in exclude or f\"metadata_{key}\" in exclude:\n                continue\n            if key in row:\n                key = f\"metadata_{key}\"\n            row[key] = value\n\n        # Append the row to the rows list\n        rows.append(row)\n\n    # Create a DataFrame from the rows\n    if rows:  # Only create DataFrame if we have data\n        df = pd.DataFrame(rows)\n        # Fill NaN with appropriate values based on column dtype\n        fill_values = {}\n        for col in df.columns:\n            if pd.api.types.is_numeric_dtype(df[col]):\n                fill_values[col] = 0\n            elif pd.api.types.is_bool_dtype(df[col]):\n                fill_values[col] = False\n            else:\n                fill_values[col] = \"\"\n\n        df = df.fillna(fill_values)  # Use assignment instead of inplace\n        return df\n    else:\n        # Return empty DataFrame with basic columns if no records\n        return pd.DataFrame(columns=[\"id\", \"name\", \"is_active\"])\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.validate_analysis_compatibility","title":"<code>validate_analysis_compatibility(module_name: str) -&gt; dict[str, Any]</code>","text":"<p>Validate if stored analysis results are compatible with current corpus state.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module to validate</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing validation results and recommendations</p> Source code in <code>lexos/corpus/corpus.py</code> <pre><code>@validate_call(config=model_config)\ndef validate_analysis_compatibility(self, module_name: str) -&gt; dict[str, Any]:\n    \"\"\"Validate if stored analysis results are compatible with current corpus state.\n\n    Args:\n        module_name: Name of the module to validate\n\n    Returns:\n        Dictionary containing validation results and recommendations\n    \"\"\"\n    if module_name not in self.analysis_results:\n        return {\n            \"compatible\": False,\n            \"reason\": f\"No analysis results found for module '{module_name}'\",\n        }\n\n    stored_results = self.analysis_results[module_name]\n    stored_state = stored_results.get(\"corpus_state\", {})\n    current_fingerprint = self._generate_corpus_fingerprint()\n    stored_fingerprint = stored_state.get(\"corpus_fingerprint\", \"\")\n\n    compatibility = {\n        \"compatible\": stored_fingerprint == current_fingerprint,\n        \"current_fingerprint\": current_fingerprint,\n        \"stored_fingerprint\": stored_fingerprint,\n        \"stored_timestamp\": stored_results.get(\"timestamp\", \"unknown\"),\n        \"stored_version\": stored_results.get(\"version\", \"unknown\"),\n    }\n\n    if not compatibility[\"compatible\"]:\n        compatibility[\"reason\"] = (\n            \"Corpus state has changed since analysis was performed\"\n        )\n        compatibility[\"recommendation\"] = (\n            f\"Re-run {module_name} analysis with current corpus state\"\n        )\n\n        # Detailed state comparison\n        compatibility[\"state_changes\"] = {\n            \"num_docs\": {\n                \"stored\": stored_state.get(\"num_docs\", 0),\n                \"current\": self.num_docs,\n                \"changed\": stored_state.get(\"num_docs\", 0) != self.num_docs,\n            },\n            \"num_active_docs\": {\n                \"stored\": stored_state.get(\"num_active_docs\", 0),\n                \"current\": self.num_active_docs,\n                \"changed\": stored_state.get(\"num_active_docs\", 0)\n                != self.num_active_docs,\n            },\n        }\n\n    return compatibility\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.__init__","title":"<code>__init__(**data: Any)</code>","text":"<p>Initialize corpus with optional database integration.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Standard Corpus initialization parameters</p> <code>{}</code> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialize corpus with optional database integration.\n\n    Args:\n        **data (Any): Standard Corpus initialization parameters\n    \"\"\"\n    # Extract database-specific parameters\n    sqlite_path = data.pop(\"sqlite_path\", None)\n    use_sqlite = data.pop(\"use_sqlite\", False)\n    sqlite_only = data.pop(\"sqlite_only\", False)\n\n    # Set the database fields\n    data[\"use_sqlite\"] = use_sqlite\n    data[\"sqlite_only\"] = sqlite_only\n    data[\"sqlite_path\"] = sqlite_path\n\n    # Initialize parent class\n    super().__init__(**data)\n\n    # Initialize database if enabled\n    if self.use_sqlite or self.sqlite_only:\n        db_path = sqlite_path or f\"{self.corpus_dir}/corpus.db\"\n        self.db = SQLiteBackend(database_path=db_path)\n        self._initialize_metadata()\n    else:\n        self.db = None\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus._add_to_backend","title":"<code>_add_to_backend(content, name: Optional[str] = None, is_active: Optional[bool] = True, model: Optional[str] = None, extensions: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, id_type: Optional[str] = 'uuid4')</code>","text":"<p>Add records in database-only mode without file storage.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def _add_to_backend(\n    self,\n    content,\n    name: Optional[str] = None,\n    is_active: Optional[bool] = True,\n    model: Optional[str] = None,\n    extensions: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    id_type: Optional[str] = \"uuid4\",\n):\n    \"\"\"Add records in database-only mode without file storage.\"\"\"\n    from spacy.tokens import Doc\n\n    # Sanitize metadata to ensure JSON-serializable types (defensive)\n    if metadata is not None:\n        metadata = self._sanitize_metadata(metadata)\n\n    # Handle single or multiple content items\n    if isinstance(content, (Doc, Record, str)):\n        items = [content]\n    else:\n        items = list(content)\n\n    for item in items:\n        # Generate unique ID\n        new_id = self._generate_unique_id(type=id_type)\n\n        if isinstance(item, Record):\n            record = item\n        else:\n            record_kwargs = dict(\n                id=new_id,\n                name=self._ensure_unique_name(name),\n                is_active=is_active,\n                content=item,\n                model=model,\n                data_source=None,\n            )\n            if extensions is not None:\n                record_kwargs[\"extensions\"] = extensions\n            if metadata is not None:\n                record_kwargs[\"meta\"] = metadata\n            record = Record(**record_kwargs)\n\n            # Note: Records are created with string content and can be parsed later if needed\n            # The database stores both parsed and unparsed content efficiently\n\n        # Add to in-memory records\n        record_id_str = str(record.id)\n        self.records[record_id_str] = record\n        if record.name not in self.names:\n            self.names[record.name] = []\n        self.names[record.name].append(record_id_str)\n        # Add a meta entry similar to file-based add to keep Corpus metadata consistent\n        try:\n            meta_entry = record.model_dump(\n                exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n            )\n            # Ensure id is a string and annotate token/term counts\n            meta_entry[\"id\"] = str(meta_entry.get(\"id\", record_id_str))\n            meta_entry[\"num_tokens\"] = (\n                record.num_tokens() if record.is_parsed else 0\n            )\n            meta_entry[\"num_terms\"] = record.num_terms() if record.is_parsed else 0\n            self.meta[record_id_str] = meta_entry\n        except Exception:\n            # Fallback minimal meta if model_dump fails\n            self.meta[record_id_str] = {\n                \"id\": record_id_str,\n                \"name\": record.name,\n                \"is_active\": record.is_active,\n                \"num_tokens\": record.num_tokens() if record.is_parsed else 0,\n                \"num_terms\": record.num_terms() if record.is_parsed else 0,\n            }\n\n        # Store in database\n        if self.db:\n            self.db.add_record(record)\n\n    # Update corpus state\n    self._update_corpus_state()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.__del__","title":"<code>__del__()</code>","text":"<p>Destructor to ensure database connections are closed.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor to ensure database connections are closed.\"\"\"\n    try:\n        self.close()\n    except:\n        pass  # Ignore errors during cleanup\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus._get_timestamp","title":"<code>_get_timestamp() -&gt; str</code>","text":"<p>Get current timestamp as ISO string.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def _get_timestamp(self) -&gt; str:\n    \"\"\"Get current timestamp as ISO string.\"\"\"\n    from datetime import datetime\n\n    return datetime.now().isoformat()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus._load_records_from_disk","title":"<code>_load_records_from_disk()</code>","text":"<p>Load records from the corpus directory into memory.</p> <p>This is a helper method for sync() to load file-based records from disk before syncing them to the database.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def _load_records_from_disk(self):\n    \"\"\"Load records from the corpus directory into memory.\n\n    This is a helper method for sync() to load file-based records\n    from disk before syncing them to the database.\n    \"\"\"\n    corpus_dir = Path(self.corpus_dir)\n    metadata_path = corpus_dir / self.corpus_metadata_file\n\n    # Check if corpus directory and metadata exist\n    if not corpus_dir.exists():\n        return\n\n    if not metadata_path.exists():\n        return\n\n    # Load metadata\n    try:\n        import srsly\n\n        metadata = srsly.read_json(metadata_path)\n\n        # Load record metadata\n        if \"meta\" in metadata and metadata[\"meta\"]:\n            for record_id, record_meta in metadata[\"meta\"].items():\n                # Load the record from disk\n                data_dir = corpus_dir / \"data\"\n                record_file = data_dir / f\"{record_id}.bin\"\n\n                if record_file.exists():\n                    # Create a Record object and load from disk\n                    record = Record(id=record_id, name=record_meta.get(\"name\", \"\"))\n                    record.from_disk(\n                        str(record_file),\n                        model=record_meta.get(\"model\"),\n                        model_cache=self.model_cache,\n                    )\n\n                    # Add to in-memory structures\n                    self.records[record_id] = record\n                    if record.name not in self.names:\n                        self.names[record.name] = []\n                    self.names[record.name].append(record_id)\n\n    except Exception as e:\n        # If loading fails, just continue with empty records\n        print(f\"Warning: Failed to load records from disk: {str(e)}\")\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus._initialize_metadata","title":"<code>_initialize_metadata()</code>","text":"<p>Initialize corpus metadata in the database.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def _initialize_metadata(self):\n    \"\"\"Initialize corpus metadata in the database.\"\"\"\n    if not self.db:\n        return\n\n    with self.db.SessionLocal() as session:\n        # Check if corpus metadata exists\n        corpus_id = self.name or \"default\"\n        existing = (\n            session.query(SQLiteMetadata)\n            .filter(SQLiteMetadata.corpus_id == corpus_id)\n            .first()\n        )\n\n        if not existing:\n            # Create new corpus metadata\n            corpus_metadata = SQLiteMetadata()\n            corpus_metadata.corpus_id = corpus_id\n            corpus_metadata.name = self.name\n            corpus_metadata.num_docs = self.num_docs\n            corpus_metadata.num_active_docs = self.num_active_docs\n            corpus_metadata.num_tokens = self.num_tokens\n            corpus_metadata.num_terms = self.num_terms\n            corpus_metadata.corpus_dir = self.corpus_dir\n            corpus_metadata.metadata_json = json.dumps(self.meta, default=str)\n            corpus_metadata.analysis_results_json = json.dumps(\n                self.analysis_results, default=str\n            )\n            corpus_metadata.corpus_fingerprint = self._generate_corpus_fingerprint()\n            corpus_metadata.created_at = self._get_timestamp()\n            corpus_metadata.updated_at = self._get_timestamp()\n            session.add(corpus_metadata)\n            session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus._sanitize_metadata","title":"<code>_sanitize_metadata(metadata: dict[str, Any]) -&gt; dict[str, Any]</code>","text":"<p>Convert non-JSON-serializable types to strings.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[str, Any]</code> <p>Original metadata dictionary</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Sanitized metadata dictionary with JSON-serializable values</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def _sanitize_metadata(self, metadata: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Convert non-JSON-serializable types to strings.\n\n    Args:\n        metadata: Original metadata dictionary\n\n    Returns:\n        Sanitized metadata dictionary with JSON-serializable values\n    \"\"\"\n    from datetime import date, datetime\n    from pathlib import Path\n    from uuid import UUID\n\n    sanitized = {}\n    for key, value in metadata.items():\n        if isinstance(value, UUID):\n            sanitized[key] = str(value)\n        elif isinstance(value, (datetime, date)):\n            sanitized[key] = value.isoformat()\n        elif isinstance(value, Path):\n            sanitized[key] = str(value)\n        elif isinstance(value, dict):\n            sanitized[key] = self._sanitize_metadata(value)  # Recursive\n        elif isinstance(value, list):\n            sanitized[key] = [\n                self._sanitize_metadata({\"item\": item})[\"item\"]\n                if isinstance(item, dict)\n                else str(item)\n                if isinstance(item, (UUID, datetime, date, Path))\n                else item\n                for item in value\n            ]\n        else:\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus._update_corpus_state","title":"<code>_update_corpus_state()</code>","text":"<p>Update corpus state in both memory and database.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def _update_corpus_state(self):\n    \"\"\"Update corpus state in both memory and database.\"\"\"\n    # Update in-memory state\n    super()._update_corpus_state()\n\n    # Update database metadata if enabled\n    if self.db:\n        with self.db.SessionLocal() as session:\n            corpus_id = self.name or \"default\"\n            corpus_metadata = (\n                session.query(SQLiteMetadata)\n                .filter(SQLiteMetadata.corpus_id == corpus_id)\n                .first()\n            )\n\n            if corpus_metadata:\n                corpus_metadata.num_docs = self.num_docs\n                corpus_metadata.num_active_docs = self.num_active_docs\n                corpus_metadata.num_tokens = self.num_tokens\n                corpus_metadata.num_terms = self.num_terms\n                corpus_metadata.metadata_json = json.dumps(self.meta, default=str)\n                corpus_metadata.analysis_results_json = json.dumps(\n                    self.analysis_results, default=str\n                )\n                corpus_metadata.corpus_fingerprint = (\n                    self._generate_corpus_fingerprint()\n                )\n                corpus_metadata.updated_at = self._get_timestamp()\n\n                session.commit()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.add","title":"<code>add(content, name: Optional[str] = None, is_active: Optional[bool] = True, model: Optional[str] = None, extensions: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, id_type: Optional[str] = 'uuid4', cache: Optional[bool] = False, store_in_db: Optional[bool] = None)</code>","text":"<p>Add a record to the corpus with optional database storage.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | Doc | Record</code> <p>The content of the record</p> required <code>name</code> <code>Optional[str]</code> <p>Optional name for the record</p> <code>None</code> <code>is_active</code> <code>Optional[bool]</code> <p>Whether the record is active</p> <code>True</code> <code>model</code> <code>Optional[str]</code> <p>spaCy model name for parsing</p> <code>None</code> <code>extensions</code> <code>Optional[list[str]]</code> <p>List of spaCy extensions to add</p> <code>None</code> <code>metadata</code> <code>Optional[dict[str, Any]]</code> <p>Optional metadata dictionary</p> <code>None</code> <code>id_type</code> <code>Optional[str]</code> <p>Type of ID to generate ('uuid4' or 'int')</p> <code>'uuid4'</code> <code>cache</code> <code>Optional[bool]</code> <p>Whether to cache the record in memory</p> <code>False</code> <code>store_in_db</code> <code>Optional[bool]</code> <p>Whether to store the record in the database</p> <code>None</code> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef add(\n    self,\n    content,\n    name: Optional[str] = None,\n    is_active: Optional[bool] = True,\n    model: Optional[str] = None,\n    extensions: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n    id_type: Optional[str] = \"uuid4\",\n    cache: Optional[bool] = False,\n    store_in_db: Optional[bool] = None,\n):\n    \"\"\"Add a record to the corpus with optional database storage.\n\n    Args:\n        content (str | Doc | Record): The content of the record\n        name (Optional[str]): Optional name for the record\n        is_active (Optional[bool]): Whether the record is active\n        model (Optional[str]): spaCy model name for parsing\n        extensions (Optional[list[str]]): List of spaCy extensions to add\n        metadata (Optional[dict[str, Any]]): Optional metadata dictionary\n        id_type (Optional[str]): Type of ID to generate ('uuid4' or 'int')\n        cache (Optional[bool]): Whether to cache the record in memory\n        store_in_db (Optional[bool]): Whether to store the record in the database\n    \"\"\"\n    # Sanitize metadata to ensure JSON-serializable types\n    if metadata is not None:\n        metadata = self._sanitize_metadata(metadata)\n\n    # Determine storage strategy\n    use_db = (\n        store_in_db\n        if store_in_db is not None\n        else self.use_sqlite or self.sqlite_only\n    )\n    use_files = not self.sqlite_only\n\n    # Get current record count to track new additions\n    initial_record_count = len(self.records)\n\n    # Add using parent implementation if using files\n    if use_files:\n        super().add(\n            content=content,\n            name=name,\n            is_active=is_active,\n            model=model,\n            extensions=extensions,\n            metadata=metadata,\n            id_type=id_type,\n            cache=cache,\n        )\n    else:\n        # Database-only mode - implement add logic without file storage\n        self._add_to_backend(\n            content=content,\n            name=name,\n            is_active=is_active,\n            model=model,\n            extensions=extensions,\n            metadata=metadata,\n            id_type=id_type,\n        )\n\n    # Also store in database if enabled and we're using file storage\n    if use_db and self.db and use_files:\n        # Get the newly added records\n        current_records = list(self.records.values())\n        new_records = current_records[initial_record_count:]\n\n        for record in new_records:\n            try:\n                # Note: Records can be parsed later if needed\n                # The database efficiently stores both parsed and unparsed content\n\n                self.db.add_record(record)\n            except Exception as e:\n                # Log error but don't fail the entire operation\n                print(f\"Warning: Failed to add record {record.id} to database: {e}\")\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.filter_records","title":"<code>filter_records(is_active: Optional[bool] = None, is_parsed: Optional[bool] = None, model: Optional[str] = None, min_tokens: Optional[int] = None, max_tokens: Optional[int] = None, limit: Optional[int] = None, use_database: bool = True) -&gt; list[Record]</code>","text":"<p>Filter records by various criteria.</p> <p>Parameters:</p> Name Type Description Default <code>is_active</code> <code>Optional[bool]</code> <p>Filter by active status</p> <code>None</code> <code>is_parsed</code> <code>Optional[bool]</code> <p>Filter by parsed status</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Filter by spaCy model name</p> <code>None</code> <code>min_tokens</code> <code>Optional[int]</code> <p>Minimum number of tokens</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Maximum number of results</p> <code>None</code> <code>use_database</code> <code>bool</code> <p>Whether to use database filtering (vs in-memory)</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Record]</code> <p>List of matching Record objects</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef filter_records(\n    self,\n    is_active: Optional[bool] = None,\n    is_parsed: Optional[bool] = None,\n    model: Optional[str] = None,\n    min_tokens: Optional[int] = None,\n    max_tokens: Optional[int] = None,\n    limit: Optional[int] = None,\n    use_database: bool = True,\n) -&gt; list[Record]:\n    \"\"\"Filter records by various criteria.\n\n    Args:\n        is_active: Filter by active status\n        is_parsed: Filter by parsed status\n        model: Filter by spaCy model name\n        min_tokens: Minimum number of tokens\n        max_tokens: Maximum number of tokens\n        limit: Maximum number of results\n        use_database: Whether to use database filtering (vs in-memory)\n\n    Returns:\n        List of matching Record objects\n    \"\"\"\n    if use_database and self.db:\n        return self.db.filter_records(\n            is_active=is_active,\n            is_parsed=is_parsed,\n            model=model,\n            min_tokens=min_tokens,\n            max_tokens=max_tokens,\n            limit=limit,\n        )\n    else:\n        # Fallback to in-memory filtering\n        filtered_records = []\n        for record in self.records.values():\n            if is_active is not None and record.is_active != is_active:\n                continue\n            if is_parsed is not None and record.is_parsed != is_parsed:\n                continue\n            if model is not None and record.model != model:\n                continue\n            if min_tokens is not None:\n                try:\n                    if record.num_tokens() &lt; min_tokens:\n                        continue\n                except:\n                    continue\n            if max_tokens is not None:\n                try:\n                    if record.num_tokens() &gt; max_tokens:\n                        continue\n                except:\n                    continue\n\n            filtered_records.append(record)\n\n            if limit and len(filtered_records) &gt;= limit:\n                break\n\n        return filtered_records\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.get_stats","title":"<code>get_stats() -&gt; dict[str, Any]</code>","text":"<p>Get corpus statistics from the database.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing database-derived statistics</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get corpus statistics from the database.\n\n    Returns:\n        Dictionary containing database-derived statistics\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n        )\n\n    return self.db.get_stats()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.search","title":"<code>search(query: str, limit: int = 100, include_inactive: bool = False, model_filter: Optional[str] = None, load_from_db: bool = True) -&gt; list[Record]</code>","text":"<p>Perform full-text search on corpus records.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>FTS5 search query string</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return</p> <code>100</code> <code>include_inactive</code> <code>bool</code> <p>Whether to include inactive records</p> <code>False</code> <code>model_filter</code> <code>Optional[str]</code> <p>Optional filter by spaCy model name</p> <code>None</code> <code>load_from_db</code> <code>bool</code> <p>Whether to load results from database (vs memory)</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Record]</code> <p>List of matching Record objects</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef search(\n    self,\n    query: str,\n    limit: int = 100,\n    include_inactive: bool = False,\n    model_filter: Optional[str] = None,\n    load_from_db: bool = True,\n) -&gt; list[Record]:\n    \"\"\"Perform full-text search on corpus records.\n\n    Args:\n        query: FTS5 search query string\n        limit: Maximum number of results to return\n        include_inactive: Whether to include inactive records\n        model_filter: Optional filter by spaCy model name\n        load_from_db: Whether to load results from database (vs memory)\n\n    Returns:\n        List of matching Record objects\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True to use search.\"\n        )\n\n    return self.db.search_records(\n        query=query,\n        limit=limit,\n        include_inactive=include_inactive,\n        model_filter=model_filter,\n    )\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.sync","title":"<code>sync(overwrite: bool = False) -&gt; int</code>","text":"<p>Synchronize existing file-based records to the database.</p> <p>This method loads records from the corpus directory on disk and adds them to the database. If records are already in memory, they will be used instead.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing database records</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of records synchronized</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef sync(self, overwrite: bool = False) -&gt; int:\n    \"\"\"Synchronize existing file-based records to the database.\n\n    This method loads records from the corpus directory on disk and adds them\n    to the database. If records are already in memory, they will be used instead.\n\n    Args:\n        overwrite: Whether to overwrite existing database records\n\n    Returns:\n        Number of records synchronized\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n        )\n\n    # Load records from disk if not already in memory\n    if not self.records:\n        self._load_records_from_disk()\n\n    synced_count = 0\n\n    for record in self.records.values():\n        try:\n            if overwrite:\n                # Check if exists and update\n                existing = self.db.get_record(str(record.id), include_doc=False)\n                if existing:\n                    self.db.update_record(record)\n                else:\n                    self.db.add_record(record)\n            else:\n                # Only add if doesn't exist\n                existing = self.db.get_record(str(record.id), include_doc=False)\n                if not existing:\n                    self.db.add_record(record)\n\n            synced_count += 1\n\n        except Exception as e:\n            # Log error but continue with other records\n            print(f\"Warning: Failed to sync record {record.id}: {str(e)}\")\n\n    return synced_count\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.load","title":"<code>load(include_docs: bool = False, active_only: bool = True) -&gt; int</code>","text":"<p>Load records from database into memory.</p> <p>Parameters:</p> Name Type Description Default <code>include_docs</code> <code>bool</code> <p>Whether to deserialize spaCy Doc content</p> <code>False</code> <code>active_only</code> <code>bool</code> <p>Whether to load only active records</p> <code>True</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of records loaded</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If database is not enabled</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>@validate_call\ndef load(self, include_docs: bool = False, active_only: bool = True) -&gt; int:\n    \"\"\"Load records from database into memory.\n\n    Args:\n        include_docs: Whether to deserialize spaCy Doc content\n        active_only: Whether to load only active records\n\n    Returns:\n        Number of records loaded\n\n    Raises:\n        LexosException: If database is not enabled\n    \"\"\"\n    if not self.db:\n        raise LexosException(\n            \"Database is not enabled. Initialize corpus with use_sqlite=True.\"\n        )\n\n    # Clear existing records if loading from database\n    self.records.clear()\n    self.names.clear()\n\n    # Load records from database\n    filters = {\"is_active\": True} if active_only else {}\n    db_records = self.db.filter_records(**filters)\n\n    loaded_count = 0\n    for record in db_records:\n        # Add to in-memory structures\n        record_id_str = str(record.id)\n        self.records[record_id_str] = record\n        if record.name not in self.names:\n            self.names[record.name] = []\n        self.names[record.name].append(record_id_str)\n        # Populate meta for loaded record so Corpus metadata is consistent\n        try:\n            meta_entry = record.model_dump(\n                exclude=[\"content\", \"terms\", \"text\", \"tokens\"], mode=\"json\"\n            )\n            if \"id\" in meta_entry:\n                meta_entry[\"id\"] = str(meta_entry[\"id\"])\n            meta_entry[\"num_tokens\"] = (\n                record.num_tokens() if record.is_parsed else 0\n            )\n            meta_entry[\"num_terms\"] = record.num_terms() if record.is_parsed else 0\n            self.meta[record_id_str] = meta_entry\n        except Exception:\n            self.meta[record_id_str] = {\n                \"id\": record_id_str,\n                \"name\": record.name,\n                \"is_active\": record.is_active,\n                \"num_tokens\": record.num_tokens() if record.is_parsed else 0,\n                \"num_terms\": record.num_terms() if record.is_parsed else 0,\n            }\n        loaded_count += 1\n\n    # Update corpus state\n    self._update_corpus_state()\n\n    return loaded_count\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.SQLiteCorpus.close","title":"<code>close()</code>","text":"<p>Close database connections and clean up resources.</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def close(self):\n    \"\"\"Close database connections and clean up resources.\"\"\"\n    if self.db:\n        self.db.close()\n</code></pre>"},{"location":"api/corpus/sqlite/integration/#lexos.corpus.sqlite.integration.create_corpus","title":"<code>create_corpus(corpus_dir: str = 'corpus', sqlite_path: Optional[Union[str, Path]] = None, name: Optional[str] = None, sqlite_only: bool = False, **kwargs: Any) -&gt; SQLiteCorpus</code>","text":"<p>Convenience function to create a SQLite-enabled corpus with sensible defaults.</p> <p>Parameters:</p> Name Type Description Default <code>corpus_dir</code> <code>str</code> <p>Directory for file-based storage</p> <code>'corpus'</code> <code>sqlite_path</code> <code>Optional[Union[str, Path]]</code> <p>Path to SQLite database (None for auto-generated)</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Corpus name</p> <code>None</code> <code>sqlite_only</code> <code>bool</code> <p>Whether to use database-only mode</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional Corpus initialization parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>SQLiteCorpus</code> <p>SQLiteCorpus instance</p> Source code in <code>lexos/corpus/sqlite/integration.py</code> <pre><code>def create_corpus(\n    corpus_dir: str = \"corpus\",\n    sqlite_path: Optional[Union[str, Path]] = None,\n    name: Optional[str] = None,\n    sqlite_only: bool = False,\n    **kwargs: Any,\n) -&gt; SQLiteCorpus:\n    \"\"\"Convenience function to create a SQLite-enabled corpus with sensible defaults.\n\n    Args:\n        corpus_dir (str): Directory for file-based storage\n        sqlite_path (Optional[Union[str, Path]]): Path to SQLite database (None for auto-generated)\n        name (Optional[str]): Corpus name\n        sqlite_only (bool): Whether to use database-only mode\n        **kwargs (Any): Additional Corpus initialization parameters\n\n    Returns:\n        SQLiteCorpus instance\n    \"\"\"\n    if sqlite_path is None:\n        sqlite_path = f\"{corpus_dir}/corpus.db\"\n\n    return SQLiteCorpus(\n        corpus_dir=corpus_dir,\n        name=name,\n        sqlite_path=sqlite_path,\n        use_sqlite=True,\n        sqlite_only=sqlite_only,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/cutter/","title":"Cutter","text":"<p>The <code>cutter</code> module provides methods of cutting (splitting) and merging documents.</p>"},{"location":"api/cutter/#cutting-character-strings","title":"Cutting Character Strings","text":"<p>Tools for cutting strings of characters are implemented in <code>text_cutter</code>. Tools for cutting on token boundaries are found in <code>token_cutter</code></p> <p>The <code>spacy_attrs</code> module provides helpers for handling spaCy Token attributes.</p>"},{"location":"api/cutter/spacy_attrs/","title":"Spacy_Attrs","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/cutter/spacy_attrs/#lexos.cutter.spacy_attrs.SPACY_ATTRS","title":"<code>SPACY_ATTRS = ['DEP', 'ENT_ID', 'ENT_IOB', 'ENT_TYPE', 'IS_ALPHA', 'IS_ASCII', 'IS_DIGIT', 'IS_LOWER', 'IS_PUNCT', 'IS_SPACE', 'IS_STOP', 'IS_TITLE', 'IS_UPPER', 'LEMMA', 'LENGTH', 'LIKE_EMAIL', 'LIKE_NUM', 'LIKE_URL', 'LOWER', 'MORPH', 'NORM', 'ORTH', 'POS', 'SENT_START', 'SHAPE', 'SPACY', 'TAG']</code>  <code>module-attribute</code>","text":""},{"location":"api/cutter/spacy_attrs/#lexos.cutter.spacy_attrs.ENTITY_HEADER","title":"<code>ENTITY_HEADER = ['ENT_IOB', 'ENT_TYPE']</code>  <code>module-attribute</code>","text":""},{"location":"api/cutter/text_cutter/","title":"Text_Cutter","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter","title":"<code>TextCutter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TextCutter class for chunking files and strings containing untokenised text.</p> <p>Fields:</p> <ul> <li> <code>chunks</code>                 (<code>list[list[str]]</code>)             </li> <li> <code>docs</code>                 (<code>Optional[Path | str | list[Path | str]]</code>)             </li> <li> <code>chunksize</code>                 (<code>Optional[int]</code>)             </li> <li> <code>n</code>                 (<code>Optional[int]</code>)             </li> <li> <code>names</code>                 (<code>Optional[list[str | None]]</code>)             </li> <li> <code>newline</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>overlap</code>                 (<code>Optional[int]</code>)             </li> <li> <code>by_bytes</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>output_dir</code>                 (<code>Optional[Path | str]</code>)             </li> <li> <code>merge_threshold</code>                 (<code>Optional[float]</code>)             </li> <li> <code>merge_final</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>delimiter</code>                 (<code>str</code>)             </li> <li> <code>pad</code>                 (<code>int</code>)             </li> <li> <code>strip_chunks</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>class TextCutter(BaseModel, validate_assignment=True):\n    \"\"\"TextCutter class for chunking files and strings containing untokenised text.\"\"\"\n\n    chunks: list[list[str]] = []\n\n    docs: Optional[Path | str | list[Path | str]] = Field(\n        default=None,\n        description=\"The documents to be split.\",\n    )\n    chunksize: Optional[int] = Field(\n        default=1_000_000,\n        description=\"The desired chunk size in characters (or bytes if by_bytes=True). When newline=True, this refers to the number of lines per chunk.\",\n    )\n    n: Optional[int] = Field(\n        default=None,\n        description=\"When newline=False: the number of chunks to split into. When newline=True: the number of lines per chunk (equivalent to chunksize).\",\n    )\n    names: Optional[list[str | None]] = Field(\n        default=[], description=\"A list of names for the doc files/strings.\"\n    )\n    newline: Optional[bool] = Field(\n        default=False, description=\"Whether to chunk by lines.\"\n    )\n    overlap: Optional[int] = Field(\n        default=None, description=\"The number of characters to overlap between chunks.\"\n    )\n    by_bytes: Optional[bool] = Field(\n        default=False, description=\"Whether to chunk by bytes instead of characters.\"\n    )\n    output_dir: Optional[Path | str] = Field(\n        default=None, description=\"The output directory to save the chunks to.\"\n    )\n    merge_threshold: Optional[float] = Field(\n        default=0.5, description=\"The threshold for merging the last two chunks.\"\n    )\n    merge_final: Optional[bool] = Field(\n        default=False,\n        description=\"Whether to merge the last two chunks.\",\n    )\n    delimiter: str = Field(\n        default=\"_\",\n        description=\"The delimiter to use for the chunk names.\",\n    )\n    pad: int = Field(default=3, description=\"The padding for the chunk names.\")\n    strip_chunks: bool = Field(\n        default=True,\n        description=\"Whether to strip leading and trailing whitespace in the chunks.\",\n    )\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Make the class iterable.\n\n        Returns:\n            Iterator: An iterator containing the object's chunks.\n        \"\"\"\n        return iter([chunk for chunk in self.chunks])\n\n    def __len__(self):\n        \"\"\"Return the number of docs in the instance.\"\"\"\n        if not self.docs:\n            return 0\n        return len(self.docs)\n\n    def _calculate_chunk_size(self, size: int, n: int) -&gt; tuple[int, int]:\n        \"\"\"Calculate chunk size and remainder for n chunks.\n\n        Args:\n            size (int): Total size of file in bytes.\n            n (int): Number of chunks to create.\n\n        Returns:\n            tuple [int, int]: (chunk_size, remainder)\n        \"\"\"\n        chunk_size = size // n\n        remainder = size % n\n        return chunk_size, remainder\n\n    def _get_name(self, doc: Path | str, index: int) -&gt; str:\n        \"\"\"Generate a filename based on doc or fallback rules.\n\n        Args:\n            doc (Path | str): Original file path or doc label.\n            index (int): Index of the doc being processed.\n\n        Returns:\n            str: A formatted name for saving the chunked output.\n        \"\"\"\n        if len(self.names) &gt; 0:\n            return self.names[index]\n        elif isinstance(doc, Path):\n            return Path(doc).stem\n        else:\n            return f\"doc{str(index).zfill(self.pad)}\"\n\n    def _merge_final_chunks(\n        self, chunks: Generator[str, None, None]\n    ) -&gt; Generator[str, None, None]:\n        \"\"\"Merge the last two chunks if the final one is below the merge threshold.\n\n        Args:\n            chunks (Generator[str]): Chunks of text to evaluate.\n\n        Yields:\n            str: Finalized chunks after merging (if needed).\n        \"\"\"\n        buffer = []\n        for item in chunks:\n            buffer.append(item)\n            if len(buffer) &gt; 2:\n                yield buffer.pop(0)\n        if len(buffer) == 2:\n            yield \"\".join([buffer[0], buffer[1]])\n        elif len(buffer) == 1:\n            yield buffer[0]\n\n    def _apply_overlap(self, chunks: list[str]) -&gt; list[str]:\n        \"\"\"Apply overlap to chunks by adding trailing characters from previous chunk.\n\n        Args:\n            chunks (list[str]): The list of chunks to apply overlap to.\n\n        Returns:\n            list[str]: The list of chunks with overlap applied.\n        \"\"\"\n        if not self.overlap or len(chunks) &lt;= 1:\n            return chunks\n\n        overlapped_chunks = [chunks[0]]\n        for i in range(1, len(chunks)):\n            prev_chunk = chunks[i - 1]\n            current_chunk = chunks[i]\n            # Get the last `overlap` characters from the previous chunk\n            overlap_text = (\n                prev_chunk[-self.overlap :]\n                if len(prev_chunk) &gt;= self.overlap\n                else prev_chunk\n            )\n            # Prepend the overlap text to the current chunk\n            overlapped_chunks.append(overlap_text + current_chunk)\n\n        return overlapped_chunks\n\n    def _process_buffer(\n        self,\n        doc: bytes | str,\n        n: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"Process single buffer in chunks.\n\n        Args:\n            doc (bytes | str): The string or bytes doc.\n            n (bool): Whether to chunk by n.\n\n        Returns:\n            list[str]: The chunks.\n        \"\"\"\n        # If chunking by bytes, use the legacy byte-based approach\n        if self.by_bytes:\n            if isinstance(doc, str):\n                doc = doc.encode()\n            chunks = []\n            with BytesIO(doc) as buffer:\n                if n is True:\n                    if self.newline:\n                        # When newline=True, n means \"N lines per chunk\" (same as chunksize)\n                        lines_per_chunk = self.n\n                        while True:\n                            chunk_lines = []\n                            for _ in range(lines_per_chunk):\n                                line = buffer.readline()\n                                if not line:\n                                    break\n                                chunk_lines.append(line)\n                            if not chunk_lines:\n                                break\n                            chunk = b\"\".join(chunk_lines)\n                            chunks.append(chunk.decode(\"utf-8\"))\n                    else:\n                        file_size = buffer.getbuffer().nbytes\n                        chunk_size, remainder = self._calculate_chunk_size(\n                            file_size, self.n\n                        )\n                        try:\n                            for i in range(self.n):\n                                size = (\n                                    chunk_size + remainder\n                                    if i == self.n - 1\n                                    else chunk_size\n                                )\n                                chunk = buffer.read(size)\n                                if not chunk:\n                                    break\n                                # Convert to string\n                                chunks.append(\n                                    chunk.decode(\"utf-8\")\n                                    if isinstance(chunk, bytes)\n                                    else chunk\n                                )\n                        finally:\n                            buffer.close()\n                else:\n                    if self.newline:\n                        # Read chunksize lines at a time\n                        while True:\n                            chunk_lines = []\n                            for _ in range(self.chunksize):\n                                line = buffer.readline()\n                                if not line:\n                                    break\n                                chunk_lines.append(line)\n                            if not chunk_lines:\n                                break\n                            chunk = b\"\".join(chunk_lines)\n                            chunks.append(chunk.decode(\"utf-8\"))\n                    else:\n                        while chunk := buffer.read(self.chunksize):\n                            chunks.append(chunk.decode(\"utf-8\"))\n            return chunks\n\n        # Character-based chunking (default)\n        if isinstance(doc, bytes):\n            doc = doc.decode(\"utf-8\")\n        chunks = []\n\n        if self.newline:\n            # Split text into lines first\n            lines = doc.splitlines(keepends=True)\n\n            # When newline=True, both n and chunksize mean \"N lines per chunk\"\n            lines_per_chunk = self.n if n is True else self.chunksize\n\n            # Split by lines_per_chunk LINES\n            for i in range(0, len(lines), lines_per_chunk):\n                chunk_lines = lines[i : i + lines_per_chunk]\n                if chunk_lines:\n                    chunks.append(\"\".join(chunk_lines))\n        elif n is True:\n            total_len = len(doc)\n            chunk_size, remainder = self._calculate_chunk_size(total_len, self.n)\n            start = 0\n            for i in range(self.n):\n                size = chunk_size + remainder if i == self.n - 1 else chunk_size\n                end = start + size\n                chunk = doc[start:end]\n                # If not the last chunk and chunk doesn't end with newline, extend to line end\n                if i &lt; self.n - 1 and chunk and not chunk.endswith(\"\\n\"):\n                    # Find the next newline\n                    next_newline = doc.find(\"\\n\", end)\n                    if next_newline != -1:\n                        chunk = doc[start:next_newline]\n                        start = next_newline\n                    else:\n                        start = end\n                else:\n                    start = end\n\n                if not chunk:\n                    break\n                chunks.append(chunk)\n        else:\n            # Simple character-based chunking\n            for i in range(0, len(doc), self.chunksize):\n                chunk = doc[i : i + self.chunksize]\n                if chunk:\n                    chunks.append(chunk)\n        return chunks\n\n    def _process_file(\n        self,\n        path: Path | str,\n        n: bool = False,\n    ) -&gt; list[str]:\n        \"\"\"Split the contents of a file into chunks.\n\n        Args:\n            path (Path | str): Path to the input file.\n            n (bool): Whether to split into a fixed number of parts.\n\n        Returns:\n            list[str]: List of chunked text segments.\n        \"\"\"\n        # If chunking by bytes, use the legacy byte-based file reading\n        if self.by_bytes:\n            chunks = []\n            with open(path, \"rb\") as f:\n                if n is True:\n                    if self.newline:\n                        # When newline=True, n means \"N lines per chunk\" (same as chunksize)\n                        lines_per_chunk = self.n\n                        while True:\n                            chunk_lines = []\n                            for _ in range(lines_per_chunk):\n                                line = f.readline()\n                                if not line:\n                                    break\n                                chunk_lines.append(line)\n                            if not chunk_lines:\n                                break\n                            chunk = b\"\".join(chunk_lines).decode(\"utf-8\")\n                            chunk = chunk.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n                            chunks.append(chunk)\n                    else:\n                        file_size = os.path.getsize(str(path))\n                        chunk_size, remainder = self._calculate_chunk_size(\n                            file_size, self.n\n                        )\n                        try:\n                            for i in range(self.n):\n                                size = (\n                                    chunk_size + remainder\n                                    if i == self.n - 1\n                                    else chunk_size\n                                )\n                                chunk = f.read(size)\n                                if not chunk:\n                                    break\n                                chunk = (\n                                    chunk.decode(\"utf-8\")\n                                    .replace(\"\\r\\n\", \"\\n\")\n                                    .replace(\"\\r\", \"\\n\")\n                                )\n\n                                # Extend to end of line if not last chunk\n                                if (\n                                    i &lt; self.n - 1\n                                    and chunk\n                                    and not chunk.endswith(\"\\n\")\n                                ):\n                                    rest_of_line = f.readline()\n                                    if rest_of_line:\n                                        rest_of_line = rest_of_line.decode(\"utf-8\")\n                                        if rest_of_line.endswith(\"\\n\"):\n                                            rest_of_line = rest_of_line[:-1]\n                                            f.seek(f.tell() - 1)\n                                        chunk = chunk + rest_of_line\n                                chunks.append(chunk)\n                        finally:\n                            f.close()\n                else:\n                    if self.newline:\n                        # Read chunksize lines at a time\n                        while True:\n                            chunk_lines = []\n                            for _ in range(self.chunksize):\n                                line = f.readline()\n                                if not line:\n                                    break\n                                chunk_lines.append(line)\n                            if not chunk_lines:\n                                break\n                            chunk = b\"\".join(chunk_lines).decode(\"utf-8\")\n                            chunk = chunk.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n                            chunks.append(chunk)\n                    else:\n                        while chunk := f.read(self.chunksize):\n                            chunk = (\n                                chunk.decode(\"utf-8\")\n                                .replace(\"\\r\\n\", \"\\n\")\n                                .replace(\"\\r\", \"\\n\")\n                            )\n                            chunks.append(chunk)\n            return chunks\n\n        # Character-based chunking (default) - read entire file as text\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n        text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n        return self._process_buffer(text, n=n)\n\n    def _read_by_lines(self, file_or_buf: BinaryIO, size: int) -&gt; str:\n        \"\"\"Read file by lines up to size limit.\n\n        Args:\n            file_or_buf (BinaryIO): The file object or buffer to read from.\n            size (int): Maximum bytes to read.\n\n        Returns:\n            str: Concatenated lines up to size limit.\n        \"\"\"\n        chunks: list[bytes] = []\n        bytes_read = 0\n\n        while bytes_read &lt; size and (line := file_or_buf.readline()):\n            chunks.append(line.decode(\"utf-8\") if isinstance(line, bytes) else line)\n            bytes_read += len(line)\n\n        return \"\".join(chunks)\n\n    def _read_chunks(self, buffer: BytesIO, size: int) -&gt; bytes:\n        \"\"\"Read a fixed number of bytes from a memory buffer.\n\n        Args:\n            buffer (BytesIO): The buffer to read from.\n            size (int): Number of bytes to read.\n\n        Returns:\n            bytes: A chunk of text from the buffer.\n        \"\"\"\n        chunk = buffer.read(size)\n        return chunk\n\n    def _set_attributes(self, **data: Any) -&gt; None:\n        \"\"\"Update multiple attributes on the TextCutter instance.\n\n        Args:\n            **data (Any): Arbitrary keyword arguments matching attribute names.\n        \"\"\"\n        for key, value in data.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n\n    def _write_chunk(\n        self, path: Path | str, n: int, chunk: str, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Write chunk to file with formatted name.\n\n        Args:\n            path (Path | str): The path of the original file.\n            n (int): The number of the chunk.\n            chunk (str): The chunk to save.\n            output_dir (Path): The output directory for the chunk.\n        \"\"\"\n        path = Path(path)\n        output_file = f\"{path.stem}{self.delimiter}{str(n).zfill(self.pad)}.txt\"\n        output_path = output_dir / output_file\n        # Ensure the output directory exists\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        output_path.write_text(chunk)\n\n    @validate_call\n    def merge(self, chunks: list[str], sep: str = \" \") -&gt; str:\n        \"\"\"Merge a list of chunks into a single str.\n\n        Args:\n            chunks (list[str]): The list of chunks to merge.\n            sep (str): The separator to use.\n\n        Returns:\n            str: The merged string.\n        \"\"\"\n        if len(chunks) == 0:\n            raise LexosException(\"No chunks to merge.\")\n        return f\"{sep}\".join(string for string in chunks)\n\n    @validate_call\n    def save(\n        self,\n        output_dir: Path | str,\n        names: Optional[list[str]] = None,\n        delimiter: Optional[str] = \"_\",\n        pad: Optional[int] = 3,\n        strip_chunks: Optional[bool] = True,\n    ) -&gt; None:\n        \"\"\"Save the chunks to disk.\n\n        Args:\n            output_dir (Path | str): The output directory to save the chunks to.\n            names (Optional[list[str]]): The doc names.\n            delimiter (str): The delimiter to use for the chunk names.\n            pad (int): The padding for the chunk names.\n            strip_chunks (bool): Whether to strip leading and trailing whitespace in the chunks.\n        \"\"\"\n        self._set_attributes(\n            output_dir=output_dir,\n            delimiter=delimiter,\n            names=names,\n            pad=pad,\n            strip_chunks=strip_chunks,\n        )\n        if not self.chunks:\n            raise LexosException(\"No chunks to save.\")\n        if self.names:\n            if len(self.names) != len(self.chunks):\n                raise LexosException(\n                    f\"The number of docs in `names` ({len(self.names)}) must equal the number of docs in `chunks` ({len(self.chunks)}).\"\n                )\n        else:\n            self.names = [\n                f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n            ]\n        for i, doc in enumerate(self.chunks):\n            for num, chunk in enumerate(doc):\n                if strip_chunks:\n                    chunk = chunk.strip()\n                self._write_chunk(self.names[i], num + 1, chunk, Path(output_dir))\n\n    @validate_call\n    def split(\n        self,\n        docs: Optional[Path | str | list[Path | str]] = None,\n        chunksize: Optional[int] = None,\n        names: Optional[str | list[str]] = None,\n        delimiter: Optional[str] = \"_\",\n        pad: Optional[int] = 3,\n        n: Optional[int] = None,\n        newline: Optional[bool] = None,\n        overlap: Optional[int] = None,\n        by_bytes: Optional[bool] = None,\n        file: Optional[bool] = False,\n        merge_threshold: Optional[float] = 0.5,\n        merge_final: Optional[bool] = False,\n    ) -&gt; list[list[str]]:\n        \"\"\"Chunk the file or buffer.\n\n        Args:\n            docs (Optional[Path | str | list[Path | str]]): The file path or buffer.\n            chunksize (Optional[int]): The size of the chunks in characters (or bytes if by_bytes=True).\n            names (Optional[str | list[str | None]]): The doc names.\n            delimiter (Optional[str]): The delimiter to use for the chunk names.\n            pad (Optional[int]): The padding for the chunk names.\n            n (Optional[int]): The number of chunks.\n            newline (Optional[bool]): Whether to chunk by lines.\n            overlap (Optional[int]): The number of characters to overlap between chunks.\n            by_bytes (Optional[bool]): Whether to chunk by bytes instead of characters.\n            file (Optional[bool]): Whether to chunk the file or buffer.\n            merge_threshold (Optional[float]): The threshold for merging the last two chunks.\n            merge_final (Optional[bool]): Whether to merge the last two chunks.\n\n        Returns:\n            list[list[str]]: A list of chunked strings.\n        \"\"\"\n        if docs:\n            self.docs = ensure_list(docs)\n        if not self.docs:\n            raise LexosException(\"No documents provided for splitting.\")\n        self._set_attributes(\n            n=n,\n            newline=newline,\n            overlap=overlap,\n            by_bytes=by_bytes if by_bytes is not None else self.by_bytes,\n            merge_threshold=merge_threshold,\n            merge_final=merge_final,\n            delimiter=delimiter,\n            pad=pad,\n        )\n        if chunksize:\n            self.chunksize = chunksize\n        if names:\n            self.names = ensure_list(names)\n        elif file:\n            self.names = [Path(name).stem for name in ensure_list(docs)]\n        else:\n            self.names = [\n                f\"doc{str(i).zfill(self.pad)}\" for i in range(1, len(self.docs) + 1)\n            ]\n        for doc in self.docs:\n            split_by_num = False\n            if isinstance(self.n, int):\n                split_by_num = True\n            chunks = (\n                self._process_file(doc, n=split_by_num)\n                if file\n                else self._process_buffer(doc, n=split_by_num)\n            )\n            # Calculate the threshold here.\n            threshold = self.chunksize * self.merge_threshold\n            if chunks and (self.merge_final is True or len(chunks[-1]) &lt; threshold):\n                chunks = list(self._merge_final_chunks(chunks))\n            # Apply overlap if specified\n            if self.overlap and chunks:\n                chunks = self._apply_overlap(chunks)\n            self.chunks.append(chunks)\n\n        return self.chunks\n\n    @validate_call\n    def split_on_milestones(\n        self,\n        milestones: list[StringSpan],\n        docs: Optional[Path | str | list[Path | str]] = None,\n        names: Optional[Path | str | list[Path | str]] = None,\n        delimiter: Optional[str] = \"_\",\n        pad: Optional[int] = 3,\n        keep_spans: Optional[bool | str] = False,\n        strip: Optional[bool] = True,\n        file: Optional[bool] = False,\n    ) -&gt; list[list[str]]:\n        \"\"\"Split text at each milestone span, optionally retaining the milestone text.\n\n        Args:\n            milestones (list[StringSpan]): A list of milestone StringSpans to split the text at.\n            docs (Optional[Path | str | list[Path | str]]): List of file paths or buffers.\n            names (Optional[Path | str | list[Path | str]]): List of doc names.\n            delimiter (Optional[str]): The delimiter to use for the chunk names.\n            pad (Optional[int]): The padding for the chunk names.\n            keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n            strip (Optional[bool]): Whether to strip the text. Defaults to True.\n            file (Optional[bool]): Set to True if reading from file(s), False for strings.\n\n        Returns:\n            list[list[str]]: A list of chunked strings.\n        \"\"\"\n        if docs:\n            self.docs = ensure_list(docs)\n        if not self.docs:\n            raise LexosException(\"No documents provided for splitting.\")\n        self._set_attributes(\n            delimiter=delimiter,\n            pad=pad,\n        )\n        if names:\n            self.names = ensure_list(names)\n        elif file:\n            self.names = [Path(name).stem for name in ensure_list(docs)]\n        else:\n            self.names = [\n                f\"doc{str(i).zfill(self.pad)}\" for i in range(1, len(self.docs) + 1)\n            ]\n        for doc in self.docs:\n            text = doc\n            if file:\n                try:\n                    with open(doc, \"r\", newline=\"\") as f:\n                        text = f.read()\n                except BaseException as e:\n                    raise LexosException(e)\n            chunks = []\n            start = 0\n            for i, span in enumerate(milestones):\n                end = span.start\n                # Preceding: milestone text goes at end of previous chunk\n                if keep_spans == \"preceding\":\n                    chunk = text[start:end] + text[span.start : span.end + 1]\n                    chunks.append(chunk)\n                # Following: milestone text goes at start of next chunk\n                elif keep_spans == \"following\":\n                    chunk = text[start:end]\n                    # Store the milestone text to prepend to the next chunk\n                    chunks.append(chunk)\n                else:\n                    chunk = text[start:end]\n                    chunks.append(chunk)\n                start = span.end + 1\n            # Last chunk\n            last_chunk = text[start:]\n            if keep_spans == \"following\" and milestones:\n                # Prepend each milestone text to the next chunk (except the first chunk)\n                for idx in range(1, len(chunks)):\n                    milestone_text = text[\n                        milestones[idx - 1].start : milestones[idx - 1].end + 1\n                    ]\n                    chunks[idx] = milestone_text + chunks[idx]\n                # Also prepend the last milestone to the last chunk\n                last_span = milestones[-1]\n                last_chunk = text[last_span.start : last_span.end + 1] + last_chunk\n            chunks.append(last_chunk)\n            if strip:\n                chunks = [doc.strip() for doc in chunks]\n            self.chunks.append(chunks)\n\n        return self.chunks\n\n    @validate_call\n    def to_dict(\n        self, names: Optional[Path | str | list[Path | str]] = None\n    ) -&gt; dict[str, list[str]]:\n        \"\"\"Return the chunks as a dictionary.\n\n        Args:\n            names (Optional[Path | str | list[Path | str]]): The doc names.\n\n        Returns:\n            dict[str, list[str]]: The chunks as a dictionary.\n        \"\"\"\n        if names:\n            self.names = ensure_list(names)\n        if self.names == [] or self.names is None:\n            self.names = [\n                f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n            ]\n        return {str(doc): chunks for doc, chunks in zip(self.names, self.chunks)}\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.by_bytes","title":"<code>by_bytes: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to chunk by bytes instead of characters.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.chunksize","title":"<code>chunksize: Optional[int] = 1000000</code>  <code>pydantic-field</code>","text":"<p>The desired chunk size in characters (or bytes if by_bytes=True). When newline=True, this refers to the number of lines per chunk.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.delimiter","title":"<code>delimiter: str = '_'</code>  <code>pydantic-field</code>","text":"<p>The delimiter to use for the chunk names.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.docs","title":"<code>docs: Optional[Path | str | list[Path | str]] = None</code>  <code>pydantic-field</code>","text":"<p>The documents to be split.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.merge_final","title":"<code>merge_final: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to merge the last two chunks.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.merge_threshold","title":"<code>merge_threshold: Optional[float] = 0.5</code>  <code>pydantic-field</code>","text":"<p>The threshold for merging the last two chunks.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.n","title":"<code>n: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>When newline=False: the number of chunks to split into. When newline=True: the number of lines per chunk (equivalent to chunksize).</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.names","title":"<code>names: Optional[list[str | None]] = []</code>  <code>pydantic-field</code>","text":"<p>A list of names for the doc files/strings.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.newline","title":"<code>newline: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to chunk by lines.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.output_dir","title":"<code>output_dir: Optional[Path | str] = None</code>  <code>pydantic-field</code>","text":"<p>The output directory to save the chunks to.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.overlap","title":"<code>overlap: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The number of characters to overlap between chunks.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.pad","title":"<code>pad: int = 3</code>  <code>pydantic-field</code>","text":"<p>The padding for the chunk names.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.strip_chunks","title":"<code>strip_chunks: bool = True</code>  <code>pydantic-field</code>","text":"<p>Whether to strip leading and trailing whitespace in the chunks.</p>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>An iterator containing the object's chunks.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: An iterator containing the object's chunks.\n    \"\"\"\n    return iter([chunk for chunk in self.chunks])\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of docs in the instance.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of docs in the instance.\"\"\"\n    if not self.docs:\n        return 0\n    return len(self.docs)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.merge","title":"<code>merge(chunks: list[str], sep: str = ' ') -&gt; str</code>","text":"<p>Merge a list of chunks into a single str.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[str]</code> <p>The list of chunks to merge.</p> required <code>sep</code> <code>str</code> <p>The separator to use.</p> <code>' '</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The merged string.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef merge(self, chunks: list[str], sep: str = \" \") -&gt; str:\n    \"\"\"Merge a list of chunks into a single str.\n\n    Args:\n        chunks (list[str]): The list of chunks to merge.\n        sep (str): The separator to use.\n\n    Returns:\n        str: The merged string.\n    \"\"\"\n    if len(chunks) == 0:\n        raise LexosException(\"No chunks to merge.\")\n    return f\"{sep}\".join(string for string in chunks)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.save","title":"<code>save(output_dir: Path | str, names: Optional[list[str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, strip_chunks: Optional[bool] = True) -&gt; None</code>","text":"<p>Save the chunks to disk.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path | str</code> <p>The output directory to save the chunks to.</p> required <code>names</code> <code>Optional[list[str]]</code> <p>The doc names.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>int</code> <p>The padding for the chunk names.</p> <code>3</code> <code>strip_chunks</code> <code>bool</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef save(\n    self,\n    output_dir: Path | str,\n    names: Optional[list[str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    strip_chunks: Optional[bool] = True,\n) -&gt; None:\n    \"\"\"Save the chunks to disk.\n\n    Args:\n        output_dir (Path | str): The output directory to save the chunks to.\n        names (Optional[list[str]]): The doc names.\n        delimiter (str): The delimiter to use for the chunk names.\n        pad (int): The padding for the chunk names.\n        strip_chunks (bool): Whether to strip leading and trailing whitespace in the chunks.\n    \"\"\"\n    self._set_attributes(\n        output_dir=output_dir,\n        delimiter=delimiter,\n        names=names,\n        pad=pad,\n        strip_chunks=strip_chunks,\n    )\n    if not self.chunks:\n        raise LexosException(\"No chunks to save.\")\n    if self.names:\n        if len(self.names) != len(self.chunks):\n            raise LexosException(\n                f\"The number of docs in `names` ({len(self.names)}) must equal the number of docs in `chunks` ({len(self.chunks)}).\"\n            )\n    else:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    for i, doc in enumerate(self.chunks):\n        for num, chunk in enumerate(doc):\n            if strip_chunks:\n                chunk = chunk.strip()\n            self._write_chunk(self.names[i], num + 1, chunk, Path(output_dir))\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.split","title":"<code>split(docs: Optional[Path | str | list[Path | str]] = None, chunksize: Optional[int] = None, names: Optional[str | list[str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, n: Optional[int] = None, newline: Optional[bool] = None, overlap: Optional[int] = None, by_bytes: Optional[bool] = None, file: Optional[bool] = False, merge_threshold: Optional[float] = 0.5, merge_final: Optional[bool] = False) -&gt; list[list[str]]</code>","text":"<p>Chunk the file or buffer.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Optional[Path | str | list[Path | str]]</code> <p>The file path or buffer.</p> <code>None</code> <code>chunksize</code> <code>Optional[int]</code> <p>The size of the chunks in characters (or bytes if by_bytes=True).</p> <code>None</code> <code>names</code> <code>Optional[str | list[str | None]]</code> <p>The doc names.</p> <code>None</code> <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>Optional[int]</code> <p>The padding for the chunk names.</p> <code>3</code> <code>n</code> <code>Optional[int]</code> <p>The number of chunks.</p> <code>None</code> <code>newline</code> <code>Optional[bool]</code> <p>Whether to chunk by lines.</p> <code>None</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of characters to overlap between chunks.</p> <code>None</code> <code>by_bytes</code> <code>Optional[bool]</code> <p>Whether to chunk by bytes instead of characters.</p> <code>None</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to chunk the file or buffer.</p> <code>False</code> <code>merge_threshold</code> <code>Optional[float]</code> <p>The threshold for merging the last two chunks.</p> <code>0.5</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to merge the last two chunks.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: A list of chunked strings.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef split(\n    self,\n    docs: Optional[Path | str | list[Path | str]] = None,\n    chunksize: Optional[int] = None,\n    names: Optional[str | list[str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    n: Optional[int] = None,\n    newline: Optional[bool] = None,\n    overlap: Optional[int] = None,\n    by_bytes: Optional[bool] = None,\n    file: Optional[bool] = False,\n    merge_threshold: Optional[float] = 0.5,\n    merge_final: Optional[bool] = False,\n) -&gt; list[list[str]]:\n    \"\"\"Chunk the file or buffer.\n\n    Args:\n        docs (Optional[Path | str | list[Path | str]]): The file path or buffer.\n        chunksize (Optional[int]): The size of the chunks in characters (or bytes if by_bytes=True).\n        names (Optional[str | list[str | None]]): The doc names.\n        delimiter (Optional[str]): The delimiter to use for the chunk names.\n        pad (Optional[int]): The padding for the chunk names.\n        n (Optional[int]): The number of chunks.\n        newline (Optional[bool]): Whether to chunk by lines.\n        overlap (Optional[int]): The number of characters to overlap between chunks.\n        by_bytes (Optional[bool]): Whether to chunk by bytes instead of characters.\n        file (Optional[bool]): Whether to chunk the file or buffer.\n        merge_threshold (Optional[float]): The threshold for merging the last two chunks.\n        merge_final (Optional[bool]): Whether to merge the last two chunks.\n\n    Returns:\n        list[list[str]]: A list of chunked strings.\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        n=n,\n        newline=newline,\n        overlap=overlap,\n        by_bytes=by_bytes if by_bytes is not None else self.by_bytes,\n        merge_threshold=merge_threshold,\n        merge_final=merge_final,\n        delimiter=delimiter,\n        pad=pad,\n    )\n    if chunksize:\n        self.chunksize = chunksize\n    if names:\n        self.names = ensure_list(names)\n    elif file:\n        self.names = [Path(name).stem for name in ensure_list(docs)]\n    else:\n        self.names = [\n            f\"doc{str(i).zfill(self.pad)}\" for i in range(1, len(self.docs) + 1)\n        ]\n    for doc in self.docs:\n        split_by_num = False\n        if isinstance(self.n, int):\n            split_by_num = True\n        chunks = (\n            self._process_file(doc, n=split_by_num)\n            if file\n            else self._process_buffer(doc, n=split_by_num)\n        )\n        # Calculate the threshold here.\n        threshold = self.chunksize * self.merge_threshold\n        if chunks and (self.merge_final is True or len(chunks[-1]) &lt; threshold):\n            chunks = list(self._merge_final_chunks(chunks))\n        # Apply overlap if specified\n        if self.overlap and chunks:\n            chunks = self._apply_overlap(chunks)\n        self.chunks.append(chunks)\n\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.split_on_milestones","title":"<code>split_on_milestones(milestones: list[StringSpan], docs: Optional[Path | str | list[Path | str]] = None, names: Optional[Path | str | list[Path | str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, keep_spans: Optional[bool | str] = False, strip: Optional[bool] = True, file: Optional[bool] = False) -&gt; list[list[str]]</code>","text":"<p>Split text at each milestone span, optionally retaining the milestone text.</p> <p>Parameters:</p> Name Type Description Default <code>milestones</code> <code>list[StringSpan]</code> <p>A list of milestone StringSpans to split the text at.</p> required <code>docs</code> <code>Optional[Path | str | list[Path | str]]</code> <p>List of file paths or buffers.</p> <code>None</code> <code>names</code> <code>Optional[Path | str | list[Path | str]]</code> <p>List of doc names.</p> <code>None</code> <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>Optional[int]</code> <p>The padding for the chunk names.</p> <code>3</code> <code>keep_spans</code> <code>Optional[bool | str]</code> <p>Whether to keep the spans in the split strings. Defaults to False.</p> <code>False</code> <code>strip</code> <code>Optional[bool]</code> <p>Whether to strip the text. Defaults to True.</p> <code>True</code> <code>file</code> <code>Optional[bool]</code> <p>Set to True if reading from file(s), False for strings.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: A list of chunked strings.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef split_on_milestones(\n    self,\n    milestones: list[StringSpan],\n    docs: Optional[Path | str | list[Path | str]] = None,\n    names: Optional[Path | str | list[Path | str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    keep_spans: Optional[bool | str] = False,\n    strip: Optional[bool] = True,\n    file: Optional[bool] = False,\n) -&gt; list[list[str]]:\n    \"\"\"Split text at each milestone span, optionally retaining the milestone text.\n\n    Args:\n        milestones (list[StringSpan]): A list of milestone StringSpans to split the text at.\n        docs (Optional[Path | str | list[Path | str]]): List of file paths or buffers.\n        names (Optional[Path | str | list[Path | str]]): List of doc names.\n        delimiter (Optional[str]): The delimiter to use for the chunk names.\n        pad (Optional[int]): The padding for the chunk names.\n        keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n        strip (Optional[bool]): Whether to strip the text. Defaults to True.\n        file (Optional[bool]): Set to True if reading from file(s), False for strings.\n\n    Returns:\n        list[list[str]]: A list of chunked strings.\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        delimiter=delimiter,\n        pad=pad,\n    )\n    if names:\n        self.names = ensure_list(names)\n    elif file:\n        self.names = [Path(name).stem for name in ensure_list(docs)]\n    else:\n        self.names = [\n            f\"doc{str(i).zfill(self.pad)}\" for i in range(1, len(self.docs) + 1)\n        ]\n    for doc in self.docs:\n        text = doc\n        if file:\n            try:\n                with open(doc, \"r\", newline=\"\") as f:\n                    text = f.read()\n            except BaseException as e:\n                raise LexosException(e)\n        chunks = []\n        start = 0\n        for i, span in enumerate(milestones):\n            end = span.start\n            # Preceding: milestone text goes at end of previous chunk\n            if keep_spans == \"preceding\":\n                chunk = text[start:end] + text[span.start : span.end + 1]\n                chunks.append(chunk)\n            # Following: milestone text goes at start of next chunk\n            elif keep_spans == \"following\":\n                chunk = text[start:end]\n                # Store the milestone text to prepend to the next chunk\n                chunks.append(chunk)\n            else:\n                chunk = text[start:end]\n                chunks.append(chunk)\n            start = span.end + 1\n        # Last chunk\n        last_chunk = text[start:]\n        if keep_spans == \"following\" and milestones:\n            # Prepend each milestone text to the next chunk (except the first chunk)\n            for idx in range(1, len(chunks)):\n                milestone_text = text[\n                    milestones[idx - 1].start : milestones[idx - 1].end + 1\n                ]\n                chunks[idx] = milestone_text + chunks[idx]\n            # Also prepend the last milestone to the last chunk\n            last_span = milestones[-1]\n            last_chunk = text[last_span.start : last_span.end + 1] + last_chunk\n        chunks.append(last_chunk)\n        if strip:\n            chunks = [doc.strip() for doc in chunks]\n        self.chunks.append(chunks)\n\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.to_dict","title":"<code>to_dict(names: Optional[Path | str | list[Path | str]] = None) -&gt; dict[str, list[str]]</code>","text":"<p>Return the chunks as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Optional[Path | str | list[Path | str]]</code> <p>The doc names.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: The chunks as a dictionary.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef to_dict(\n    self, names: Optional[Path | str | list[Path | str]] = None\n) -&gt; dict[str, list[str]]:\n    \"\"\"Return the chunks as a dictionary.\n\n    Args:\n        names (Optional[Path | str | list[Path | str]]): The doc names.\n\n    Returns:\n        dict[str, list[str]]: The chunks as a dictionary.\n    \"\"\"\n    if names:\n        self.names = ensure_list(names)\n    if self.names == [] or self.names is None:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    return {str(doc): chunks for doc, chunks in zip(self.names, self.chunks)}\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>An iterator containing the object's chunks.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: An iterator containing the object's chunks.\n    \"\"\"\n    return iter([chunk for chunk in self.chunks])\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of docs in the instance.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of docs in the instance.\"\"\"\n    if not self.docs:\n        return 0\n    return len(self.docs)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._calculate_chunk_size","title":"<code>_calculate_chunk_size(size: int, n: int) -&gt; tuple[int, int]</code>","text":"<p>Calculate chunk size and remainder for n chunks.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Total size of file in bytes.</p> required <code>n</code> <code>int</code> <p>Number of chunks to create.</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>tuple [int, int]: (chunk_size, remainder)</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _calculate_chunk_size(self, size: int, n: int) -&gt; tuple[int, int]:\n    \"\"\"Calculate chunk size and remainder for n chunks.\n\n    Args:\n        size (int): Total size of file in bytes.\n        n (int): Number of chunks to create.\n\n    Returns:\n        tuple [int, int]: (chunk_size, remainder)\n    \"\"\"\n    chunk_size = size // n\n    remainder = size % n\n    return chunk_size, remainder\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._get_name","title":"<code>_get_name(doc: Path | str, index: int) -&gt; str</code>","text":"<p>Generate a filename based on doc or fallback rules.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Path | str</code> <p>Original file path or doc label.</p> required <code>index</code> <code>int</code> <p>Index of the doc being processed.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted name for saving the chunked output.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _get_name(self, doc: Path | str, index: int) -&gt; str:\n    \"\"\"Generate a filename based on doc or fallback rules.\n\n    Args:\n        doc (Path | str): Original file path or doc label.\n        index (int): Index of the doc being processed.\n\n    Returns:\n        str: A formatted name for saving the chunked output.\n    \"\"\"\n    if len(self.names) &gt; 0:\n        return self.names[index]\n    elif isinstance(doc, Path):\n        return Path(doc).stem\n    else:\n        return f\"doc{str(index).zfill(self.pad)}\"\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._merge_final_chunks","title":"<code>_merge_final_chunks(chunks: Generator[str, None, None]) -&gt; Generator[str, None, None]</code>","text":"<p>Merge the last two chunks if the final one is below the merge threshold.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>Generator[str]</code> <p>Chunks of text to evaluate.</p> required <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>Finalized chunks after merging (if needed).</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _merge_final_chunks(\n    self, chunks: Generator[str, None, None]\n) -&gt; Generator[str, None, None]:\n    \"\"\"Merge the last two chunks if the final one is below the merge threshold.\n\n    Args:\n        chunks (Generator[str]): Chunks of text to evaluate.\n\n    Yields:\n        str: Finalized chunks after merging (if needed).\n    \"\"\"\n    buffer = []\n    for item in chunks:\n        buffer.append(item)\n        if len(buffer) &gt; 2:\n            yield buffer.pop(0)\n    if len(buffer) == 2:\n        yield \"\".join([buffer[0], buffer[1]])\n    elif len(buffer) == 1:\n        yield buffer[0]\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._process_buffer","title":"<code>_process_buffer(doc: bytes | str, n: bool = False) -&gt; list[str]</code>","text":"<p>Process single buffer in chunks.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>bytes | str</code> <p>The string or bytes doc.</p> required <code>n</code> <code>bool</code> <p>Whether to chunk by n.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The chunks.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _process_buffer(\n    self,\n    doc: bytes | str,\n    n: bool = False,\n) -&gt; list[str]:\n    \"\"\"Process single buffer in chunks.\n\n    Args:\n        doc (bytes | str): The string or bytes doc.\n        n (bool): Whether to chunk by n.\n\n    Returns:\n        list[str]: The chunks.\n    \"\"\"\n    # If chunking by bytes, use the legacy byte-based approach\n    if self.by_bytes:\n        if isinstance(doc, str):\n            doc = doc.encode()\n        chunks = []\n        with BytesIO(doc) as buffer:\n            if n is True:\n                if self.newline:\n                    # When newline=True, n means \"N lines per chunk\" (same as chunksize)\n                    lines_per_chunk = self.n\n                    while True:\n                        chunk_lines = []\n                        for _ in range(lines_per_chunk):\n                            line = buffer.readline()\n                            if not line:\n                                break\n                            chunk_lines.append(line)\n                        if not chunk_lines:\n                            break\n                        chunk = b\"\".join(chunk_lines)\n                        chunks.append(chunk.decode(\"utf-8\"))\n                else:\n                    file_size = buffer.getbuffer().nbytes\n                    chunk_size, remainder = self._calculate_chunk_size(\n                        file_size, self.n\n                    )\n                    try:\n                        for i in range(self.n):\n                            size = (\n                                chunk_size + remainder\n                                if i == self.n - 1\n                                else chunk_size\n                            )\n                            chunk = buffer.read(size)\n                            if not chunk:\n                                break\n                            # Convert to string\n                            chunks.append(\n                                chunk.decode(\"utf-8\")\n                                if isinstance(chunk, bytes)\n                                else chunk\n                            )\n                    finally:\n                        buffer.close()\n            else:\n                if self.newline:\n                    # Read chunksize lines at a time\n                    while True:\n                        chunk_lines = []\n                        for _ in range(self.chunksize):\n                            line = buffer.readline()\n                            if not line:\n                                break\n                            chunk_lines.append(line)\n                        if not chunk_lines:\n                            break\n                        chunk = b\"\".join(chunk_lines)\n                        chunks.append(chunk.decode(\"utf-8\"))\n                else:\n                    while chunk := buffer.read(self.chunksize):\n                        chunks.append(chunk.decode(\"utf-8\"))\n        return chunks\n\n    # Character-based chunking (default)\n    if isinstance(doc, bytes):\n        doc = doc.decode(\"utf-8\")\n    chunks = []\n\n    if self.newline:\n        # Split text into lines first\n        lines = doc.splitlines(keepends=True)\n\n        # When newline=True, both n and chunksize mean \"N lines per chunk\"\n        lines_per_chunk = self.n if n is True else self.chunksize\n\n        # Split by lines_per_chunk LINES\n        for i in range(0, len(lines), lines_per_chunk):\n            chunk_lines = lines[i : i + lines_per_chunk]\n            if chunk_lines:\n                chunks.append(\"\".join(chunk_lines))\n    elif n is True:\n        total_len = len(doc)\n        chunk_size, remainder = self._calculate_chunk_size(total_len, self.n)\n        start = 0\n        for i in range(self.n):\n            size = chunk_size + remainder if i == self.n - 1 else chunk_size\n            end = start + size\n            chunk = doc[start:end]\n            # If not the last chunk and chunk doesn't end with newline, extend to line end\n            if i &lt; self.n - 1 and chunk and not chunk.endswith(\"\\n\"):\n                # Find the next newline\n                next_newline = doc.find(\"\\n\", end)\n                if next_newline != -1:\n                    chunk = doc[start:next_newline]\n                    start = next_newline\n                else:\n                    start = end\n            else:\n                start = end\n\n            if not chunk:\n                break\n            chunks.append(chunk)\n    else:\n        # Simple character-based chunking\n        for i in range(0, len(doc), self.chunksize):\n            chunk = doc[i : i + self.chunksize]\n            if chunk:\n                chunks.append(chunk)\n    return chunks\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._process_file","title":"<code>_process_file(path: Path | str, n: bool = False) -&gt; list[str]</code>","text":"<p>Split the contents of a file into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the input file.</p> required <code>n</code> <code>bool</code> <p>Whether to split into a fixed number of parts.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of chunked text segments.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _process_file(\n    self,\n    path: Path | str,\n    n: bool = False,\n) -&gt; list[str]:\n    \"\"\"Split the contents of a file into chunks.\n\n    Args:\n        path (Path | str): Path to the input file.\n        n (bool): Whether to split into a fixed number of parts.\n\n    Returns:\n        list[str]: List of chunked text segments.\n    \"\"\"\n    # If chunking by bytes, use the legacy byte-based file reading\n    if self.by_bytes:\n        chunks = []\n        with open(path, \"rb\") as f:\n            if n is True:\n                if self.newline:\n                    # When newline=True, n means \"N lines per chunk\" (same as chunksize)\n                    lines_per_chunk = self.n\n                    while True:\n                        chunk_lines = []\n                        for _ in range(lines_per_chunk):\n                            line = f.readline()\n                            if not line:\n                                break\n                            chunk_lines.append(line)\n                        if not chunk_lines:\n                            break\n                        chunk = b\"\".join(chunk_lines).decode(\"utf-8\")\n                        chunk = chunk.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n                        chunks.append(chunk)\n                else:\n                    file_size = os.path.getsize(str(path))\n                    chunk_size, remainder = self._calculate_chunk_size(\n                        file_size, self.n\n                    )\n                    try:\n                        for i in range(self.n):\n                            size = (\n                                chunk_size + remainder\n                                if i == self.n - 1\n                                else chunk_size\n                            )\n                            chunk = f.read(size)\n                            if not chunk:\n                                break\n                            chunk = (\n                                chunk.decode(\"utf-8\")\n                                .replace(\"\\r\\n\", \"\\n\")\n                                .replace(\"\\r\", \"\\n\")\n                            )\n\n                            # Extend to end of line if not last chunk\n                            if (\n                                i &lt; self.n - 1\n                                and chunk\n                                and not chunk.endswith(\"\\n\")\n                            ):\n                                rest_of_line = f.readline()\n                                if rest_of_line:\n                                    rest_of_line = rest_of_line.decode(\"utf-8\")\n                                    if rest_of_line.endswith(\"\\n\"):\n                                        rest_of_line = rest_of_line[:-1]\n                                        f.seek(f.tell() - 1)\n                                    chunk = chunk + rest_of_line\n                            chunks.append(chunk)\n                    finally:\n                        f.close()\n            else:\n                if self.newline:\n                    # Read chunksize lines at a time\n                    while True:\n                        chunk_lines = []\n                        for _ in range(self.chunksize):\n                            line = f.readline()\n                            if not line:\n                                break\n                            chunk_lines.append(line)\n                        if not chunk_lines:\n                            break\n                        chunk = b\"\".join(chunk_lines).decode(\"utf-8\")\n                        chunk = chunk.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n                        chunks.append(chunk)\n                else:\n                    while chunk := f.read(self.chunksize):\n                        chunk = (\n                            chunk.decode(\"utf-8\")\n                            .replace(\"\\r\\n\", \"\\n\")\n                            .replace(\"\\r\", \"\\n\")\n                        )\n                        chunks.append(chunk)\n        return chunks\n\n    # Character-based chunking (default) - read entire file as text\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    return self._process_buffer(text, n=n)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._read_by_lines","title":"<code>_read_by_lines(file_or_buf: BinaryIO, size: int) -&gt; str</code>","text":"<p>Read file by lines up to size limit.</p> <p>Parameters:</p> Name Type Description Default <code>file_or_buf</code> <code>BinaryIO</code> <p>The file object or buffer to read from.</p> required <code>size</code> <code>int</code> <p>Maximum bytes to read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Concatenated lines up to size limit.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _read_by_lines(self, file_or_buf: BinaryIO, size: int) -&gt; str:\n    \"\"\"Read file by lines up to size limit.\n\n    Args:\n        file_or_buf (BinaryIO): The file object or buffer to read from.\n        size (int): Maximum bytes to read.\n\n    Returns:\n        str: Concatenated lines up to size limit.\n    \"\"\"\n    chunks: list[bytes] = []\n    bytes_read = 0\n\n    while bytes_read &lt; size and (line := file_or_buf.readline()):\n        chunks.append(line.decode(\"utf-8\") if isinstance(line, bytes) else line)\n        bytes_read += len(line)\n\n    return \"\".join(chunks)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._read_chunks","title":"<code>_read_chunks(buffer: BytesIO, size: int) -&gt; bytes</code>","text":"<p>Read a fixed number of bytes from a memory buffer.</p> <p>Parameters:</p> Name Type Description Default <code>buffer</code> <code>BytesIO</code> <p>The buffer to read from.</p> required <code>size</code> <code>int</code> <p>Number of bytes to read.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>A chunk of text from the buffer.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _read_chunks(self, buffer: BytesIO, size: int) -&gt; bytes:\n    \"\"\"Read a fixed number of bytes from a memory buffer.\n\n    Args:\n        buffer (BytesIO): The buffer to read from.\n        size (int): Number of bytes to read.\n\n    Returns:\n        bytes: A chunk of text from the buffer.\n    \"\"\"\n    chunk = buffer.read(size)\n    return chunk\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._set_attributes","title":"<code>_set_attributes(**data: Any) -&gt; None</code>","text":"<p>Update multiple attributes on the TextCutter instance.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Arbitrary keyword arguments matching attribute names.</p> <code>{}</code> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _set_attributes(self, **data: Any) -&gt; None:\n    \"\"\"Update multiple attributes on the TextCutter instance.\n\n    Args:\n        **data (Any): Arbitrary keyword arguments matching attribute names.\n    \"\"\"\n    for key, value in data.items():\n        if hasattr(self, key):\n            setattr(self, key, value)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter._write_chunk","title":"<code>_write_chunk(path: Path | str, n: int, chunk: str, output_dir: Path) -&gt; None</code>","text":"<p>Write chunk to file with formatted name.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path of the original file.</p> required <code>n</code> <code>int</code> <p>The number of the chunk.</p> required <code>chunk</code> <code>str</code> <p>The chunk to save.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory for the chunk.</p> required Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>def _write_chunk(\n    self, path: Path | str, n: int, chunk: str, output_dir: Path\n) -&gt; None:\n    \"\"\"Write chunk to file with formatted name.\n\n    Args:\n        path (Path | str): The path of the original file.\n        n (int): The number of the chunk.\n        chunk (str): The chunk to save.\n        output_dir (Path): The output directory for the chunk.\n    \"\"\"\n    path = Path(path)\n    output_file = f\"{path.stem}{self.delimiter}{str(n).zfill(self.pad)}.txt\"\n    output_path = output_dir / output_file\n    # Ensure the output directory exists\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    output_path.write_text(chunk)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.merge","title":"<code>merge(chunks: list[str], sep: str = ' ') -&gt; str</code>","text":"<p>Merge a list of chunks into a single str.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[str]</code> <p>The list of chunks to merge.</p> required <code>sep</code> <code>str</code> <p>The separator to use.</p> <code>' '</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The merged string.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef merge(self, chunks: list[str], sep: str = \" \") -&gt; str:\n    \"\"\"Merge a list of chunks into a single str.\n\n    Args:\n        chunks (list[str]): The list of chunks to merge.\n        sep (str): The separator to use.\n\n    Returns:\n        str: The merged string.\n    \"\"\"\n    if len(chunks) == 0:\n        raise LexosException(\"No chunks to merge.\")\n    return f\"{sep}\".join(string for string in chunks)\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.save","title":"<code>save(output_dir: Path | str, names: Optional[list[str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, strip_chunks: Optional[bool] = True) -&gt; None</code>","text":"<p>Save the chunks to disk.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path | str</code> <p>The output directory to save the chunks to.</p> required <code>names</code> <code>Optional[list[str]]</code> <p>The doc names.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>int</code> <p>The padding for the chunk names.</p> <code>3</code> <code>strip_chunks</code> <code>bool</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef save(\n    self,\n    output_dir: Path | str,\n    names: Optional[list[str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    strip_chunks: Optional[bool] = True,\n) -&gt; None:\n    \"\"\"Save the chunks to disk.\n\n    Args:\n        output_dir (Path | str): The output directory to save the chunks to.\n        names (Optional[list[str]]): The doc names.\n        delimiter (str): The delimiter to use for the chunk names.\n        pad (int): The padding for the chunk names.\n        strip_chunks (bool): Whether to strip leading and trailing whitespace in the chunks.\n    \"\"\"\n    self._set_attributes(\n        output_dir=output_dir,\n        delimiter=delimiter,\n        names=names,\n        pad=pad,\n        strip_chunks=strip_chunks,\n    )\n    if not self.chunks:\n        raise LexosException(\"No chunks to save.\")\n    if self.names:\n        if len(self.names) != len(self.chunks):\n            raise LexosException(\n                f\"The number of docs in `names` ({len(self.names)}) must equal the number of docs in `chunks` ({len(self.chunks)}).\"\n            )\n    else:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    for i, doc in enumerate(self.chunks):\n        for num, chunk in enumerate(doc):\n            if strip_chunks:\n                chunk = chunk.strip()\n            self._write_chunk(self.names[i], num + 1, chunk, Path(output_dir))\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.split","title":"<code>split(docs: Optional[Path | str | list[Path | str]] = None, chunksize: Optional[int] = None, names: Optional[str | list[str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, n: Optional[int] = None, newline: Optional[bool] = None, overlap: Optional[int] = None, by_bytes: Optional[bool] = None, file: Optional[bool] = False, merge_threshold: Optional[float] = 0.5, merge_final: Optional[bool] = False) -&gt; list[list[str]]</code>","text":"<p>Chunk the file or buffer.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Optional[Path | str | list[Path | str]]</code> <p>The file path or buffer.</p> <code>None</code> <code>chunksize</code> <code>Optional[int]</code> <p>The size of the chunks in characters (or bytes if by_bytes=True).</p> <code>None</code> <code>names</code> <code>Optional[str | list[str | None]]</code> <p>The doc names.</p> <code>None</code> <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>Optional[int]</code> <p>The padding for the chunk names.</p> <code>3</code> <code>n</code> <code>Optional[int]</code> <p>The number of chunks.</p> <code>None</code> <code>newline</code> <code>Optional[bool]</code> <p>Whether to chunk by lines.</p> <code>None</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of characters to overlap between chunks.</p> <code>None</code> <code>by_bytes</code> <code>Optional[bool]</code> <p>Whether to chunk by bytes instead of characters.</p> <code>None</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to chunk the file or buffer.</p> <code>False</code> <code>merge_threshold</code> <code>Optional[float]</code> <p>The threshold for merging the last two chunks.</p> <code>0.5</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to merge the last two chunks.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: A list of chunked strings.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef split(\n    self,\n    docs: Optional[Path | str | list[Path | str]] = None,\n    chunksize: Optional[int] = None,\n    names: Optional[str | list[str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    n: Optional[int] = None,\n    newline: Optional[bool] = None,\n    overlap: Optional[int] = None,\n    by_bytes: Optional[bool] = None,\n    file: Optional[bool] = False,\n    merge_threshold: Optional[float] = 0.5,\n    merge_final: Optional[bool] = False,\n) -&gt; list[list[str]]:\n    \"\"\"Chunk the file or buffer.\n\n    Args:\n        docs (Optional[Path | str | list[Path | str]]): The file path or buffer.\n        chunksize (Optional[int]): The size of the chunks in characters (or bytes if by_bytes=True).\n        names (Optional[str | list[str | None]]): The doc names.\n        delimiter (Optional[str]): The delimiter to use for the chunk names.\n        pad (Optional[int]): The padding for the chunk names.\n        n (Optional[int]): The number of chunks.\n        newline (Optional[bool]): Whether to chunk by lines.\n        overlap (Optional[int]): The number of characters to overlap between chunks.\n        by_bytes (Optional[bool]): Whether to chunk by bytes instead of characters.\n        file (Optional[bool]): Whether to chunk the file or buffer.\n        merge_threshold (Optional[float]): The threshold for merging the last two chunks.\n        merge_final (Optional[bool]): Whether to merge the last two chunks.\n\n    Returns:\n        list[list[str]]: A list of chunked strings.\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        n=n,\n        newline=newline,\n        overlap=overlap,\n        by_bytes=by_bytes if by_bytes is not None else self.by_bytes,\n        merge_threshold=merge_threshold,\n        merge_final=merge_final,\n        delimiter=delimiter,\n        pad=pad,\n    )\n    if chunksize:\n        self.chunksize = chunksize\n    if names:\n        self.names = ensure_list(names)\n    elif file:\n        self.names = [Path(name).stem for name in ensure_list(docs)]\n    else:\n        self.names = [\n            f\"doc{str(i).zfill(self.pad)}\" for i in range(1, len(self.docs) + 1)\n        ]\n    for doc in self.docs:\n        split_by_num = False\n        if isinstance(self.n, int):\n            split_by_num = True\n        chunks = (\n            self._process_file(doc, n=split_by_num)\n            if file\n            else self._process_buffer(doc, n=split_by_num)\n        )\n        # Calculate the threshold here.\n        threshold = self.chunksize * self.merge_threshold\n        if chunks and (self.merge_final is True or len(chunks[-1]) &lt; threshold):\n            chunks = list(self._merge_final_chunks(chunks))\n        # Apply overlap if specified\n        if self.overlap and chunks:\n            chunks = self._apply_overlap(chunks)\n        self.chunks.append(chunks)\n\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.split_on_milestones","title":"<code>split_on_milestones(milestones: list[StringSpan], docs: Optional[Path | str | list[Path | str]] = None, names: Optional[Path | str | list[Path | str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, keep_spans: Optional[bool | str] = False, strip: Optional[bool] = True, file: Optional[bool] = False) -&gt; list[list[str]]</code>","text":"<p>Split text at each milestone span, optionally retaining the milestone text.</p> <p>Parameters:</p> Name Type Description Default <code>milestones</code> <code>list[StringSpan]</code> <p>A list of milestone StringSpans to split the text at.</p> required <code>docs</code> <code>Optional[Path | str | list[Path | str]]</code> <p>List of file paths or buffers.</p> <code>None</code> <code>names</code> <code>Optional[Path | str | list[Path | str]]</code> <p>List of doc names.</p> <code>None</code> <code>delimiter</code> <code>Optional[str]</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>Optional[int]</code> <p>The padding for the chunk names.</p> <code>3</code> <code>keep_spans</code> <code>Optional[bool | str]</code> <p>Whether to keep the spans in the split strings. Defaults to False.</p> <code>False</code> <code>strip</code> <code>Optional[bool]</code> <p>Whether to strip the text. Defaults to True.</p> <code>True</code> <code>file</code> <code>Optional[bool]</code> <p>Set to True if reading from file(s), False for strings.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: A list of chunked strings.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef split_on_milestones(\n    self,\n    milestones: list[StringSpan],\n    docs: Optional[Path | str | list[Path | str]] = None,\n    names: Optional[Path | str | list[Path | str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    keep_spans: Optional[bool | str] = False,\n    strip: Optional[bool] = True,\n    file: Optional[bool] = False,\n) -&gt; list[list[str]]:\n    \"\"\"Split text at each milestone span, optionally retaining the milestone text.\n\n    Args:\n        milestones (list[StringSpan]): A list of milestone StringSpans to split the text at.\n        docs (Optional[Path | str | list[Path | str]]): List of file paths or buffers.\n        names (Optional[Path | str | list[Path | str]]): List of doc names.\n        delimiter (Optional[str]): The delimiter to use for the chunk names.\n        pad (Optional[int]): The padding for the chunk names.\n        keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n        strip (Optional[bool]): Whether to strip the text. Defaults to True.\n        file (Optional[bool]): Set to True if reading from file(s), False for strings.\n\n    Returns:\n        list[list[str]]: A list of chunked strings.\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        delimiter=delimiter,\n        pad=pad,\n    )\n    if names:\n        self.names = ensure_list(names)\n    elif file:\n        self.names = [Path(name).stem for name in ensure_list(docs)]\n    else:\n        self.names = [\n            f\"doc{str(i).zfill(self.pad)}\" for i in range(1, len(self.docs) + 1)\n        ]\n    for doc in self.docs:\n        text = doc\n        if file:\n            try:\n                with open(doc, \"r\", newline=\"\") as f:\n                    text = f.read()\n            except BaseException as e:\n                raise LexosException(e)\n        chunks = []\n        start = 0\n        for i, span in enumerate(milestones):\n            end = span.start\n            # Preceding: milestone text goes at end of previous chunk\n            if keep_spans == \"preceding\":\n                chunk = text[start:end] + text[span.start : span.end + 1]\n                chunks.append(chunk)\n            # Following: milestone text goes at start of next chunk\n            elif keep_spans == \"following\":\n                chunk = text[start:end]\n                # Store the milestone text to prepend to the next chunk\n                chunks.append(chunk)\n            else:\n                chunk = text[start:end]\n                chunks.append(chunk)\n            start = span.end + 1\n        # Last chunk\n        last_chunk = text[start:]\n        if keep_spans == \"following\" and milestones:\n            # Prepend each milestone text to the next chunk (except the first chunk)\n            for idx in range(1, len(chunks)):\n                milestone_text = text[\n                    milestones[idx - 1].start : milestones[idx - 1].end + 1\n                ]\n                chunks[idx] = milestone_text + chunks[idx]\n            # Also prepend the last milestone to the last chunk\n            last_span = milestones[-1]\n            last_chunk = text[last_span.start : last_span.end + 1] + last_chunk\n        chunks.append(last_chunk)\n        if strip:\n            chunks = [doc.strip() for doc in chunks]\n        self.chunks.append(chunks)\n\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/text_cutter/#lexos.cutter.text_cutter.TextCutter.to_dict","title":"<code>to_dict(names: Optional[Path | str | list[Path | str]] = None) -&gt; dict[str, list[str]]</code>","text":"<p>Return the chunks as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Optional[Path | str | list[Path | str]]</code> <p>The doc names.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: The chunks as a dictionary.</p> Source code in <code>lexos/cutter/text_cutter.py</code> <pre><code>@validate_call\ndef to_dict(\n    self, names: Optional[Path | str | list[Path | str]] = None\n) -&gt; dict[str, list[str]]:\n    \"\"\"Return the chunks as a dictionary.\n\n    Args:\n        names (Optional[Path | str | list[Path | str]]): The doc names.\n\n    Returns:\n        dict[str, list[str]]: The chunks as a dictionary.\n    \"\"\"\n    if names:\n        self.names = ensure_list(names)\n    if self.names == [] or self.names is None:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    return {str(doc): chunks for doc, chunks in zip(self.names, self.chunks)}\n</code></pre>"},{"location":"api/cutter/token_cutter/","title":"Token_Cutter","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter","title":"<code>TokenCutter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TokenCutter class for chunking spaCy Doc objects into smaller segments.</p> <p>based on token count, line breaks, sentences, or custom milestones. Supports overlapping, merging, and export to disk.</p> <p>Config:</p> <ul> <li><code>default</code>: <code>validation_config</code></li> </ul> <p>Fields:</p> <ul> <li> <code>chunks</code>                 (<code>list[list[Doc]]</code>)             </li> <li> <code>docs</code>                 (<code>Optional[Doc | list[Doc] | Path | str | list[Path | str]]</code>)             </li> <li> <code>chunksize</code>                 (<code>Optional[int]</code>)             </li> <li> <code>n</code>                 (<code>Optional[int]</code>)             </li> <li> <code>names</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>newline</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>merge_threshold</code>                 (<code>Optional[float]</code>)             </li> <li> <code>overlap</code>                 (<code>Optional[int]</code>)             </li> <li> <code>output_dir</code>                 (<code>Optional[Path | str]</code>)             </li> <li> <code>delimiter</code>                 (<code>str</code>)             </li> <li> <code>pad</code>                 (<code>int</code>)             </li> <li> <code>strip_chunks</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>class TokenCutter(BaseModel, validate_assignment=True):\n    \"\"\"TokenCutter class for chunking spaCy Doc objects into smaller segments.\n\n    based on token count, line breaks, sentences, or custom milestones.\n    Supports overlapping, merging, and export to disk.\n    \"\"\"\n\n    chunks: list[list[Doc]] = Field(default=[], description=\"The list of chunks.\")\n\n    docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = Field(\n        default=None,\n        description=\"The documents to be split.\",\n    )\n    chunksize: Optional[int] = Field(\n        default=1000, gt=0, description=\"The desired chunk size in tokens.\"\n    )\n    n: Optional[int] = Field(\n        default=None,\n        # gt=0, Removed to allow runtime validation via LexosException instead of Pydantic pre-validation for testing coverage.\n        description=\"The number of chunks or the number of lines or sentences per chunk.\",\n    )\n    names: Optional[list[str]] = Field(\n        default=[], description=\"A list of names for the source docs.\"\n    )\n    newline: Optional[bool] = Field(\n        default=False, description=\"Whether to chunk by lines.\"\n    )\n    merge_threshold: Optional[float] = Field(\n        default=0.5, ge=0, le=1, description=\"The threshold to merge the last segment.\"\n    )\n    overlap: Optional[int] = Field(\n        default=None, gt=0, description=\"The number of tokens to overlap.\"\n    )\n    output_dir: Optional[Path | str] = Field(\n        default=None, description=\"The output directory to save the chunks to.\"\n    )\n    delimiter: str = Field(\n        default=\"_\", description=\"The delimiter to use for the chunk names.\"\n    )\n    pad: int = Field(default=3, gt=0, description=\"The padding for the chunk names.\")\n    strip_chunks: bool = Field(\n        default=True,\n        description=\"Whether to strip leading and trailing whitespace in the chunks.\",\n    )\n\n    model_config = validation_config\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Iterate over the object's chunks.\n\n        Returns:\n            Iterator: An iterator containing the object's chunks.\n        \"\"\"\n        return iter(self.chunks)\n\n    def __len__(self):\n        \"\"\"Return the number of source docs in the instance.\"\"\"\n        if not self.docs:\n            return 0\n        return len(self.docs)\n\n    @staticmethod\n    def list_start_end_indexes(arrays: list[np.ndarray]) -&gt; list[tuple[int, int]]:\n        \"\"\"List start and end indexes for a list of numpy arrays.\n\n        Args:\n            arrays (list[np.ndarray]): List of numpy arrays.\n\n        Returns:\n            list[tuple[int, int]]: List of tuples with start and end indexes.\n        \"\"\"\n        indexes = []\n        start = 0\n\n        for array in arrays:\n            end = start + len(array)\n            indexes.append((start, end))\n            start = end\n\n        return indexes\n\n    def _apply_merge_threshold(\n        self, chunks: list[Doc], force: bool = False\n    ) -&gt; list[Doc]:\n        \"\"\"Apply the merge threshold to the last chunk.\n\n        Args:\n            chunks (list[Doc]): The list of chunks.\n            force (bool, optional): Whether to force the merge. Defaults to False.\n\n        Returns:\n            list[Doc]: The list of chunks with the last chunk merged if necessary.\n\n        Notes:\n          - Whitespace is supplied between merged chunks.\n          - Length of final chunk is measured in number tokens or number of sentences.\n        \"\"\"\n        if len(chunks) == 1:\n            return chunks\n        merge_threshold = (\n            self.merge_threshold if self.merge_threshold is not None else 0.5\n        )\n        if isinstance(self.n, int):\n            threshold = max([len(chunk) for chunk in chunks]) * merge_threshold\n        else:\n            threshold = (\n                self.chunksize if self.chunksize is not None else 1\n            ) * merge_threshold\n        # If the length of the last chunk &lt; threshold, merge it with the previous chunk\n        if force is True or len(chunks[-1]) &lt; threshold:\n            # Get rid of the last chunk\n            last_chunk = chunks.pop(-1)\n            # Combine the last two segments into a single doc\n            chunks[-1] = Doc.from_docs([chunks[-1], last_chunk])\n        return chunks\n\n    def _apply_overlap(\n        self,\n        chunks: list[Doc],\n    ) -&gt; list[Doc]:\n        \"\"\"Create overlapping chunks.\n\n        Args:\n            chunks (list[Doc]): A list of spaCy docs.\n\n        Returns:\n            list[Doc]: A list of spaCy docs.\n        \"\"\"\n        overlapped_chunks = []\n        for i, chunk in enumerate(chunks):\n            if i &lt; len(chunks) - 1:\n                overlap_doc = chunks[i + 1][: self.overlap].as_doc()\n                overlapped_doc = Doc.from_docs([chunk, overlap_doc])\n                overlapped_chunks.append(overlapped_doc)\n            elif i == len(chunks) - 1:\n                overlapped_chunks.append(chunk)\n        return overlapped_chunks\n\n    def _chunk_doc(\n        self,\n        doc: Doc,\n        attrs: \"Sequence[int | str]\" = SPACY_ATTRS,\n        header: Sequence[int | str] = ENTITY_HEADER,\n    ) -&gt; list[Doc]:\n        \"\"\"Split a Doc into chunks.\n\n        Args:\n            doc: The Doc to split.\n            attrs: The attributes to include in the chunks.\n            header: The NER attributes to include in the chunks.\n\n        Returns:\n            list[Doc]: List of Doc chunks.\n        \"\"\"\n        # Check that the document is not empty\n        if len(doc) == 0:\n            raise LexosException(\"Document is empty.\")\n\n        # Return the whole doc if it is less than the chunksize\n        if self.n is None and self.chunksize is not None and len(doc) &lt;= self.chunksize:\n            return [doc]\n\n        # Get the names of the custom extensions\n        extension_names = [name for name in doc[0]._.__dict__[\"_extensions\"].keys()]\n\n        # Split the doc into n chunks\n        if isinstance(self.n, int):\n            chunks_arr = np.array_split(doc.to_array(list(attrs)), self.n)\n            # If there is only one chunk, skip the rest of the function\n            if len(chunks_arr) == 1:\n                return [doc]\n        else:\n            chunks_arr = np.array_split(\n                doc.to_array(list(attrs)),\n                np.arange(self.chunksize, len(attrs), self.chunksize),\n            )\n            # Remove empty elements\n            chunks_arr = [x for x in chunks_arr if x.size &gt; 0]\n\n        # Create a list to hold the chunks and get the chunk indexes\n        chunks = []\n        chunk_indexes = TokenCutter.list_start_end_indexes(chunks_arr)\n\n        # Iterate over the chunks\n        for i, chunk in enumerate(chunks_arr):\n            # Get chunk start and end indexes\n            start = chunk_indexes[i][0]\n            end = chunk_indexes[i][1]\n            span = doc[start:end]\n            words = [token.text for token in span]\n\n            # Make a new doc for the chunk\n            new_doc = Doc(doc.vocab, words=words)\n\n            # Add the attributes to the new chunk doc\n            new_doc.from_array(list(attrs), chunk)\n\n            # Add entities to the new chunk doc\n            if doc.ents and len(doc.ents) &gt; 0:\n                ent_array = np.empty((len(chunk), len(header)), dtype=\"uint64\")\n                for i, token in enumerate(span):\n                    ent_array[i, 0] = token.ent_iob\n                    ent_array[i, 1] = token.ent_type\n                new_doc.from_array(list(header), ent_array)\n\n            # Add custom attributes to doc\n            if len(extension_names) &gt; 0:\n                for i, token in enumerate(span):\n                    for ext in extension_names:\n                        new_doc[i]._.set(ext, token._.get(ext))\n\n            # Add the chunk to the chunks list\n            chunks.append(new_doc)\n\n        # Return the list of chunks\n        return chunks\n\n    def _keep_milestones_bool(\n        self, doc: Doc, milestones: list[Span], keep_spans: bool = False\n    ) -&gt; list[Doc]:\n        \"\"\"Split a spaCy Doc into chunks on milestones, optionally keeping milestones.\n\n        Args:\n            doc (Doc): The spaCy Doc to split.\n            milestones (list[Span]): The milestones to split on.\n            keep_spans (bool): Whether to keep the spans in the split strings.\n\n        Returns:\n            list[Doc]: A list of spaCy Docs.\n        \"\"\"\n        chunks = []\n        start = 0\n        for span in milestones:\n            if span.start == 0 or span.end == doc[-1].i:\n                if keep_spans:\n                    chunks.append(span)\n            elif start &lt; span.start:\n                chunks.append(doc[start : span.start])\n                if keep_spans:\n                    chunks.append(span)\n            start = span.end\n        if start &lt; len(doc):\n            chunks.append(doc[start:])\n        return chunks\n\n    def _keep_milestones_following(self, doc: Doc, milestones: list[Span]) -&gt; list[Doc]:\n        \"\"\"Split a spaCy Doc into chunks on milestones preserving milestones in the following chunk.\n\n        Args:\n            doc (Doc): The spaCy Doc to split.\n            milestones (list[Span]): The milestones to split on.\n\n        Returns:\n            list[Doc]: A list of spaCy Docs.\n        \"\"\"\n        chunks = []\n        start = 0\n        for index, span in enumerate(milestones):\n            # Text before milestone\n            if start &lt; span.start:\n                chunks.append(doc[start : span.start])\n\n            # Find end of chunk (next milestone or doc end)\n            end = (\n                milestones[index + 1].start if index &lt; len(milestones) - 1 else len(doc)\n            )\n\n            # Milestone + following text as one chunk\n            chunks.append(doc[span.start : end])\n            start = end\n        return chunks\n\n    def _keep_milestones_preceding(self, doc: Doc, milestones: list[Span]) -&gt; list[Doc]:\n        \"\"\"Split a spaCy Doc into chunks on milestones preserving milestones in the preceding chunk.\n\n        Args:\n            doc (Doc): The spaCy Doc to split.\n            milestones (list[Span]): The milestones to split on.\n\n        Returns:\n            list[Doc]: A list of spaCy Docs.\n        \"\"\"\n        # Check that the document is not empty\n        if len(doc) == 0:\n            raise LexosException(\"Document is empty.\")\n        if len(milestones) == 0:\n            return [doc]\n        chunks = []\n        start = 0\n        for span in milestones:\n            index = span.start\n            if index != -1:\n                chunks.append(doc[start : index + len(span)])\n                start = index + len(span)\n        if start &lt; len(doc):\n            chunks.append(doc[start:])\n        if milestones[0].start == 0:\n            _ = chunks.pop(0)\n            chunks[0] = doc[: chunks[0].end]\n        return chunks\n\n    def _set_attributes(self, **data) -&gt; None:\n        \"\"\"Set attributes after initialization.\"\"\"\n        for key, value in data.items():\n            setattr(self, key, value)\n\n    def _split_doc(\n        self,\n        doc: Doc,\n        attrs: Optional[Sequence[int | str]] = SPACY_ATTRS,\n        merge_final: Optional[bool] = False,\n    ) -&gt; list[Doc]:\n        \"\"\"Split a spaCy doc into chunks by a fixed number of tokens.\n\n        Args:\n            doc (Doc): A spaCy doc.\n            attrs (Optional[int | str]): The spaCy attributes to include in the chunks.\n            merge_final (Optional[bool]): Whether to merge the final segment.\n\n        Returns:\n            list[Doc]: A list of spaCy docs.\n        \"\"\"\n        if len(doc) == 0:\n            raise LexosException(\"Document is empty.\")\n\n        attrs = attrs if attrs is not None else SPACY_ATTRS\n        chunks = self._chunk_doc(doc, attrs)\n        chunks = self._apply_merge_threshold(\n            chunks, force=merge_final if merge_final is not None else False\n        )\n        if self.overlap:\n            chunks = self._apply_overlap(chunks)\n        if self.strip_chunks:\n            return [strip_doc(chunk) for chunk in chunks]\n        # Ensure that all chunks are spaCy docs\n        else:\n            return [\n                chunk.as_doc() if isinstance(chunk, Span) else chunk for chunk in chunks\n            ]\n\n    def _split_doc_by_lines(\n        self, doc: Doc, merge_final: Optional[bool] = False\n    ) -&gt; list[Doc]:\n        \"\"\"Split a spaCy Doc into chunks of n lines.\n\n        Args:\n            doc: spaCy Doc to split.\n            merge_final: Whether to merge the final segment.\n\n        Returns:\n            list[Doc]: Chunks of the doc split by lines.\n        \"\"\"\n        if len(doc) == 0:\n            raise LexosException(\"Document is empty.\")\n\n        indices = []  # The indices immediately following the newline tokens\n        count = 0\n        chunks = []\n        for token in doc:\n            if \"\\n\" in token.text:\n                count += 1\n                if (\n                    self.n is not None and count % self.n == 0\n                ):  # Check if it's the nth occurrence\n                    indices.append(token.i + 1)\n        if len(indices) == 0:\n            chunks.append(doc)\n        else:\n            prev_index = 0\n            for index in indices:\n                chunks.append(doc[prev_index:index].as_doc())\n                prev_index = index\n            chunks.append(doc[prev_index:].as_doc())  # Append the remaining elements\n\n        # Ensure there are no empty docs\n        chunks = [chunk for chunk in chunks if len(chunk) &gt; 0]\n\n        # Apply the merge threshold and overlap\n        chunks = self._apply_merge_threshold(\n            chunks, force=merge_final if merge_final is not None else False\n        )\n        if self.overlap:\n            chunks = self._apply_overlap(chunks)\n\n        if self.strip_chunks:\n            return [strip_doc(chunk) for chunk in chunks]\n\n        return chunks\n\n    def _split_doc_by_sentences(\n        self, doc: Doc, merge_final: Optional[bool] = False\n    ) -&gt; list[Doc]:\n        \"\"\"Split a spaCy Doc into chunks of n sentences.\n\n        Args:\n            doc: A spaCy Doc object.\n            merge_final: Whether to merge the final segment.\n\n        Returns:\n            Doc: Chunks containing n sentences each (last chunk may have fewer).\n        \"\"\"\n        if len(doc) == 0:\n            raise LexosException(\"Document is empty.\")\n\n        try:\n            next(doc.sents)\n        except (StopIteration, ValueError):\n            raise LexosException(\"The document has no assigned sentences.\")\n\n        # Split the doc into chunks of n sentences\n        sents = list(doc.sents)\n        chunks = []\n        n = self.n if self.n is not None else 1\n        for i in range(0, len(sents), n):\n            chunk_sents = sents[i : i + n]\n            start_idx = chunk_sents[0].start\n            end_idx = chunk_sents[-1].end\n            chunks.append(doc[start_idx:end_idx].as_doc())\n        # No need to append doc[end_idx:] since all sentences are already included in the chunks\n\n        # Ensure there are no empty docs\n        chunks = [chunk for chunk in chunks if len(chunk) &gt; 0]\n\n        # Apply the merge threshold and overlap\n        chunks = self._apply_merge_threshold(\n            chunks, force=merge_final if merge_final is not None else False\n        )\n        if self.overlap:\n            chunks = self._apply_overlap(chunks)\n\n        if self.strip_chunks:\n            return [strip_doc(chunk) for chunk in chunks]\n\n        return chunks\n\n    def _split_doc_on_milestones(\n        self,\n        doc: Doc,\n        milestones: Span | list[Span],\n        keep_spans: Optional[bool | str] = False,\n        merge_final: Optional[bool] = False,\n    ) -&gt; list[Doc]:\n        \"\"\"Split document on a milestone.\n\n        Args:\n            doc (Doc): The document to be split.\n            milestones (Span | list[Span]): A Span or list of Spans to be matched.\n            keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n            merge_final (Optional[bool]): Whether to force the merge of the last segment. Defaults to False.\n\n        Returns:\n            list[Doc]: A list of chunked spaCy Doc objects.\n        \"\"\"\n        if len(doc) == 0:\n            raise LexosException(\"Document is empty.\")\n\n        milestones = ensure_list(milestones)\n        if keep_spans == \"following\":\n            chunks = self._keep_milestones_following(doc, milestones)\n        elif keep_spans == \"preceding\":\n            chunks = self._keep_milestones_preceding(doc, milestones)\n        else:\n            # Only pass a boolean to keep_spans\n            chunks = self._keep_milestones_bool(\n                doc, milestones, keep_spans=bool(keep_spans)\n            )\n\n        # Ensure that all chunks are spaCy docs\n        chunks = [\n            chunk.as_doc() if isinstance(chunk, Span) else chunk for chunk in chunks\n        ]\n\n        # Apply the merge threshold and overlap\n        chunks = self._apply_merge_threshold(\n            chunks, force=merge_final if merge_final is not None else False\n        )\n        if self.overlap:\n            chunks = self._apply_overlap(chunks)\n\n        if self.strip_chunks:\n            return [strip_doc(chunk) for chunk in chunks]\n\n        return chunks\n\n    def _write_chunk(\n        self, path: str, n: int, chunk: Doc, output_dir: Path, as_text: bool = True\n    ) -&gt; None:\n        \"\"\"Write chunk text to file with formatted name.\n\n        Args:\n            path (str): The path of the original file.\n            n (int): The number of the chunk.\n            chunk (Doc): The chunk to save.\n            output_dir (Path): The output directory for the chunk.\n            as_text (bool): Whether to save the chunk as a text file or a spaCy Doc object.\n        \"\"\"\n        output_file = f\"{path}{self.delimiter}{str(n).zfill(self.pad)}.txt\"\n        output_path = output_dir / output_file\n        if as_text:\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(chunk.text)\n        else:\n            chunk.to_disk(output_path)\n\n    def merge(self, chunks: list[Doc]) -&gt; Doc:\n        \"\"\"Merge a list of chunks into a single Doc.\n\n        Args:\n            chunks (list[Doc]): The list of chunks to merge.\n\n        Returns:\n            Doc: The merged doc.\n\n        Note:\n            - The user_data dict of the docs will be ignored. If they contain information\n              that needs to be preserved, it should be stored in the doc extensions.\n              See https://github.com/explosion/spaCy/discussions/9106.\n        \"\"\"\n        if len(chunks) == 0:\n            raise LexosException(\"No chunks to merge.\")\n        return Doc.from_docs(chunks)\n\n    @validate_call(config=validation_config)\n    def save(\n        self,\n        output_dir: Path | str,\n        names: Optional[str | list[str]] = None,\n        delimiter: Optional[str] = \"_\",\n        pad: Optional[int] = 3,\n        strip_chunks: Optional[bool] = True,\n        as_text: Optional[bool] = True,\n    ) -&gt; None:\n        \"\"\"Save the chunks to disk.\n\n        Args:\n            output_dir (Path | str): The output directory to save the chunks to.\n            names (Optional[str | list[str]]): The doc names.\n            delimiter (str): The delimiter to use for the chunk names.\n            pad (int): The padding for the chunk names.\n            strip_chunks (bool): Whether to strip leading and trailing whitespace in the chunks.\n            as_text (Optional[bool]): Whether to save the chunks as text files or spaCy Doc objects (bytes).\n        \"\"\"\n        self._set_attributes(\n            output_dir=output_dir,\n            delimiter=delimiter,\n            names=names,\n            pad=pad,\n            strip_chunks=strip_chunks,\n        )\n        if not self.chunks or self.chunks == []:\n            raise LexosException(\"No chunks to save.\")\n        if self.names:\n            if len(self.names) != len(self.chunks):\n                raise LexosException(\n                    f\"The number of docs in `names` ({len(self.names)}) must equal the number of docs in `chunks` ({len(self.chunks)}).\"\n                )\n        elif self.names == [] or self.names is None:\n            self.names = [\n                f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n            ]\n        for i, doc in enumerate(self.chunks):\n            for num, chunk in enumerate(doc):\n                if strip_chunks:\n                    chunk = strip_doc(chunk)\n                self._write_chunk(\n                    self.names[i], num + 1, chunk, Path(output_dir), as_text\n                )\n\n    @validate_call(config=validation_config)\n    def split(\n        self,\n        docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None,\n        chunksize: Optional[int] = None,\n        n: Optional[int] = None,\n        merge_threshold: Optional[float] = 0.5,\n        overlap: Optional[int] = None,\n        names: Optional[str | list[str]] = None,\n        newline: Optional[bool] = None,\n        strip_chunks: Optional[bool] = True,\n        file: Optional[bool] = False,\n        model: Optional[str] = None,\n        merge_final: Optional[bool] = False,\n    ) -&gt; list[list[Doc]]:\n        \"\"\"Split spaCy docs into chunks by a fixed number of tokens.\n\n        Args:\n            docs (Optional[Doc | list[Doc] | Path | str | list[Path | str]]): A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().\n            chunksize (Optional[int]): The number of tokens to split on.\n            n (Optional[int]): The number of chunks to produce.\n            merge_threshold (Optional[float]): The threshold to merge the last segment.\n            overlap (Optional[int]): The number of tokens to overlap.\n            names (Optional[str | list[str]]): The doc names.\n            newline (Optional[bool]): Whether to chunk by lines.\n            strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n            file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n            model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n            merge_final (Optional[bool]): Whether to force the merge of the last segment.\n\n        Returns:\n            list[list[Doc]]: A list of spaCy docs (chunks).\n        \"\"\"\n        if docs:\n            self.docs = ensure_list(docs)\n        if not self.docs:\n            raise LexosException(\"No documents provided for splitting.\")\n        self._set_attributes(\n            chunksize=chunksize,\n            n=n,\n            merge_threshold=merge_threshold,\n            overlap=overlap,\n            names=names,\n            newline=newline,\n            strip_chunks=strip_chunks,\n        )\n\n        # Load docs from files if file=True\n        if file:\n            if model is None:\n                raise LexosException(\"model parameter is required when file=True\")\n            nlp = spacy.load(model)\n            loaded_docs = []\n            for doc in ensure_list(docs):\n                try:\n                    doc = Doc(nlp.vocab).from_disk(doc)\n                except ValueError:\n                    raise LexosException(\n                        f\"Error loading doc from disk. Doc file must be in a valid spaCy serialization format: see https://spacy.io/api/doc#to_disk\"\n                    )\n                loaded_docs.append(doc)\n            docs = loaded_docs\n\n        if self.newline:\n            if not self.n:\n                self.n = self.chunksize\n            if not self.n or self.n &lt; 1:\n                raise LexosException(\"n must be greater than 0.\")\n            for doc in ensure_list(docs):\n                self.chunks.append(\n                    self._split_doc_by_lines(doc, merge_final=merge_final)\n                )\n        else:\n            for doc in ensure_list(docs):\n                self.chunks.append(self._split_doc(doc, merge_final=merge_final))\n\n        return self.chunks\n\n    @validate_call(config=validation_config)\n    def split_on_milestones(\n        self,\n        milestones: Span | list[Span],\n        docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None,\n        merge_threshold: Optional[float] = 0.5,\n        merge_final: Optional[bool] = False,\n        overlap: Optional[int] = None,\n        keep_spans: Optional[bool | str] = False,\n        strip_chunks: Optional[bool] = True,\n        names: Optional[str | list[str]] = None,\n        file: Optional[bool] = False,\n        model: Optional[str] = None,\n    ) -&gt; list[list[Doc]]:\n        \"\"\"Split document on a milestone.\n\n        Args:\n            milestones (Span | list[Span]): A milestone span or list of milestone spans to be matched.\n            docs (Optional[Doc | list[Doc] | Path | str | list[Path | str]]): The document(s) to be split, or file paths to spaCy docs saved with Doc.to_disk().\n            merge_threshold (Optional[float]): The threshold to merge the last segment.\n            merge_final (Optional[bool]): Whether to force the merge of the last segment.\n            overlap (Optional[int]): The number of tokens to overlap.\n            keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n            strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n            names (Optional[str | list[str]]): The doc names.\n            file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n            model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n\n        Returns:\n            list[list[Doc]]: A list of spaCy docs (chunks).\n        \"\"\"\n        if docs:\n            self.docs = ensure_list(docs)\n        if not self.docs:\n            raise LexosException(\"No documents provided for splitting.\")\n        self._set_attributes(\n            merge_threshold=merge_threshold,\n            overlap=overlap,\n            strip_chunks=strip_chunks,\n            names=names,\n        )\n\n        # Load docs from files if file=True\n        if file:\n            if model is None:\n                raise LexosException(\"model parameter is required when file=True\")\n            nlp = spacy.load(model)\n            loaded_docs = []\n            for doc in ensure_list(docs):\n                doc = Doc(nlp.vocab).from_disk(doc)\n                loaded_docs.append(doc)\n            docs = loaded_docs\n\n        for doc in ensure_list(docs):\n            chunks = self._split_doc_on_milestones(\n                doc, milestones, keep_spans=keep_spans, merge_final=merge_final\n            )\n            self.chunks.append(chunks)\n        return self.chunks\n\n    @validate_call(config=validation_config)\n    def split_on_sentences(\n        self,\n        docs: Doc | list[Doc] | Path | str | list[Path | str],\n        n: Optional[int] = None,\n        merge_final: Optional[bool] = False,\n        overlap: Optional[int] = None,\n        strip_chunks: Optional[bool] = True,\n        names: Optional[str | list[str]] = None,\n        file: Optional[bool] = False,\n        model: Optional[str] = None,\n    ) -&gt; list[list[Doc]]:\n        \"\"\"Split spaCy docs into chunks by a fixed number of sentences.\n\n        Args:\n            docs (Doc | list[Doc] | Path | str | list[Path | str]): A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().\n            n (Optional[int]): The number of sentences per chunk.\n            merge_final (Optional[bool]): Whether to merge the last segment.\n            overlap (Optional[int]): The number of tokens to overlap.\n            strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n            names (Optional[str | list[str]]): The doc names.\n            file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n            model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n\n        Returns:\n            list[list[Doc]]: A list of spaCy docs (chunks).\n\n        Raises:\n            ValueError: If n is less than or equal to 0.\n            ValueError: If the model has no sentences.\n        \"\"\"\n        self._set_attributes(\n            n=n,\n            overlap=overlap,\n            strip_chunks=strip_chunks,\n            names=names,\n        )\n\n        # Load docs from files if file=True\n        if file:\n            if model is None:\n                raise LexosException(\"model parameter is required when file=True\")\n            nlp = spacy.load(model)\n            loaded_docs = []\n            for doc in ensure_list(docs):\n                doc = Doc(nlp.vocab).from_disk(doc)\n                loaded_docs.append(doc)\n            docs = loaded_docs\n\n        if not self.n:\n            self.n = self.chunksize\n        if not self.n or self.n &lt; 1:\n            raise LexosException(\"n must be greater than 0.\")\n        for i, doc in enumerate(ensure_list(docs)):\n            if not doc.has_annotation(\"SENT_START\"):\n                raise LexosException(\n                    f\"The spaCy model used to create the Doc {i} does not have sentence boundary detection. Please use a model that includes the 'senter' or 'parser' pipeline component.\"\n                )\n            else:\n                next(doc.sents)\n            self.chunks.append(\n                self._split_doc_by_sentences(doc, merge_final=merge_final)\n            )\n        return self.chunks\n\n    @validate_call(config=validation_config)\n    def to_dict(self, names: Optional[list[str]] = None) -&gt; dict[str, list[str]]:\n        \"\"\"Return the chunks as a dictionary.\n\n        Args:\n            names (Optional[list[str]]): A list of names for the doc Docs.\n\n        Returns:\n            dict[str, list[str]]: The chunks as a dictionary.\n        \"\"\"\n        if names:\n            self.names = names\n        if not self.names:\n            self.names = [\n                f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n            ]\n        return {\n            str(name): [chunk.text for chunk in chunks]\n            for name, chunks in zip(self.names, self.chunks)\n        }\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.chunks","title":"<code>chunks: list[list[Doc]] = []</code>  <code>pydantic-field</code>","text":"<p>The list of chunks.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.chunksize","title":"<code>chunksize: Optional[int] = 1000</code>  <code>pydantic-field</code>","text":"<p>The desired chunk size in tokens.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.delimiter","title":"<code>delimiter: str = '_'</code>  <code>pydantic-field</code>","text":"<p>The delimiter to use for the chunk names.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.docs","title":"<code>docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None</code>  <code>pydantic-field</code>","text":"<p>The documents to be split.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.merge_threshold","title":"<code>merge_threshold: Optional[float] = 0.5</code>  <code>pydantic-field</code>","text":"<p>The threshold to merge the last segment.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.n","title":"<code>n: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The number of chunks or the number of lines or sentences per chunk.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.names","title":"<code>names: Optional[list[str]] = []</code>  <code>pydantic-field</code>","text":"<p>A list of names for the source docs.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.newline","title":"<code>newline: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to chunk by lines.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.output_dir","title":"<code>output_dir: Optional[Path | str] = None</code>  <code>pydantic-field</code>","text":"<p>The output directory to save the chunks to.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.overlap","title":"<code>overlap: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The number of tokens to overlap.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.pad","title":"<code>pad: int = 3</code>  <code>pydantic-field</code>","text":"<p>The padding for the chunk names.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.strip_chunks","title":"<code>strip_chunks: bool = True</code>  <code>pydantic-field</code>","text":"<p>Whether to strip leading and trailing whitespace in the chunks.</p>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Iterate over the object's chunks.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>An iterator containing the object's chunks.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Iterate over the object's chunks.\n\n    Returns:\n        Iterator: An iterator containing the object's chunks.\n    \"\"\"\n    return iter(self.chunks)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of source docs in the instance.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of source docs in the instance.\"\"\"\n    if not self.docs:\n        return 0\n    return len(self.docs)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.list_start_end_indexes","title":"<code>list_start_end_indexes(arrays: list[np.ndarray]) -&gt; list[tuple[int, int]]</code>  <code>staticmethod</code>","text":"<p>List start and end indexes for a list of numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list[ndarray]</code> <p>List of numpy arrays.</p> required <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>list[tuple[int, int]]: List of tuples with start and end indexes.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@staticmethod\ndef list_start_end_indexes(arrays: list[np.ndarray]) -&gt; list[tuple[int, int]]:\n    \"\"\"List start and end indexes for a list of numpy arrays.\n\n    Args:\n        arrays (list[np.ndarray]): List of numpy arrays.\n\n    Returns:\n        list[tuple[int, int]]: List of tuples with start and end indexes.\n    \"\"\"\n    indexes = []\n    start = 0\n\n    for array in arrays:\n        end = start + len(array)\n        indexes.append((start, end))\n        start = end\n\n    return indexes\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.merge","title":"<code>merge(chunks: list[Doc]) -&gt; Doc</code>","text":"<p>Merge a list of chunks into a single Doc.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Doc]</code> <p>The list of chunks to merge.</p> required <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The merged doc.</p> Note <ul> <li>The user_data dict of the docs will be ignored. If they contain information   that needs to be preserved, it should be stored in the doc extensions.   See https://github.com/explosion/spaCy/discussions/9106.</li> </ul> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def merge(self, chunks: list[Doc]) -&gt; Doc:\n    \"\"\"Merge a list of chunks into a single Doc.\n\n    Args:\n        chunks (list[Doc]): The list of chunks to merge.\n\n    Returns:\n        Doc: The merged doc.\n\n    Note:\n        - The user_data dict of the docs will be ignored. If they contain information\n          that needs to be preserved, it should be stored in the doc extensions.\n          See https://github.com/explosion/spaCy/discussions/9106.\n    \"\"\"\n    if len(chunks) == 0:\n        raise LexosException(\"No chunks to merge.\")\n    return Doc.from_docs(chunks)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.save","title":"<code>save(output_dir: Path | str, names: Optional[str | list[str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, strip_chunks: Optional[bool] = True, as_text: Optional[bool] = True) -&gt; None</code>","text":"<p>Save the chunks to disk.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path | str</code> <p>The output directory to save the chunks to.</p> required <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>int</code> <p>The padding for the chunk names.</p> <code>3</code> <code>strip_chunks</code> <code>bool</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>as_text</code> <code>Optional[bool]</code> <p>Whether to save the chunks as text files or spaCy Doc objects (bytes).</p> <code>True</code> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef save(\n    self,\n    output_dir: Path | str,\n    names: Optional[str | list[str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    strip_chunks: Optional[bool] = True,\n    as_text: Optional[bool] = True,\n) -&gt; None:\n    \"\"\"Save the chunks to disk.\n\n    Args:\n        output_dir (Path | str): The output directory to save the chunks to.\n        names (Optional[str | list[str]]): The doc names.\n        delimiter (str): The delimiter to use for the chunk names.\n        pad (int): The padding for the chunk names.\n        strip_chunks (bool): Whether to strip leading and trailing whitespace in the chunks.\n        as_text (Optional[bool]): Whether to save the chunks as text files or spaCy Doc objects (bytes).\n    \"\"\"\n    self._set_attributes(\n        output_dir=output_dir,\n        delimiter=delimiter,\n        names=names,\n        pad=pad,\n        strip_chunks=strip_chunks,\n    )\n    if not self.chunks or self.chunks == []:\n        raise LexosException(\"No chunks to save.\")\n    if self.names:\n        if len(self.names) != len(self.chunks):\n            raise LexosException(\n                f\"The number of docs in `names` ({len(self.names)}) must equal the number of docs in `chunks` ({len(self.chunks)}).\"\n            )\n    elif self.names == [] or self.names is None:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    for i, doc in enumerate(self.chunks):\n        for num, chunk in enumerate(doc):\n            if strip_chunks:\n                chunk = strip_doc(chunk)\n            self._write_chunk(\n                self.names[i], num + 1, chunk, Path(output_dir), as_text\n            )\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.split","title":"<code>split(docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None, chunksize: Optional[int] = None, n: Optional[int] = None, merge_threshold: Optional[float] = 0.5, overlap: Optional[int] = None, names: Optional[str | list[str]] = None, newline: Optional[bool] = None, strip_chunks: Optional[bool] = True, file: Optional[bool] = False, model: Optional[str] = None, merge_final: Optional[bool] = False) -&gt; list[list[Doc]]</code>","text":"<p>Split spaCy docs into chunks by a fixed number of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Optional[Doc | list[Doc] | Path | str | list[Path | str]]</code> <p>A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().</p> <code>None</code> <code>chunksize</code> <code>Optional[int]</code> <p>The number of tokens to split on.</p> <code>None</code> <code>n</code> <code>Optional[int]</code> <p>The number of chunks to produce.</p> <code>None</code> <code>merge_threshold</code> <code>Optional[float]</code> <p>The threshold to merge the last segment.</p> <code>0.5</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of tokens to overlap.</p> <code>None</code> <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>newline</code> <code>Optional[bool]</code> <p>Whether to chunk by lines.</p> <code>None</code> <code>strip_chunks</code> <code>Optional[bool]</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to load docs from files using Doc.from_disk().</p> <code>False</code> <code>model</code> <code>Optional[str]</code> <p>The name of the spaCy model to use when loading docs from files. Required when file=True.</p> <code>None</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to force the merge of the last segment.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[Doc]]</code> <p>list[list[Doc]]: A list of spaCy docs (chunks).</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef split(\n    self,\n    docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None,\n    chunksize: Optional[int] = None,\n    n: Optional[int] = None,\n    merge_threshold: Optional[float] = 0.5,\n    overlap: Optional[int] = None,\n    names: Optional[str | list[str]] = None,\n    newline: Optional[bool] = None,\n    strip_chunks: Optional[bool] = True,\n    file: Optional[bool] = False,\n    model: Optional[str] = None,\n    merge_final: Optional[bool] = False,\n) -&gt; list[list[Doc]]:\n    \"\"\"Split spaCy docs into chunks by a fixed number of tokens.\n\n    Args:\n        docs (Optional[Doc | list[Doc] | Path | str | list[Path | str]]): A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().\n        chunksize (Optional[int]): The number of tokens to split on.\n        n (Optional[int]): The number of chunks to produce.\n        merge_threshold (Optional[float]): The threshold to merge the last segment.\n        overlap (Optional[int]): The number of tokens to overlap.\n        names (Optional[str | list[str]]): The doc names.\n        newline (Optional[bool]): Whether to chunk by lines.\n        strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n        file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n        model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n        merge_final (Optional[bool]): Whether to force the merge of the last segment.\n\n    Returns:\n        list[list[Doc]]: A list of spaCy docs (chunks).\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        chunksize=chunksize,\n        n=n,\n        merge_threshold=merge_threshold,\n        overlap=overlap,\n        names=names,\n        newline=newline,\n        strip_chunks=strip_chunks,\n    )\n\n    # Load docs from files if file=True\n    if file:\n        if model is None:\n            raise LexosException(\"model parameter is required when file=True\")\n        nlp = spacy.load(model)\n        loaded_docs = []\n        for doc in ensure_list(docs):\n            try:\n                doc = Doc(nlp.vocab).from_disk(doc)\n            except ValueError:\n                raise LexosException(\n                    f\"Error loading doc from disk. Doc file must be in a valid spaCy serialization format: see https://spacy.io/api/doc#to_disk\"\n                )\n            loaded_docs.append(doc)\n        docs = loaded_docs\n\n    if self.newline:\n        if not self.n:\n            self.n = self.chunksize\n        if not self.n or self.n &lt; 1:\n            raise LexosException(\"n must be greater than 0.\")\n        for doc in ensure_list(docs):\n            self.chunks.append(\n                self._split_doc_by_lines(doc, merge_final=merge_final)\n            )\n    else:\n        for doc in ensure_list(docs):\n            self.chunks.append(self._split_doc(doc, merge_final=merge_final))\n\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.split_on_milestones","title":"<code>split_on_milestones(milestones: Span | list[Span], docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None, merge_threshold: Optional[float] = 0.5, merge_final: Optional[bool] = False, overlap: Optional[int] = None, keep_spans: Optional[bool | str] = False, strip_chunks: Optional[bool] = True, names: Optional[str | list[str]] = None, file: Optional[bool] = False, model: Optional[str] = None) -&gt; list[list[Doc]]</code>","text":"<p>Split document on a milestone.</p> <p>Parameters:</p> Name Type Description Default <code>milestones</code> <code>Span | list[Span]</code> <p>A milestone span or list of milestone spans to be matched.</p> required <code>docs</code> <code>Optional[Doc | list[Doc] | Path | str | list[Path | str]]</code> <p>The document(s) to be split, or file paths to spaCy docs saved with Doc.to_disk().</p> <code>None</code> <code>merge_threshold</code> <code>Optional[float]</code> <p>The threshold to merge the last segment.</p> <code>0.5</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to force the merge of the last segment.</p> <code>False</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of tokens to overlap.</p> <code>None</code> <code>keep_spans</code> <code>Optional[bool | str]</code> <p>Whether to keep the spans in the split strings. Defaults to False.</p> <code>False</code> <code>strip_chunks</code> <code>Optional[bool]</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to load docs from files using Doc.from_disk().</p> <code>False</code> <code>model</code> <code>Optional[str]</code> <p>The name of the spaCy model to use when loading docs from files. Required when file=True.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Doc]]</code> <p>list[list[Doc]]: A list of spaCy docs (chunks).</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef split_on_milestones(\n    self,\n    milestones: Span | list[Span],\n    docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None,\n    merge_threshold: Optional[float] = 0.5,\n    merge_final: Optional[bool] = False,\n    overlap: Optional[int] = None,\n    keep_spans: Optional[bool | str] = False,\n    strip_chunks: Optional[bool] = True,\n    names: Optional[str | list[str]] = None,\n    file: Optional[bool] = False,\n    model: Optional[str] = None,\n) -&gt; list[list[Doc]]:\n    \"\"\"Split document on a milestone.\n\n    Args:\n        milestones (Span | list[Span]): A milestone span or list of milestone spans to be matched.\n        docs (Optional[Doc | list[Doc] | Path | str | list[Path | str]]): The document(s) to be split, or file paths to spaCy docs saved with Doc.to_disk().\n        merge_threshold (Optional[float]): The threshold to merge the last segment.\n        merge_final (Optional[bool]): Whether to force the merge of the last segment.\n        overlap (Optional[int]): The number of tokens to overlap.\n        keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n        strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n        names (Optional[str | list[str]]): The doc names.\n        file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n        model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n\n    Returns:\n        list[list[Doc]]: A list of spaCy docs (chunks).\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        merge_threshold=merge_threshold,\n        overlap=overlap,\n        strip_chunks=strip_chunks,\n        names=names,\n    )\n\n    # Load docs from files if file=True\n    if file:\n        if model is None:\n            raise LexosException(\"model parameter is required when file=True\")\n        nlp = spacy.load(model)\n        loaded_docs = []\n        for doc in ensure_list(docs):\n            doc = Doc(nlp.vocab).from_disk(doc)\n            loaded_docs.append(doc)\n        docs = loaded_docs\n\n    for doc in ensure_list(docs):\n        chunks = self._split_doc_on_milestones(\n            doc, milestones, keep_spans=keep_spans, merge_final=merge_final\n        )\n        self.chunks.append(chunks)\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.split_on_sentences","title":"<code>split_on_sentences(docs: Doc | list[Doc] | Path | str | list[Path | str], n: Optional[int] = None, merge_final: Optional[bool] = False, overlap: Optional[int] = None, strip_chunks: Optional[bool] = True, names: Optional[str | list[str]] = None, file: Optional[bool] = False, model: Optional[str] = None) -&gt; list[list[Doc]]</code>","text":"<p>Split spaCy docs into chunks by a fixed number of sentences.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Doc | list[Doc] | Path | str | list[Path | str]</code> <p>A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().</p> required <code>n</code> <code>Optional[int]</code> <p>The number of sentences per chunk.</p> <code>None</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to merge the last segment.</p> <code>False</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of tokens to overlap.</p> <code>None</code> <code>strip_chunks</code> <code>Optional[bool]</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to load docs from files using Doc.from_disk().</p> <code>False</code> <code>model</code> <code>Optional[str]</code> <p>The name of the spaCy model to use when loading docs from files. Required when file=True.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Doc]]</code> <p>list[list[Doc]]: A list of spaCy docs (chunks).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n is less than or equal to 0.</p> <code>ValueError</code> <p>If the model has no sentences.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef split_on_sentences(\n    self,\n    docs: Doc | list[Doc] | Path | str | list[Path | str],\n    n: Optional[int] = None,\n    merge_final: Optional[bool] = False,\n    overlap: Optional[int] = None,\n    strip_chunks: Optional[bool] = True,\n    names: Optional[str | list[str]] = None,\n    file: Optional[bool] = False,\n    model: Optional[str] = None,\n) -&gt; list[list[Doc]]:\n    \"\"\"Split spaCy docs into chunks by a fixed number of sentences.\n\n    Args:\n        docs (Doc | list[Doc] | Path | str | list[Path | str]): A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().\n        n (Optional[int]): The number of sentences per chunk.\n        merge_final (Optional[bool]): Whether to merge the last segment.\n        overlap (Optional[int]): The number of tokens to overlap.\n        strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n        names (Optional[str | list[str]]): The doc names.\n        file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n        model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n\n    Returns:\n        list[list[Doc]]: A list of spaCy docs (chunks).\n\n    Raises:\n        ValueError: If n is less than or equal to 0.\n        ValueError: If the model has no sentences.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        overlap=overlap,\n        strip_chunks=strip_chunks,\n        names=names,\n    )\n\n    # Load docs from files if file=True\n    if file:\n        if model is None:\n            raise LexosException(\"model parameter is required when file=True\")\n        nlp = spacy.load(model)\n        loaded_docs = []\n        for doc in ensure_list(docs):\n            doc = Doc(nlp.vocab).from_disk(doc)\n            loaded_docs.append(doc)\n        docs = loaded_docs\n\n    if not self.n:\n        self.n = self.chunksize\n    if not self.n or self.n &lt; 1:\n        raise LexosException(\"n must be greater than 0.\")\n    for i, doc in enumerate(ensure_list(docs)):\n        if not doc.has_annotation(\"SENT_START\"):\n            raise LexosException(\n                f\"The spaCy model used to create the Doc {i} does not have sentence boundary detection. Please use a model that includes the 'senter' or 'parser' pipeline component.\"\n            )\n        else:\n            next(doc.sents)\n        self.chunks.append(\n            self._split_doc_by_sentences(doc, merge_final=merge_final)\n        )\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.to_dict","title":"<code>to_dict(names: Optional[list[str]] = None) -&gt; dict[str, list[str]]</code>","text":"<p>Return the chunks as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Optional[list[str]]</code> <p>A list of names for the doc Docs.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: The chunks as a dictionary.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_dict(self, names: Optional[list[str]] = None) -&gt; dict[str, list[str]]:\n    \"\"\"Return the chunks as a dictionary.\n\n    Args:\n        names (Optional[list[str]]): A list of names for the doc Docs.\n\n    Returns:\n        dict[str, list[str]]: The chunks as a dictionary.\n    \"\"\"\n    if names:\n        self.names = names\n    if not self.names:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    return {\n        str(name): [chunk.text for chunk in chunks]\n        for name, chunks in zip(self.names, self.chunks)\n    }\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Iterate over the object's chunks.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>An iterator containing the object's chunks.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Iterate over the object's chunks.\n\n    Returns:\n        Iterator: An iterator containing the object's chunks.\n    \"\"\"\n    return iter(self.chunks)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of source docs in the instance.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of source docs in the instance.\"\"\"\n    if not self.docs:\n        return 0\n    return len(self.docs)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.list_start_end_indexes","title":"<code>list_start_end_indexes(arrays: list[np.ndarray]) -&gt; list[tuple[int, int]]</code>  <code>staticmethod</code>","text":"<p>List start and end indexes for a list of numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list[ndarray]</code> <p>List of numpy arrays.</p> required <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>list[tuple[int, int]]: List of tuples with start and end indexes.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@staticmethod\ndef list_start_end_indexes(arrays: list[np.ndarray]) -&gt; list[tuple[int, int]]:\n    \"\"\"List start and end indexes for a list of numpy arrays.\n\n    Args:\n        arrays (list[np.ndarray]): List of numpy arrays.\n\n    Returns:\n        list[tuple[int, int]]: List of tuples with start and end indexes.\n    \"\"\"\n    indexes = []\n    start = 0\n\n    for array in arrays:\n        end = start + len(array)\n        indexes.append((start, end))\n        start = end\n\n    return indexes\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._apply_merge_threshold","title":"<code>_apply_merge_threshold(chunks: list[Doc], force: bool = False) -&gt; list[Doc]</code>","text":"<p>Apply the merge threshold to the last chunk.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Doc]</code> <p>The list of chunks.</p> required <code>force</code> <code>bool</code> <p>Whether to force the merge. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: The list of chunks with the last chunk merged if necessary.</p> Notes <ul> <li>Whitespace is supplied between merged chunks.</li> <li>Length of final chunk is measured in number tokens or number of sentences.</li> </ul> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _apply_merge_threshold(\n    self, chunks: list[Doc], force: bool = False\n) -&gt; list[Doc]:\n    \"\"\"Apply the merge threshold to the last chunk.\n\n    Args:\n        chunks (list[Doc]): The list of chunks.\n        force (bool, optional): Whether to force the merge. Defaults to False.\n\n    Returns:\n        list[Doc]: The list of chunks with the last chunk merged if necessary.\n\n    Notes:\n      - Whitespace is supplied between merged chunks.\n      - Length of final chunk is measured in number tokens or number of sentences.\n    \"\"\"\n    if len(chunks) == 1:\n        return chunks\n    merge_threshold = (\n        self.merge_threshold if self.merge_threshold is not None else 0.5\n    )\n    if isinstance(self.n, int):\n        threshold = max([len(chunk) for chunk in chunks]) * merge_threshold\n    else:\n        threshold = (\n            self.chunksize if self.chunksize is not None else 1\n        ) * merge_threshold\n    # If the length of the last chunk &lt; threshold, merge it with the previous chunk\n    if force is True or len(chunks[-1]) &lt; threshold:\n        # Get rid of the last chunk\n        last_chunk = chunks.pop(-1)\n        # Combine the last two segments into a single doc\n        chunks[-1] = Doc.from_docs([chunks[-1], last_chunk])\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._apply_overlap","title":"<code>_apply_overlap(chunks: list[Doc]) -&gt; list[Doc]</code>","text":"<p>Create overlapping chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Doc]</code> <p>A list of spaCy docs.</p> required <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: A list of spaCy docs.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _apply_overlap(\n    self,\n    chunks: list[Doc],\n) -&gt; list[Doc]:\n    \"\"\"Create overlapping chunks.\n\n    Args:\n        chunks (list[Doc]): A list of spaCy docs.\n\n    Returns:\n        list[Doc]: A list of spaCy docs.\n    \"\"\"\n    overlapped_chunks = []\n    for i, chunk in enumerate(chunks):\n        if i &lt; len(chunks) - 1:\n            overlap_doc = chunks[i + 1][: self.overlap].as_doc()\n            overlapped_doc = Doc.from_docs([chunk, overlap_doc])\n            overlapped_chunks.append(overlapped_doc)\n        elif i == len(chunks) - 1:\n            overlapped_chunks.append(chunk)\n    return overlapped_chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._chunk_doc","title":"<code>_chunk_doc(doc: Doc, attrs: Sequence[int | str] = SPACY_ATTRS, header: Sequence[int | str] = ENTITY_HEADER) -&gt; list[Doc]</code>","text":"<p>Split a Doc into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>The Doc to split.</p> required <code>attrs</code> <code>Sequence[int | str]</code> <p>The attributes to include in the chunks.</p> <code>SPACY_ATTRS</code> <code>header</code> <code>Sequence[int | str]</code> <p>The NER attributes to include in the chunks.</p> <code>ENTITY_HEADER</code> <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: List of Doc chunks.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _chunk_doc(\n    self,\n    doc: Doc,\n    attrs: \"Sequence[int | str]\" = SPACY_ATTRS,\n    header: Sequence[int | str] = ENTITY_HEADER,\n) -&gt; list[Doc]:\n    \"\"\"Split a Doc into chunks.\n\n    Args:\n        doc: The Doc to split.\n        attrs: The attributes to include in the chunks.\n        header: The NER attributes to include in the chunks.\n\n    Returns:\n        list[Doc]: List of Doc chunks.\n    \"\"\"\n    # Check that the document is not empty\n    if len(doc) == 0:\n        raise LexosException(\"Document is empty.\")\n\n    # Return the whole doc if it is less than the chunksize\n    if self.n is None and self.chunksize is not None and len(doc) &lt;= self.chunksize:\n        return [doc]\n\n    # Get the names of the custom extensions\n    extension_names = [name for name in doc[0]._.__dict__[\"_extensions\"].keys()]\n\n    # Split the doc into n chunks\n    if isinstance(self.n, int):\n        chunks_arr = np.array_split(doc.to_array(list(attrs)), self.n)\n        # If there is only one chunk, skip the rest of the function\n        if len(chunks_arr) == 1:\n            return [doc]\n    else:\n        chunks_arr = np.array_split(\n            doc.to_array(list(attrs)),\n            np.arange(self.chunksize, len(attrs), self.chunksize),\n        )\n        # Remove empty elements\n        chunks_arr = [x for x in chunks_arr if x.size &gt; 0]\n\n    # Create a list to hold the chunks and get the chunk indexes\n    chunks = []\n    chunk_indexes = TokenCutter.list_start_end_indexes(chunks_arr)\n\n    # Iterate over the chunks\n    for i, chunk in enumerate(chunks_arr):\n        # Get chunk start and end indexes\n        start = chunk_indexes[i][0]\n        end = chunk_indexes[i][1]\n        span = doc[start:end]\n        words = [token.text for token in span]\n\n        # Make a new doc for the chunk\n        new_doc = Doc(doc.vocab, words=words)\n\n        # Add the attributes to the new chunk doc\n        new_doc.from_array(list(attrs), chunk)\n\n        # Add entities to the new chunk doc\n        if doc.ents and len(doc.ents) &gt; 0:\n            ent_array = np.empty((len(chunk), len(header)), dtype=\"uint64\")\n            for i, token in enumerate(span):\n                ent_array[i, 0] = token.ent_iob\n                ent_array[i, 1] = token.ent_type\n            new_doc.from_array(list(header), ent_array)\n\n        # Add custom attributes to doc\n        if len(extension_names) &gt; 0:\n            for i, token in enumerate(span):\n                for ext in extension_names:\n                    new_doc[i]._.set(ext, token._.get(ext))\n\n        # Add the chunk to the chunks list\n        chunks.append(new_doc)\n\n    # Return the list of chunks\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._keep_milestones_bool","title":"<code>_keep_milestones_bool(doc: Doc, milestones: list[Span], keep_spans: bool = False) -&gt; list[Doc]</code>","text":"<p>Split a spaCy Doc into chunks on milestones, optionally keeping milestones.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>The spaCy Doc to split.</p> required <code>milestones</code> <code>list[Span]</code> <p>The milestones to split on.</p> required <code>keep_spans</code> <code>bool</code> <p>Whether to keep the spans in the split strings.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: A list of spaCy Docs.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _keep_milestones_bool(\n    self, doc: Doc, milestones: list[Span], keep_spans: bool = False\n) -&gt; list[Doc]:\n    \"\"\"Split a spaCy Doc into chunks on milestones, optionally keeping milestones.\n\n    Args:\n        doc (Doc): The spaCy Doc to split.\n        milestones (list[Span]): The milestones to split on.\n        keep_spans (bool): Whether to keep the spans in the split strings.\n\n    Returns:\n        list[Doc]: A list of spaCy Docs.\n    \"\"\"\n    chunks = []\n    start = 0\n    for span in milestones:\n        if span.start == 0 or span.end == doc[-1].i:\n            if keep_spans:\n                chunks.append(span)\n        elif start &lt; span.start:\n            chunks.append(doc[start : span.start])\n            if keep_spans:\n                chunks.append(span)\n        start = span.end\n    if start &lt; len(doc):\n        chunks.append(doc[start:])\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._keep_milestones_following","title":"<code>_keep_milestones_following(doc: Doc, milestones: list[Span]) -&gt; list[Doc]</code>","text":"<p>Split a spaCy Doc into chunks on milestones preserving milestones in the following chunk.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>The spaCy Doc to split.</p> required <code>milestones</code> <code>list[Span]</code> <p>The milestones to split on.</p> required <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: A list of spaCy Docs.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _keep_milestones_following(self, doc: Doc, milestones: list[Span]) -&gt; list[Doc]:\n    \"\"\"Split a spaCy Doc into chunks on milestones preserving milestones in the following chunk.\n\n    Args:\n        doc (Doc): The spaCy Doc to split.\n        milestones (list[Span]): The milestones to split on.\n\n    Returns:\n        list[Doc]: A list of spaCy Docs.\n    \"\"\"\n    chunks = []\n    start = 0\n    for index, span in enumerate(milestones):\n        # Text before milestone\n        if start &lt; span.start:\n            chunks.append(doc[start : span.start])\n\n        # Find end of chunk (next milestone or doc end)\n        end = (\n            milestones[index + 1].start if index &lt; len(milestones) - 1 else len(doc)\n        )\n\n        # Milestone + following text as one chunk\n        chunks.append(doc[span.start : end])\n        start = end\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._keep_milestones_preceding","title":"<code>_keep_milestones_preceding(doc: Doc, milestones: list[Span]) -&gt; list[Doc]</code>","text":"<p>Split a spaCy Doc into chunks on milestones preserving milestones in the preceding chunk.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>The spaCy Doc to split.</p> required <code>milestones</code> <code>list[Span]</code> <p>The milestones to split on.</p> required <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: A list of spaCy Docs.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _keep_milestones_preceding(self, doc: Doc, milestones: list[Span]) -&gt; list[Doc]:\n    \"\"\"Split a spaCy Doc into chunks on milestones preserving milestones in the preceding chunk.\n\n    Args:\n        doc (Doc): The spaCy Doc to split.\n        milestones (list[Span]): The milestones to split on.\n\n    Returns:\n        list[Doc]: A list of spaCy Docs.\n    \"\"\"\n    # Check that the document is not empty\n    if len(doc) == 0:\n        raise LexosException(\"Document is empty.\")\n    if len(milestones) == 0:\n        return [doc]\n    chunks = []\n    start = 0\n    for span in milestones:\n        index = span.start\n        if index != -1:\n            chunks.append(doc[start : index + len(span)])\n            start = index + len(span)\n    if start &lt; len(doc):\n        chunks.append(doc[start:])\n    if milestones[0].start == 0:\n        _ = chunks.pop(0)\n        chunks[0] = doc[: chunks[0].end]\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._set_attributes","title":"<code>_set_attributes(**data) -&gt; None</code>","text":"<p>Set attributes after initialization.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _set_attributes(self, **data) -&gt; None:\n    \"\"\"Set attributes after initialization.\"\"\"\n    for key, value in data.items():\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._split_doc","title":"<code>_split_doc(doc: Doc, attrs: Optional[Sequence[int | str]] = SPACY_ATTRS, merge_final: Optional[bool] = False) -&gt; list[Doc]</code>","text":"<p>Split a spaCy doc into chunks by a fixed number of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>A spaCy doc.</p> required <code>attrs</code> <code>Optional[int | str]</code> <p>The spaCy attributes to include in the chunks.</p> <code>SPACY_ATTRS</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to merge the final segment.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: A list of spaCy docs.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _split_doc(\n    self,\n    doc: Doc,\n    attrs: Optional[Sequence[int | str]] = SPACY_ATTRS,\n    merge_final: Optional[bool] = False,\n) -&gt; list[Doc]:\n    \"\"\"Split a spaCy doc into chunks by a fixed number of tokens.\n\n    Args:\n        doc (Doc): A spaCy doc.\n        attrs (Optional[int | str]): The spaCy attributes to include in the chunks.\n        merge_final (Optional[bool]): Whether to merge the final segment.\n\n    Returns:\n        list[Doc]: A list of spaCy docs.\n    \"\"\"\n    if len(doc) == 0:\n        raise LexosException(\"Document is empty.\")\n\n    attrs = attrs if attrs is not None else SPACY_ATTRS\n    chunks = self._chunk_doc(doc, attrs)\n    chunks = self._apply_merge_threshold(\n        chunks, force=merge_final if merge_final is not None else False\n    )\n    if self.overlap:\n        chunks = self._apply_overlap(chunks)\n    if self.strip_chunks:\n        return [strip_doc(chunk) for chunk in chunks]\n    # Ensure that all chunks are spaCy docs\n    else:\n        return [\n            chunk.as_doc() if isinstance(chunk, Span) else chunk for chunk in chunks\n        ]\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._split_doc_by_lines","title":"<code>_split_doc_by_lines(doc: Doc, merge_final: Optional[bool] = False) -&gt; list[Doc]</code>","text":"<p>Split a spaCy Doc into chunks of n lines.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>spaCy Doc to split.</p> required <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to merge the final segment.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: Chunks of the doc split by lines.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _split_doc_by_lines(\n    self, doc: Doc, merge_final: Optional[bool] = False\n) -&gt; list[Doc]:\n    \"\"\"Split a spaCy Doc into chunks of n lines.\n\n    Args:\n        doc: spaCy Doc to split.\n        merge_final: Whether to merge the final segment.\n\n    Returns:\n        list[Doc]: Chunks of the doc split by lines.\n    \"\"\"\n    if len(doc) == 0:\n        raise LexosException(\"Document is empty.\")\n\n    indices = []  # The indices immediately following the newline tokens\n    count = 0\n    chunks = []\n    for token in doc:\n        if \"\\n\" in token.text:\n            count += 1\n            if (\n                self.n is not None and count % self.n == 0\n            ):  # Check if it's the nth occurrence\n                indices.append(token.i + 1)\n    if len(indices) == 0:\n        chunks.append(doc)\n    else:\n        prev_index = 0\n        for index in indices:\n            chunks.append(doc[prev_index:index].as_doc())\n            prev_index = index\n        chunks.append(doc[prev_index:].as_doc())  # Append the remaining elements\n\n    # Ensure there are no empty docs\n    chunks = [chunk for chunk in chunks if len(chunk) &gt; 0]\n\n    # Apply the merge threshold and overlap\n    chunks = self._apply_merge_threshold(\n        chunks, force=merge_final if merge_final is not None else False\n    )\n    if self.overlap:\n        chunks = self._apply_overlap(chunks)\n\n    if self.strip_chunks:\n        return [strip_doc(chunk) for chunk in chunks]\n\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._split_doc_by_sentences","title":"<code>_split_doc_by_sentences(doc: Doc, merge_final: Optional[bool] = False) -&gt; list[Doc]</code>","text":"<p>Split a spaCy Doc into chunks of n sentences.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>A spaCy Doc object.</p> required <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to merge the final segment.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>list[Doc]</code> <p>Chunks containing n sentences each (last chunk may have fewer).</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _split_doc_by_sentences(\n    self, doc: Doc, merge_final: Optional[bool] = False\n) -&gt; list[Doc]:\n    \"\"\"Split a spaCy Doc into chunks of n sentences.\n\n    Args:\n        doc: A spaCy Doc object.\n        merge_final: Whether to merge the final segment.\n\n    Returns:\n        Doc: Chunks containing n sentences each (last chunk may have fewer).\n    \"\"\"\n    if len(doc) == 0:\n        raise LexosException(\"Document is empty.\")\n\n    try:\n        next(doc.sents)\n    except (StopIteration, ValueError):\n        raise LexosException(\"The document has no assigned sentences.\")\n\n    # Split the doc into chunks of n sentences\n    sents = list(doc.sents)\n    chunks = []\n    n = self.n if self.n is not None else 1\n    for i in range(0, len(sents), n):\n        chunk_sents = sents[i : i + n]\n        start_idx = chunk_sents[0].start\n        end_idx = chunk_sents[-1].end\n        chunks.append(doc[start_idx:end_idx].as_doc())\n    # No need to append doc[end_idx:] since all sentences are already included in the chunks\n\n    # Ensure there are no empty docs\n    chunks = [chunk for chunk in chunks if len(chunk) &gt; 0]\n\n    # Apply the merge threshold and overlap\n    chunks = self._apply_merge_threshold(\n        chunks, force=merge_final if merge_final is not None else False\n    )\n    if self.overlap:\n        chunks = self._apply_overlap(chunks)\n\n    if self.strip_chunks:\n        return [strip_doc(chunk) for chunk in chunks]\n\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._split_doc_on_milestones","title":"<code>_split_doc_on_milestones(doc: Doc, milestones: Span | list[Span], keep_spans: Optional[bool | str] = False, merge_final: Optional[bool] = False) -&gt; list[Doc]</code>","text":"<p>Split document on a milestone.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>The document to be split.</p> required <code>milestones</code> <code>Span | list[Span]</code> <p>A Span or list of Spans to be matched.</p> required <code>keep_spans</code> <code>Optional[bool | str]</code> <p>Whether to keep the spans in the split strings. Defaults to False.</p> <code>False</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to force the merge of the last segment. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Doc]</code> <p>list[Doc]: A list of chunked spaCy Doc objects.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _split_doc_on_milestones(\n    self,\n    doc: Doc,\n    milestones: Span | list[Span],\n    keep_spans: Optional[bool | str] = False,\n    merge_final: Optional[bool] = False,\n) -&gt; list[Doc]:\n    \"\"\"Split document on a milestone.\n\n    Args:\n        doc (Doc): The document to be split.\n        milestones (Span | list[Span]): A Span or list of Spans to be matched.\n        keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n        merge_final (Optional[bool]): Whether to force the merge of the last segment. Defaults to False.\n\n    Returns:\n        list[Doc]: A list of chunked spaCy Doc objects.\n    \"\"\"\n    if len(doc) == 0:\n        raise LexosException(\"Document is empty.\")\n\n    milestones = ensure_list(milestones)\n    if keep_spans == \"following\":\n        chunks = self._keep_milestones_following(doc, milestones)\n    elif keep_spans == \"preceding\":\n        chunks = self._keep_milestones_preceding(doc, milestones)\n    else:\n        # Only pass a boolean to keep_spans\n        chunks = self._keep_milestones_bool(\n            doc, milestones, keep_spans=bool(keep_spans)\n        )\n\n    # Ensure that all chunks are spaCy docs\n    chunks = [\n        chunk.as_doc() if isinstance(chunk, Span) else chunk for chunk in chunks\n    ]\n\n    # Apply the merge threshold and overlap\n    chunks = self._apply_merge_threshold(\n        chunks, force=merge_final if merge_final is not None else False\n    )\n    if self.overlap:\n        chunks = self._apply_overlap(chunks)\n\n    if self.strip_chunks:\n        return [strip_doc(chunk) for chunk in chunks]\n\n    return chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter._write_chunk","title":"<code>_write_chunk(path: str, n: int, chunk: Doc, output_dir: Path, as_text: bool = True) -&gt; None</code>","text":"<p>Write chunk text to file with formatted name.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the original file.</p> required <code>n</code> <code>int</code> <p>The number of the chunk.</p> required <code>chunk</code> <code>Doc</code> <p>The chunk to save.</p> required <code>output_dir</code> <code>Path</code> <p>The output directory for the chunk.</p> required <code>as_text</code> <code>bool</code> <p>Whether to save the chunk as a text file or a spaCy Doc object.</p> <code>True</code> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def _write_chunk(\n    self, path: str, n: int, chunk: Doc, output_dir: Path, as_text: bool = True\n) -&gt; None:\n    \"\"\"Write chunk text to file with formatted name.\n\n    Args:\n        path (str): The path of the original file.\n        n (int): The number of the chunk.\n        chunk (Doc): The chunk to save.\n        output_dir (Path): The output directory for the chunk.\n        as_text (bool): Whether to save the chunk as a text file or a spaCy Doc object.\n    \"\"\"\n    output_file = f\"{path}{self.delimiter}{str(n).zfill(self.pad)}.txt\"\n    output_path = output_dir / output_file\n    if as_text:\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(chunk.text)\n    else:\n        chunk.to_disk(output_path)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.merge","title":"<code>merge(chunks: list[Doc]) -&gt; Doc</code>","text":"<p>Merge a list of chunks into a single Doc.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Doc]</code> <p>The list of chunks to merge.</p> required <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The merged doc.</p> Note <ul> <li>The user_data dict of the docs will be ignored. If they contain information   that needs to be preserved, it should be stored in the doc extensions.   See https://github.com/explosion/spaCy/discussions/9106.</li> </ul> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>def merge(self, chunks: list[Doc]) -&gt; Doc:\n    \"\"\"Merge a list of chunks into a single Doc.\n\n    Args:\n        chunks (list[Doc]): The list of chunks to merge.\n\n    Returns:\n        Doc: The merged doc.\n\n    Note:\n        - The user_data dict of the docs will be ignored. If they contain information\n          that needs to be preserved, it should be stored in the doc extensions.\n          See https://github.com/explosion/spaCy/discussions/9106.\n    \"\"\"\n    if len(chunks) == 0:\n        raise LexosException(\"No chunks to merge.\")\n    return Doc.from_docs(chunks)\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.save","title":"<code>save(output_dir: Path | str, names: Optional[str | list[str]] = None, delimiter: Optional[str] = '_', pad: Optional[int] = 3, strip_chunks: Optional[bool] = True, as_text: Optional[bool] = True) -&gt; None</code>","text":"<p>Save the chunks to disk.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path | str</code> <p>The output directory to save the chunks to.</p> required <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>The delimiter to use for the chunk names.</p> <code>'_'</code> <code>pad</code> <code>int</code> <p>The padding for the chunk names.</p> <code>3</code> <code>strip_chunks</code> <code>bool</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>as_text</code> <code>Optional[bool]</code> <p>Whether to save the chunks as text files or spaCy Doc objects (bytes).</p> <code>True</code> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef save(\n    self,\n    output_dir: Path | str,\n    names: Optional[str | list[str]] = None,\n    delimiter: Optional[str] = \"_\",\n    pad: Optional[int] = 3,\n    strip_chunks: Optional[bool] = True,\n    as_text: Optional[bool] = True,\n) -&gt; None:\n    \"\"\"Save the chunks to disk.\n\n    Args:\n        output_dir (Path | str): The output directory to save the chunks to.\n        names (Optional[str | list[str]]): The doc names.\n        delimiter (str): The delimiter to use for the chunk names.\n        pad (int): The padding for the chunk names.\n        strip_chunks (bool): Whether to strip leading and trailing whitespace in the chunks.\n        as_text (Optional[bool]): Whether to save the chunks as text files or spaCy Doc objects (bytes).\n    \"\"\"\n    self._set_attributes(\n        output_dir=output_dir,\n        delimiter=delimiter,\n        names=names,\n        pad=pad,\n        strip_chunks=strip_chunks,\n    )\n    if not self.chunks or self.chunks == []:\n        raise LexosException(\"No chunks to save.\")\n    if self.names:\n        if len(self.names) != len(self.chunks):\n            raise LexosException(\n                f\"The number of docs in `names` ({len(self.names)}) must equal the number of docs in `chunks` ({len(self.chunks)}).\"\n            )\n    elif self.names == [] or self.names is None:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    for i, doc in enumerate(self.chunks):\n        for num, chunk in enumerate(doc):\n            if strip_chunks:\n                chunk = strip_doc(chunk)\n            self._write_chunk(\n                self.names[i], num + 1, chunk, Path(output_dir), as_text\n            )\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.split","title":"<code>split(docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None, chunksize: Optional[int] = None, n: Optional[int] = None, merge_threshold: Optional[float] = 0.5, overlap: Optional[int] = None, names: Optional[str | list[str]] = None, newline: Optional[bool] = None, strip_chunks: Optional[bool] = True, file: Optional[bool] = False, model: Optional[str] = None, merge_final: Optional[bool] = False) -&gt; list[list[Doc]]</code>","text":"<p>Split spaCy docs into chunks by a fixed number of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Optional[Doc | list[Doc] | Path | str | list[Path | str]]</code> <p>A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().</p> <code>None</code> <code>chunksize</code> <code>Optional[int]</code> <p>The number of tokens to split on.</p> <code>None</code> <code>n</code> <code>Optional[int]</code> <p>The number of chunks to produce.</p> <code>None</code> <code>merge_threshold</code> <code>Optional[float]</code> <p>The threshold to merge the last segment.</p> <code>0.5</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of tokens to overlap.</p> <code>None</code> <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>newline</code> <code>Optional[bool]</code> <p>Whether to chunk by lines.</p> <code>None</code> <code>strip_chunks</code> <code>Optional[bool]</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to load docs from files using Doc.from_disk().</p> <code>False</code> <code>model</code> <code>Optional[str]</code> <p>The name of the spaCy model to use when loading docs from files. Required when file=True.</p> <code>None</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to force the merge of the last segment.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[Doc]]</code> <p>list[list[Doc]]: A list of spaCy docs (chunks).</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef split(\n    self,\n    docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None,\n    chunksize: Optional[int] = None,\n    n: Optional[int] = None,\n    merge_threshold: Optional[float] = 0.5,\n    overlap: Optional[int] = None,\n    names: Optional[str | list[str]] = None,\n    newline: Optional[bool] = None,\n    strip_chunks: Optional[bool] = True,\n    file: Optional[bool] = False,\n    model: Optional[str] = None,\n    merge_final: Optional[bool] = False,\n) -&gt; list[list[Doc]]:\n    \"\"\"Split spaCy docs into chunks by a fixed number of tokens.\n\n    Args:\n        docs (Optional[Doc | list[Doc] | Path | str | list[Path | str]]): A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().\n        chunksize (Optional[int]): The number of tokens to split on.\n        n (Optional[int]): The number of chunks to produce.\n        merge_threshold (Optional[float]): The threshold to merge the last segment.\n        overlap (Optional[int]): The number of tokens to overlap.\n        names (Optional[str | list[str]]): The doc names.\n        newline (Optional[bool]): Whether to chunk by lines.\n        strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n        file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n        model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n        merge_final (Optional[bool]): Whether to force the merge of the last segment.\n\n    Returns:\n        list[list[Doc]]: A list of spaCy docs (chunks).\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        chunksize=chunksize,\n        n=n,\n        merge_threshold=merge_threshold,\n        overlap=overlap,\n        names=names,\n        newline=newline,\n        strip_chunks=strip_chunks,\n    )\n\n    # Load docs from files if file=True\n    if file:\n        if model is None:\n            raise LexosException(\"model parameter is required when file=True\")\n        nlp = spacy.load(model)\n        loaded_docs = []\n        for doc in ensure_list(docs):\n            try:\n                doc = Doc(nlp.vocab).from_disk(doc)\n            except ValueError:\n                raise LexosException(\n                    f\"Error loading doc from disk. Doc file must be in a valid spaCy serialization format: see https://spacy.io/api/doc#to_disk\"\n                )\n            loaded_docs.append(doc)\n        docs = loaded_docs\n\n    if self.newline:\n        if not self.n:\n            self.n = self.chunksize\n        if not self.n or self.n &lt; 1:\n            raise LexosException(\"n must be greater than 0.\")\n        for doc in ensure_list(docs):\n            self.chunks.append(\n                self._split_doc_by_lines(doc, merge_final=merge_final)\n            )\n    else:\n        for doc in ensure_list(docs):\n            self.chunks.append(self._split_doc(doc, merge_final=merge_final))\n\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.split_on_milestones","title":"<code>split_on_milestones(milestones: Span | list[Span], docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None, merge_threshold: Optional[float] = 0.5, merge_final: Optional[bool] = False, overlap: Optional[int] = None, keep_spans: Optional[bool | str] = False, strip_chunks: Optional[bool] = True, names: Optional[str | list[str]] = None, file: Optional[bool] = False, model: Optional[str] = None) -&gt; list[list[Doc]]</code>","text":"<p>Split document on a milestone.</p> <p>Parameters:</p> Name Type Description Default <code>milestones</code> <code>Span | list[Span]</code> <p>A milestone span or list of milestone spans to be matched.</p> required <code>docs</code> <code>Optional[Doc | list[Doc] | Path | str | list[Path | str]]</code> <p>The document(s) to be split, or file paths to spaCy docs saved with Doc.to_disk().</p> <code>None</code> <code>merge_threshold</code> <code>Optional[float]</code> <p>The threshold to merge the last segment.</p> <code>0.5</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to force the merge of the last segment.</p> <code>False</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of tokens to overlap.</p> <code>None</code> <code>keep_spans</code> <code>Optional[bool | str]</code> <p>Whether to keep the spans in the split strings. Defaults to False.</p> <code>False</code> <code>strip_chunks</code> <code>Optional[bool]</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to load docs from files using Doc.from_disk().</p> <code>False</code> <code>model</code> <code>Optional[str]</code> <p>The name of the spaCy model to use when loading docs from files. Required when file=True.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Doc]]</code> <p>list[list[Doc]]: A list of spaCy docs (chunks).</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef split_on_milestones(\n    self,\n    milestones: Span | list[Span],\n    docs: Optional[Doc | list[Doc] | Path | str | list[Path | str]] = None,\n    merge_threshold: Optional[float] = 0.5,\n    merge_final: Optional[bool] = False,\n    overlap: Optional[int] = None,\n    keep_spans: Optional[bool | str] = False,\n    strip_chunks: Optional[bool] = True,\n    names: Optional[str | list[str]] = None,\n    file: Optional[bool] = False,\n    model: Optional[str] = None,\n) -&gt; list[list[Doc]]:\n    \"\"\"Split document on a milestone.\n\n    Args:\n        milestones (Span | list[Span]): A milestone span or list of milestone spans to be matched.\n        docs (Optional[Doc | list[Doc] | Path | str | list[Path | str]]): The document(s) to be split, or file paths to spaCy docs saved with Doc.to_disk().\n        merge_threshold (Optional[float]): The threshold to merge the last segment.\n        merge_final (Optional[bool]): Whether to force the merge of the last segment.\n        overlap (Optional[int]): The number of tokens to overlap.\n        keep_spans (Optional[bool | str]): Whether to keep the spans in the split strings. Defaults to False.\n        strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n        names (Optional[str | list[str]]): The doc names.\n        file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n        model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n\n    Returns:\n        list[list[Doc]]: A list of spaCy docs (chunks).\n    \"\"\"\n    if docs:\n        self.docs = ensure_list(docs)\n    if not self.docs:\n        raise LexosException(\"No documents provided for splitting.\")\n    self._set_attributes(\n        merge_threshold=merge_threshold,\n        overlap=overlap,\n        strip_chunks=strip_chunks,\n        names=names,\n    )\n\n    # Load docs from files if file=True\n    if file:\n        if model is None:\n            raise LexosException(\"model parameter is required when file=True\")\n        nlp = spacy.load(model)\n        loaded_docs = []\n        for doc in ensure_list(docs):\n            doc = Doc(nlp.vocab).from_disk(doc)\n            loaded_docs.append(doc)\n        docs = loaded_docs\n\n    for doc in ensure_list(docs):\n        chunks = self._split_doc_on_milestones(\n            doc, milestones, keep_spans=keep_spans, merge_final=merge_final\n        )\n        self.chunks.append(chunks)\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.split_on_sentences","title":"<code>split_on_sentences(docs: Doc | list[Doc] | Path | str | list[Path | str], n: Optional[int] = None, merge_final: Optional[bool] = False, overlap: Optional[int] = None, strip_chunks: Optional[bool] = True, names: Optional[str | list[str]] = None, file: Optional[bool] = False, model: Optional[str] = None) -&gt; list[list[Doc]]</code>","text":"<p>Split spaCy docs into chunks by a fixed number of sentences.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Doc | list[Doc] | Path | str | list[Path | str]</code> <p>A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().</p> required <code>n</code> <code>Optional[int]</code> <p>The number of sentences per chunk.</p> <code>None</code> <code>merge_final</code> <code>Optional[bool]</code> <p>Whether to merge the last segment.</p> <code>False</code> <code>overlap</code> <code>Optional[int]</code> <p>The number of tokens to overlap.</p> <code>None</code> <code>strip_chunks</code> <code>Optional[bool]</code> <p>Whether to strip leading and trailing whitespace in the chunks.</p> <code>True</code> <code>names</code> <code>Optional[str | list[str]]</code> <p>The doc names.</p> <code>None</code> <code>file</code> <code>Optional[bool]</code> <p>Whether to load docs from files using Doc.from_disk().</p> <code>False</code> <code>model</code> <code>Optional[str]</code> <p>The name of the spaCy model to use when loading docs from files. Required when file=True.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Doc]]</code> <p>list[list[Doc]]: A list of spaCy docs (chunks).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n is less than or equal to 0.</p> <code>ValueError</code> <p>If the model has no sentences.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef split_on_sentences(\n    self,\n    docs: Doc | list[Doc] | Path | str | list[Path | str],\n    n: Optional[int] = None,\n    merge_final: Optional[bool] = False,\n    overlap: Optional[int] = None,\n    strip_chunks: Optional[bool] = True,\n    names: Optional[str | list[str]] = None,\n    file: Optional[bool] = False,\n    model: Optional[str] = None,\n) -&gt; list[list[Doc]]:\n    \"\"\"Split spaCy docs into chunks by a fixed number of sentences.\n\n    Args:\n        docs (Doc | list[Doc] | Path | str | list[Path | str]): A spaCy doc, list of spaCy docs, or file paths to spaCy docs saved with Doc.to_disk().\n        n (Optional[int]): The number of sentences per chunk.\n        merge_final (Optional[bool]): Whether to merge the last segment.\n        overlap (Optional[int]): The number of tokens to overlap.\n        strip_chunks (Optional[bool]): Whether to strip leading and trailing whitespace in the chunks.\n        names (Optional[str | list[str]]): The doc names.\n        file (Optional[bool]): Whether to load docs from files using Doc.from_disk().\n        model (Optional[str]): The name of the spaCy model to use when loading docs from files. Required when file=True.\n\n    Returns:\n        list[list[Doc]]: A list of spaCy docs (chunks).\n\n    Raises:\n        ValueError: If n is less than or equal to 0.\n        ValueError: If the model has no sentences.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        overlap=overlap,\n        strip_chunks=strip_chunks,\n        names=names,\n    )\n\n    # Load docs from files if file=True\n    if file:\n        if model is None:\n            raise LexosException(\"model parameter is required when file=True\")\n        nlp = spacy.load(model)\n        loaded_docs = []\n        for doc in ensure_list(docs):\n            doc = Doc(nlp.vocab).from_disk(doc)\n            loaded_docs.append(doc)\n        docs = loaded_docs\n\n    if not self.n:\n        self.n = self.chunksize\n    if not self.n or self.n &lt; 1:\n        raise LexosException(\"n must be greater than 0.\")\n    for i, doc in enumerate(ensure_list(docs)):\n        if not doc.has_annotation(\"SENT_START\"):\n            raise LexosException(\n                f\"The spaCy model used to create the Doc {i} does not have sentence boundary detection. Please use a model that includes the 'senter' or 'parser' pipeline component.\"\n            )\n        else:\n            next(doc.sents)\n        self.chunks.append(\n            self._split_doc_by_sentences(doc, merge_final=merge_final)\n        )\n    return self.chunks\n</code></pre>"},{"location":"api/cutter/token_cutter/#lexos.cutter.token_cutter.TokenCutter.to_dict","title":"<code>to_dict(names: Optional[list[str]] = None) -&gt; dict[str, list[str]]</code>","text":"<p>Return the chunks as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Optional[list[str]]</code> <p>A list of names for the doc Docs.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: The chunks as a dictionary.</p> Source code in <code>lexos/cutter/token_cutter.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_dict(self, names: Optional[list[str]] = None) -&gt; dict[str, list[str]]:\n    \"\"\"Return the chunks as a dictionary.\n\n    Args:\n        names (Optional[list[str]]): A list of names for the doc Docs.\n\n    Returns:\n        dict[str, list[str]]: The chunks as a dictionary.\n    \"\"\"\n    if names:\n        self.names = names\n    if not self.names:\n        self.names = [\n            f\"doc{str(i + 1).zfill(self.pad)}\" for i in range(len(self.chunks))\n        ]\n    return {\n        str(name): [chunk.text for chunk in chunks]\n        for name, chunks in zip(self.names, self.chunks)\n    }\n</code></pre>"},{"location":"api/dtm/","title":"DTM","text":"<p>The <code>dtm</code> module is used to generate and manage document-term matrices (DTMs).</p>"},{"location":"api/dtm/dtm/","title":"DTM","text":""},{"location":"api/dtm/dtm/#lexos.dtm.DTM","title":"<code>DTM</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for a document-term matrix.</p> <p>Config:</p> <ul> <li><code>default</code>: <code>validation_config</code></li> </ul> <p>Fields:</p> <ul> <li> <code>docs</code>                 (<code>Optional[list[list[str] | Doc]]</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>vectorizer</code>                 (<code>Optional[Callable]</code>)             </li> <li> <code>alg</code>                 (<code>Optional[ns]</code>)             </li> <li> <code>doc_term_matrix</code>                 (<code>Optional[spmatrix]</code>)             </li> </ul> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>class DTM(BaseModel):\n    \"\"\"Class for a document-term matrix.\"\"\"\n\n    docs: Optional[list[list[str] | Doc]] = Field(\n        default=None,\n        description=\"A list of spaCy docs or a list of token lists.\"\n    )\n    labels: Optional[list[str]] = Field(\n        default=None,\n        description=\"A list of labels for the documents.\"\n    )\n    vectorizer: Optional[Callable] = Field(\n        default=TextacyVectorizer,\n        description=\"A callable Vectorizer. Must have a fit_transform() method.\"\n    )\n    alg: Optional[ns] = Field(\n        default=ns.LOCALE,\n        description=\"The sorting algorithm to use.\"\n    )\n    doc_term_matrix: Optional[sp.spmatrix] = Field(\n        default=None, description=\"The document-term matrix.\"\n    )\n\n    model_config = validation_config\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Return the shape of the DTM.\n\n        Returns:\n            tuple[int, int]: The shape of the DTM.\n        \"\"\"\n        if self.doc_term_matrix is None:\n            raise LexosException(\"DTM must be built before accessing its shape\")\n        return self.doc_term_matrix.shape\n\n    @property\n    def sorted_terms_list(self) -&gt; list[str]:\n        \"\"\"Return a natsorted list of terms in the DTM.\n\n        Returns:\n            list[str]: A natsorted list of terms in the DTM.\n        \"\"\"\n        if self.vectorizer is None or not hasattr(self.vectorizer, \"terms_list\"):\n            # This handles cases where vectorizer might be None or not properly set up\n            # before attempting to access terms_list.\n            raise LexosException(\n                \"Vectorizer or its 'terms_list' attribute is not available to get sorted terms.\"\n            )\n        return natsorted(self.vectorizer.terms_list, reverse=False, alg=self.alg)\n\n    @property\n    def sorted_term_counts(self) -&gt; dict[str, int]:\n        \"\"\"Return a natsorted dict of terms and their TOTAL counts across all documents in the DTM.\n\n        Returns:\n            dict[str, int]: A natsorted dict of terms and their total counts.\n        \"\"\"\n        # 1. Handle edge cases: DTM not built or empty\n        if self.doc_term_matrix is None or self.doc_term_matrix.shape[1] == 0:\n            return {}  # Return an empty dictionary if no DTM or no terms\n\n        # 2. Get the terms (column names) from the vectorizer\n        if not hasattr(self.vectorizer, \"terms_list\") or not self.vectorizer.terms_list:\n            # Fallback for unexpected mock scenarios or custom vectorizers\n            # A well-formed DTM should always have terms_list if it has a non-empty matrix\n            raise LexosException(\n                \"Vectorizer must have 'terms_list' attribute to get sorted term counts.\"\n            )\n        terms = self.vectorizer.terms_list\n\n        # 3. Calculate the sum of each term (column) across all documents\n        # .sum(axis=0) returns a 1xN matrix. .A1 flattens it to a 1D numpy array for sparse matrices.\n        term_totals = self.doc_term_matrix.sum(axis=0).A1.astype(int)\n\n        # 4. Create a dictionary mapping terms to their total counts\n        # Ensure terms and term_totals have the same length\n        if len(terms) != len(term_totals):\n            raise LexosException(\n                f\"Mismatch between number of terms ({len(terms)}) \"\n                f\"and calculated term totals ({len(term_totals)}).\"\n            )\n        term_counts_dict = dict(zip(terms, term_totals))\n\n        # 5. Natsort the items (term-count pairs) by term name, then convert back to a dictionary\n        # This uses the natsort library based on the instance's 'alg'\n        sorted_items = natsorted(\n            term_counts_dict.items(), key=lambda item: item[0], alg=self.alg\n        )\n        return dict(sorted_items)\n\n    def __init__(\n        self, **data: dict[str, list | str | Callable | ns | sp.spmatrix]\n    ) -&gt; None:\n        \"\"\"Initialize the DTM class.\n\n        Args:\n            data (dict): A dictionary of data to initialize the DTM.\n                - docs: A list of spaCy docs or a list of token lists.\n                - labels: A list of labels for the documents.\n                - vectorizer: A callable Vectorizer. Must have a fit_transform() method.\n                - alg: The sorting algorithm to use (default is ns.LOCALE).\n                - doc_term_matrix: The document-term matrix (optional).\n                - **kwargs: Additional keyword arguments to pass to the vectorizer.\n        \"\"\"\n        super().__init__(**data)\n        # Make sure that a vectorizer instance is called with any keyword arguments\n        kwargs = {\n            k: v\n            for k, v in data.items()\n            if k not in [\"docs\", \"labels\", \"vectorizer\", \"alg\", \"doc_term_matrix\"]\n        }\n        self.vectorizer = self.vectorizer(**kwargs)\n\n    def __call__(\n        self,\n        docs: Optional[list[list[str] | Doc]],\n        labels: Optional[Iterable[str]],\n        **kwargs: dict[str, str | int | float | bool],\n    ) -&gt; None:\n        \"\"\"Call method for DTM class.\n\n        Args:\n            docs (list[list[str] | Doc]): A list of spaCy docs or a list of token lists.\n            labels (list[str]): A list of labels for the documents.\n            **kwargs (dict): Additional keyword arguments to pass to the vectorizer.\n\n        Note:\n            - If you want to filter the docs by token attributes, you can do so beforehand\n            and pass the filtered docs to this method.\n            - If you want to sort the dataframe, use pandas sort_values(), but make sure to\n              pass `SORTING_ALGORITHM` or `self.alg` to the `key` parameter for natsorting.\n        \"\"\"\n        if docs is None or len(docs) == 0:\n            raise LexosException(\n                \"You must provide a list of docs or a list of lists of token strings.\"\n            )\n\n        # Ensure that the vectorizer is not None\n        if self.vectorizer is None:\n            self.vectorizer = TextacyVectorizer()\n\n        # Update the vectorizer with any additional keyword arguments\n        self._update_vectorizer(**kwargs)\n\n        # Make sure the sorting algorithm is valid\n        self._validate_sorting_algorithm()\n\n        # Coerce the docs to a list of token lists\n        if docs:\n            self.docs = docs\n        self.docs = [\n            [token.text for token in doc] if isinstance(doc, Doc) else doc\n            for doc in self.docs\n        ]\n        # Set the instance labels\n        if labels:\n            self.labels = labels\n        elif self.labels is None:\n            self.labels = [f\"Doc{i + 1}\" for i in range(len(self.docs))]\n\n        # Make sure the number of docs matches the number of labels\n        if len(self.docs) != len(self.labels):\n            raise LexosException(\"The number of docs must match the number of labels.\")\n\n        # Call the vectorizer to build the DTM\n        try:\n            # Fit the vectorizer to the docs and build the document-term matrix\n            self.doc_term_matrix = self.vectorizer.fit_transform(self.docs)\n\n        except Exception as e:\n            raise LexosException(f\"Error building DTM: {e}\")\n\n    def _get_term_percentages(\n        self,\n        df: pd.DataFrame,\n        rounding: int = 3,\n        as_str: bool | str = \"string\",\n        sum: bool = False,\n        mean: bool = False,\n        median: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Return a dataframe with term frequencies.\n\n        Args:\n            df (pd.DataFrame): The dataframe to convert to percentages.\n            rounding (int): The number of decimal places to round to.\n            as_str (bool | str): Whether to return the terms as strings.\n            sum (bool): Whether to include a column for the sum of each row.\n            mean (bool): Whether to include a column for the mean of each row.\n            median (bool): Whether to include a column for the median of each row.\n\n        Returns:\n            pd.DataFrame: A dataframe with term frequencies.\n        \"\"\"\n        dense_array = df.to_numpy()\n        total_sum = np.sum(dense_array)\n        if total_sum != 0:\n            percentage_array = (dense_array / total_sum) * 100\n        else:\n            percentage_array = np.zeros_like(dense_array)\n        df = pd.DataFrame(percentage_array, columns=df.columns, index=df.index)\n        if sum:\n            df[\"Total\"] = df.sum(numeric_only=True, axis=1)\n        if mean:\n            df[\"Mean\"] = df.mean(numeric_only=True, axis=1)\n        if median:\n            df[\"Median\"] = df.median(numeric_only=True, axis=1)\n        if rounding:\n            df = df.round(rounding)\n        if as_str == \"string\":\n            df = df.map(lambda x: f\"{x}%\")\n        return df\n\n    def _update_vectorizer(self, **kwargs: dict[str, str | int | float | bool]) -&gt; None:\n        \"\"\"Update the vectorizer with additional keyword arguments.\n\n        Args:\n            kwargs (dict): Additional keyword arguments to update the vectorizer.\n        \"\"\"\n        # Get parameters from the vectorizer attribute in case they have been set directly\n        params = {\n            \"tf_type\": self.vectorizer.tf_type,\n            \"idf_type\": self.vectorizer.idf_type,\n            \"dl_type\": self.vectorizer.dl_type,\n            \"norm\": self.vectorizer.norm,\n            \"min_df\": self.vectorizer.min_df,\n            \"max_df\": self.vectorizer.max_df,\n            \"max_n_terms\": self.vectorizer.max_n_terms,\n            \"vocabulary_terms\": self.vectorizer.vocabulary_terms,\n        }\n        # Override the settings with any additional keyword arguments\n        for key, value in kwargs.items():\n            params[key] = value\n\n        # Create a new vectorizer instance with the updated parameters\n        self.vectorizer = TextacyVectorizer(**params)\n\n    def _validate_sorting_algorithm(self) -&gt; bool:\n        \"\"\"Ensure that the specified sorting algorithm is a valid natsort locale.\n\n        Returns:\n            bool: Whether the sorting algorithm is valid.\n        \"\"\"\n        if self.alg not in [e for e in ns]:\n            locales = \", \".join([f\"ns.{e.name}\" for e in ns])\n            err = (\n                f\"Invalid sorting algorithm: {self.alg}.\",\n                f\"Valid algorithms for `alg` are: {locales}.\",\n                \"See https://natsort.readthedocs.io/en/stable/api.html#natsort.ns.\",\n            )\n            raise LexosException(\" \".join(err))\n        return True\n\n    def to_df(\n        self,\n        by: Optional[list | list[str]] = None,\n        ascending: Optional[bool | list[bool]] = True,\n        as_percent: Optional[bool] = False,\n        rounding: Optional[int] = 3,\n        transpose: Optional[bool] = False,\n        sum: Optional[bool] = False,\n        mean: Optional[bool] = False,\n        median: Optional[bool] = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Return the whole DTM as a pandas dataframe.\n\n        Args:\n            by (Optional[list | list[str]]): The column(s) to sort by.\n            ascending (Optional[bool | list[bool]]): Whether to sort in ascending order.\n            as_percent (Optional[bool]): Whether to return the terms as percentages.\n            rounding (Optional[int]): The number of decimal places to round to.\n            transpose (Optional[bool]): Whether to transpose the dataframe.\n            sum (Optional[bool]): Whether to include a column for the sum of each row.\n            mean (Optional[bool]): Whether to include a column for the mean of each row.\n            median (Optional[bool]): Whether to include a column for the median of each row.\n\n        Returns:\n            pd.DataFrame: The DTM as a pandas dataframe.\n        \"\"\"\n        if by is None:\n            by = self.labels[0]\n        try:\n            df = pd.DataFrame.sparse.from_spmatrix(\n                self.doc_term_matrix,\n                columns=self.vectorizer.terms_list,\n                index=self.labels,\n            ).T\n        except AttributeError:\n            df = pd.DataFrame(\n                self.doc_term_matrix,\n                columns=self.vectorizer.terms_list,\n                index=self.labels,\n            ).T\n        except Exception as e:\n            raise LexosException(f\"Error converting DTM to DataFrame: {e}\")\n        if as_percent:\n            df = self._get_term_percentages(\n                df,\n                rounding=rounding,\n                as_str=as_percent,\n                sum=sum,\n                mean=mean,\n                median=median,\n            )\n        else:\n            if sum:\n                df[\"Total\"] = df.sum(numeric_only=True, axis=1)\n            if mean:\n                df[\"Mean\"] = df.mean(numeric_only=True, axis=1)\n            if median:\n                df[\"Median\"] = np.median(df.to_numpy())\n        df = df.sort_values(by=by, ascending=ascending)\n        if transpose:\n            df = df.T\n        # NOTE: Sorting may need to be made conditional\n        # if transpose:\n        #     df = df.T\n        #     # After transpose, sort by index or don't sort\n        # else:\n        #     df = df.sort_values(by=by, ascending=ascending)\n        return df\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.alg","title":"<code>alg: Optional[ns] = ns.LOCALE</code>  <code>pydantic-field</code>","text":"<p>The sorting algorithm to use.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.doc_term_matrix","title":"<code>doc_term_matrix: Optional[sp.spmatrix] = None</code>  <code>pydantic-field</code>","text":"<p>The document-term matrix.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.docs","title":"<code>docs: Optional[list[list[str] | Doc]] = None</code>  <code>pydantic-field</code>","text":"<p>A list of spaCy docs or a list of token lists.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.labels","title":"<code>labels: Optional[list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>A list of labels for the documents.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Return the shape of the DTM.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>tuple[int, int]: The shape of the DTM.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.sorted_term_counts","title":"<code>sorted_term_counts: dict[str, int]</code>  <code>property</code>","text":"<p>Return a natsorted dict of terms and their TOTAL counts across all documents in the DTM.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: A natsorted dict of terms and their total counts.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.sorted_terms_list","title":"<code>sorted_terms_list: list[str]</code>  <code>property</code>","text":"<p>Return a natsorted list of terms in the DTM.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A natsorted list of terms in the DTM.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.__call__","title":"<code>__call__(docs: Optional[list[list[str] | Doc]], labels: Optional[Iterable[str]], **kwargs: dict[str, str | int | float | bool]) -&gt; None</code>","text":"<p>Call method for DTM class.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list[list[str] | Doc]</code> <p>A list of spaCy docs or a list of token lists.</p> required <code>labels</code> <code>list[str]</code> <p>A list of labels for the documents.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the vectorizer.</p> <code>{}</code> Note <ul> <li>If you want to filter the docs by token attributes, you can do so beforehand and pass the filtered docs to this method.</li> <li>If you want to sort the dataframe, use pandas sort_values(), but make sure to   pass <code>SORTING_ALGORITHM</code> or <code>self.alg</code> to the <code>key</code> parameter for natsorting.</li> </ul> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def __call__(\n    self,\n    docs: Optional[list[list[str] | Doc]],\n    labels: Optional[Iterable[str]],\n    **kwargs: dict[str, str | int | float | bool],\n) -&gt; None:\n    \"\"\"Call method for DTM class.\n\n    Args:\n        docs (list[list[str] | Doc]): A list of spaCy docs or a list of token lists.\n        labels (list[str]): A list of labels for the documents.\n        **kwargs (dict): Additional keyword arguments to pass to the vectorizer.\n\n    Note:\n        - If you want to filter the docs by token attributes, you can do so beforehand\n        and pass the filtered docs to this method.\n        - If you want to sort the dataframe, use pandas sort_values(), but make sure to\n          pass `SORTING_ALGORITHM` or `self.alg` to the `key` parameter for natsorting.\n    \"\"\"\n    if docs is None or len(docs) == 0:\n        raise LexosException(\n            \"You must provide a list of docs or a list of lists of token strings.\"\n        )\n\n    # Ensure that the vectorizer is not None\n    if self.vectorizer is None:\n        self.vectorizer = TextacyVectorizer()\n\n    # Update the vectorizer with any additional keyword arguments\n    self._update_vectorizer(**kwargs)\n\n    # Make sure the sorting algorithm is valid\n    self._validate_sorting_algorithm()\n\n    # Coerce the docs to a list of token lists\n    if docs:\n        self.docs = docs\n    self.docs = [\n        [token.text for token in doc] if isinstance(doc, Doc) else doc\n        for doc in self.docs\n    ]\n    # Set the instance labels\n    if labels:\n        self.labels = labels\n    elif self.labels is None:\n        self.labels = [f\"Doc{i + 1}\" for i in range(len(self.docs))]\n\n    # Make sure the number of docs matches the number of labels\n    if len(self.docs) != len(self.labels):\n        raise LexosException(\"The number of docs must match the number of labels.\")\n\n    # Call the vectorizer to build the DTM\n    try:\n        # Fit the vectorizer to the docs and build the document-term matrix\n        self.doc_term_matrix = self.vectorizer.fit_transform(self.docs)\n\n    except Exception as e:\n        raise LexosException(f\"Error building DTM: {e}\")\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.__init__","title":"<code>__init__(**data: dict[str, list | str | Callable | ns | sp.spmatrix]) -&gt; None</code>","text":"<p>Initialize the DTM class.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary of data to initialize the DTM. - docs: A list of spaCy docs or a list of token lists. - labels: A list of labels for the documents. - vectorizer: A callable Vectorizer. Must have a fit_transform() method. - alg: The sorting algorithm to use (default is ns.LOCALE). - doc_term_matrix: The document-term matrix (optional). - **kwargs: Additional keyword arguments to pass to the vectorizer.</p> <code>{}</code> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def __init__(\n    self, **data: dict[str, list | str | Callable | ns | sp.spmatrix]\n) -&gt; None:\n    \"\"\"Initialize the DTM class.\n\n    Args:\n        data (dict): A dictionary of data to initialize the DTM.\n            - docs: A list of spaCy docs or a list of token lists.\n            - labels: A list of labels for the documents.\n            - vectorizer: A callable Vectorizer. Must have a fit_transform() method.\n            - alg: The sorting algorithm to use (default is ns.LOCALE).\n            - doc_term_matrix: The document-term matrix (optional).\n            - **kwargs: Additional keyword arguments to pass to the vectorizer.\n    \"\"\"\n    super().__init__(**data)\n    # Make sure that a vectorizer instance is called with any keyword arguments\n    kwargs = {\n        k: v\n        for k, v in data.items()\n        if k not in [\"docs\", \"labels\", \"vectorizer\", \"alg\", \"doc_term_matrix\"]\n    }\n    self.vectorizer = self.vectorizer(**kwargs)\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.to_df","title":"<code>to_df(by: Optional[list | list[str]] = None, ascending: Optional[bool | list[bool]] = True, as_percent: Optional[bool] = False, rounding: Optional[int] = 3, transpose: Optional[bool] = False, sum: Optional[bool] = False, mean: Optional[bool] = False, median: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Return the whole DTM as a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>Optional[list | list[str]]</code> <p>The column(s) to sort by.</p> <code>None</code> <code>ascending</code> <code>Optional[bool | list[bool]]</code> <p>Whether to sort in ascending order.</p> <code>True</code> <code>as_percent</code> <code>Optional[bool]</code> <p>Whether to return the terms as percentages.</p> <code>False</code> <code>rounding</code> <code>Optional[int]</code> <p>The number of decimal places to round to.</p> <code>3</code> <code>transpose</code> <code>Optional[bool]</code> <p>Whether to transpose the dataframe.</p> <code>False</code> <code>sum</code> <code>Optional[bool]</code> <p>Whether to include a column for the sum of each row.</p> <code>False</code> <code>mean</code> <code>Optional[bool]</code> <p>Whether to include a column for the mean of each row.</p> <code>False</code> <code>median</code> <code>Optional[bool]</code> <p>Whether to include a column for the median of each row.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DTM as a pandas dataframe.</p> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def to_df(\n    self,\n    by: Optional[list | list[str]] = None,\n    ascending: Optional[bool | list[bool]] = True,\n    as_percent: Optional[bool] = False,\n    rounding: Optional[int] = 3,\n    transpose: Optional[bool] = False,\n    sum: Optional[bool] = False,\n    mean: Optional[bool] = False,\n    median: Optional[bool] = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Return the whole DTM as a pandas dataframe.\n\n    Args:\n        by (Optional[list | list[str]]): The column(s) to sort by.\n        ascending (Optional[bool | list[bool]]): Whether to sort in ascending order.\n        as_percent (Optional[bool]): Whether to return the terms as percentages.\n        rounding (Optional[int]): The number of decimal places to round to.\n        transpose (Optional[bool]): Whether to transpose the dataframe.\n        sum (Optional[bool]): Whether to include a column for the sum of each row.\n        mean (Optional[bool]): Whether to include a column for the mean of each row.\n        median (Optional[bool]): Whether to include a column for the median of each row.\n\n    Returns:\n        pd.DataFrame: The DTM as a pandas dataframe.\n    \"\"\"\n    if by is None:\n        by = self.labels[0]\n    try:\n        df = pd.DataFrame.sparse.from_spmatrix(\n            self.doc_term_matrix,\n            columns=self.vectorizer.terms_list,\n            index=self.labels,\n        ).T\n    except AttributeError:\n        df = pd.DataFrame(\n            self.doc_term_matrix,\n            columns=self.vectorizer.terms_list,\n            index=self.labels,\n        ).T\n    except Exception as e:\n        raise LexosException(f\"Error converting DTM to DataFrame: {e}\")\n    if as_percent:\n        df = self._get_term_percentages(\n            df,\n            rounding=rounding,\n            as_str=as_percent,\n            sum=sum,\n            mean=mean,\n            median=median,\n        )\n    else:\n        if sum:\n            df[\"Total\"] = df.sum(numeric_only=True, axis=1)\n        if mean:\n            df[\"Mean\"] = df.mean(numeric_only=True, axis=1)\n        if median:\n            df[\"Median\"] = np.median(df.to_numpy())\n    df = df.sort_values(by=by, ascending=ascending)\n    if transpose:\n        df = df.T\n    # NOTE: Sorting may need to be made conditional\n    # if transpose:\n    #     df = df.T\n    #     # After transpose, sort by index or don't sort\n    # else:\n    #     df = df.sort_values(by=by, ascending=ascending)\n    return df\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.__init__","title":"<code>__init__(**data: dict[str, list | str | Callable | ns | sp.spmatrix]) -&gt; None</code>","text":"<p>Initialize the DTM class.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary of data to initialize the DTM. - docs: A list of spaCy docs or a list of token lists. - labels: A list of labels for the documents. - vectorizer: A callable Vectorizer. Must have a fit_transform() method. - alg: The sorting algorithm to use (default is ns.LOCALE). - doc_term_matrix: The document-term matrix (optional). - **kwargs: Additional keyword arguments to pass to the vectorizer.</p> <code>{}</code> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def __init__(\n    self, **data: dict[str, list | str | Callable | ns | sp.spmatrix]\n) -&gt; None:\n    \"\"\"Initialize the DTM class.\n\n    Args:\n        data (dict): A dictionary of data to initialize the DTM.\n            - docs: A list of spaCy docs or a list of token lists.\n            - labels: A list of labels for the documents.\n            - vectorizer: A callable Vectorizer. Must have a fit_transform() method.\n            - alg: The sorting algorithm to use (default is ns.LOCALE).\n            - doc_term_matrix: The document-term matrix (optional).\n            - **kwargs: Additional keyword arguments to pass to the vectorizer.\n    \"\"\"\n    super().__init__(**data)\n    # Make sure that a vectorizer instance is called with any keyword arguments\n    kwargs = {\n        k: v\n        for k, v in data.items()\n        if k not in [\"docs\", \"labels\", \"vectorizer\", \"alg\", \"doc_term_matrix\"]\n    }\n    self.vectorizer = self.vectorizer(**kwargs)\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Return the shape of the DTM.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>tuple[int, int]: The shape of the DTM.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.sorted_terms_list","title":"<code>sorted_terms_list: list[str]</code>  <code>property</code>","text":"<p>Return a natsorted list of terms in the DTM.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A natsorted list of terms in the DTM.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.sorted_term_counts","title":"<code>sorted_term_counts: dict[str, int]</code>  <code>property</code>","text":"<p>Return a natsorted dict of terms and their TOTAL counts across all documents in the DTM.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: A natsorted dict of terms and their total counts.</p>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM._get_term_percentages","title":"<code>_get_term_percentages(df: pd.DataFrame, rounding: int = 3, as_str: bool | str = 'string', sum: bool = False, mean: bool = False, median: bool = False) -&gt; pd.DataFrame</code>","text":"<p>Return a dataframe with term frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to convert to percentages.</p> required <code>rounding</code> <code>int</code> <p>The number of decimal places to round to.</p> <code>3</code> <code>as_str</code> <code>bool | str</code> <p>Whether to return the terms as strings.</p> <code>'string'</code> <code>sum</code> <code>bool</code> <p>Whether to include a column for the sum of each row.</p> <code>False</code> <code>mean</code> <code>bool</code> <p>Whether to include a column for the mean of each row.</p> <code>False</code> <code>median</code> <code>bool</code> <p>Whether to include a column for the median of each row.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with term frequencies.</p> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def _get_term_percentages(\n    self,\n    df: pd.DataFrame,\n    rounding: int = 3,\n    as_str: bool | str = \"string\",\n    sum: bool = False,\n    mean: bool = False,\n    median: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Return a dataframe with term frequencies.\n\n    Args:\n        df (pd.DataFrame): The dataframe to convert to percentages.\n        rounding (int): The number of decimal places to round to.\n        as_str (bool | str): Whether to return the terms as strings.\n        sum (bool): Whether to include a column for the sum of each row.\n        mean (bool): Whether to include a column for the mean of each row.\n        median (bool): Whether to include a column for the median of each row.\n\n    Returns:\n        pd.DataFrame: A dataframe with term frequencies.\n    \"\"\"\n    dense_array = df.to_numpy()\n    total_sum = np.sum(dense_array)\n    if total_sum != 0:\n        percentage_array = (dense_array / total_sum) * 100\n    else:\n        percentage_array = np.zeros_like(dense_array)\n    df = pd.DataFrame(percentage_array, columns=df.columns, index=df.index)\n    if sum:\n        df[\"Total\"] = df.sum(numeric_only=True, axis=1)\n    if mean:\n        df[\"Mean\"] = df.mean(numeric_only=True, axis=1)\n    if median:\n        df[\"Median\"] = df.median(numeric_only=True, axis=1)\n    if rounding:\n        df = df.round(rounding)\n    if as_str == \"string\":\n        df = df.map(lambda x: f\"{x}%\")\n    return df\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM._update_vectorizer","title":"<code>_update_vectorizer(**kwargs: dict[str, str | int | float | bool]) -&gt; None</code>","text":"<p>Update the vectorizer with additional keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to update the vectorizer.</p> <code>{}</code> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def _update_vectorizer(self, **kwargs: dict[str, str | int | float | bool]) -&gt; None:\n    \"\"\"Update the vectorizer with additional keyword arguments.\n\n    Args:\n        kwargs (dict): Additional keyword arguments to update the vectorizer.\n    \"\"\"\n    # Get parameters from the vectorizer attribute in case they have been set directly\n    params = {\n        \"tf_type\": self.vectorizer.tf_type,\n        \"idf_type\": self.vectorizer.idf_type,\n        \"dl_type\": self.vectorizer.dl_type,\n        \"norm\": self.vectorizer.norm,\n        \"min_df\": self.vectorizer.min_df,\n        \"max_df\": self.vectorizer.max_df,\n        \"max_n_terms\": self.vectorizer.max_n_terms,\n        \"vocabulary_terms\": self.vectorizer.vocabulary_terms,\n    }\n    # Override the settings with any additional keyword arguments\n    for key, value in kwargs.items():\n        params[key] = value\n\n    # Create a new vectorizer instance with the updated parameters\n    self.vectorizer = TextacyVectorizer(**params)\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM._validate_sorting_algorithm","title":"<code>_validate_sorting_algorithm() -&gt; bool</code>","text":"<p>Ensure that the specified sorting algorithm is a valid natsort locale.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the sorting algorithm is valid.</p> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def _validate_sorting_algorithm(self) -&gt; bool:\n    \"\"\"Ensure that the specified sorting algorithm is a valid natsort locale.\n\n    Returns:\n        bool: Whether the sorting algorithm is valid.\n    \"\"\"\n    if self.alg not in [e for e in ns]:\n        locales = \", \".join([f\"ns.{e.name}\" for e in ns])\n        err = (\n            f\"Invalid sorting algorithm: {self.alg}.\",\n            f\"Valid algorithms for `alg` are: {locales}.\",\n            \"See https://natsort.readthedocs.io/en/stable/api.html#natsort.ns.\",\n        )\n        raise LexosException(\" \".join(err))\n    return True\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.DTM.to_df","title":"<code>to_df(by: Optional[list | list[str]] = None, ascending: Optional[bool | list[bool]] = True, as_percent: Optional[bool] = False, rounding: Optional[int] = 3, transpose: Optional[bool] = False, sum: Optional[bool] = False, mean: Optional[bool] = False, median: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Return the whole DTM as a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>Optional[list | list[str]]</code> <p>The column(s) to sort by.</p> <code>None</code> <code>ascending</code> <code>Optional[bool | list[bool]]</code> <p>Whether to sort in ascending order.</p> <code>True</code> <code>as_percent</code> <code>Optional[bool]</code> <p>Whether to return the terms as percentages.</p> <code>False</code> <code>rounding</code> <code>Optional[int]</code> <p>The number of decimal places to round to.</p> <code>3</code> <code>transpose</code> <code>Optional[bool]</code> <p>Whether to transpose the dataframe.</p> <code>False</code> <code>sum</code> <code>Optional[bool]</code> <p>Whether to include a column for the sum of each row.</p> <code>False</code> <code>mean</code> <code>Optional[bool]</code> <p>Whether to include a column for the mean of each row.</p> <code>False</code> <code>median</code> <code>Optional[bool]</code> <p>Whether to include a column for the median of each row.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DTM as a pandas dataframe.</p> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>def to_df(\n    self,\n    by: Optional[list | list[str]] = None,\n    ascending: Optional[bool | list[bool]] = True,\n    as_percent: Optional[bool] = False,\n    rounding: Optional[int] = 3,\n    transpose: Optional[bool] = False,\n    sum: Optional[bool] = False,\n    mean: Optional[bool] = False,\n    median: Optional[bool] = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Return the whole DTM as a pandas dataframe.\n\n    Args:\n        by (Optional[list | list[str]]): The column(s) to sort by.\n        ascending (Optional[bool | list[bool]]): Whether to sort in ascending order.\n        as_percent (Optional[bool]): Whether to return the terms as percentages.\n        rounding (Optional[int]): The number of decimal places to round to.\n        transpose (Optional[bool]): Whether to transpose the dataframe.\n        sum (Optional[bool]): Whether to include a column for the sum of each row.\n        mean (Optional[bool]): Whether to include a column for the mean of each row.\n        median (Optional[bool]): Whether to include a column for the median of each row.\n\n    Returns:\n        pd.DataFrame: The DTM as a pandas dataframe.\n    \"\"\"\n    if by is None:\n        by = self.labels[0]\n    try:\n        df = pd.DataFrame.sparse.from_spmatrix(\n            self.doc_term_matrix,\n            columns=self.vectorizer.terms_list,\n            index=self.labels,\n        ).T\n    except AttributeError:\n        df = pd.DataFrame(\n            self.doc_term_matrix,\n            columns=self.vectorizer.terms_list,\n            index=self.labels,\n        ).T\n    except Exception as e:\n        raise LexosException(f\"Error converting DTM to DataFrame: {e}\")\n    if as_percent:\n        df = self._get_term_percentages(\n            df,\n            rounding=rounding,\n            as_str=as_percent,\n            sum=sum,\n            mean=mean,\n            median=median,\n        )\n    else:\n        if sum:\n            df[\"Total\"] = df.sum(numeric_only=True, axis=1)\n        if mean:\n            df[\"Mean\"] = df.mean(numeric_only=True, axis=1)\n        if median:\n            df[\"Median\"] = np.median(df.to_numpy())\n    df = df.sort_values(by=by, ascending=ascending)\n    if transpose:\n        df = df.T\n    # NOTE: Sorting may need to be made conditional\n    # if transpose:\n    #     df = df.T\n    #     # After transpose, sort by index or don't sort\n    # else:\n    #     df = df.sort_values(by=by, ascending=ascending)\n    return df\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.Vectorizer","title":"<code>Vectorizer</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for Textacy's Vectorizer.</p> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>class Vectorizer(BaseModel):\n    \"\"\"Wrapper class for Textacy's Vectorizer.\"\"\"\n\n    @validate_call(config=validation_config)\n    def __call__(\n        self,\n        *,\n        tf_type: Literal[\"linear\", \"sqrt\", \"log\", \"binary\"] = \"linear\",\n        idf_type: Optional[Literal[\"linear\", \"sqrt\", \"log\"]] = None,\n        dl_type: Optional[Literal[\"linear\", \"sqrt\", \"log\"]] = None,\n        norm: Optional[Literal[\"l1\", \"l2\"]] = None,\n        min_df: int | float = 1,\n        max_df: int | float = 1.0,\n        max_n_terms: Optional[int] = None,\n        vocabulary_terms: Optional[dict[str, int] | Iterable[str]] = None,\n    ) -&gt; TextacyVectorizer:\n        \"\"\"Return a Textacy Vectorizer object.\n\n        Args:\n            tf_type (str): Term frequency type.\n            idf_type (str): Inverse document frequency type.\n            dl_type (str): Document length type.\n            norm (str): Normalization type.\n            min_df (int | float): Minimum document frequency.\n            max_df (int | float): Maximum document frequency.\n            max_n_terms (int): Maximum number of terms.\n            vocabulary_terms (dict | Iterable[str]): Vocabulary terms.\n\n        Returns:\n            TextacyVectorizer: A Textacy Vectorizer object.\n        \"\"\"\n        return TextacyVectorizer(\n            tf_type=tf_type,\n            idf_type=idf_type,\n            dl_type=dl_type,\n            norm=norm,\n            min_df=min_df,\n            max_df=max_df,\n            max_n_terms=max_n_terms,\n            vocabulary_terms=vocabulary_terms,\n        )\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.Vectorizer.__call__","title":"<code>__call__(*, tf_type: Literal['linear', 'sqrt', 'log', 'binary'] = 'linear', idf_type: Optional[Literal['linear', 'sqrt', 'log']] = None, dl_type: Optional[Literal['linear', 'sqrt', 'log']] = None, norm: Optional[Literal['l1', 'l2']] = None, min_df: int | float = 1, max_df: int | float = 1.0, max_n_terms: Optional[int] = None, vocabulary_terms: Optional[dict[str, int] | Iterable[str]] = None) -&gt; TextacyVectorizer</code>","text":"<p>Return a Textacy Vectorizer object.</p> <p>Parameters:</p> Name Type Description Default <code>tf_type</code> <code>str</code> <p>Term frequency type.</p> <code>'linear'</code> <code>idf_type</code> <code>str</code> <p>Inverse document frequency type.</p> <code>None</code> <code>dl_type</code> <code>str</code> <p>Document length type.</p> <code>None</code> <code>norm</code> <code>str</code> <p>Normalization type.</p> <code>None</code> <code>min_df</code> <code>int | float</code> <p>Minimum document frequency.</p> <code>1</code> <code>max_df</code> <code>int | float</code> <p>Maximum document frequency.</p> <code>1.0</code> <code>max_n_terms</code> <code>int</code> <p>Maximum number of terms.</p> <code>None</code> <code>vocabulary_terms</code> <code>dict | Iterable[str]</code> <p>Vocabulary terms.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TextacyVectorizer</code> <code>Vectorizer</code> <p>A Textacy Vectorizer object.</p> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>@validate_call(config=validation_config)\ndef __call__(\n    self,\n    *,\n    tf_type: Literal[\"linear\", \"sqrt\", \"log\", \"binary\"] = \"linear\",\n    idf_type: Optional[Literal[\"linear\", \"sqrt\", \"log\"]] = None,\n    dl_type: Optional[Literal[\"linear\", \"sqrt\", \"log\"]] = None,\n    norm: Optional[Literal[\"l1\", \"l2\"]] = None,\n    min_df: int | float = 1,\n    max_df: int | float = 1.0,\n    max_n_terms: Optional[int] = None,\n    vocabulary_terms: Optional[dict[str, int] | Iterable[str]] = None,\n) -&gt; TextacyVectorizer:\n    \"\"\"Return a Textacy Vectorizer object.\n\n    Args:\n        tf_type (str): Term frequency type.\n        idf_type (str): Inverse document frequency type.\n        dl_type (str): Document length type.\n        norm (str): Normalization type.\n        min_df (int | float): Minimum document frequency.\n        max_df (int | float): Maximum document frequency.\n        max_n_terms (int): Maximum number of terms.\n        vocabulary_terms (dict | Iterable[str]): Vocabulary terms.\n\n    Returns:\n        TextacyVectorizer: A Textacy Vectorizer object.\n    \"\"\"\n    return TextacyVectorizer(\n        tf_type=tf_type,\n        idf_type=idf_type,\n        dl_type=dl_type,\n        norm=norm,\n        min_df=min_df,\n        max_df=max_df,\n        max_n_terms=max_n_terms,\n        vocabulary_terms=vocabulary_terms,\n    )\n</code></pre>"},{"location":"api/dtm/dtm/#lexos.dtm.Vectorizer.__call__","title":"<code>__call__(*, tf_type: Literal['linear', 'sqrt', 'log', 'binary'] = 'linear', idf_type: Optional[Literal['linear', 'sqrt', 'log']] = None, dl_type: Optional[Literal['linear', 'sqrt', 'log']] = None, norm: Optional[Literal['l1', 'l2']] = None, min_df: int | float = 1, max_df: int | float = 1.0, max_n_terms: Optional[int] = None, vocabulary_terms: Optional[dict[str, int] | Iterable[str]] = None) -&gt; TextacyVectorizer</code>","text":"<p>Return a Textacy Vectorizer object.</p> <p>Parameters:</p> Name Type Description Default <code>tf_type</code> <code>str</code> <p>Term frequency type.</p> <code>'linear'</code> <code>idf_type</code> <code>str</code> <p>Inverse document frequency type.</p> <code>None</code> <code>dl_type</code> <code>str</code> <p>Document length type.</p> <code>None</code> <code>norm</code> <code>str</code> <p>Normalization type.</p> <code>None</code> <code>min_df</code> <code>int | float</code> <p>Minimum document frequency.</p> <code>1</code> <code>max_df</code> <code>int | float</code> <p>Maximum document frequency.</p> <code>1.0</code> <code>max_n_terms</code> <code>int</code> <p>Maximum number of terms.</p> <code>None</code> <code>vocabulary_terms</code> <code>dict | Iterable[str]</code> <p>Vocabulary terms.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TextacyVectorizer</code> <code>Vectorizer</code> <p>A Textacy Vectorizer object.</p> Source code in <code>lexos/dtm/__init__.py</code> <pre><code>@validate_call(config=validation_config)\ndef __call__(\n    self,\n    *,\n    tf_type: Literal[\"linear\", \"sqrt\", \"log\", \"binary\"] = \"linear\",\n    idf_type: Optional[Literal[\"linear\", \"sqrt\", \"log\"]] = None,\n    dl_type: Optional[Literal[\"linear\", \"sqrt\", \"log\"]] = None,\n    norm: Optional[Literal[\"l1\", \"l2\"]] = None,\n    min_df: int | float = 1,\n    max_df: int | float = 1.0,\n    max_n_terms: Optional[int] = None,\n    vocabulary_terms: Optional[dict[str, int] | Iterable[str]] = None,\n) -&gt; TextacyVectorizer:\n    \"\"\"Return a Textacy Vectorizer object.\n\n    Args:\n        tf_type (str): Term frequency type.\n        idf_type (str): Inverse document frequency type.\n        dl_type (str): Document length type.\n        norm (str): Normalization type.\n        min_df (int | float): Minimum document frequency.\n        max_df (int | float): Maximum document frequency.\n        max_n_terms (int): Maximum number of terms.\n        vocabulary_terms (dict | Iterable[str]): Vocabulary terms.\n\n    Returns:\n        TextacyVectorizer: A Textacy Vectorizer object.\n    \"\"\"\n    return TextacyVectorizer(\n        tf_type=tf_type,\n        idf_type=idf_type,\n        dl_type=dl_type,\n        norm=norm,\n        min_df=min_df,\n        max_df=max_df,\n        max_n_terms=max_n_terms,\n        vocabulary_terms=vocabulary_terms,\n    )\n</code></pre>"},{"location":"api/filter/","title":"Filter","text":"<p>The <code>filter</code> module provides a base class for applying filters to a document and returning a new document, as well as extracting tokens or ids form filtered docs. Filters are applied by passing a spaCy matcher to identify matches to the filter criteria. Since the doc's language model may not supply the required token attributes, you can create a custom filter to add and set the attributes as custom extensions. Examples of custom filter classes are <code>IsRomanFilter</code> and <code>IsWordFilter</code>.</p> <p>The module also provides a useful <code>StopwordFilter</code> class to add or remove stop words from a spaCy <code>Doc</code> without retokenising. Note, however, that it works by changing the model's defaults, so they will apply to any <code>Doc</code> created with that model unless the model is reloaded.</p>"},{"location":"api/filter/filter/","title":"Filters","text":""},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter","title":"<code>BaseFilter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>BaseFilter class.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>doc</code>                 (<code>Optional[Doc]</code>)             </li> <li> <code>matcher</code>                 (<code>Optional[Matcher]</code>)             </li> <li> <code>matches</code>                 (<code>Optional[list[tuple[int, int, int]]]</code>)             </li> </ul> Source code in <code>lexos/filter/filters.py</code> <pre><code>class BaseFilter(BaseModel):\n    \"\"\"BaseFilter class.\"\"\"\n\n    id: ClassVar[str] = \"base_filter\"\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\")\n    matcher: Optional[Matcher] = Field(default=None, description=\"A spaCy matcher.\")\n    matches: Optional[list[tuple[int, int, int]]] = Field(\n        default=None, description=\"List of matches.\"\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    @validate_call(config=model_config)\n    def __call__(self, doc: Optional[Doc], matcher: Optional[Matcher] = None) -&gt; Doc:\n        \"\"\"Call the filter function.\n\n        Args:\n            doc (Optional[Doc]): A spaCy doc.\n            matcher (Optional[Matcher]): A spaCy matcher.\n\n        Returns:\n            Doc: The filtered doc.\n        \"\"\"\n        # Validate the inputs\n        if not doc and not self.doc:\n            raise LexosException(\"No doc has been assigned to the filter.\")\n        if not matcher and not self.matcher:\n            raise LexosException(\"No matcher has been assigned to the filter.\")\n        if doc:\n            self.doc = doc\n        if matcher:\n            self.matcher = matcher\n        # Get the matches\n        self.matches = self.matcher(doc)\n\n    @property\n    def matched_token_ids(self) -&gt; set[int]:\n        \"\"\"A list of matched token ids after the filter was applied.\"\"\"\n        if not self.matches:\n            return None\n        token_ids = set()\n        for _, start, end in self.matches:\n            for i in range(start, end):\n                token_ids.add(i)\n        return token_ids\n\n    @property\n    def matched_tokens(self) -&gt; list[int]:\n        \"\"\"A list of matched tokens after the filter was applied.\"\"\"\n        return [self.doc[i] for i in self.matched_token_ids]\n\n    @property\n    def filtered_tokens(self) -&gt; list[int]:\n        \"\"\"A list of filtered tokens after the filter was applied.\"\"\"\n        return [self.doc[i] for i in self.filtered_token_ids]\n\n    @property\n    def filtered_token_ids(self) -&gt; set[int]:\n        \"\"\"A list of filtered token ids after the filter was applied.\"\"\"\n        if not self.matches:\n            return None\n        return set(range(len(self.doc))) - self.matched_token_ids\n\n    def _set_extensions(self, attr: str, default: Any):\n        \"\"\"Set the extensions.\"\"\"\n        if not Token.has_extension(attr):\n            Token.set_extension(attr, default=default, force=True)\n\n    def get_matched_doc(self, add_spaces: bool = False) -&gt; Doc:\n        \"\"\"Get a new doc from the matched tokens.\n\n        Args:\n            add_spaces (bool): If True, add a space after every token.\n                             If False, preserve original whitespace. Default is False.\n\n        Returns:\n            Doc: A new spaCy Doc containing only the matched tokens.\n        \"\"\"\n        words = [t.text for t in self.matched_tokens]\n        if add_spaces:\n            spaces = [\" \" for _ in self.matched_tokens]\n        else:\n            spaces = [t.whitespace_ for t in self.matched_tokens]\n        return Doc(self.doc.vocab, words=words, spaces=spaces)\n\n    def get_filtered_doc(self, add_spaces: bool = False) -&gt; Doc:\n        \"\"\"Get a new doc from the filtered tokens.\n\n        Args:\n            add_spaces (bool): If True, add a space after every token.\n                             If False, preserve original whitespace. Default is False.\n\n        Returns:\n            Doc: A new spaCy Doc containing only the filtered tokens.\n        \"\"\"\n        words = [t.text for t in self.filtered_tokens]\n        if add_spaces:\n            spaces = [\" \" for _ in self.filtered_tokens]\n        else:\n            spaces = [t.whitespace_ for t in self.filtered_tokens]\n        return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.filtered_token_ids","title":"<code>filtered_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of filtered token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.filtered_tokens","title":"<code>filtered_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of filtered tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matched_token_ids","title":"<code>matched_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of matched token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matched_tokens","title":"<code>matched_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of matched tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matcher","title":"<code>matcher: Optional[Matcher] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy matcher.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matches","title":"<code>matches: Optional[list[tuple[int, int, int]]] = None</code>  <code>pydantic-field</code>","text":"<p>List of matches.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.__call__","title":"<code>__call__(doc: Optional[Doc], matcher: Optional[Matcher] = None) -&gt; Doc</code>","text":"<p>Call the filter function.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Optional[Doc]</code> <p>A spaCy doc.</p> required <code>matcher</code> <code>Optional[Matcher]</code> <p>A spaCy matcher.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(self, doc: Optional[Doc], matcher: Optional[Matcher] = None) -&gt; Doc:\n    \"\"\"Call the filter function.\n\n    Args:\n        doc (Optional[Doc]): A spaCy doc.\n        matcher (Optional[Matcher]): A spaCy matcher.\n\n    Returns:\n        Doc: The filtered doc.\n    \"\"\"\n    # Validate the inputs\n    if not doc and not self.doc:\n        raise LexosException(\"No doc has been assigned to the filter.\")\n    if not matcher and not self.matcher:\n        raise LexosException(\"No matcher has been assigned to the filter.\")\n    if doc:\n        self.doc = doc\n    if matcher:\n        self.matcher = matcher\n    # Get the matches\n    self.matches = self.matcher(doc)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.get_filtered_doc","title":"<code>get_filtered_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the filtered tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the filtered tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_filtered_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the filtered tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the filtered tokens.\n    \"\"\"\n    words = [t.text for t in self.filtered_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.filtered_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.filtered_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.get_matched_doc","title":"<code>get_matched_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the matched tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the matched tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_matched_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the matched tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the matched tokens.\n    \"\"\"\n    words = [t.text for t in self.matched_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.matched_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.matched_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.id","title":"<code>id: str = 'base_filter'</code>  <code>class-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matcher","title":"<code>matcher: Optional[Matcher] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy matcher.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matches","title":"<code>matches: Optional[list[tuple[int, int, int]]] = None</code>  <code>pydantic-field</code>","text":"<p>List of matches.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.__call__","title":"<code>__call__(doc: Optional[Doc], matcher: Optional[Matcher] = None) -&gt; Doc</code>","text":"<p>Call the filter function.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Optional[Doc]</code> <p>A spaCy doc.</p> required <code>matcher</code> <code>Optional[Matcher]</code> <p>A spaCy matcher.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(self, doc: Optional[Doc], matcher: Optional[Matcher] = None) -&gt; Doc:\n    \"\"\"Call the filter function.\n\n    Args:\n        doc (Optional[Doc]): A spaCy doc.\n        matcher (Optional[Matcher]): A spaCy matcher.\n\n    Returns:\n        Doc: The filtered doc.\n    \"\"\"\n    # Validate the inputs\n    if not doc and not self.doc:\n        raise LexosException(\"No doc has been assigned to the filter.\")\n    if not matcher and not self.matcher:\n        raise LexosException(\"No matcher has been assigned to the filter.\")\n    if doc:\n        self.doc = doc\n    if matcher:\n        self.matcher = matcher\n    # Get the matches\n    self.matches = self.matcher(doc)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matched_token_ids","title":"<code>matched_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of matched token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.matched_tokens","title":"<code>matched_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of matched tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.filtered_token_ids","title":"<code>filtered_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of filtered token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.filtered_tokens","title":"<code>filtered_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of filtered tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter._set_extensions","title":"<code>_set_extensions(attr: str, default: Any)</code>","text":"<p>Set the extensions.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def _set_extensions(self, attr: str, default: Any):\n    \"\"\"Set the extensions.\"\"\"\n    if not Token.has_extension(attr):\n        Token.set_extension(attr, default=default, force=True)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.get_matched_doc","title":"<code>get_matched_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the matched tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the matched tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_matched_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the matched tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the matched tokens.\n    \"\"\"\n    words = [t.text for t in self.matched_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.matched_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.matched_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.BaseFilter.get_filtered_doc","title":"<code>get_filtered_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the filtered tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the filtered tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_filtered_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the filtered tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the filtered tokens.\n    \"\"\"\n    words = [t.text for t in self.filtered_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.filtered_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.filtered_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter","title":"<code>IsRomanFilter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A filter for Roman numerals.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>matcher</code>                 (<code>Optional[Matcher]</code>)             </li> <li> <code>matches</code>                 (<code>Optional[list[tuple[int, int, int]]]</code>)             </li> <li> <code>doc</code>                 (<code>Optional[Doc]</code>)             </li> <li> <code>attr</code>                 (<code>Optional[str]</code>)             </li> <li> <code>default</code>                 (<code>Optional[Any]</code>)             </li> </ul> Source code in <code>lexos/filter/filters.py</code> <pre><code>class IsRomanFilter(BaseFilter):\n    \"\"\"A filter for Roman numerals.\"\"\"\n\n    id: ClassVar[str] = \"is_roman\"\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\")\n    attr: Optional[str] = Field(\n        default=\"is_roman\",\n        description=\"The name of the attribute to add to the tokens.\",\n    )\n    default: Optional[Any] = Field(\n        default=None, description=\"The default value of the attribute.\"\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    def __init__(self, **data):\n        \"\"\"Initialise the filter object and set custom attribute extensions.\"\"\"\n        super().__init__(**data)\n        if self.attr:\n            self._set_extensions(self.attr, self.default)\n\n    @validate_call(config=model_config)\n    def __call__(\n        self,\n        doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n        attr: Optional[str] = Field(\n            default=None, description=\"The name of the attribute to add to the tokens.\"\n        ),\n        default: Optional[Any] = Field(\n            default=None, description=\"The default value of the attribute.\"\n        ),\n    ) -&gt; Doc:\n        \"\"\"Apply the filter.\n\n        Returns:\n            Doc: The filtered doc.\n        \"\"\"\n        # Validation\n        if doc:\n            self.doc = doc\n        if attr:\n            self.attr = attr\n        if default is not None:\n            self.default = default\n\n        # Use instance attributes if we have them\n        working_doc = self.doc if self.doc is not None else doc\n        working_attr = (\n            self.attr if hasattr(self, \"attr\") and self.attr is not None else attr\n        )\n        working_default = (\n            self.default\n            if hasattr(self, \"default\") and self.default is not None\n            else default\n        )\n\n        # Set custom extensions\n        if working_attr:\n            self._set_extensions(working_attr, working_default)\n\n        # Apply the filter only if we have a valid doc\n        if working_doc is not None:\n            # Store matches for tokens that are Roman numerals\n            self.matches = []\n            for i, token in enumerate(working_doc):\n                is_roman_result = self.is_roman(token)\n                setattr(working_doc[i]._, working_attr, is_roman_result)\n                if is_roman_result:\n                    self.matches.append((0, i, i + 1))\n\n        return working_doc if working_doc is not None else doc\n\n    @validate_call(config=model_config)\n    def is_roman(self, token: Token) -&gt; bool:\n        \"\"\"Detect Roman numerals (capitals only).\n\n        Args:\n            token (Token): A spaCy token.\n\n        Returns:\n            bool: True if the token is a Roman numeral.\n        \"\"\"\n        if token.text == \"\":\n            return False\n        pattern = r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\"\n        return bool(re.search(pattern, token.text))\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.attr","title":"<code>attr: Optional[str] = 'is_roman'</code>  <code>pydantic-field</code>","text":"<p>The name of the attribute to add to the tokens.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.default","title":"<code>default: Optional[Any] = None</code>  <code>pydantic-field</code>","text":"<p>The default value of the attribute.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.filtered_token_ids","title":"<code>filtered_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of filtered token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.filtered_tokens","title":"<code>filtered_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of filtered tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.matched_token_ids","title":"<code>matched_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of matched token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.matched_tokens","title":"<code>matched_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of matched tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.matcher","title":"<code>matcher: Optional[Matcher] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy matcher.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.matches","title":"<code>matches: Optional[list[tuple[int, int, int]]] = None</code>  <code>pydantic-field</code>","text":"<p>List of matches.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.__call__","title":"<code>__call__(doc: Optional[Doc] = Field(default=None, description='A spaCy doc.'), attr: Optional[str] = Field(default=None, description='The name of the attribute to add to the tokens.'), default: Optional[Any] = Field(default=None, description='The default value of the attribute.')) -&gt; Doc</code>","text":"<p>Apply the filter.</p> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(\n    self,\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n    attr: Optional[str] = Field(\n        default=None, description=\"The name of the attribute to add to the tokens.\"\n    ),\n    default: Optional[Any] = Field(\n        default=None, description=\"The default value of the attribute.\"\n    ),\n) -&gt; Doc:\n    \"\"\"Apply the filter.\n\n    Returns:\n        Doc: The filtered doc.\n    \"\"\"\n    # Validation\n    if doc:\n        self.doc = doc\n    if attr:\n        self.attr = attr\n    if default is not None:\n        self.default = default\n\n    # Use instance attributes if we have them\n    working_doc = self.doc if self.doc is not None else doc\n    working_attr = (\n        self.attr if hasattr(self, \"attr\") and self.attr is not None else attr\n    )\n    working_default = (\n        self.default\n        if hasattr(self, \"default\") and self.default is not None\n        else default\n    )\n\n    # Set custom extensions\n    if working_attr:\n        self._set_extensions(working_attr, working_default)\n\n    # Apply the filter only if we have a valid doc\n    if working_doc is not None:\n        # Store matches for tokens that are Roman numerals\n        self.matches = []\n        for i, token in enumerate(working_doc):\n            is_roman_result = self.is_roman(token)\n            setattr(working_doc[i]._, working_attr, is_roman_result)\n            if is_roman_result:\n                self.matches.append((0, i, i + 1))\n\n    return working_doc if working_doc is not None else doc\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialise the filter object and set custom attribute extensions.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialise the filter object and set custom attribute extensions.\"\"\"\n    super().__init__(**data)\n    if self.attr:\n        self._set_extensions(self.attr, self.default)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.get_filtered_doc","title":"<code>get_filtered_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the filtered tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the filtered tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_filtered_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the filtered tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the filtered tokens.\n    \"\"\"\n    words = [t.text for t in self.filtered_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.filtered_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.filtered_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.get_matched_doc","title":"<code>get_matched_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the matched tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the matched tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_matched_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the matched tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the matched tokens.\n    \"\"\"\n    words = [t.text for t in self.matched_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.matched_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.matched_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.is_roman","title":"<code>is_roman(token: Token) -&gt; bool</code>","text":"<p>Detect Roman numerals (capitals only).</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Token</code> <p>A spaCy token.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the token is a Roman numeral.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef is_roman(self, token: Token) -&gt; bool:\n    \"\"\"Detect Roman numerals (capitals only).\n\n    Args:\n        token (Token): A spaCy token.\n\n    Returns:\n        bool: True if the token is a Roman numeral.\n    \"\"\"\n    if token.text == \"\":\n        return False\n    pattern = r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\"\n    return bool(re.search(pattern, token.text))\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.id","title":"<code>id: str = 'is_roman'</code>  <code>class-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.attr","title":"<code>attr: Optional[str] = 'is_roman'</code>  <code>pydantic-field</code>","text":"<p>The name of the attribute to add to the tokens.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.default","title":"<code>default: Optional[Any] = None</code>  <code>pydantic-field</code>","text":"<p>The default value of the attribute.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialise the filter object and set custom attribute extensions.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialise the filter object and set custom attribute extensions.\"\"\"\n    super().__init__(**data)\n    if self.attr:\n        self._set_extensions(self.attr, self.default)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.__call__","title":"<code>__call__(doc: Optional[Doc] = Field(default=None, description='A spaCy doc.'), attr: Optional[str] = Field(default=None, description='The name of the attribute to add to the tokens.'), default: Optional[Any] = Field(default=None, description='The default value of the attribute.')) -&gt; Doc</code>","text":"<p>Apply the filter.</p> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(\n    self,\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n    attr: Optional[str] = Field(\n        default=None, description=\"The name of the attribute to add to the tokens.\"\n    ),\n    default: Optional[Any] = Field(\n        default=None, description=\"The default value of the attribute.\"\n    ),\n) -&gt; Doc:\n    \"\"\"Apply the filter.\n\n    Returns:\n        Doc: The filtered doc.\n    \"\"\"\n    # Validation\n    if doc:\n        self.doc = doc\n    if attr:\n        self.attr = attr\n    if default is not None:\n        self.default = default\n\n    # Use instance attributes if we have them\n    working_doc = self.doc if self.doc is not None else doc\n    working_attr = (\n        self.attr if hasattr(self, \"attr\") and self.attr is not None else attr\n    )\n    working_default = (\n        self.default\n        if hasattr(self, \"default\") and self.default is not None\n        else default\n    )\n\n    # Set custom extensions\n    if working_attr:\n        self._set_extensions(working_attr, working_default)\n\n    # Apply the filter only if we have a valid doc\n    if working_doc is not None:\n        # Store matches for tokens that are Roman numerals\n        self.matches = []\n        for i, token in enumerate(working_doc):\n            is_roman_result = self.is_roman(token)\n            setattr(working_doc[i]._, working_attr, is_roman_result)\n            if is_roman_result:\n                self.matches.append((0, i, i + 1))\n\n    return working_doc if working_doc is not None else doc\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsRomanFilter.is_roman","title":"<code>is_roman(token: Token) -&gt; bool</code>","text":"<p>Detect Roman numerals (capitals only).</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Token</code> <p>A spaCy token.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the token is a Roman numeral.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef is_roman(self, token: Token) -&gt; bool:\n    \"\"\"Detect Roman numerals (capitals only).\n\n    Args:\n        token (Token): A spaCy token.\n\n    Returns:\n        bool: True if the token is a Roman numeral.\n    \"\"\"\n    if token.text == \"\":\n        return False\n    pattern = r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\"\n    return bool(re.search(pattern, token.text))\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter","title":"<code>IsStopwordFilter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A filter to detect stop words in a spaCy doc.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>matcher</code>                 (<code>Optional[Matcher]</code>)             </li> <li> <code>matches</code>                 (<code>Optional[list[tuple[int, int, int]]]</code>)             </li> <li> <code>doc</code>                 (<code>Optional[Doc]</code>)             </li> <li> <code>stopwords</code>                 (<code>Optional[list | str]</code>)             </li> <li> <code>remove</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>case_sensitive</code>                 (<code>Optional[bool]</code>)             </li> </ul> Source code in <code>lexos/filter/filters.py</code> <pre><code>class IsStopwordFilter(BaseFilter):\n    \"\"\"A filter to detect stop words in a spaCy doc.\"\"\"\n\n    id: ClassVar[str] = \"is_stopword\"\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\")\n    stopwords: Optional[list | str] = Field(\n        default=None,\n        description=\"A list or string containing the stop word(s) to add or remove.\",\n    )\n    remove: Optional[bool] = Field(\n        default=False,\n        description=\"If True, the stop word(s) will be removed from the model.\",\n    )\n    case_sensitive: Optional[bool] = Field(\n        default=False,\n        description=\"If False (default), stop word changes apply to all case variations. If True, only the exact case provided is modified.\",\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    def __init__(self, **data: Any):\n        \"\"\"Initialise the filter object with configuration.\n\n        Args:\n            **data (Any): Configuration data\n        \"\"\"\n        super().__init__(**data)\n        self.stopwords = ensure_list(self.stopwords)\n\n    @validate_call(config=model_config)\n    def __call__(\n        self,\n        doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n        stopwords: Optional[list | str] = Field(\n            default=None,\n            description=\"A list or string containing the stop word(s) to add or remove.\",\n        ),\n        remove: Optional[bool] = Field(\n            default=None,\n            description=\"If True, the stop word(s) will be removed from the model.\",\n        ),\n        case_sensitive: Optional[bool] = Field(\n            default=None,\n            description=\"If False (default), stop word changes apply to all case variations. If True, only the exact case provided is modified.\",\n        ),\n    ) -&gt; Doc:\n        \"\"\"Apply the filter.\n\n        Returns:\n            Doc: The filtered doc.\n\n        Note:\n            This filter modifies the model defaults. If you need the model's original default stop words.\n            you will need to re-load the model.\n        \"\"\"\n        # Validation\n        if doc:\n            self.doc = doc\n        if stopwords is not None:\n            self.stopwords = ensure_list(stopwords)\n        if remove is not None:\n            self.remove = remove\n        if case_sensitive is not None:\n            self.case_sensitive = case_sensitive\n\n        # Use instance attributes if parameters are None\n        working_doc = self.doc if self.doc is not None else doc\n        # Handle stopwords carefully - convert to list if it's a pydantic ValidatorIterator\n        if (\n            stopwords is None\n            and hasattr(self, \"stopwords\")\n            and self.stopwords is not None\n        ):\n            try:\n                working_stopwords = list(self.stopwords)\n            except (TypeError, AttributeError):\n                working_stopwords = self.stopwords\n        else:\n            working_stopwords = stopwords\n        working_remove = (\n            self.remove\n            if hasattr(self, \"remove\") and self.remove is not None\n            else remove\n        )\n        working_case_sensitive = (\n            self.case_sensitive\n            if hasattr(self, \"case_sensitive\") and self.case_sensitive is not None\n            else case_sensitive\n            if case_sensitive is not None\n            else False\n        )\n\n        # Apply the filter only if we have valid inputs\n        if working_doc is not None and working_stopwords is not None:\n            # Ensure stopwords is iterable and properly formatted\n            if not isinstance(working_stopwords, list):\n                working_stopwords = ensure_list(working_stopwords)\n\n            # Get the vocab (shared across all docs from the same model)\n            vocab = working_doc.vocab\n\n            if working_remove:\n                for item in working_stopwords:\n                    if item is not None:  # Skip None values\n                        if working_case_sensitive:\n                            # Only modify the exact case provided\n                            vocab[item].is_stop = False\n                        else:\n                            # Modify common case variations\n                            vocab[item.lower()].is_stop = False\n                            vocab[item].is_stop = False\n                            vocab[item.capitalize()].is_stop = False\n            else:\n                for item in working_stopwords:\n                    if item is not None:  # Skip None values\n                        if working_case_sensitive:\n                            # Only modify the exact case provided\n                            vocab[item].is_stop = True\n                        else:\n                            # Modify common case variations\n                            vocab[item.lower()].is_stop = True\n                            vocab[item].is_stop = True\n                            vocab[item.capitalize()].is_stop = True\n\n            # Store matches for stopwords\n            self.matches = []\n            for i, token in enumerate(working_doc):\n                if token.is_stop:\n                    self.matches.append((0, i, i + 1))\n\n        return working_doc if working_doc is not None else doc\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.case_sensitive","title":"<code>case_sensitive: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>If False (default), stop word changes apply to all case variations. If True, only the exact case provided is modified.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.filtered_token_ids","title":"<code>filtered_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of filtered token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.filtered_tokens","title":"<code>filtered_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of filtered tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.matched_token_ids","title":"<code>matched_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of matched token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.matched_tokens","title":"<code>matched_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of matched tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.matcher","title":"<code>matcher: Optional[Matcher] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy matcher.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.matches","title":"<code>matches: Optional[list[tuple[int, int, int]]] = None</code>  <code>pydantic-field</code>","text":"<p>List of matches.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.remove","title":"<code>remove: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>If True, the stop word(s) will be removed from the model.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.__call__","title":"<code>__call__(doc: Optional[Doc] = Field(default=None, description='A spaCy doc.'), stopwords: Optional[list | str] = Field(default=None, description='A list or string containing the stop word(s) to add or remove.'), remove: Optional[bool] = Field(default=None, description='If True, the stop word(s) will be removed from the model.'), case_sensitive: Optional[bool] = Field(default=None, description='If False (default), stop word changes apply to all case variations. If True, only the exact case provided is modified.')) -&gt; Doc</code>","text":"<p>Apply the filter.</p> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Note <p>This filter modifies the model defaults. If you need the model's original default stop words. you will need to re-load the model.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(\n    self,\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n    stopwords: Optional[list | str] = Field(\n        default=None,\n        description=\"A list or string containing the stop word(s) to add or remove.\",\n    ),\n    remove: Optional[bool] = Field(\n        default=None,\n        description=\"If True, the stop word(s) will be removed from the model.\",\n    ),\n    case_sensitive: Optional[bool] = Field(\n        default=None,\n        description=\"If False (default), stop word changes apply to all case variations. If True, only the exact case provided is modified.\",\n    ),\n) -&gt; Doc:\n    \"\"\"Apply the filter.\n\n    Returns:\n        Doc: The filtered doc.\n\n    Note:\n        This filter modifies the model defaults. If you need the model's original default stop words.\n        you will need to re-load the model.\n    \"\"\"\n    # Validation\n    if doc:\n        self.doc = doc\n    if stopwords is not None:\n        self.stopwords = ensure_list(stopwords)\n    if remove is not None:\n        self.remove = remove\n    if case_sensitive is not None:\n        self.case_sensitive = case_sensitive\n\n    # Use instance attributes if parameters are None\n    working_doc = self.doc if self.doc is not None else doc\n    # Handle stopwords carefully - convert to list if it's a pydantic ValidatorIterator\n    if (\n        stopwords is None\n        and hasattr(self, \"stopwords\")\n        and self.stopwords is not None\n    ):\n        try:\n            working_stopwords = list(self.stopwords)\n        except (TypeError, AttributeError):\n            working_stopwords = self.stopwords\n    else:\n        working_stopwords = stopwords\n    working_remove = (\n        self.remove\n        if hasattr(self, \"remove\") and self.remove is not None\n        else remove\n    )\n    working_case_sensitive = (\n        self.case_sensitive\n        if hasattr(self, \"case_sensitive\") and self.case_sensitive is not None\n        else case_sensitive\n        if case_sensitive is not None\n        else False\n    )\n\n    # Apply the filter only if we have valid inputs\n    if working_doc is not None and working_stopwords is not None:\n        # Ensure stopwords is iterable and properly formatted\n        if not isinstance(working_stopwords, list):\n            working_stopwords = ensure_list(working_stopwords)\n\n        # Get the vocab (shared across all docs from the same model)\n        vocab = working_doc.vocab\n\n        if working_remove:\n            for item in working_stopwords:\n                if item is not None:  # Skip None values\n                    if working_case_sensitive:\n                        # Only modify the exact case provided\n                        vocab[item].is_stop = False\n                    else:\n                        # Modify common case variations\n                        vocab[item.lower()].is_stop = False\n                        vocab[item].is_stop = False\n                        vocab[item.capitalize()].is_stop = False\n        else:\n            for item in working_stopwords:\n                if item is not None:  # Skip None values\n                    if working_case_sensitive:\n                        # Only modify the exact case provided\n                        vocab[item].is_stop = True\n                    else:\n                        # Modify common case variations\n                        vocab[item.lower()].is_stop = True\n                        vocab[item].is_stop = True\n                        vocab[item.capitalize()].is_stop = True\n\n        # Store matches for stopwords\n        self.matches = []\n        for i, token in enumerate(working_doc):\n            if token.is_stop:\n                self.matches.append((0, i, i + 1))\n\n    return working_doc if working_doc is not None else doc\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.__init__","title":"<code>__init__(**data: Any)</code>","text":"<p>Initialise the filter object with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Configuration data</p> <code>{}</code> Source code in <code>lexos/filter/filters.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialise the filter object with configuration.\n\n    Args:\n        **data (Any): Configuration data\n    \"\"\"\n    super().__init__(**data)\n    self.stopwords = ensure_list(self.stopwords)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.get_filtered_doc","title":"<code>get_filtered_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the filtered tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the filtered tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_filtered_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the filtered tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the filtered tokens.\n    \"\"\"\n    words = [t.text for t in self.filtered_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.filtered_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.filtered_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.get_matched_doc","title":"<code>get_matched_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the matched tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the matched tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_matched_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the matched tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the matched tokens.\n    \"\"\"\n    words = [t.text for t in self.matched_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.matched_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.matched_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.id","title":"<code>id: str = 'is_stopword'</code>  <code>class-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.stopwords","title":"<code>stopwords: Optional[list | str]</code>  <code>pydantic-field</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.remove","title":"<code>remove: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>If True, the stop word(s) will be removed from the model.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.__init__","title":"<code>__init__(**data: Any)</code>","text":"<p>Initialise the filter object with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Configuration data</p> <code>{}</code> Source code in <code>lexos/filter/filters.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialise the filter object with configuration.\n\n    Args:\n        **data (Any): Configuration data\n    \"\"\"\n    super().__init__(**data)\n    self.stopwords = ensure_list(self.stopwords)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsStopwordFilter.__call__","title":"<code>__call__(doc: Optional[Doc] = Field(default=None, description='A spaCy doc.'), stopwords: Optional[list | str] = Field(default=None, description='A list or string containing the stop word(s) to add or remove.'), remove: Optional[bool] = Field(default=None, description='If True, the stop word(s) will be removed from the model.'), case_sensitive: Optional[bool] = Field(default=None, description='If False (default), stop word changes apply to all case variations. If True, only the exact case provided is modified.')) -&gt; Doc</code>","text":"<p>Apply the filter.</p> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Note <p>This filter modifies the model defaults. If you need the model's original default stop words. you will need to re-load the model.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(\n    self,\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n    stopwords: Optional[list | str] = Field(\n        default=None,\n        description=\"A list or string containing the stop word(s) to add or remove.\",\n    ),\n    remove: Optional[bool] = Field(\n        default=None,\n        description=\"If True, the stop word(s) will be removed from the model.\",\n    ),\n    case_sensitive: Optional[bool] = Field(\n        default=None,\n        description=\"If False (default), stop word changes apply to all case variations. If True, only the exact case provided is modified.\",\n    ),\n) -&gt; Doc:\n    \"\"\"Apply the filter.\n\n    Returns:\n        Doc: The filtered doc.\n\n    Note:\n        This filter modifies the model defaults. If you need the model's original default stop words.\n        you will need to re-load the model.\n    \"\"\"\n    # Validation\n    if doc:\n        self.doc = doc\n    if stopwords is not None:\n        self.stopwords = ensure_list(stopwords)\n    if remove is not None:\n        self.remove = remove\n    if case_sensitive is not None:\n        self.case_sensitive = case_sensitive\n\n    # Use instance attributes if parameters are None\n    working_doc = self.doc if self.doc is not None else doc\n    # Handle stopwords carefully - convert to list if it's a pydantic ValidatorIterator\n    if (\n        stopwords is None\n        and hasattr(self, \"stopwords\")\n        and self.stopwords is not None\n    ):\n        try:\n            working_stopwords = list(self.stopwords)\n        except (TypeError, AttributeError):\n            working_stopwords = self.stopwords\n    else:\n        working_stopwords = stopwords\n    working_remove = (\n        self.remove\n        if hasattr(self, \"remove\") and self.remove is not None\n        else remove\n    )\n    working_case_sensitive = (\n        self.case_sensitive\n        if hasattr(self, \"case_sensitive\") and self.case_sensitive is not None\n        else case_sensitive\n        if case_sensitive is not None\n        else False\n    )\n\n    # Apply the filter only if we have valid inputs\n    if working_doc is not None and working_stopwords is not None:\n        # Ensure stopwords is iterable and properly formatted\n        if not isinstance(working_stopwords, list):\n            working_stopwords = ensure_list(working_stopwords)\n\n        # Get the vocab (shared across all docs from the same model)\n        vocab = working_doc.vocab\n\n        if working_remove:\n            for item in working_stopwords:\n                if item is not None:  # Skip None values\n                    if working_case_sensitive:\n                        # Only modify the exact case provided\n                        vocab[item].is_stop = False\n                    else:\n                        # Modify common case variations\n                        vocab[item.lower()].is_stop = False\n                        vocab[item].is_stop = False\n                        vocab[item.capitalize()].is_stop = False\n        else:\n            for item in working_stopwords:\n                if item is not None:  # Skip None values\n                    if working_case_sensitive:\n                        # Only modify the exact case provided\n                        vocab[item].is_stop = True\n                    else:\n                        # Modify common case variations\n                        vocab[item.lower()].is_stop = True\n                        vocab[item].is_stop = True\n                        vocab[item.capitalize()].is_stop = True\n\n        # Store matches for stopwords\n        self.matches = []\n        for i, token in enumerate(working_doc):\n            if token.is_stop:\n                self.matches.append((0, i, i + 1))\n\n    return working_doc if working_doc is not None else doc\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter","title":"<code>IsWordFilter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFilter</code></p> <p>A filter to detect words in a spaCy doc.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>matcher</code>                 (<code>Optional[Matcher]</code>)             </li> <li> <code>matches</code>                 (<code>Optional[list[tuple[int, int, int]]]</code>)             </li> <li> <code>doc</code>                 (<code>Optional[Doc]</code>)             </li> <li> <code>attr</code>                 (<code>Optional[str]</code>)             </li> <li> <code>default</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>exclude</code>                 (<code>Optional[Optional[list[str] | str]]</code>)             </li> <li> <code>exclude_digits</code>                 (<code>Optional[Optional[bool]]</code>)             </li> <li> <code>exclude_roman_numerals</code>                 (<code>Optional[Optional[bool]]</code>)             </li> <li> <code>exclude_pattern</code>                 (<code>Optional[list[str] | str]</code>)             </li> </ul> Source code in <code>lexos/filter/filters.py</code> <pre><code>class IsWordFilter(BaseFilter):\n    \"\"\"A filter to detect words in a spaCy doc.\"\"\"\n\n    id: ClassVar[str] = \"is_word\"\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\")\n    attr: Optional[str] = Field(\n        default=\"is_word\", description=\"The name of the attribute to add to the tokens.\"\n    )\n    default: Optional[bool] = Field(\n        default=False, description=\"The default value of the attribute.\"\n    )\n    exclude: Optional[Optional[list[str] | str]] = Field(\n        default=[\" \", \"\\n\"],\n        description=\"A string/regex or list of strings/regex patterns to exclude.\",\n    )\n    exclude_digits: Optional[Optional[bool]] = Field(\n        default=False, description=\"If True, digits will not be treated as words.\"\n    )\n    exclude_roman_numerals: Optional[Optional[bool]] = Field(\n        default=False,\n        description=\"Same as above for Roman numerals, but only works on capital letters.\",\n    )\n    exclude_pattern: Optional[list[str] | str] = Field(\n        default=None,\n        description=\"Additional patterns to add to the default exclude list.\",\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    def __init__(self, **data: Any):\n        \"\"\"Initialise the filter object with configuration.\n\n        Args:\n            **data (Any): Configuration data\n        \"\"\"\n        super().__init__(**data)\n        if self.attr:\n            self._set_extensions(self.attr, self.default)\n\n    @validate_call(config=model_config)\n    def __call__(\n        self,\n        doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n        attr: Optional[str] = Field(\n            default=None,\n            description=\"The name of the attribute to add to the tokens.\",\n        ),\n        default: Optional[bool] = Field(\n            default=None, description=\"The default value of the attribute.\"\n        ),\n        exclude: Optional[list[str] | str] = Field(\n            default=None,\n            description=\"A string/regex or list of strings/regex patterns to exclude.\",\n        ),\n        exclude_digits: Optional[bool] = Field(\n            default=None, description=\"If True, digits will not be treated as words.\"\n        ),\n        exclude_roman_numerals: Optional[bool] = Field(\n            default=None,\n            description=\"Same as above for Roman numerals, but only works on capital letters.\",\n        ),\n        exclude_pattern: Optional[list[str] | str] = Field(\n            default=None,\n            description=\"Additional patterns to add to the default exclude list.\",\n        ),\n    ) -&gt; Doc:\n        \"\"\"Apply the filter.\n\n        Returns:\n            Doc: The filtered doc.\n        \"\"\"\n        # Assign keyword variables to the instance attributes\n        if doc:\n            self.doc = doc\n        if exclude:\n            self.exclude = ensure_list(exclude)\n        if exclude_digits is not None:\n            self.exclude_digits = exclude_digits\n        if exclude_roman_numerals is not None:\n            self.exclude_roman_numerals = exclude_roman_numerals\n        if exclude_pattern:\n            self.exclude_pattern = ensure_list(exclude_pattern)\n        if attr:\n            self.attr = attr\n        if default is not None:\n            self.default = default\n\n        # Use instance attributes if we have them\n        working_doc = self.doc if self.doc is not None else doc\n        working_attr = (\n            self.attr if hasattr(self, \"attr\") and self.attr is not None else attr\n        )\n        working_default = (\n            self.default\n            if hasattr(self, \"default\") and self.default is not None\n            else default\n        )\n\n        # Set ._is_word extension\n        if working_attr:\n            self._set_extensions(working_attr, working_default)\n\n        # Apply the filter only if we have a valid doc\n        if working_doc is not None:\n            # Store matches for tokens that are words\n            self.matches = []\n            for i, token in enumerate(working_doc):\n                is_word_result = self.is_word(token)\n                setattr(working_doc[i]._, working_attr, is_word_result)\n                if is_word_result:\n                    self.matches.append((0, i, i + 1))\n\n        return working_doc if working_doc is not None else doc\n\n    def _is_roman_numeral(self, string: str) -&gt; bool:\n        \"\"\"Check if a string is a Roman numeral.\n\n        Args:\n            string (str): A string.\n\n        Returns:\n            bool: True if the string is a Roman numeral.\n        \"\"\"\n        if string == \"\":\n            return False\n        pattern = r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\"\n        return bool(re.search(pattern, string))\n\n    @validate_call(config=model_config)\n    def is_word(self, token: Token) -&gt; bool:\n        \"\"\"Detect words.\n\n        Args:\n            token (Token): A spaCy token.\n\n        Returns:\n            bool: True if the token is a word.\n        \"\"\"\n        predicates = []\n        if self.exclude_digits:\n            predicates.append(lambda token: token.text.isalpha())\n        else:\n            predicates.append(\n                lambda token: token.text.isalpha() or token.text.isdigit()\n            )\n        if self.exclude_roman_numerals:\n            predicates.append(lambda token: not self._is_roman_numeral(token.text))\n        if self.exclude_pattern:\n            self.exclude += self.exclude_pattern\n        if len(self.exclude) &gt; 0:\n            exclude_pat = \"|\".join(self.exclude)\n            predicates.append(lambda token: re.search(exclude_pat, token.text) is None)\n        return all([f(token) for f in predicates])\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.attr","title":"<code>attr: Optional[str] = 'is_word'</code>  <code>pydantic-field</code>","text":"<p>The name of the attribute to add to the tokens.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.default","title":"<code>default: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>The default value of the attribute.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude","title":"<code>exclude: Optional[Optional[list[str] | str]] = [' ', '\\n']</code>  <code>pydantic-field</code>","text":"<p>A string/regex or list of strings/regex patterns to exclude.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude_digits","title":"<code>exclude_digits: Optional[Optional[bool]] = False</code>  <code>pydantic-field</code>","text":"<p>If True, digits will not be treated as words.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude_pattern","title":"<code>exclude_pattern: Optional[list[str] | str] = None</code>  <code>pydantic-field</code>","text":"<p>Additional patterns to add to the default exclude list.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude_roman_numerals","title":"<code>exclude_roman_numerals: Optional[Optional[bool]] = False</code>  <code>pydantic-field</code>","text":"<p>Same as above for Roman numerals, but only works on capital letters.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.filtered_token_ids","title":"<code>filtered_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of filtered token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.filtered_tokens","title":"<code>filtered_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of filtered tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.matched_token_ids","title":"<code>matched_token_ids: set[int]</code>  <code>property</code>","text":"<p>A list of matched token ids after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.matched_tokens","title":"<code>matched_tokens: list[int]</code>  <code>property</code>","text":"<p>A list of matched tokens after the filter was applied.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.matcher","title":"<code>matcher: Optional[Matcher] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy matcher.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.matches","title":"<code>matches: Optional[list[tuple[int, int, int]]] = None</code>  <code>pydantic-field</code>","text":"<p>List of matches.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.__call__","title":"<code>__call__(doc: Optional[Doc] = Field(default=None, description='A spaCy doc.'), attr: Optional[str] = Field(default=None, description='The name of the attribute to add to the tokens.'), default: Optional[bool] = Field(default=None, description='The default value of the attribute.'), exclude: Optional[list[str] | str] = Field(default=None, description='A string/regex or list of strings/regex patterns to exclude.'), exclude_digits: Optional[bool] = Field(default=None, description='If True, digits will not be treated as words.'), exclude_roman_numerals: Optional[bool] = Field(default=None, description='Same as above for Roman numerals, but only works on capital letters.'), exclude_pattern: Optional[list[str] | str] = Field(default=None, description='Additional patterns to add to the default exclude list.')) -&gt; Doc</code>","text":"<p>Apply the filter.</p> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(\n    self,\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n    attr: Optional[str] = Field(\n        default=None,\n        description=\"The name of the attribute to add to the tokens.\",\n    ),\n    default: Optional[bool] = Field(\n        default=None, description=\"The default value of the attribute.\"\n    ),\n    exclude: Optional[list[str] | str] = Field(\n        default=None,\n        description=\"A string/regex or list of strings/regex patterns to exclude.\",\n    ),\n    exclude_digits: Optional[bool] = Field(\n        default=None, description=\"If True, digits will not be treated as words.\"\n    ),\n    exclude_roman_numerals: Optional[bool] = Field(\n        default=None,\n        description=\"Same as above for Roman numerals, but only works on capital letters.\",\n    ),\n    exclude_pattern: Optional[list[str] | str] = Field(\n        default=None,\n        description=\"Additional patterns to add to the default exclude list.\",\n    ),\n) -&gt; Doc:\n    \"\"\"Apply the filter.\n\n    Returns:\n        Doc: The filtered doc.\n    \"\"\"\n    # Assign keyword variables to the instance attributes\n    if doc:\n        self.doc = doc\n    if exclude:\n        self.exclude = ensure_list(exclude)\n    if exclude_digits is not None:\n        self.exclude_digits = exclude_digits\n    if exclude_roman_numerals is not None:\n        self.exclude_roman_numerals = exclude_roman_numerals\n    if exclude_pattern:\n        self.exclude_pattern = ensure_list(exclude_pattern)\n    if attr:\n        self.attr = attr\n    if default is not None:\n        self.default = default\n\n    # Use instance attributes if we have them\n    working_doc = self.doc if self.doc is not None else doc\n    working_attr = (\n        self.attr if hasattr(self, \"attr\") and self.attr is not None else attr\n    )\n    working_default = (\n        self.default\n        if hasattr(self, \"default\") and self.default is not None\n        else default\n    )\n\n    # Set ._is_word extension\n    if working_attr:\n        self._set_extensions(working_attr, working_default)\n\n    # Apply the filter only if we have a valid doc\n    if working_doc is not None:\n        # Store matches for tokens that are words\n        self.matches = []\n        for i, token in enumerate(working_doc):\n            is_word_result = self.is_word(token)\n            setattr(working_doc[i]._, working_attr, is_word_result)\n            if is_word_result:\n                self.matches.append((0, i, i + 1))\n\n    return working_doc if working_doc is not None else doc\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.__init__","title":"<code>__init__(**data: Any)</code>","text":"<p>Initialise the filter object with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Configuration data</p> <code>{}</code> Source code in <code>lexos/filter/filters.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialise the filter object with configuration.\n\n    Args:\n        **data (Any): Configuration data\n    \"\"\"\n    super().__init__(**data)\n    if self.attr:\n        self._set_extensions(self.attr, self.default)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.get_filtered_doc","title":"<code>get_filtered_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the filtered tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the filtered tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_filtered_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the filtered tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the filtered tokens.\n    \"\"\"\n    words = [t.text for t in self.filtered_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.filtered_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.filtered_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.get_matched_doc","title":"<code>get_matched_doc(add_spaces: bool = False) -&gt; Doc</code>","text":"<p>Get a new doc from the matched tokens.</p> <p>Parameters:</p> Name Type Description Default <code>add_spaces</code> <code>bool</code> <p>If True, add a space after every token.              If False, preserve original whitespace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A new spaCy Doc containing only the matched tokens.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def get_matched_doc(self, add_spaces: bool = False) -&gt; Doc:\n    \"\"\"Get a new doc from the matched tokens.\n\n    Args:\n        add_spaces (bool): If True, add a space after every token.\n                         If False, preserve original whitespace. Default is False.\n\n    Returns:\n        Doc: A new spaCy Doc containing only the matched tokens.\n    \"\"\"\n    words = [t.text for t in self.matched_tokens]\n    if add_spaces:\n        spaces = [\" \" for _ in self.matched_tokens]\n    else:\n        spaces = [t.whitespace_ for t in self.matched_tokens]\n    return Doc(self.doc.vocab, words=words, spaces=spaces)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.is_word","title":"<code>is_word(token: Token) -&gt; bool</code>","text":"<p>Detect words.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Token</code> <p>A spaCy token.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the token is a word.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef is_word(self, token: Token) -&gt; bool:\n    \"\"\"Detect words.\n\n    Args:\n        token (Token): A spaCy token.\n\n    Returns:\n        bool: True if the token is a word.\n    \"\"\"\n    predicates = []\n    if self.exclude_digits:\n        predicates.append(lambda token: token.text.isalpha())\n    else:\n        predicates.append(\n            lambda token: token.text.isalpha() or token.text.isdigit()\n        )\n    if self.exclude_roman_numerals:\n        predicates.append(lambda token: not self._is_roman_numeral(token.text))\n    if self.exclude_pattern:\n        self.exclude += self.exclude_pattern\n    if len(self.exclude) &gt; 0:\n        exclude_pat = \"|\".join(self.exclude)\n        predicates.append(lambda token: re.search(exclude_pat, token.text) is None)\n    return all([f(token) for f in predicates])\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.id","title":"<code>id: str = 'is_word'</code>  <code>class-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.doc","title":"<code>doc: Optional[Doc] = None</code>  <code>pydantic-field</code>","text":"<p>A spaCy doc.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.attr","title":"<code>attr: Optional[str] = 'is_word'</code>  <code>pydantic-field</code>","text":"<p>The name of the attribute to add to the tokens.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.default","title":"<code>default: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>The default value of the attribute.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude","title":"<code>exclude: Optional[Optional[list[str] | str]] = [' ', '\\n']</code>  <code>pydantic-field</code>","text":"<p>A string/regex or list of strings/regex patterns to exclude.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude_digits","title":"<code>exclude_digits: Optional[Optional[bool]] = False</code>  <code>pydantic-field</code>","text":"<p>If True, digits will not be treated as words.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude_roman_numerals","title":"<code>exclude_roman_numerals: Optional[Optional[bool]] = False</code>  <code>pydantic-field</code>","text":"<p>Same as above for Roman numerals, but only works on capital letters.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.exclude_pattern","title":"<code>exclude_pattern: Optional[list[str] | str] = None</code>  <code>pydantic-field</code>","text":"<p>Additional patterns to add to the default exclude list.</p>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.__init__","title":"<code>__init__(**data: Any)</code>","text":"<p>Initialise the filter object with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Configuration data</p> <code>{}</code> Source code in <code>lexos/filter/filters.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initialise the filter object with configuration.\n\n    Args:\n        **data (Any): Configuration data\n    \"\"\"\n    super().__init__(**data)\n    if self.attr:\n        self._set_extensions(self.attr, self.default)\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.__call__","title":"<code>__call__(doc: Optional[Doc] = Field(default=None, description='A spaCy doc.'), attr: Optional[str] = Field(default=None, description='The name of the attribute to add to the tokens.'), default: Optional[bool] = Field(default=None, description='The default value of the attribute.'), exclude: Optional[list[str] | str] = Field(default=None, description='A string/regex or list of strings/regex patterns to exclude.'), exclude_digits: Optional[bool] = Field(default=None, description='If True, digits will not be treated as words.'), exclude_roman_numerals: Optional[bool] = Field(default=None, description='Same as above for Roman numerals, but only works on capital letters.'), exclude_pattern: Optional[list[str] | str] = Field(default=None, description='Additional patterns to add to the default exclude list.')) -&gt; Doc</code>","text":"<p>Apply the filter.</p> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>The filtered doc.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef __call__(\n    self,\n    doc: Optional[Doc] = Field(default=None, description=\"A spaCy doc.\"),\n    attr: Optional[str] = Field(\n        default=None,\n        description=\"The name of the attribute to add to the tokens.\",\n    ),\n    default: Optional[bool] = Field(\n        default=None, description=\"The default value of the attribute.\"\n    ),\n    exclude: Optional[list[str] | str] = Field(\n        default=None,\n        description=\"A string/regex or list of strings/regex patterns to exclude.\",\n    ),\n    exclude_digits: Optional[bool] = Field(\n        default=None, description=\"If True, digits will not be treated as words.\"\n    ),\n    exclude_roman_numerals: Optional[bool] = Field(\n        default=None,\n        description=\"Same as above for Roman numerals, but only works on capital letters.\",\n    ),\n    exclude_pattern: Optional[list[str] | str] = Field(\n        default=None,\n        description=\"Additional patterns to add to the default exclude list.\",\n    ),\n) -&gt; Doc:\n    \"\"\"Apply the filter.\n\n    Returns:\n        Doc: The filtered doc.\n    \"\"\"\n    # Assign keyword variables to the instance attributes\n    if doc:\n        self.doc = doc\n    if exclude:\n        self.exclude = ensure_list(exclude)\n    if exclude_digits is not None:\n        self.exclude_digits = exclude_digits\n    if exclude_roman_numerals is not None:\n        self.exclude_roman_numerals = exclude_roman_numerals\n    if exclude_pattern:\n        self.exclude_pattern = ensure_list(exclude_pattern)\n    if attr:\n        self.attr = attr\n    if default is not None:\n        self.default = default\n\n    # Use instance attributes if we have them\n    working_doc = self.doc if self.doc is not None else doc\n    working_attr = (\n        self.attr if hasattr(self, \"attr\") and self.attr is not None else attr\n    )\n    working_default = (\n        self.default\n        if hasattr(self, \"default\") and self.default is not None\n        else default\n    )\n\n    # Set ._is_word extension\n    if working_attr:\n        self._set_extensions(working_attr, working_default)\n\n    # Apply the filter only if we have a valid doc\n    if working_doc is not None:\n        # Store matches for tokens that are words\n        self.matches = []\n        for i, token in enumerate(working_doc):\n            is_word_result = self.is_word(token)\n            setattr(working_doc[i]._, working_attr, is_word_result)\n            if is_word_result:\n                self.matches.append((0, i, i + 1))\n\n    return working_doc if working_doc is not None else doc\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter._is_roman_numeral","title":"<code>_is_roman_numeral(string: str) -&gt; bool</code>","text":"<p>Check if a string is a Roman numeral.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>A string.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the string is a Roman numeral.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>def _is_roman_numeral(self, string: str) -&gt; bool:\n    \"\"\"Check if a string is a Roman numeral.\n\n    Args:\n        string (str): A string.\n\n    Returns:\n        bool: True if the string is a Roman numeral.\n    \"\"\"\n    if string == \"\":\n        return False\n    pattern = r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\"\n    return bool(re.search(pattern, string))\n</code></pre>"},{"location":"api/filter/filter/#lexos.filter.filters.IsWordFilter.is_word","title":"<code>is_word(token: Token) -&gt; bool</code>","text":"<p>Detect words.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Token</code> <p>A spaCy token.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the token is a word.</p> Source code in <code>lexos/filter/filters.py</code> <pre><code>@validate_call(config=model_config)\ndef is_word(self, token: Token) -&gt; bool:\n    \"\"\"Detect words.\n\n    Args:\n        token (Token): A spaCy token.\n\n    Returns:\n        bool: True if the token is a word.\n    \"\"\"\n    predicates = []\n    if self.exclude_digits:\n        predicates.append(lambda token: token.text.isalpha())\n    else:\n        predicates.append(\n            lambda token: token.text.isalpha() or token.text.isdigit()\n        )\n    if self.exclude_roman_numerals:\n        predicates.append(lambda token: not self._is_roman_numeral(token.text))\n    if self.exclude_pattern:\n        self.exclude += self.exclude_pattern\n    if len(self.exclude) &gt; 0:\n        exclude_pat = \"|\".join(self.exclude)\n        predicates.append(lambda token: re.search(exclude_pat, token.text) is None)\n    return all([f(token) for f in predicates])\n</code></pre>"},{"location":"api/io/","title":"IO","text":"<p>The <code>io</code> module manages input and output functions. It contains three main loader modules: <code>loader</code>, <code>parallel_loader</code>, and <code>data_loader</code>. The <code>loader</code> provides an interface for loading texts in a variety of formats, whether from local files or from urls. The <code>parallel_loader</code> module provides an optimized loader for large data sets using parallel I/O operations. The <code>data_loader</code> module provides method for loading or downloading large numbers of texts that are generally stored in a single file. All three modules inherit from a <code>BaseLoader</code> class, which provides common functionality for loading texts.</p>"},{"location":"api/io/base_loader/","title":"base_loader","text":""},{"location":"api/io/base_loader/#the-baseloader-class","title":"The BaseLoader Class","text":"<p>The <code>BaseLoader</code> class is the base class for all loaders in the IO module. Additional modules should inherit from BaseLoader.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader","title":"<code>BaseLoader</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>BaseLoader.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>paths</code>                 (<code>list</code>)             </li> <li> <code>mime_types</code>                 (<code>list</code>)             </li> <li> <code>names</code>                 (<code>list</code>)             </li> <li> <code>texts</code>                 (<code>list</code>)             </li> <li> <code>errors</code>                 (<code>list</code>)             </li> </ul> Source code in <code>lexos/io/base_loader.py</code> <pre><code>class BaseLoader(BaseModel, ABC):\n    \"\"\"BaseLoader.\"\"\"\n\n    paths: list = Field(default=[], description=\"The list of paths.\")\n    mime_types: list = Field(default=[], description=\"The list of text mime types.\")\n    names: list = Field(default=[], description=\"The list of text names.\")\n    texts: list = Field(default=[], description=\"The list of loaded texts.\")\n    errors: list = Field(default=[], description=\"The list of loading errors.\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __iter__(self) -&gt; Generator[dict, None, None]:\n        \"\"\"Iterate through the records.\"\"\"\n        return (record for record in self.records)\n\n    @property\n    def data(self) -&gt; dict[str, list]:\n        \"\"\"Get the data as a dictionary.\n\n        Returns:\n            dict[str, list]: A dictionary containing the paths, mime_types, names, texts, and errors.\n        \"\"\"\n        return {\n            \"paths\": self.paths,\n            \"mime_types\": self.mime_types,\n            \"names\": self.names,\n            \"texts\": self.texts,\n            \"errors\": self.errors,\n        }\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        \"\"\"Get a pandas DataFrame of file records.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing file metadata and content.\n        \"\"\"\n        return pd.DataFrame(self.records)\n\n    @property\n    def records(self) -&gt; list[dict[str, str]]:\n        \"\"\"Get a list of file records.\n\n        Returns:\n            list[dict]: List of dictionaries containing file metadata and content.\n            Each dict has keys: path, mime_type, name, text\n\n        Raises:\n            ValueError: If the lengths of paths, mime_types, names and texts don't match.\n\n        Note:\n            Validates that all lists have the same length before returning the records.\n        \"\"\"\n        if not (\n            len(self.paths)\n            == len(self.mime_types)\n            == len(self.names)\n            == len(self.texts)\n        ):\n            raise LexosException(\"Mismatched lengths in file records data\")\n\n        return [\n            {\"name\": name, \"path\": path, \"mime_type\": mime_type, \"text\": text}\n            for name, path, mime_type, text in zip(\n                self.names, self.paths, self.mime_types, self.texts\n            )\n        ]\n\n    # Abstract method, skipped for coverage\n    @validate_call(config=model_config)  # pragma: no cover\n    @abstractmethod  # pragma: no cover\n    def load_dataset(self, dataset) -&gt; None:  # pragma: no cover\n        \"\"\"Load a dataset.\n\n        Args:\n            dataset (DataLoader): The dataset to load.\n        \"\"\"\n        ...\n\n    @validate_call(config=model_config)\n    def dedupe(self, subset: Optional[list[str]] = None) -&gt; pd.DataFrame:\n        \"\"\"Deduplicate a DataFrame.\n\n        Args:\n            subset (Optional[list[str]]): The columns to consider for deduplication.\n\n        Returns:\n            pd.DataFrame: The deduplicated DataFrame.\n        \"\"\"\n        if not self.df.empty:\n            df = self.df.copy()\n            df.drop_duplicates(\n                subset=subset, keep=\"first\", inplace=True, ignore_index=True\n            )\n            self.paths = df[\"path\"].tolist()\n            self.mime_types = df[\"mime_type\"].tolist()\n            self.names = df[\"name\"].tolist()\n            self.texts = df[\"text\"].tolist()\n\n    @validate_call(config=model_config)\n    def show_duplicates(\n        self, subset: Optional[list[str]] = None\n    ) -&gt; pd.DataFrame | None:\n        \"\"\"Show duplicates in a DataFrame.\n\n        Args:\n            subset (Optional[list[str]] = None): The columns to consider for checking duplicates.\n\n        Returns:\n            pd.DataFrame: The DataFrame with duplicates.\n        \"\"\"\n        if not self.df.empty:\n            df = self.df.copy()\n            return df[df.duplicated(subset=subset)]\n        return None\n\n    @validate_call(config=model_config)\n    def to_csv(self, path: Path | str, **kwargs) -&gt; None:\n        \"\"\"Save the data to a csv file.\n\n        Args:\n            path (Path | str): The path to save the csv file.\n        \"\"\"\n        self.df.to_csv(path, **kwargs)\n\n    @validate_call(config=model_config)\n    def to_excel(self, path: Path | str, **kwargs) -&gt; None:\n        \"\"\"Save the data to an Excel file.\n\n        Args:\n            path (Path | str): The path to save the csv file.\n        \"\"\"\n        self.df.to_csv(path, **kwargs)\n\n    @validate_call(config=model_config)\n    def to_json(self, path: Path | str, **kwargs) -&gt; None:\n        \"\"\"Save the data to a json file.\n\n        Args:\n            path (Path | str): The path to save the csv file.\n        \"\"\"\n        self.df.to_json(path, **kwargs)\n\n    @validate_call(config=model_config)\n    def reset(self) -&gt; None:\n        \"\"\"Reset the class attributes to empty lists.\"\"\"\n        self.paths = []\n        self.mime_types = []\n        self.names = []\n        self.texts = []\n        self.errors = []\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.data","title":"<code>data: dict[str, list]</code>  <code>property</code>","text":"<p>Get the data as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>dict[str, list]: A dictionary containing the paths, mime_types, names, texts, and errors.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Get a pandas DataFrame of file records.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame containing file metadata and content.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.errors","title":"<code>errors: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of loading errors.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.mime_types","title":"<code>mime_types: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of text mime types.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.names","title":"<code>names: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of text names.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.paths","title":"<code>paths: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of paths.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.records","title":"<code>records: list[dict[str, str]]</code>  <code>property</code>","text":"<p>Get a list of file records.</p> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict]: List of dictionaries containing file metadata and content.</p> <code>list[dict[str, str]]</code> <p>Each dict has keys: path, mime_type, name, text</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of paths, mime_types, names and texts don't match.</p> Note <p>Validates that all lists have the same length before returning the records.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.texts","title":"<code>texts: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of loaded texts.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.__iter__","title":"<code>__iter__() -&gt; Generator[dict, None, None]</code>","text":"<p>Iterate through the records.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>def __iter__(self) -&gt; Generator[dict, None, None]:\n    \"\"\"Iterate through the records.\"\"\"\n    return (record for record in self.records)\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.dedupe","title":"<code>dedupe(subset: Optional[list[str]] = None) -&gt; pd.DataFrame</code>","text":"<p>Deduplicate a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]]</code> <p>The columns to consider for deduplication.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The deduplicated DataFrame.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef dedupe(self, subset: Optional[list[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"Deduplicate a DataFrame.\n\n    Args:\n        subset (Optional[list[str]]): The columns to consider for deduplication.\n\n    Returns:\n        pd.DataFrame: The deduplicated DataFrame.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        df.drop_duplicates(\n            subset=subset, keep=\"first\", inplace=True, ignore_index=True\n        )\n        self.paths = df[\"path\"].tolist()\n        self.mime_types = df[\"mime_type\"].tolist()\n        self.names = df[\"name\"].tolist()\n        self.texts = df[\"text\"].tolist()\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.load_dataset","title":"<code>load_dataset(dataset) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataLoader</code> <p>The dataset to load.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)  # pragma: no cover\n@abstractmethod  # pragma: no cover\ndef load_dataset(self, dataset) -&gt; None:  # pragma: no cover\n    \"\"\"Load a dataset.\n\n    Args:\n        dataset (DataLoader): The dataset to load.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset the class attributes to empty lists.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef reset(self) -&gt; None:\n    \"\"\"Reset the class attributes to empty lists.\"\"\"\n    self.paths = []\n    self.mime_types = []\n    self.names = []\n    self.texts = []\n    self.errors = []\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.show_duplicates","title":"<code>show_duplicates(subset: Optional[list[str]] = None) -&gt; pd.DataFrame | None</code>","text":"<p>Show duplicates in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]] = None</code> <p>The columns to consider for checking duplicates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>pd.DataFrame: The DataFrame with duplicates.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef show_duplicates(\n    self, subset: Optional[list[str]] = None\n) -&gt; pd.DataFrame | None:\n    \"\"\"Show duplicates in a DataFrame.\n\n    Args:\n        subset (Optional[list[str]] = None): The columns to consider for checking duplicates.\n\n    Returns:\n        pd.DataFrame: The DataFrame with duplicates.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        return df[df.duplicated(subset=subset)]\n    return None\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.to_csv","title":"<code>to_csv(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_csv(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a csv file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.to_excel","title":"<code>to_excel(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_excel(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to an Excel file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.to_json","title":"<code>to_json(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_json(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a json file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_json(path, **kwargs)\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.__iter__","title":"<code>__iter__() -&gt; Generator[dict, None, None]</code>","text":"<p>Iterate through the records.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>def __iter__(self) -&gt; Generator[dict, None, None]:\n    \"\"\"Iterate through the records.\"\"\"\n    return (record for record in self.records)\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.data","title":"<code>data: dict[str, list]</code>  <code>property</code>","text":"<p>Get the data as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>dict[str, list]: A dictionary containing the paths, mime_types, names, texts, and errors.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Get a pandas DataFrame of file records.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame containing file metadata and content.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.records","title":"<code>records: list[dict[str, str]]</code>  <code>property</code>","text":"<p>Get a list of file records.</p> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict]: List of dictionaries containing file metadata and content.</p> <code>list[dict[str, str]]</code> <p>Each dict has keys: path, mime_type, name, text</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of paths, mime_types, names and texts don't match.</p> Note <p>Validates that all lists have the same length before returning the records.</p>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.load_dataset","title":"<code>load_dataset(dataset) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataLoader</code> <p>The dataset to load.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)  # pragma: no cover\n@abstractmethod  # pragma: no cover\ndef load_dataset(self, dataset) -&gt; None:  # pragma: no cover\n    \"\"\"Load a dataset.\n\n    Args:\n        dataset (DataLoader): The dataset to load.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.dedupe","title":"<code>dedupe(subset: Optional[list[str]] = None) -&gt; pd.DataFrame</code>","text":"<p>Deduplicate a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]]</code> <p>The columns to consider for deduplication.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The deduplicated DataFrame.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef dedupe(self, subset: Optional[list[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"Deduplicate a DataFrame.\n\n    Args:\n        subset (Optional[list[str]]): The columns to consider for deduplication.\n\n    Returns:\n        pd.DataFrame: The deduplicated DataFrame.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        df.drop_duplicates(\n            subset=subset, keep=\"first\", inplace=True, ignore_index=True\n        )\n        self.paths = df[\"path\"].tolist()\n        self.mime_types = df[\"mime_type\"].tolist()\n        self.names = df[\"name\"].tolist()\n        self.texts = df[\"text\"].tolist()\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset the class attributes to empty lists.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef reset(self) -&gt; None:\n    \"\"\"Reset the class attributes to empty lists.\"\"\"\n    self.paths = []\n    self.mime_types = []\n    self.names = []\n    self.texts = []\n    self.errors = []\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.show_duplicates","title":"<code>show_duplicates(subset: Optional[list[str]] = None) -&gt; pd.DataFrame | None</code>","text":"<p>Show duplicates in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]] = None</code> <p>The columns to consider for checking duplicates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>pd.DataFrame: The DataFrame with duplicates.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef show_duplicates(\n    self, subset: Optional[list[str]] = None\n) -&gt; pd.DataFrame | None:\n    \"\"\"Show duplicates in a DataFrame.\n\n    Args:\n        subset (Optional[list[str]] = None): The columns to consider for checking duplicates.\n\n    Returns:\n        pd.DataFrame: The DataFrame with duplicates.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        return df[df.duplicated(subset=subset)]\n    return None\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.to_csv","title":"<code>to_csv(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_csv(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a csv file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.to_excel","title":"<code>to_excel(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_excel(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to an Excel file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/base_loader/#lexos.io.base_loader.BaseLoader.to_json","title":"<code>to_json(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_json(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a json file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_json(path, **kwargs)\n</code></pre>"},{"location":"api/io/data_loader/","title":"Data Loader","text":""},{"location":"api/io/data_loader/#the-dataloader-class","title":"The <code>DataLoader</code> Class","text":"<p>The <code>DataLoader</code> class is the main class for loading datasets in various formats. It tries to be \"smart\" detecting the format as well as can be done so that you can use a common interface to load content regardless of source.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.Dataset","title":"<code>Dataset</code>  <code>dataclass</code>","text":"<p>Dataset class.</p> Source code in <code>lexos/io/data_loader.py</code> <pre><code>@dataclass\nclass Dataset:\n    \"\"\"Dataset class.\"\"\"\n\n    name: str\n    path: str\n    mime_type: str\n    text: str\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader","title":"<code>DataLoader</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLoader</code></p> <p>DataLoader.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>paths</code>                 (<code>list</code>)             </li> <li> <code>mime_types</code>                 (<code>list</code>)             </li> <li> <code>names</code>                 (<code>list</code>)             </li> <li> <code>texts</code>                 (<code>list</code>)             </li> <li> <code>errors</code>                 (<code>list</code>)             </li> </ul> Source code in <code>lexos/io/data_loader.py</code> <pre><code>class DataLoader(BaseLoader):\n    \"\"\"DataLoader.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self):\n        \"\"\"Initialize the DataLoader.\"\"\"\n        super().__init__()\n\n    def __iter__(self) -&gt; Generator[Dataset, None, None]:\n        \"\"\"Make the class iterable.\n\n        Yields:\n            Dataset: A Dataset object containing the name, path, mime_type, and text of each dataset item.\n\n        Note: Overrides the BaseLoader's __iter__ method to yield Dataset objects.\n        \"\"\"\n        for i in range(len(self.data[\"paths\"])):\n            yield Dataset(\n                name=self.data[\"names\"][i],\n                path=self.data[\"paths\"][i],\n                mime_type=self.data[\"mime_types\"][i],\n                text=self.data[\"texts\"][i],\n            )\n\n    def _update_data(\n        self, path: Path | str, df: pd.DataFrame, mime_type: str = \"text/plain\"\n    ) -&gt; None:\n        \"\"\"Update the DataLoader.\n\n        Args:\n            path (Path | str): The path to the file.\n            df (pd.DataFrame): The DataFrame to update with.\n            mime_type (str): The mime type of the file.\n        \"\"\"\n        self.names = self.names + df[\"name\"].tolist()\n        length = len(self.names)\n        self.paths = self.paths + [str(path)] * length\n        self.mime_types = self.mime_types + [mime_type] * length\n        self.texts = self.texts + [decode(text) for text in df[\"text\"].tolist()]\n\n    @validate_call(config=model_config)\n    def load_csv(\n        self,\n        path: io.StringIO | os.PathLike | Path | str,\n        name_col: Optional[str] = \"name\",\n        text_col: Optional[str] = \"text\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Load a csv file.\n\n        Args:\n            path (io.StringIO | os.PathLike | Path | str): The path to the file.\n            name_col (Optional[str]): The column name for the names.\n            text_col (Optional[str]): The column name for the texts.\n        \"\"\"\n        try:\n            df = pd.read_csv(path, **kwargs)\n        except BaseException as e:\n            raise LexosException(e)\n        if not isinstance(path, (Path, str)):\n            path = \"csv_string\"\n        if \"sep\" in kwargs and kwargs[\"sep\"] == \"\\t\":\n            mime_type = \"text/tab-separated-values\"\n        else:\n            mime_type = \"text/csv\"\n        if name_col:\n            df = df.rename(columns={name_col: \"name\"})\n        if text_col:\n            df = df.rename(columns={text_col: \"text\"})\n        if \"name\" not in df.columns or \"text\" not in df.columns:\n            err = (\n                \"CSV and TSV files must contain headers named `name` and `text`. \",\n                \"You can convert the names of existing headers to these with the \",\n                \"`name_col` and `text_col` parameters.\",\n            )\n            raise LexosException(\"\".join(err))\n        self._update_data(path, df, mime_type)\n\n    # @validate_call(config=model_config)\n    def load_dataset(self, dataset: Self) -&gt; None:\n        \"\"\"Load a dataset.\n\n        Args:\n            dataset (DataLoader): The dataset to load.\n\n        Note: As of v2.10.5, Pydantic does not support recursive types (Self).\n            As a result, this method performs its own check to see if the\n            value of `dataset` is of type `DataSet`.\n        \"\"\"\n        if not isinstance(dataset, DataLoader):\n            raise LexosException(\"Invalid dataset type.\")\n        self.paths = self.paths + dataset.paths\n        self.mime_types = self.mime_types + dataset.mime_types\n        self.names = self.names + dataset.names\n        self.texts = self.texts + dataset.texts\n\n    # Skipped for coverage, same method as load_csv\n    @validate_call(config=model_config)  # pragma: no cover\n    def load_excel(  # pragma: no cover\n        self, path: Path | str, name_col: str, text_col: str, **kwargs\n    ) -&gt; None:\n        \"\"\"Load an Excel file.\n\n        Args:\n            path (Path | str): The path to the file.\n            name_col (str): The column name for the names.\n            text_col (str): The column name for the texts.\n        \"\"\"\n        try:\n            df = pd.read_csv(path, **kwargs)\n        except BaseException as e:\n            raise LexosException(e)\n        if not isinstance(path, (Path, str)):\n            path = \"buffer\"\n        if name_col:\n            df = df.rename(columns={name_col: \"name\"})\n        if text_col:\n            df = df.rename(columns={text_col: \"text\"})\n        if \"name\" not in df.columns or \"text\" not in df.columns:\n            err = (\n                \"Excel files must contain headers named `name` and `text`. \",\n                \"You can convert the names of existing headers to these with the \",\n                \"`name_col` and `text_col` parameters.\",\n            )\n            raise LexosException(\"\".join(err))\n        self._update(\n            path,\n            df,\n            \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n        )\n\n    @validate_call(config=model_config)\n    def load_json(\n        self,\n        path: io.StringIO | os.PathLike | Path | str,\n        name_field: Optional[str] = \"name\",\n        text_field: Optional[str] = \"text\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Load a JSON file.\n\n        Args:\n            path (io.StringIO | os.PathLike | Path | str): The path to the file.\n            name_field (Optional[str] = ): The field name for the names.\n            text_field (Optional[str] = ): The field name for the texts.\n        \"\"\"\n        try:\n            df = pd.read_json(path, **kwargs)\n        except BaseException as e:\n            raise LexosException(e)\n        if not isinstance(path, (Path, str)):\n            path = \"json_string\"\n        if name_field:\n            df = df.rename(columns={name_field: \"name\"})\n        if text_field:\n            df = df.rename(columns={text_field: \"text\"})\n        if \"name\" not in df.columns or \"text\" not in df.columns:\n            err = (\n                \"JSON files must contain fields named `name` and `text`. \",\n                \"You can convert the names of existing headers to these with the \",\n                \"`name_field` and `text_field` parameters.\",\n            )\n            raise LexosException(\"\".join(err))\n        self._update_data(path, df, \"application/json\")\n\n    @validate_call(config=model_config)\n    def load_lineated_text(\n        self,\n        path: io.StringIO | os.PathLike | Path | str,\n        names: Optional[list[str]] = None,\n        start: Optional[int] = 1,\n        zero_pad: Optional[str] = \"03\",\n    ) -&gt; None:\n        \"\"\"Load a list of texts.\n\n        Args:\n            path (io.StringIO | os.PathLike | Path | str): The path to the file.\n            names (Optional[list[str]]): The list of names for the texts.\n            start (Optional[int]): The starting index for the names if no list is provided.\n            zero_pad (Optional[str]): The zero padding for the names increments if no list is provided.\n        \"\"\"\n        try:\n            with open(path, \"rb\") as f:\n                texts = f.readlines()\n        except (FileNotFoundError, IOError, OSError):\n            texts = path.split(\"\\n\")\n        except BaseException as e:\n            raise LexosException(e)\n        if names is None:\n            names = [f\"text{i + start:{zero_pad}d}\" for i in range(len(texts))]\n        self.paths = [\"text_string\"] * len(texts)\n        self.names = names\n        self.mime_types = [\"text/plain\"] * len(texts)\n        self.texts = [decode(text) for text in texts]\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.data","title":"<code>data: dict[str, list]</code>  <code>property</code>","text":"<p>Get the data as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>dict[str, list]: A dictionary containing the paths, mime_types, names, texts, and errors.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Get a pandas DataFrame of file records.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame containing file metadata and content.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.errors","title":"<code>errors: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of loading errors.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.mime_types","title":"<code>mime_types: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of text mime types.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.names","title":"<code>names: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of text names.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.paths","title":"<code>paths: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of paths.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.records","title":"<code>records: list[dict[str, str]]</code>  <code>property</code>","text":"<p>Get a list of file records.</p> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict]: List of dictionaries containing file metadata and content.</p> <code>list[dict[str, str]]</code> <p>Each dict has keys: path, mime_type, name, text</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of paths, mime_types, names and texts don't match.</p> Note <p>Validates that all lists have the same length before returning the records.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.texts","title":"<code>texts: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of loaded texts.</p>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the DataLoader.</p> Source code in <code>lexos/io/data_loader.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the DataLoader.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.__iter__","title":"<code>__iter__() -&gt; Generator[Dataset, None, None]</code>","text":"<p>Make the class iterable.</p> <p>Yields:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>A Dataset object containing the name, path, mime_type, and text of each dataset item.</p> <p>Note: Overrides the BaseLoader's iter method to yield Dataset objects.</p> Source code in <code>lexos/io/data_loader.py</code> <pre><code>def __iter__(self) -&gt; Generator[Dataset, None, None]:\n    \"\"\"Make the class iterable.\n\n    Yields:\n        Dataset: A Dataset object containing the name, path, mime_type, and text of each dataset item.\n\n    Note: Overrides the BaseLoader's __iter__ method to yield Dataset objects.\n    \"\"\"\n    for i in range(len(self.data[\"paths\"])):\n        yield Dataset(\n            name=self.data[\"names\"][i],\n            path=self.data[\"paths\"][i],\n            mime_type=self.data[\"mime_types\"][i],\n            text=self.data[\"texts\"][i],\n        )\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.dedupe","title":"<code>dedupe(subset: Optional[list[str]] = None) -&gt; pd.DataFrame</code>","text":"<p>Deduplicate a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]]</code> <p>The columns to consider for deduplication.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The deduplicated DataFrame.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef dedupe(self, subset: Optional[list[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"Deduplicate a DataFrame.\n\n    Args:\n        subset (Optional[list[str]]): The columns to consider for deduplication.\n\n    Returns:\n        pd.DataFrame: The deduplicated DataFrame.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        df.drop_duplicates(\n            subset=subset, keep=\"first\", inplace=True, ignore_index=True\n        )\n        self.paths = df[\"path\"].tolist()\n        self.mime_types = df[\"mime_type\"].tolist()\n        self.names = df[\"name\"].tolist()\n        self.texts = df[\"text\"].tolist()\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_csv","title":"<code>load_csv(path: io.StringIO | os.PathLike | Path | str, name_col: Optional[str] = 'name', text_col: Optional[str] = 'text', **kwargs) -&gt; None</code>","text":"<p>Load a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>StringIO | PathLike | Path | str</code> <p>The path to the file.</p> required <code>name_col</code> <code>Optional[str]</code> <p>The column name for the names.</p> <code>'name'</code> <code>text_col</code> <code>Optional[str]</code> <p>The column name for the texts.</p> <code>'text'</code> Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load_csv(\n    self,\n    path: io.StringIO | os.PathLike | Path | str,\n    name_col: Optional[str] = \"name\",\n    text_col: Optional[str] = \"text\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Load a csv file.\n\n    Args:\n        path (io.StringIO | os.PathLike | Path | str): The path to the file.\n        name_col (Optional[str]): The column name for the names.\n        text_col (Optional[str]): The column name for the texts.\n    \"\"\"\n    try:\n        df = pd.read_csv(path, **kwargs)\n    except BaseException as e:\n        raise LexosException(e)\n    if not isinstance(path, (Path, str)):\n        path = \"csv_string\"\n    if \"sep\" in kwargs and kwargs[\"sep\"] == \"\\t\":\n        mime_type = \"text/tab-separated-values\"\n    else:\n        mime_type = \"text/csv\"\n    if name_col:\n        df = df.rename(columns={name_col: \"name\"})\n    if text_col:\n        df = df.rename(columns={text_col: \"text\"})\n    if \"name\" not in df.columns or \"text\" not in df.columns:\n        err = (\n            \"CSV and TSV files must contain headers named `name` and `text`. \",\n            \"You can convert the names of existing headers to these with the \",\n            \"`name_col` and `text_col` parameters.\",\n        )\n        raise LexosException(\"\".join(err))\n    self._update_data(path, df, mime_type)\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_dataset","title":"<code>load_dataset(dataset: Self) -&gt; None</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataLoader</code> <p>The dataset to load.</p> required As of v2.10.5, Pydantic does not support recursive types (Self). <p>As a result, this method performs its own check to see if the value of <code>dataset</code> is of type <code>DataSet</code>.</p> Source code in <code>lexos/io/data_loader.py</code> <pre><code>def load_dataset(self, dataset: Self) -&gt; None:\n    \"\"\"Load a dataset.\n\n    Args:\n        dataset (DataLoader): The dataset to load.\n\n    Note: As of v2.10.5, Pydantic does not support recursive types (Self).\n        As a result, this method performs its own check to see if the\n        value of `dataset` is of type `DataSet`.\n    \"\"\"\n    if not isinstance(dataset, DataLoader):\n        raise LexosException(\"Invalid dataset type.\")\n    self.paths = self.paths + dataset.paths\n    self.mime_types = self.mime_types + dataset.mime_types\n    self.names = self.names + dataset.names\n    self.texts = self.texts + dataset.texts\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_excel","title":"<code>load_excel(path: Path | str, name_col: str, text_col: str, **kwargs) -&gt; None</code>","text":"<p>Load an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <code>name_col</code> <code>str</code> <p>The column name for the names.</p> required <code>text_col</code> <code>str</code> <p>The column name for the texts.</p> required Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)  # pragma: no cover\ndef load_excel(  # pragma: no cover\n    self, path: Path | str, name_col: str, text_col: str, **kwargs\n) -&gt; None:\n    \"\"\"Load an Excel file.\n\n    Args:\n        path (Path | str): The path to the file.\n        name_col (str): The column name for the names.\n        text_col (str): The column name for the texts.\n    \"\"\"\n    try:\n        df = pd.read_csv(path, **kwargs)\n    except BaseException as e:\n        raise LexosException(e)\n    if not isinstance(path, (Path, str)):\n        path = \"buffer\"\n    if name_col:\n        df = df.rename(columns={name_col: \"name\"})\n    if text_col:\n        df = df.rename(columns={text_col: \"text\"})\n    if \"name\" not in df.columns or \"text\" not in df.columns:\n        err = (\n            \"Excel files must contain headers named `name` and `text`. \",\n            \"You can convert the names of existing headers to these with the \",\n            \"`name_col` and `text_col` parameters.\",\n        )\n        raise LexosException(\"\".join(err))\n    self._update(\n        path,\n        df,\n        \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n    )\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_json","title":"<code>load_json(path: io.StringIO | os.PathLike | Path | str, name_field: Optional[str] = 'name', text_field: Optional[str] = 'text', **kwargs) -&gt; None</code>","text":"<p>Load a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>StringIO | PathLike | Path | str</code> <p>The path to the file.</p> required <code>name_field</code> <code>Optional[str] =</code> <p>The field name for the names.</p> <code>'name'</code> <code>text_field</code> <code>Optional[str] =</code> <p>The field name for the texts.</p> <code>'text'</code> Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load_json(\n    self,\n    path: io.StringIO | os.PathLike | Path | str,\n    name_field: Optional[str] = \"name\",\n    text_field: Optional[str] = \"text\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Load a JSON file.\n\n    Args:\n        path (io.StringIO | os.PathLike | Path | str): The path to the file.\n        name_field (Optional[str] = ): The field name for the names.\n        text_field (Optional[str] = ): The field name for the texts.\n    \"\"\"\n    try:\n        df = pd.read_json(path, **kwargs)\n    except BaseException as e:\n        raise LexosException(e)\n    if not isinstance(path, (Path, str)):\n        path = \"json_string\"\n    if name_field:\n        df = df.rename(columns={name_field: \"name\"})\n    if text_field:\n        df = df.rename(columns={text_field: \"text\"})\n    if \"name\" not in df.columns or \"text\" not in df.columns:\n        err = (\n            \"JSON files must contain fields named `name` and `text`. \",\n            \"You can convert the names of existing headers to these with the \",\n            \"`name_field` and `text_field` parameters.\",\n        )\n        raise LexosException(\"\".join(err))\n    self._update_data(path, df, \"application/json\")\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_lineated_text","title":"<code>load_lineated_text(path: io.StringIO | os.PathLike | Path | str, names: Optional[list[str]] = None, start: Optional[int] = 1, zero_pad: Optional[str] = '03') -&gt; None</code>","text":"<p>Load a list of texts.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>StringIO | PathLike | Path | str</code> <p>The path to the file.</p> required <code>names</code> <code>Optional[list[str]]</code> <p>The list of names for the texts.</p> <code>None</code> <code>start</code> <code>Optional[int]</code> <p>The starting index for the names if no list is provided.</p> <code>1</code> <code>zero_pad</code> <code>Optional[str]</code> <p>The zero padding for the names increments if no list is provided.</p> <code>'03'</code> Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load_lineated_text(\n    self,\n    path: io.StringIO | os.PathLike | Path | str,\n    names: Optional[list[str]] = None,\n    start: Optional[int] = 1,\n    zero_pad: Optional[str] = \"03\",\n) -&gt; None:\n    \"\"\"Load a list of texts.\n\n    Args:\n        path (io.StringIO | os.PathLike | Path | str): The path to the file.\n        names (Optional[list[str]]): The list of names for the texts.\n        start (Optional[int]): The starting index for the names if no list is provided.\n        zero_pad (Optional[str]): The zero padding for the names increments if no list is provided.\n    \"\"\"\n    try:\n        with open(path, \"rb\") as f:\n            texts = f.readlines()\n    except (FileNotFoundError, IOError, OSError):\n        texts = path.split(\"\\n\")\n    except BaseException as e:\n        raise LexosException(e)\n    if names is None:\n        names = [f\"text{i + start:{zero_pad}d}\" for i in range(len(texts))]\n    self.paths = [\"text_string\"] * len(texts)\n    self.names = names\n    self.mime_types = [\"text/plain\"] * len(texts)\n    self.texts = [decode(text) for text in texts]\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset the class attributes to empty lists.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef reset(self) -&gt; None:\n    \"\"\"Reset the class attributes to empty lists.\"\"\"\n    self.paths = []\n    self.mime_types = []\n    self.names = []\n    self.texts = []\n    self.errors = []\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.show_duplicates","title":"<code>show_duplicates(subset: Optional[list[str]] = None) -&gt; pd.DataFrame | None</code>","text":"<p>Show duplicates in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]] = None</code> <p>The columns to consider for checking duplicates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>pd.DataFrame: The DataFrame with duplicates.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef show_duplicates(\n    self, subset: Optional[list[str]] = None\n) -&gt; pd.DataFrame | None:\n    \"\"\"Show duplicates in a DataFrame.\n\n    Args:\n        subset (Optional[list[str]] = None): The columns to consider for checking duplicates.\n\n    Returns:\n        pd.DataFrame: The DataFrame with duplicates.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        return df[df.duplicated(subset=subset)]\n    return None\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.to_csv","title":"<code>to_csv(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_csv(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a csv file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.to_excel","title":"<code>to_excel(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_excel(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to an Excel file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.to_json","title":"<code>to_json(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_json(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a json file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_json(path, **kwargs)\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the DataLoader.</p> Source code in <code>lexos/io/data_loader.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the DataLoader.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.__iter__","title":"<code>__iter__() -&gt; Generator[Dataset, None, None]</code>","text":"<p>Make the class iterable.</p> <p>Yields:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>A Dataset object containing the name, path, mime_type, and text of each dataset item.</p> <p>Note: Overrides the BaseLoader's iter method to yield Dataset objects.</p> Source code in <code>lexos/io/data_loader.py</code> <pre><code>def __iter__(self) -&gt; Generator[Dataset, None, None]:\n    \"\"\"Make the class iterable.\n\n    Yields:\n        Dataset: A Dataset object containing the name, path, mime_type, and text of each dataset item.\n\n    Note: Overrides the BaseLoader's __iter__ method to yield Dataset objects.\n    \"\"\"\n    for i in range(len(self.data[\"paths\"])):\n        yield Dataset(\n            name=self.data[\"names\"][i],\n            path=self.data[\"paths\"][i],\n            mime_type=self.data[\"mime_types\"][i],\n            text=self.data[\"texts\"][i],\n        )\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader._update_data","title":"<code>_update_data(path: Path | str, df: pd.DataFrame, mime_type: str = 'text/plain') -&gt; None</code>","text":"<p>Update the DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <code>df</code> <code>DataFrame</code> <p>The DataFrame to update with.</p> required <code>mime_type</code> <code>str</code> <p>The mime type of the file.</p> <code>'text/plain'</code> Source code in <code>lexos/io/data_loader.py</code> <pre><code>def _update_data(\n    self, path: Path | str, df: pd.DataFrame, mime_type: str = \"text/plain\"\n) -&gt; None:\n    \"\"\"Update the DataLoader.\n\n    Args:\n        path (Path | str): The path to the file.\n        df (pd.DataFrame): The DataFrame to update with.\n        mime_type (str): The mime type of the file.\n    \"\"\"\n    self.names = self.names + df[\"name\"].tolist()\n    length = len(self.names)\n    self.paths = self.paths + [str(path)] * length\n    self.mime_types = self.mime_types + [mime_type] * length\n    self.texts = self.texts + [decode(text) for text in df[\"text\"].tolist()]\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_csv","title":"<code>load_csv(path: io.StringIO | os.PathLike | Path | str, name_col: Optional[str] = 'name', text_col: Optional[str] = 'text', **kwargs) -&gt; None</code>","text":"<p>Load a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>StringIO | PathLike | Path | str</code> <p>The path to the file.</p> required <code>name_col</code> <code>Optional[str]</code> <p>The column name for the names.</p> <code>'name'</code> <code>text_col</code> <code>Optional[str]</code> <p>The column name for the texts.</p> <code>'text'</code> Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load_csv(\n    self,\n    path: io.StringIO | os.PathLike | Path | str,\n    name_col: Optional[str] = \"name\",\n    text_col: Optional[str] = \"text\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Load a csv file.\n\n    Args:\n        path (io.StringIO | os.PathLike | Path | str): The path to the file.\n        name_col (Optional[str]): The column name for the names.\n        text_col (Optional[str]): The column name for the texts.\n    \"\"\"\n    try:\n        df = pd.read_csv(path, **kwargs)\n    except BaseException as e:\n        raise LexosException(e)\n    if not isinstance(path, (Path, str)):\n        path = \"csv_string\"\n    if \"sep\" in kwargs and kwargs[\"sep\"] == \"\\t\":\n        mime_type = \"text/tab-separated-values\"\n    else:\n        mime_type = \"text/csv\"\n    if name_col:\n        df = df.rename(columns={name_col: \"name\"})\n    if text_col:\n        df = df.rename(columns={text_col: \"text\"})\n    if \"name\" not in df.columns or \"text\" not in df.columns:\n        err = (\n            \"CSV and TSV files must contain headers named `name` and `text`. \",\n            \"You can convert the names of existing headers to these with the \",\n            \"`name_col` and `text_col` parameters.\",\n        )\n        raise LexosException(\"\".join(err))\n    self._update_data(path, df, mime_type)\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_dataset","title":"<code>load_dataset(dataset: Self) -&gt; None</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataLoader</code> <p>The dataset to load.</p> required As of v2.10.5, Pydantic does not support recursive types (Self). <p>As a result, this method performs its own check to see if the value of <code>dataset</code> is of type <code>DataSet</code>.</p> Source code in <code>lexos/io/data_loader.py</code> <pre><code>def load_dataset(self, dataset: Self) -&gt; None:\n    \"\"\"Load a dataset.\n\n    Args:\n        dataset (DataLoader): The dataset to load.\n\n    Note: As of v2.10.5, Pydantic does not support recursive types (Self).\n        As a result, this method performs its own check to see if the\n        value of `dataset` is of type `DataSet`.\n    \"\"\"\n    if not isinstance(dataset, DataLoader):\n        raise LexosException(\"Invalid dataset type.\")\n    self.paths = self.paths + dataset.paths\n    self.mime_types = self.mime_types + dataset.mime_types\n    self.names = self.names + dataset.names\n    self.texts = self.texts + dataset.texts\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_excel","title":"<code>load_excel(path: Path | str, name_col: str, text_col: str, **kwargs) -&gt; None</code>","text":"<p>Load an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <code>name_col</code> <code>str</code> <p>The column name for the names.</p> required <code>text_col</code> <code>str</code> <p>The column name for the texts.</p> required Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)  # pragma: no cover\ndef load_excel(  # pragma: no cover\n    self, path: Path | str, name_col: str, text_col: str, **kwargs\n) -&gt; None:\n    \"\"\"Load an Excel file.\n\n    Args:\n        path (Path | str): The path to the file.\n        name_col (str): The column name for the names.\n        text_col (str): The column name for the texts.\n    \"\"\"\n    try:\n        df = pd.read_csv(path, **kwargs)\n    except BaseException as e:\n        raise LexosException(e)\n    if not isinstance(path, (Path, str)):\n        path = \"buffer\"\n    if name_col:\n        df = df.rename(columns={name_col: \"name\"})\n    if text_col:\n        df = df.rename(columns={text_col: \"text\"})\n    if \"name\" not in df.columns or \"text\" not in df.columns:\n        err = (\n            \"Excel files must contain headers named `name` and `text`. \",\n            \"You can convert the names of existing headers to these with the \",\n            \"`name_col` and `text_col` parameters.\",\n        )\n        raise LexosException(\"\".join(err))\n    self._update(\n        path,\n        df,\n        \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n    )\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_json","title":"<code>load_json(path: io.StringIO | os.PathLike | Path | str, name_field: Optional[str] = 'name', text_field: Optional[str] = 'text', **kwargs) -&gt; None</code>","text":"<p>Load a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>StringIO | PathLike | Path | str</code> <p>The path to the file.</p> required <code>name_field</code> <code>Optional[str] =</code> <p>The field name for the names.</p> <code>'name'</code> <code>text_field</code> <code>Optional[str] =</code> <p>The field name for the texts.</p> <code>'text'</code> Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load_json(\n    self,\n    path: io.StringIO | os.PathLike | Path | str,\n    name_field: Optional[str] = \"name\",\n    text_field: Optional[str] = \"text\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Load a JSON file.\n\n    Args:\n        path (io.StringIO | os.PathLike | Path | str): The path to the file.\n        name_field (Optional[str] = ): The field name for the names.\n        text_field (Optional[str] = ): The field name for the texts.\n    \"\"\"\n    try:\n        df = pd.read_json(path, **kwargs)\n    except BaseException as e:\n        raise LexosException(e)\n    if not isinstance(path, (Path, str)):\n        path = \"json_string\"\n    if name_field:\n        df = df.rename(columns={name_field: \"name\"})\n    if text_field:\n        df = df.rename(columns={text_field: \"text\"})\n    if \"name\" not in df.columns or \"text\" not in df.columns:\n        err = (\n            \"JSON files must contain fields named `name` and `text`. \",\n            \"You can convert the names of existing headers to these with the \",\n            \"`name_field` and `text_field` parameters.\",\n        )\n        raise LexosException(\"\".join(err))\n    self._update_data(path, df, \"application/json\")\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.load_lineated_text","title":"<code>load_lineated_text(path: io.StringIO | os.PathLike | Path | str, names: Optional[list[str]] = None, start: Optional[int] = 1, zero_pad: Optional[str] = '03') -&gt; None</code>","text":"<p>Load a list of texts.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>StringIO | PathLike | Path | str</code> <p>The path to the file.</p> required <code>names</code> <code>Optional[list[str]]</code> <p>The list of names for the texts.</p> <code>None</code> <code>start</code> <code>Optional[int]</code> <p>The starting index for the names if no list is provided.</p> <code>1</code> <code>zero_pad</code> <code>Optional[str]</code> <p>The zero padding for the names increments if no list is provided.</p> <code>'03'</code> Source code in <code>lexos/io/data_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load_lineated_text(\n    self,\n    path: io.StringIO | os.PathLike | Path | str,\n    names: Optional[list[str]] = None,\n    start: Optional[int] = 1,\n    zero_pad: Optional[str] = \"03\",\n) -&gt; None:\n    \"\"\"Load a list of texts.\n\n    Args:\n        path (io.StringIO | os.PathLike | Path | str): The path to the file.\n        names (Optional[list[str]]): The list of names for the texts.\n        start (Optional[int]): The starting index for the names if no list is provided.\n        zero_pad (Optional[str]): The zero padding for the names increments if no list is provided.\n    \"\"\"\n    try:\n        with open(path, \"rb\") as f:\n            texts = f.readlines()\n    except (FileNotFoundError, IOError, OSError):\n        texts = path.split(\"\\n\")\n    except BaseException as e:\n        raise LexosException(e)\n    if names is None:\n        names = [f\"text{i + start:{zero_pad}d}\" for i in range(len(texts))]\n    self.paths = [\"text_string\"] * len(texts)\n    self.names = names\n    self.mime_types = [\"text/plain\"] * len(texts)\n    self.texts = [decode(text) for text in texts]\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.show_duplicates","title":"<code>show_duplicates(subset: Optional[list[str]] = None) -&gt; pd.DataFrame | None</code>","text":"<p>Show duplicates in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]] = None</code> <p>The columns to consider for checking duplicates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>pd.DataFrame: The DataFrame with duplicates.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef show_duplicates(\n    self, subset: Optional[list[str]] = None\n) -&gt; pd.DataFrame | None:\n    \"\"\"Show duplicates in a DataFrame.\n\n    Args:\n        subset (Optional[list[str]] = None): The columns to consider for checking duplicates.\n\n    Returns:\n        pd.DataFrame: The DataFrame with duplicates.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        return df[df.duplicated(subset=subset)]\n    return None\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.to_csv","title":"<code>to_csv(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_csv(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a csv file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.to_excel","title":"<code>to_excel(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_excel(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to an Excel file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/data_loader/#lexos.io.data_loader.DataLoader.to_json","title":"<code>to_json(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_json(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a json file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_json(path, **kwargs)\n</code></pre>"},{"location":"api/io/loader/","title":"loader","text":""},{"location":"api/io/loader/#the-loader-class","title":"The <code>Loader</code> Class","text":"<p>The <code>Loader</code> class is the main class for loading files in various formats. It tries to be \"smart\" detecting the format as well as can be done so that you can use a common interface to load content regardless of source.</p>"},{"location":"api/io/loader/#lexos.io.loader.VALID_FILE_TYPES","title":"<code>VALID_FILE_TYPES = {*TEXT_TYPES, *PDF_TYPES, *DOCX_TYPES, *ZIP_TYPES}</code>  <code>module-attribute</code>","text":""},{"location":"api/io/loader/#lexos.io.loader.Loader","title":"<code>Loader</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Loader.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>paths</code>                 (<code>list</code>)             </li> <li> <code>mime_types</code>                 (<code>list</code>)             </li> <li> <code>names</code>                 (<code>list</code>)             </li> <li> <code>texts</code>                 (<code>list</code>)             </li> <li> <code>errors</code>                 (<code>list</code>)             </li> </ul> Source code in <code>lexos/io/loader.py</code> <pre><code>class Loader(BaseLoader):\n    \"\"\"Loader.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self):\n        \"\"\"Initialize the Loader.\"\"\"\n        super().__init__()\n\n    def _get_mime_type(self, path: Path | str, file_start: str) -&gt; str:\n        \"\"\"Get the mime type of a file.\n\n        Args:\n            path (Path | str): The path to the file.\n\n        Returns:\n            str: The mime type of the file.\n        \"\"\"\n        if Path(path).suffix == \".pickle\":\n            return \"application/vnd.python.pickle\"\n        results = puremagic.magic_string(file_start, path)\n        if not results:\n            return None\n        else:\n            mime_type = results[0].mime_type\n            if mime_type == \"\":\n                mime_type, _ = mimetypes.guess_type(path)\n            return mime_type\n\n    def _load_docx_file(self, path: Path | str) -&gt; None:\n        \"\"\"Load a docx file.\n\n        Args:\n            path (Path | str): The path to the file.\n\n        Note:\n            Consider https://github.com/ShayHill/docx2python for greater coverage.\n        \"\"\"\n        try:\n            doc = Document(path)\n            text = \"\\n\".join([decode(p.text) for p in doc.paragraphs])\n            self.names.append(Path(path).name)\n            self.mime_types.append(\"application/docx\")\n            self.texts.append(text)\n        except BaseException as e:\n            self.errors.append(e)\n\n    def _load_pdf_file(self, path: Path | str) -&gt; None:\n        \"\"\"Load a pdf file.\n\n        Args:\n            path (Path | str): The path to the file.\n        \"\"\"\n        try:\n            reader = PdfReader(path)\n            for page in reader.pages:\n                text = decode(page.extract_text())\n                self.names.append(Path(path).name)\n                self.mime_types.append(\"application/pdf\")\n                self.texts.append(text)\n        except BaseException as e:\n            self.errors.append(e)\n\n    def _load_text_file(self, path: Path | str, mime_type: str) -&gt; None:\n        \"\"\"Load a text file.\n\n        Args:\n            path (Path | str): The path to the file.\n            mime_type (str): The mime type of the file.\n        \"\"\"\n        try:\n            with open(path, \"rb\") as f:\n                text = decode(f.read())\n                self.paths.append(Path(path).name)\n                self.names.append(Path(path).stem)\n                self.mime_types.append(mime_type)\n                self.texts.append(text)\n        except BaseException as e:\n            self.errors.append(e)\n\n    def _load_zip_file(self, path: Path | str) -&gt; None:\n        \"\"\"Handle a zip file.\n\n        Args:\n            path (Path | str): The path to the file.\n        \"\"\"\n        with open(path, \"rb\") as fin:\n            with zipfile.ZipFile(fin) as zip:\n                for info in zip.infolist():\n                    try:\n                        # Get the mime type of the file\n                        file_bytes = zip.read(info.filename)\n                        file_start = decode(file_bytes[:MIN_ENCODING_DETECT])\n                        mime_type = self._get_mime_type(info.filename, file_start)\n                    except (IOError, UnicodeDecodeError) as e:\n                        self.errors.append(e)\n                        mime_type = None\n                    try:\n                        if mime_type in VALID_FILE_TYPES:\n                            text = decode(file_bytes)\n                            self.paths.append(\n                                Path(path).as_posix() + \"/\" + info.filename\n                            )\n                            self.names.append(Path(info.filename).stem)\n                            self.mime_types.append(mime_type)\n                            self.texts.append(text)\n                        else:\n                            self.errors.append(\n                                f\"Invalid MIME type: {mime_type} for file {info.filename}.\"\n                            )\n                    except BaseException as e:\n                        self.errors.append(e)\n\n    # @validate_call(config=model_config)\n    def load_dataset(self, dataset: Self) -&gt; None:\n        \"\"\"Load a dataset.\n\n        Args:\n            dataset (DataLoader): The dataset to load.\n\n        Note: As of v2.10.5, Pydantic does not support recursive types (Self).\n            As a result, this method performs its own check to see if the\n            value of `dataset` is of type `DataLoader`.\n        \"\"\"\n        if not isinstance(dataset, DataLoader):\n            raise LexosException(\"Invalid dataset type.\")\n        self.paths = self.paths + dataset.paths\n        self.mime_types = self.mime_types + dataset.mime_types\n        self.names = self.names + dataset.names\n        self.texts = self.texts + dataset.texts\n\n    @validate_call(config=model_config)\n    def load(self, paths: Path | str | list[Path | str]) -&gt; None:\n        \"\"\"Load a list of paths.\n\n        Args:\n            paths (Path | str | list[Path | str]): The list of paths to load.\n        \"\"\"\n        paths = ensure_list(paths)\n        for path in paths:\n            if Path(path).is_dir():\n                paths = [p for p in Path(path).rglob(\"*\")]\n                self.load(paths)\n            # Get the mime type of the file\n            try:\n                with open(path, \"rb\") as f:\n                    file_start = f.read(FILE_START)\n                mime_type = self._get_mime_type(path, file_start)\n            except IOError as e:\n                self.errors.append(e)\n                mime_type = None\n            if mime_type in TEXT_TYPES:\n                self._load_text_file(path, mime_type)\n            elif mime_type in PDF_TYPES:\n                self._load_pdf_file(path)\n            elif mime_type in DOCX_TYPES:\n                self._load_docx_file(path)\n            elif mime_type in ZIP_TYPES:\n                self._load_zip_file(path)\n            else:\n                self.errors.append(f\"Invalid MIME type: {mime_type} for file {path}.\")\n\n    @validate_call(config=model_config)\n    def loads(\n        self,\n        texts: Optional[list[Path | str]] = None,\n        names: Optional[list[str]] = None,\n        start: Optional[int] = 1,\n        zero_pad: Optional[str] = \"03\",\n    ) -&gt; None:\n        \"\"\"Load a list of texts.\n\n        Args:\n            texts (Optional[list[Path | str]]): The list of texts to load.\n            names (Optional[list[str]]): The list of names for the texts.\n            start (Optional[int]): The starting index for the names if no list is provided.\n            zero_pad (Optional[str]): The zero padding for the names increments if no list is provided.\n        \"\"\"\n        texts = ensure_list(texts)\n        if names is None:\n            names = [f\"text{i + start:{zero_pad}d}\" for i in range(len(texts))]\n        for i, text in enumerate(texts):\n            self.names.append(names[i])\n            self.mime_types.append(\"text/plain\")\n            self.texts.append(text)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.data","title":"<code>data: dict[str, list]</code>  <code>property</code>","text":"<p>Get the data as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>dict[str, list]: A dictionary containing the paths, mime_types, names, texts, and errors.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.df","title":"<code>df: pd.DataFrame</code>  <code>property</code>","text":"<p>Get a pandas DataFrame of file records.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame containing file metadata and content.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.errors","title":"<code>errors: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of loading errors.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.mime_types","title":"<code>mime_types: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of text mime types.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.names","title":"<code>names: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of text names.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.paths","title":"<code>paths: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of paths.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.records","title":"<code>records: list[dict[str, str]]</code>  <code>property</code>","text":"<p>Get a list of file records.</p> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict]: List of dictionaries containing file metadata and content.</p> <code>list[dict[str, str]]</code> <p>Each dict has keys: path, mime_type, name, text</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of paths, mime_types, names and texts don't match.</p> Note <p>Validates that all lists have the same length before returning the records.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.texts","title":"<code>texts: list = []</code>  <code>pydantic-field</code>","text":"<p>The list of loaded texts.</p>"},{"location":"api/io/loader/#lexos.io.loader.Loader.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Loader.</p> Source code in <code>lexos/io/loader.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the Loader.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.__iter__","title":"<code>__iter__() -&gt; Generator[dict, None, None]</code>","text":"<p>Iterate through the records.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>def __iter__(self) -&gt; Generator[dict, None, None]:\n    \"\"\"Iterate through the records.\"\"\"\n    return (record for record in self.records)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.dedupe","title":"<code>dedupe(subset: Optional[list[str]] = None) -&gt; pd.DataFrame</code>","text":"<p>Deduplicate a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]]</code> <p>The columns to consider for deduplication.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The deduplicated DataFrame.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef dedupe(self, subset: Optional[list[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"Deduplicate a DataFrame.\n\n    Args:\n        subset (Optional[list[str]]): The columns to consider for deduplication.\n\n    Returns:\n        pd.DataFrame: The deduplicated DataFrame.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        df.drop_duplicates(\n            subset=subset, keep=\"first\", inplace=True, ignore_index=True\n        )\n        self.paths = df[\"path\"].tolist()\n        self.mime_types = df[\"mime_type\"].tolist()\n        self.names = df[\"name\"].tolist()\n        self.texts = df[\"text\"].tolist()\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.load","title":"<code>load(paths: Path | str | list[Path | str]) -&gt; None</code>","text":"<p>Load a list of paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Path | str | list[Path | str]</code> <p>The list of paths to load.</p> required Source code in <code>lexos/io/loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load(self, paths: Path | str | list[Path | str]) -&gt; None:\n    \"\"\"Load a list of paths.\n\n    Args:\n        paths (Path | str | list[Path | str]): The list of paths to load.\n    \"\"\"\n    paths = ensure_list(paths)\n    for path in paths:\n        if Path(path).is_dir():\n            paths = [p for p in Path(path).rglob(\"*\")]\n            self.load(paths)\n        # Get the mime type of the file\n        try:\n            with open(path, \"rb\") as f:\n                file_start = f.read(FILE_START)\n            mime_type = self._get_mime_type(path, file_start)\n        except IOError as e:\n            self.errors.append(e)\n            mime_type = None\n        if mime_type in TEXT_TYPES:\n            self._load_text_file(path, mime_type)\n        elif mime_type in PDF_TYPES:\n            self._load_pdf_file(path)\n        elif mime_type in DOCX_TYPES:\n            self._load_docx_file(path)\n        elif mime_type in ZIP_TYPES:\n            self._load_zip_file(path)\n        else:\n            self.errors.append(f\"Invalid MIME type: {mime_type} for file {path}.\")\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.load_dataset","title":"<code>load_dataset(dataset: Self) -&gt; None</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataLoader</code> <p>The dataset to load.</p> required As of v2.10.5, Pydantic does not support recursive types (Self). <p>As a result, this method performs its own check to see if the value of <code>dataset</code> is of type <code>DataLoader</code>.</p> Source code in <code>lexos/io/loader.py</code> <pre><code>def load_dataset(self, dataset: Self) -&gt; None:\n    \"\"\"Load a dataset.\n\n    Args:\n        dataset (DataLoader): The dataset to load.\n\n    Note: As of v2.10.5, Pydantic does not support recursive types (Self).\n        As a result, this method performs its own check to see if the\n        value of `dataset` is of type `DataLoader`.\n    \"\"\"\n    if not isinstance(dataset, DataLoader):\n        raise LexosException(\"Invalid dataset type.\")\n    self.paths = self.paths + dataset.paths\n    self.mime_types = self.mime_types + dataset.mime_types\n    self.names = self.names + dataset.names\n    self.texts = self.texts + dataset.texts\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.loads","title":"<code>loads(texts: Optional[list[Path | str]] = None, names: Optional[list[str]] = None, start: Optional[int] = 1, zero_pad: Optional[str] = '03') -&gt; None</code>","text":"<p>Load a list of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Optional[list[Path | str]]</code> <p>The list of texts to load.</p> <code>None</code> <code>names</code> <code>Optional[list[str]]</code> <p>The list of names for the texts.</p> <code>None</code> <code>start</code> <code>Optional[int]</code> <p>The starting index for the names if no list is provided.</p> <code>1</code> <code>zero_pad</code> <code>Optional[str]</code> <p>The zero padding for the names increments if no list is provided.</p> <code>'03'</code> Source code in <code>lexos/io/loader.py</code> <pre><code>@validate_call(config=model_config)\ndef loads(\n    self,\n    texts: Optional[list[Path | str]] = None,\n    names: Optional[list[str]] = None,\n    start: Optional[int] = 1,\n    zero_pad: Optional[str] = \"03\",\n) -&gt; None:\n    \"\"\"Load a list of texts.\n\n    Args:\n        texts (Optional[list[Path | str]]): The list of texts to load.\n        names (Optional[list[str]]): The list of names for the texts.\n        start (Optional[int]): The starting index for the names if no list is provided.\n        zero_pad (Optional[str]): The zero padding for the names increments if no list is provided.\n    \"\"\"\n    texts = ensure_list(texts)\n    if names is None:\n        names = [f\"text{i + start:{zero_pad}d}\" for i in range(len(texts))]\n    for i, text in enumerate(texts):\n        self.names.append(names[i])\n        self.mime_types.append(\"text/plain\")\n        self.texts.append(text)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset the class attributes to empty lists.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef reset(self) -&gt; None:\n    \"\"\"Reset the class attributes to empty lists.\"\"\"\n    self.paths = []\n    self.mime_types = []\n    self.names = []\n    self.texts = []\n    self.errors = []\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.show_duplicates","title":"<code>show_duplicates(subset: Optional[list[str]] = None) -&gt; pd.DataFrame | None</code>","text":"<p>Show duplicates in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subset</code> <code>Optional[list[str]] = None</code> <p>The columns to consider for checking duplicates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>pd.DataFrame: The DataFrame with duplicates.</p> Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef show_duplicates(\n    self, subset: Optional[list[str]] = None\n) -&gt; pd.DataFrame | None:\n    \"\"\"Show duplicates in a DataFrame.\n\n    Args:\n        subset (Optional[list[str]] = None): The columns to consider for checking duplicates.\n\n    Returns:\n        pd.DataFrame: The DataFrame with duplicates.\n    \"\"\"\n    if not self.df.empty:\n        df = self.df.copy()\n        return df[df.duplicated(subset=subset)]\n    return None\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.to_csv","title":"<code>to_csv(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_csv(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a csv file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.to_excel","title":"<code>to_excel(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_excel(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to an Excel file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_csv(path, **kwargs)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.to_json","title":"<code>to_json(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the data to a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to save the csv file.</p> required Source code in <code>lexos/io/base_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef to_json(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the data to a json file.\n\n    Args:\n        path (Path | str): The path to save the csv file.\n    \"\"\"\n    self.df.to_json(path, **kwargs)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Loader.</p> Source code in <code>lexos/io/loader.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the Loader.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader._get_mime_type","title":"<code>_get_mime_type(path: Path | str, file_start: str) -&gt; str</code>","text":"<p>Get the mime type of a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The mime type of the file.</p> Source code in <code>lexos/io/loader.py</code> <pre><code>def _get_mime_type(self, path: Path | str, file_start: str) -&gt; str:\n    \"\"\"Get the mime type of a file.\n\n    Args:\n        path (Path | str): The path to the file.\n\n    Returns:\n        str: The mime type of the file.\n    \"\"\"\n    if Path(path).suffix == \".pickle\":\n        return \"application/vnd.python.pickle\"\n    results = puremagic.magic_string(file_start, path)\n    if not results:\n        return None\n    else:\n        mime_type = results[0].mime_type\n        if mime_type == \"\":\n            mime_type, _ = mimetypes.guess_type(path)\n        return mime_type\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader._load_docx_file","title":"<code>_load_docx_file(path: Path | str) -&gt; None</code>","text":"<p>Load a docx file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required Note <p>Consider https://github.com/ShayHill/docx2python for greater coverage.</p> Source code in <code>lexos/io/loader.py</code> <pre><code>def _load_docx_file(self, path: Path | str) -&gt; None:\n    \"\"\"Load a docx file.\n\n    Args:\n        path (Path | str): The path to the file.\n\n    Note:\n        Consider https://github.com/ShayHill/docx2python for greater coverage.\n    \"\"\"\n    try:\n        doc = Document(path)\n        text = \"\\n\".join([decode(p.text) for p in doc.paragraphs])\n        self.names.append(Path(path).name)\n        self.mime_types.append(\"application/docx\")\n        self.texts.append(text)\n    except BaseException as e:\n        self.errors.append(e)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader._load_pdf_file","title":"<code>_load_pdf_file(path: Path | str) -&gt; None</code>","text":"<p>Load a pdf file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required Source code in <code>lexos/io/loader.py</code> <pre><code>def _load_pdf_file(self, path: Path | str) -&gt; None:\n    \"\"\"Load a pdf file.\n\n    Args:\n        path (Path | str): The path to the file.\n    \"\"\"\n    try:\n        reader = PdfReader(path)\n        for page in reader.pages:\n            text = decode(page.extract_text())\n            self.names.append(Path(path).name)\n            self.mime_types.append(\"application/pdf\")\n            self.texts.append(text)\n    except BaseException as e:\n        self.errors.append(e)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader._load_text_file","title":"<code>_load_text_file(path: Path | str, mime_type: str) -&gt; None</code>","text":"<p>Load a text file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <code>mime_type</code> <code>str</code> <p>The mime type of the file.</p> required Source code in <code>lexos/io/loader.py</code> <pre><code>def _load_text_file(self, path: Path | str, mime_type: str) -&gt; None:\n    \"\"\"Load a text file.\n\n    Args:\n        path (Path | str): The path to the file.\n        mime_type (str): The mime type of the file.\n    \"\"\"\n    try:\n        with open(path, \"rb\") as f:\n            text = decode(f.read())\n            self.paths.append(Path(path).name)\n            self.names.append(Path(path).stem)\n            self.mime_types.append(mime_type)\n            self.texts.append(text)\n    except BaseException as e:\n        self.errors.append(e)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader._load_zip_file","title":"<code>_load_zip_file(path: Path | str) -&gt; None</code>","text":"<p>Handle a zip file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required Source code in <code>lexos/io/loader.py</code> <pre><code>def _load_zip_file(self, path: Path | str) -&gt; None:\n    \"\"\"Handle a zip file.\n\n    Args:\n        path (Path | str): The path to the file.\n    \"\"\"\n    with open(path, \"rb\") as fin:\n        with zipfile.ZipFile(fin) as zip:\n            for info in zip.infolist():\n                try:\n                    # Get the mime type of the file\n                    file_bytes = zip.read(info.filename)\n                    file_start = decode(file_bytes[:MIN_ENCODING_DETECT])\n                    mime_type = self._get_mime_type(info.filename, file_start)\n                except (IOError, UnicodeDecodeError) as e:\n                    self.errors.append(e)\n                    mime_type = None\n                try:\n                    if mime_type in VALID_FILE_TYPES:\n                        text = decode(file_bytes)\n                        self.paths.append(\n                            Path(path).as_posix() + \"/\" + info.filename\n                        )\n                        self.names.append(Path(info.filename).stem)\n                        self.mime_types.append(mime_type)\n                        self.texts.append(text)\n                    else:\n                        self.errors.append(\n                            f\"Invalid MIME type: {mime_type} for file {info.filename}.\"\n                        )\n                except BaseException as e:\n                    self.errors.append(e)\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.load_dataset","title":"<code>load_dataset(dataset: Self) -&gt; None</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataLoader</code> <p>The dataset to load.</p> required As of v2.10.5, Pydantic does not support recursive types (Self). <p>As a result, this method performs its own check to see if the value of <code>dataset</code> is of type <code>DataLoader</code>.</p> Source code in <code>lexos/io/loader.py</code> <pre><code>def load_dataset(self, dataset: Self) -&gt; None:\n    \"\"\"Load a dataset.\n\n    Args:\n        dataset (DataLoader): The dataset to load.\n\n    Note: As of v2.10.5, Pydantic does not support recursive types (Self).\n        As a result, this method performs its own check to see if the\n        value of `dataset` is of type `DataLoader`.\n    \"\"\"\n    if not isinstance(dataset, DataLoader):\n        raise LexosException(\"Invalid dataset type.\")\n    self.paths = self.paths + dataset.paths\n    self.mime_types = self.mime_types + dataset.mime_types\n    self.names = self.names + dataset.names\n    self.texts = self.texts + dataset.texts\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.load","title":"<code>load(paths: Path | str | list[Path | str]) -&gt; None</code>","text":"<p>Load a list of paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Path | str | list[Path | str]</code> <p>The list of paths to load.</p> required Source code in <code>lexos/io/loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load(self, paths: Path | str | list[Path | str]) -&gt; None:\n    \"\"\"Load a list of paths.\n\n    Args:\n        paths (Path | str | list[Path | str]): The list of paths to load.\n    \"\"\"\n    paths = ensure_list(paths)\n    for path in paths:\n        if Path(path).is_dir():\n            paths = [p for p in Path(path).rglob(\"*\")]\n            self.load(paths)\n        # Get the mime type of the file\n        try:\n            with open(path, \"rb\") as f:\n                file_start = f.read(FILE_START)\n            mime_type = self._get_mime_type(path, file_start)\n        except IOError as e:\n            self.errors.append(e)\n            mime_type = None\n        if mime_type in TEXT_TYPES:\n            self._load_text_file(path, mime_type)\n        elif mime_type in PDF_TYPES:\n            self._load_pdf_file(path)\n        elif mime_type in DOCX_TYPES:\n            self._load_docx_file(path)\n        elif mime_type in ZIP_TYPES:\n            self._load_zip_file(path)\n        else:\n            self.errors.append(f\"Invalid MIME type: {mime_type} for file {path}.\")\n</code></pre>"},{"location":"api/io/loader/#lexos.io.loader.Loader.loads","title":"<code>loads(texts: Optional[list[Path | str]] = None, names: Optional[list[str]] = None, start: Optional[int] = 1, zero_pad: Optional[str] = '03') -&gt; None</code>","text":"<p>Load a list of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Optional[list[Path | str]]</code> <p>The list of texts to load.</p> <code>None</code> <code>names</code> <code>Optional[list[str]]</code> <p>The list of names for the texts.</p> <code>None</code> <code>start</code> <code>Optional[int]</code> <p>The starting index for the names if no list is provided.</p> <code>1</code> <code>zero_pad</code> <code>Optional[str]</code> <p>The zero padding for the names increments if no list is provided.</p> <code>'03'</code> Source code in <code>lexos/io/loader.py</code> <pre><code>@validate_call(config=model_config)\ndef loads(\n    self,\n    texts: Optional[list[Path | str]] = None,\n    names: Optional[list[str]] = None,\n    start: Optional[int] = 1,\n    zero_pad: Optional[str] = \"03\",\n) -&gt; None:\n    \"\"\"Load a list of texts.\n\n    Args:\n        texts (Optional[list[Path | str]]): The list of texts to load.\n        names (Optional[list[str]]): The list of names for the texts.\n        start (Optional[int]): The starting index for the names if no list is provided.\n        zero_pad (Optional[str]): The zero padding for the names increments if no list is provided.\n    \"\"\"\n    texts = ensure_list(texts)\n    if names is None:\n        names = [f\"text{i + start:{zero_pad}d}\" for i in range(len(texts))]\n    for i, text in enumerate(texts):\n        self.names.append(names[i])\n        self.mime_types.append(\"text/plain\")\n        self.texts.append(text)\n</code></pre>"},{"location":"api/io/parallel_loader/","title":"Parallel Loader","text":"<p>The <code>ParallelLoader</code> class is an implementation of the <code>Loader</code> class optimized for large data sets using Python's <code>ThreadPoolExecutor</code> for parallel I/O operations.</p>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.VALID_FILE_TYPES","title":"<code>VALID_FILE_TYPES = {*TEXT_TYPES, *PDF_TYPES, *DOCX_TYPES, *ZIP_TYPES}</code>  <code>module-attribute</code>","text":""},{"location":"api/io/parallel_loader/#class-parallelloader","title":"class <code>ParallelLoader</code>","text":"<p>Note</p> <p>Mkdocstrings does not properly render the the class docstrings because <code>griffe_pydantic</code> gets confused when trying to render fields inherited from <code>BaseLoader</code> combined with new fields in <code>ParallelLoader</code>. Attributes are still documented correctly, but the overall class docstring is missing.</p>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.max_workers","title":"<code>max_workers: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>Maximum number of worker threads. Can be an integer or will be auto-calculated based on worker_strategy.</p>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.worker_strategy","title":"<code>worker_strategy: str = 'auto'</code>  <code>pydantic-field</code>","text":"<p>Worker allocation strategy: 'auto' (analyze files), 'io_bound' (more workers), 'cpu_bound' (fewer workers), 'balanced' (middle ground).</p>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.batch_size","title":"<code>batch_size: int = 100</code>  <code>pydantic-field</code>","text":"<p>Number of files to process in each batch for memory management.</p>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.show_progress","title":"<code>show_progress: bool = True</code>  <code>pydantic-field</code>","text":"<p>Whether to show a progress bar during loading.</p>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.callback","title":"<code>callback: Optional[Callable[..., None]] = None</code>  <code>pydantic-field</code>","text":"<p>Optional callback function for custom progress handling.</p>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the ParallelLoader.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the ParallelLoader.\"\"\"\n    super().__init__(**data)\n    self._lock = threading.Lock()\n    self._mime_cache = {}\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._calculate_optimal_workers","title":"<code>_calculate_optimal_workers(file_list: list[tuple[Path | str, str]]) -&gt; int</code>","text":"<p>Calculate optimal worker count based on file types and strategy.</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list[tuple]</code> <p>List of (path, mime_type) tuples.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Optimal number of workers.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _calculate_optimal_workers(\n    self, file_list: list[tuple[Path | str, str]]\n) -&gt; int:\n    \"\"\"Calculate optimal worker count based on file types and strategy.\n\n    Args:\n        file_list (list[tuple]): List of (path, mime_type) tuples.\n\n    Returns:\n        int: Optimal number of workers.\n    \"\"\"\n    cpu_count = os.cpu_count() or 1\n\n    # If max_workers is explicitly set, use it\n    if self.max_workers is not None:\n        return self.max_workers\n\n    # Analyze file types for 'auto' strategy\n    if self.worker_strategy == \"auto\":\n        # Count file types\n        cpu_intensive_count = 0\n        io_intensive_count = 0\n\n        for _, mime_type in file_list:\n            if mime_type in PDF_TYPES or mime_type in DOCX_TYPES:\n                cpu_intensive_count += 1\n            elif mime_type in TEXT_TYPES:\n                io_intensive_count += 1\n\n        # Determine strategy based on file mix\n        if cpu_intensive_count &gt; len(file_list) * 0.5:\n            # More than 50% CPU-intensive files\n            return min(16, cpu_count * 2)\n        elif io_intensive_count &gt; len(file_list) * 0.8:\n            # More than 80% I/O-intensive files\n            return min(32, cpu_count * 4)\n        else:\n            # Mixed workload\n            return min(24, cpu_count * 3)\n\n    # Manual strategy selection\n    elif self.worker_strategy == \"io_bound\":\n        return min(32, cpu_count * 4)\n    elif self.worker_strategy == \"cpu_bound\":\n        return min(16, cpu_count * 2)\n    elif self.worker_strategy == \"balanced\":\n        return min(24, cpu_count * 3)\n    else:\n        # Default fallback\n        return min(32, cpu_count + 4)\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._detect_mime_types_parallel","title":"<code>_detect_mime_types_parallel(file_list: list[tuple[Path | str, Optional[str]]], progress: Optional[Progress] = None, task_id: Optional[int] = None) -&gt; list[tuple[Path | str, str]]</code>","text":"<p>Detect MIME types for all files in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list[tuple]</code> <p>List of (path, mime_type) tuples.</p> required <code>progress</code> <code>Optional[Progress]</code> <p>Rich progress bar instance.</p> <code>None</code> <code>task_id</code> <code>Optional[int]</code> <p>Task ID for progress tracking.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[Path | str, str]]</code> <p>list[tuple]: List of (path, mime_type) tuples with detected types.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _detect_mime_types_parallel(\n    self,\n    file_list: list[tuple[Path | str, Optional[str]]],\n    progress: Optional[Progress] = None,\n    task_id: Optional[int] = None,\n) -&gt; list[tuple[Path | str, str]]:\n    \"\"\"Detect MIME types for all files in parallel.\n\n    Args:\n        file_list (list[tuple]): List of (path, mime_type) tuples.\n        progress (Optional[Progress]): Rich progress bar instance.\n        task_id (Optional[int]): Task ID for progress tracking.\n\n    Returns:\n        list[tuple]: List of (path, mime_type) tuples with detected types.\n    \"\"\"\n    results = []\n\n    def detect_mime(path_tuple):\n        path, _ = path_tuple\n        try:\n            with open(path, \"rb\") as f:\n                file_start = f.read(FILE_START)\n            mime_type = self._get_mime_type(path, file_start)\n            if progress and task_id is not None:\n                progress.update(task_id, advance=1)\n            return (path, mime_type)\n        except IOError as e:\n            with self._lock:\n                self.errors.append(e)\n            if progress and task_id is not None:\n                progress.update(task_id, advance=1)\n            return (path, None)\n\n    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n        results = list(executor.map(detect_mime, file_list))\n\n    return results\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._get_mime_type","title":"<code>_get_mime_type(path: Path | str, file_start: bytes) -&gt; str</code>","text":"<p>Get the mime type of a file with caching.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <code>file_start</code> <code>bytes</code> <p>The first bytes of the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The mime type of the file.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _get_mime_type(self, path: Path | str, file_start: bytes) -&gt; str:\n    \"\"\"Get the mime type of a file with caching.\n\n    Args:\n        path (Path | str): The path to the file.\n        file_start (bytes): The first bytes of the file.\n\n    Returns:\n        str: The mime type of the file.\n    \"\"\"\n    # Check cache first\n    path_str = str(path)\n    if path_str in self._mime_cache:\n        return self._mime_cache[path_str]\n\n    if Path(path).suffix == \".pickle\":\n        mime_type = \"application/vnd.python.pickle\"\n    else:\n        try:\n            file_start_str = decode(file_start)\n        except (UnicodeDecodeError, AttributeError):\n            file_start_str = \"\"\n\n        results = puremagic.magic_string(file_start_str, path)\n        if not results:\n            mime_type = None\n        else:\n            mime_type = results[0].mime_type\n            if mime_type == \"\":\n                mime_type, _ = mimetypes.guess_type(path)\n\n    # Cache the result\n    self._mime_cache[path_str] = mime_type\n    return mime_type\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._group_by_type","title":"<code>_group_by_type(file_list: list[tuple[Path | str, str]]) -&gt; dict[str, list[Path | str]]</code>","text":"<p>Group files by MIME type for optimized processing.</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list[tuple]</code> <p>List of (path, mime_type) tuples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, list[Path | str]]</code> <p>Dictionary mapping mime_type to list of paths.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _group_by_type(\n    self, file_list: list[tuple[Path | str, str]]\n) -&gt; dict[str, list[Path | str]]:\n    \"\"\"Group files by MIME type for optimized processing.\n\n    Args:\n        file_list (list[tuple]): List of (path, mime_type) tuples.\n\n    Returns:\n        dict: Dictionary mapping mime_type to list of paths.\n    \"\"\"\n    grouped = {}\n    for path, mime_type in file_list:\n        if mime_type not in grouped:\n            grouped[mime_type] = []\n        grouped[mime_type].append(path)\n    return grouped\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._load_docx_file","title":"<code>_load_docx_file(path: Path | str) -&gt; tuple[str, str, str, str, Optional[Exception]]</code>","text":"<p>Load a docx file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str, str, str, str, Optional[Exception]]</code> <p>(path_name, name, mime_type, text, error)</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _load_docx_file(\n    self, path: Path | str\n) -&gt; tuple[str, str, str, str, Optional[Exception]]:\n    \"\"\"Load a docx file.\n\n    Args:\n        path (Path | str): The path to the file.\n\n    Returns:\n        tuple: (path_name, name, mime_type, text, error)\n    \"\"\"\n    try:\n        doc = Document(path)\n        text = \"\\n\".join([decode(p.text) for p in doc.paragraphs])\n        return (Path(path).name, Path(path).stem, \"application/docx\", text, None)\n    except Exception as e:\n        return (Path(path).name, Path(path).stem, \"application/docx\", \"\", e)\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._load_pdf_file","title":"<code>_load_pdf_file(path: Path | str) -&gt; list[tuple[str, str, str, str, Optional[Exception]]]</code>","text":"<p>Load a pdf file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, str, str, Optional[Exception]]]</code> <p>list[tuple]: List of (path_name, name, mime_type, text, error) for each page.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _load_pdf_file(\n    self, path: Path | str\n) -&gt; list[tuple[str, str, str, str, Optional[Exception]]]:\n    \"\"\"Load a pdf file.\n\n    Args:\n        path (Path | str): The path to the file.\n\n    Returns:\n        list[tuple]: List of (path_name, name, mime_type, text, error) for each page.\n    \"\"\"\n    results = []\n    try:\n        reader = PdfReader(path)\n        for page in reader.pages:\n            text = decode(page.extract_text())\n            results.append(\n                (Path(path).name, Path(path).stem, \"application/pdf\", text, None)\n            )\n    except Exception as e:\n        results.append((Path(path).name, Path(path).stem, \"application/pdf\", \"\", e))\n    return results\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._load_text_file","title":"<code>_load_text_file(path: Path | str, mime_type: str) -&gt; tuple[str, str, str, str, Optional[Exception]]</code>","text":"<p>Load a text file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <code>mime_type</code> <code>str</code> <p>The mime type of the file.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str, str, str, str, Optional[Exception]]</code> <p>(path_name, name, mime_type, text, error)</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _load_text_file(\n    self, path: Path | str, mime_type: str\n) -&gt; tuple[str, str, str, str, Optional[Exception]]:\n    \"\"\"Load a text file.\n\n    Args:\n        path (Path | str): The path to the file.\n        mime_type (str): The mime type of the file.\n\n    Returns:\n        tuple: (path_name, name, mime_type, text, error)\n    \"\"\"\n    try:\n        with open(path, \"rb\") as f:\n            text = decode(f.read())\n        return (Path(path).name, Path(path).stem, mime_type, text, None)\n    except Exception as e:\n        return (Path(path).name, Path(path).stem, mime_type, \"\", e)\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._load_zip_file","title":"<code>_load_zip_file(path: Path | str) -&gt; list[tuple[str, str, str, str, Optional[Exception]]]</code>","text":"<p>Handle a zip file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, str, str, Optional[Exception]]]</code> <p>list[tuple]: List of (path_name, name, mime_type, text, error) for each file in zip.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _load_zip_file(\n    self, path: Path | str\n) -&gt; list[tuple[str, str, str, str, Optional[Exception]]]:\n    \"\"\"Handle a zip file.\n\n    Args:\n        path (Path | str): The path to the file.\n\n    Returns:\n        list[tuple]: List of (path_name, name, mime_type, text, error) for each file in zip.\n    \"\"\"\n    results = []\n    try:\n        with open(path, \"rb\") as fin:\n            with zipfile.ZipFile(fin) as zip:\n                for info in zip.infolist():\n                    try:\n                        # Get the mime type of the file\n                        file_bytes = zip.read(info.filename)\n                        file_start = file_bytes[:MIN_ENCODING_DETECT]\n                        mime_type = self._get_mime_type(info.filename, file_start)\n                    except (IOError, UnicodeDecodeError) as e:\n                        results.append(\n                            (info.filename, Path(info.filename).stem, None, \"\", e)\n                        )\n                        continue\n\n                    try:\n                        if mime_type in VALID_FILE_TYPES:\n                            text = decode(file_bytes)\n                            full_path = Path(path).as_posix() + \"/\" + info.filename\n                            results.append(\n                                (\n                                    full_path,\n                                    Path(info.filename).stem,\n                                    mime_type,\n                                    text,\n                                    None,\n                                )\n                            )\n                        else:\n                            error = LexosException(\n                                f\"Invalid MIME type: {mime_type} for file {info.filename}.\"\n                            )\n                            results.append(\n                                (\n                                    info.filename,\n                                    Path(info.filename).stem,\n                                    mime_type,\n                                    \"\",\n                                    error,\n                                )\n                            )\n                    except Exception as e:\n                        results.append(\n                            (\n                                info.filename,\n                                Path(info.filename).stem,\n                                mime_type,\n                                \"\",\n                                e,\n                            )\n                        )\n    except Exception as e:\n        results.append((Path(path).name, Path(path).stem, None, \"\", e))\n    return results\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._load_file_concurrent","title":"<code>_load_file_concurrent(path: Path | str, mime_type: str) -&gt; list[tuple[str, str, str, str, Optional[Exception]]]</code>","text":"<p>Load a single file (wrapper for concurrent execution).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file.</p> required <code>mime_type</code> <code>str</code> <p>The mime type of the file.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, str, str, Optional[Exception]]]</code> <p>list[tuple]: List of (path_name, name, mime_type, text, error) tuples.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _load_file_concurrent(\n    self, path: Path | str, mime_type: str\n) -&gt; list[tuple[str, str, str, str, Optional[Exception]]]:\n    \"\"\"Load a single file (wrapper for concurrent execution).\n\n    Args:\n        path (Path | str): The path to the file.\n        mime_type (str): The mime type of the file.\n\n    Returns:\n        list[tuple]: List of (path_name, name, mime_type, text, error) tuples.\n    \"\"\"\n    results = []\n\n    if mime_type in TEXT_TYPES:\n        result = self._load_text_file(path, mime_type)\n        results.append(result)\n    elif mime_type in PDF_TYPES:\n        results.extend(self._load_pdf_file(path))\n    elif mime_type in DOCX_TYPES:\n        result = self._load_docx_file(path)\n        results.append(result)\n    elif mime_type in ZIP_TYPES:\n        results.extend(self._load_zip_file(path))\n    else:\n        error = LexosException(f\"Invalid MIME type: {mime_type} for file {path}.\")\n        results.append((Path(path).name, Path(path).stem, mime_type, \"\", error))\n\n    return results\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._prepare_file_list","title":"<code>_prepare_file_list(paths: list[Path | str]) -&gt; list[tuple[Path | str, str]]</code>","text":"<p>Prepare list of files with MIME types, expanding directories.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[Path | str]</code> <p>List of file or directory paths.</p> required <p>Returns:</p> Type Description <code>list[tuple[Path | str, str]]</code> <p>list[tuple]: List of (path, mime_type) tuples.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _prepare_file_list(\n    self, paths: list[Path | str]\n) -&gt; list[tuple[Path | str, str]]:\n    \"\"\"Prepare list of files with MIME types, expanding directories.\n\n    Args:\n        paths (list[Path | str]): List of file or directory paths.\n\n    Returns:\n        list[tuple]: List of (path, mime_type) tuples.\n    \"\"\"\n    file_list = []\n\n    for path in paths:\n        if Path(path).is_dir():\n            # Recursively get all files in directory\n            dir_files = [p for p in Path(path).rglob(\"*\") if p.is_file()]\n            file_list.extend([(str(f), None) for f in dir_files])\n        else:\n            file_list.append((str(path), None))\n\n    return file_list\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._process_results","title":"<code>_process_results(results: list[tuple]) -&gt; None</code>","text":"<p>Process and store results in a thread-safe manner.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple]</code> <p>List of (path_name, name, mime_type, text, error) tuples.</p> required Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _process_results(self, results: list[tuple]) -&gt; None:\n    \"\"\"Process and store results in a thread-safe manner.\n\n    Args:\n        results (list[tuple]): List of (path_name, name, mime_type, text, error) tuples.\n    \"\"\"\n    with self._lock:\n        for path_name, name, mime_type, text, error in results:\n            self.paths.append(path_name)\n            self.names.append(name)\n            self.mime_types.append(mime_type)\n            self.texts.append(text)\n            if error:\n                self.errors.append(error)\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader._sort_files_by_type","title":"<code>_sort_files_by_type(file_list: list[tuple[Path | str, str]]) -&gt; list[tuple[Path | str, str]]</code>","text":"<p>Sort files by MIME type for better cache locality.</p> <p>Groups similar file types together to improve processing efficiency through better cache utilization and reduced context switching.</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list[tuple]</code> <p>List of (path, mime_type) tuples.</p> required <p>Returns:</p> Type Description <code>list[tuple[Path | str, str]]</code> <p>list[tuple]: Sorted list of (path, mime_type) tuples.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def _sort_files_by_type(\n    self, file_list: list[tuple[Path | str, str]]\n) -&gt; list[tuple[Path | str, str]]:\n    \"\"\"Sort files by MIME type for better cache locality.\n\n    Groups similar file types together to improve processing efficiency\n    through better cache utilization and reduced context switching.\n\n    Args:\n        file_list (list[tuple]): List of (path, mime_type) tuples.\n\n    Returns:\n        list[tuple]: Sorted list of (path, mime_type) tuples.\n    \"\"\"\n    # Define priority order for file types (process simpler types first)\n    type_priority = {\n        **{t: 1 for t in TEXT_TYPES},  # Text files first (fastest)\n        **{t: 2 for t in ZIP_TYPES},  # ZIP files second\n        **{t: 3 for t in DOCX_TYPES},  # DOCX files third\n        **{t: 4 for t in PDF_TYPES},  # PDF files last (slowest)\n    }\n\n    # Sort by priority, with unknown types at the end\n    return sorted(file_list, key=lambda x: type_priority.get(x[1], 999))\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.load_dataset","title":"<code>load_dataset(dataset: Self) -&gt; None</code>","text":"<p>Load a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataLoader</code> <p>The dataset to load.</p> required As of v2.10.5, Pydantic does not support recursive types (Self). <p>As a result, this method performs its own check to see if the value of <code>dataset</code> is of type <code>DataLoader</code>.</p> Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>def load_dataset(self, dataset: Self) -&gt; None:\n    \"\"\"Load a dataset.\n\n    Args:\n        dataset (DataLoader): The dataset to load.\n\n    Note: As of v2.10.5, Pydantic does not support recursive types (Self).\n        As a result, this method performs its own check to see if the\n        value of `dataset` is of type `DataLoader`.\n    \"\"\"\n    if not isinstance(dataset, DataLoader):\n        raise LexosException(\"Invalid dataset type.\")\n\n    with self._lock:\n        self.paths = self.paths + dataset.paths\n        self.mime_types = self.mime_types + dataset.mime_types\n        self.names = self.names + dataset.names\n        self.texts = self.texts + dataset.texts\n</code></pre>"},{"location":"api/io/parallel_loader/#lexos.io.parallel_loader.ParallelLoader.load","title":"<code>load(paths: Path | str | list[Path | str]) -&gt; None</code>","text":"<p>Load files in parallel with batching and progress tracking.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Path | str | list[Path | str]</code> <p>The list of paths to load.</p> required Source code in <code>lexos/io/parallel_loader.py</code> <pre><code>@validate_call(config=model_config)\ndef load(self, paths: Path | str | list[Path | str]) -&gt; None:\n    \"\"\"Load files in parallel with batching and progress tracking.\n\n    Args:\n        paths (Path | str | list[Path | str]): The list of paths to load.\n    \"\"\"\n    paths = ensure_list(paths)\n\n    # Step 1: Prepare file list (expand directories)\n    file_list = self._prepare_file_list(paths)\n    total_files = len(file_list)\n\n    if total_files == 0:\n        return\n\n    # Setup progress bar\n    progress = None\n    detect_task = None\n    load_task = None\n\n    if self.show_progress:\n        progress = Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            TaskProgressColumn(),\n            TimeRemainingColumn(),\n        )\n        progress.start()\n        detect_task = progress.add_task(\n            \"[cyan]Detecting file types...\", total=total_files\n        )\n\n    # Step 2: Detect MIME types in parallel\n    file_list = self._detect_mime_types_parallel(file_list, progress, detect_task)\n\n    # Step 3: Calculate optimal worker count based on file types (auto-tuning)\n    if self.max_workers is None:\n        self.max_workers = self._calculate_optimal_workers(file_list)\n\n    # Step 4: Sort files by type for better cache locality (smart ordering)\n    file_list = self._sort_files_by_type(file_list)\n\n    # Step 5: Group files by type for optimized processing\n    grouped_files = self._group_by_type(file_list)\n\n    # Update progress bar for loading phase\n    if self.show_progress and progress:\n        load_task = progress.add_task(\"[green]Loading files...\", total=total_files)\n\n    # Step 6: Process files in batches by type\n    processed = 0\n    for mime_type, paths_of_type in grouped_files.items():\n        # Process this type in batches\n        for i in range(0, len(paths_of_type), self.batch_size):\n            batch = paths_of_type[i : i + self.batch_size]\n\n            # Load batch in parallel\n            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n                future_to_path = {\n                    executor.submit(\n                        self._load_file_concurrent, path, mime_type\n                    ): path\n                    for path in batch\n                }\n\n                for future in as_completed(future_to_path):\n                    path = future_to_path[future]\n                    try:\n                        results = future.result()\n                        self._process_results(results)\n                        processed += 1\n\n                        # Update progress\n                        if (\n                            self.show_progress\n                            and progress\n                            and load_task is not None\n                        ):\n                            progress.update(load_task, advance=1)\n\n                        # Call custom callback if provided\n                        if self.callback:\n                            self.callback(path, processed, total_files)\n\n                    except Exception as e:\n                        with self._lock:\n                            self.errors.append(e)\n                        processed += 1\n                        if (\n                            self.show_progress\n                            and progress\n                            and load_task is not None\n                        ):\n                            progress.update(load_task, advance=1)\n\n    # Cleanup progress bar\n    if self.show_progress and progress:\n        progress.stop()\n</code></pre>"},{"location":"api/kwic/","title":"KWIC","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre> <pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic","title":"<code>Kwic</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class for finding keywords in context (KWIC) in text or spaCy documents.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>nlp</code>                 (<code>Optional[str]</code>)             </li> <li> <code>alg</code>                 (<code>Optional[ns]</code>)             </li> </ul> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>class Kwic(BaseModel):\n    \"\"\"Class for finding keywords in context (KWIC) in text or spaCy documents.\"\"\"\n\n    nlp: Optional[str] = Field(\n        default=\"xx_sent_ud_sm\", description=\"The spaCy model to use for tokenization.\"\n    )\n    alg: Optional[ns] = Field(\n        default=ns.LOCALE, description=\"The sorting algorithm to use.\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data):\n        \"\"\"Initialize the Kwic class with a spaCy model.\"\"\"\n        super().__init__(**data)\n        self.nlp = spacy.load(self.nlp)\n\n        # Make sure the sorting algorithm is valid\n        self._validate_sorting_algorithm()\n\n    def __call__(\n        self,\n        docs: Optional[Doc | str | list[Doc | str]] = Field(\n            default_factory=list,\n            description=\"The spaCy Doc(s) or string(s) to search within.\",\n        ),\n        labels: Optional[str | list[str]] = Field(\n            None,\n            description=\"A list of labels for the documents. Defaults to None.\",\n        ),\n        patterns: list = Field(\n            default_factory=list,\n            description=\"A list of patterns to match. Can be regex strings or spaCy token patterns.\",\n        ),\n        window: Optional[int] = Field(\n            50,\n            description=\"The number of tokens or characters to include before and after the match.\",\n        ),\n        matcher: Optional[str] = Field(\n            \"characters\",\n            description=\"The type of matcher to use. Can be 'rule' for spaCy Matcher, 'phrase' for PhraseMatcher, 'tokens' for token patterns, or 'characters' for string matching.\",\n        ),\n        case_sensitive: Optional[bool] = Field(\n            False,\n            description=\"If True, the matching will be case-sensitive. Defaults to False.\",\n        ),\n        use_regex: Optional[bool] = Field(\n            False,\n            description=\"If True, use regex for matching with the 'tokens' setting. Defaults to False.\",\n        ),\n        as_df: Optional[bool] = Field(\n            True,\n            description=\"If True, return results as a pandas DataFrame. Defaults to True.\",\n        ),\n        sort_by: Optional[str] = Field(\n            \"keyword\",  # Make sure this matches the column name exactly\n            description=\"The column to sort the results by if as_df is True. Defaults to 'keyword'.\",\n        ),\n        ascending: Optional[bool] = Field(\n            True,\n            description=\"If True, sort in ascending order. Defaults to True.\",\n        ),\n    ) -&gt; list[tuple[str, str, str]] | pd.DataFrame:\n        \"\"\"Call the Kwic instance to find keywords in context.\n\n        Returns:\n            list: A list of tuples, each containing the context before, the matched keyword,\n                and the context after, or a DataFrame with the same content.\n        \"\"\"\n        # Validate input types\n        if matcher in [\"rule\", \"phrase\", \"tokens\"] and any(\n            isinstance(doc, str) for doc in docs\n        ):\n            raise LexosException(\n                \"Docs must be spaCy Doc objects when using 'rule', 'phrase', or 'tokens' matcher. To search raw text strings, use the 'characters' matcher type, setting `use_regex` if you wish to use regex patterns.\"\n            )\n\n        # Ensure that docs and labels are lists of equal length\n        docs = ensure_list(docs)\n        if isinstance(labels, list):\n            labels = ensure_list(labels)\n            if len(docs) != len(labels) and labels:\n                raise LexosException(\n                    \"The number of documents and labels must match. If you do not want to label the documents, set `labels` to None.\"\n                )\n        else:\n            labels = [f\"Doc {i + 1}\" for i in range(len(docs))]\n\n        # Assign search parameters and call match method\n        match matcher:\n            case \"rule\":\n                matcher = Matcher(self.nlp.vocab)\n                matcher.add(\"KWIC_PATTERNS\", patterns)\n                hits = self._match_tokens(docs, labels, window, matcher)\n            case \"phrase\":\n                if case_sensitive:\n                    matcher = PhraseMatcher(self.nlp.vocab)\n                else:\n                    matcher = PhraseMatcher(self.nlp.vocab, attr=\"LOWER\")\n                patterns = [self.nlp.make_doc(phrase) for phrase in patterns]\n                matcher.add(\"KWIC_PATTERNS\", patterns)\n                hits = self._match_tokens(docs, labels, window, matcher)\n            case \"tokens\":\n                matcher = Matcher(self.nlp.vocab)\n                patterns = ensure_list(patterns)\n                patterns = self._convert_patterns_to_spacy(\n                    patterns, case_sensitive, use_regex\n                )\n                matcher.add(\"KWIC_PATTERNS\", patterns)\n                hits = self._match_tokens(docs, labels, window, matcher)\n            case _:\n                docs = [doc.text if isinstance(doc, Doc) else doc for doc in docs]\n                patterns = ensure_list(patterns)\n                hits = list(\n                    self._match_strings(\n                        docs, labels, patterns, window, case_sensitive=case_sensitive\n                    )\n                )\n\n        # Convert hits to DataFrame for sorting\n        df = pd.DataFrame(\n            hits, columns=[\"doc\", \"context_before\", \"keyword\", \"context_after\"]\n        )\n\n        # Only sort if we have data and the sort_by column exists\n        if not df.empty and sort_by in df.columns:\n            df = df.sort_values(\n                by=sort_by, ascending=ascending, key=natsort_keygen(alg=self.alg)\n            )\n\n        # If as_df is False, convert DataFrame to list of dictionaries\n        if not as_df:\n            result = list(df.to_records(index=False))\n            return [tuple(item) for item in result]\n\n        return df\n\n    def _convert_patterns_to_spacy(\n        self, patterns: list, case_sensitive: bool, use_regex: bool\n    ) -&gt; list:\n        \"\"\"Convert a list of string patterns to spaCy token patterns.\n\n        Args:\n            patterns (list): A list of string patterns to convert.\n            case_sensitive (bool): If True, the patterns will be case-sensitive.\n            use_regex (bool): If True, the patterns will be treated as regex patterns.\n\n        Returns:\n            list: A list of spaCy token patterns.\n        \"\"\"\n        if use_regex:\n            if case_sensitive:\n                return [[{\"TEXT\": {\"REGEX\": pattern}}] for pattern in patterns]\n            else:\n                return [[{\"LOWER\": {\"REGEX\": pattern.lower()}}] for pattern in patterns]\n        else:\n            if case_sensitive:\n                return [[{\"TEXT\": pattern}] for pattern in patterns]\n            else:\n                return [[{\"LOWER\": pattern}] for pattern in patterns]\n\n    def _match_strings(\n        self,\n        docs: list[str],\n        labels: list[str],\n        patterns: list,\n        window: int,\n        case_sensitive: bool,\n    ):\n        \"\"\"Match keywords in a string and return their context.\n\n        Args:\n            docs (list[str]): The text to search within.\n            labels (str): A list of labels for the documents.\n            patterns (list): A list of regex patterns to match.\n            window (int): The number of characters to include before and after the match.\n            case_sensitive (bool): If True, the matching will be case-sensitive.\n\n        Yields:\n            tuple (tuple): A tuple containing the context before, the matched keyword, and the context after.\n        \"\"\"\n        flags = 0 if case_sensitive else re.IGNORECASE\n        for i, doc in enumerate(docs):\n            for pattern in patterns:\n                for match in re.finditer(pattern, doc, flags=flags):\n                    start = match.start()\n                    end = match.end()\n                    context_start = max(0, start - window)\n                    context_end = min(len(doc), end + window)\n                    context_before = doc[context_start:start]\n                    context_after = doc[end:context_end]\n                    yield (labels[i], context_before, match.group(), context_after)\n\n    def _match_tokens(\n        self, docs: list[Doc], labels: list[str], window: int, matcher: Matcher\n    ) -&gt; list[tuple[str, str, str, str]]:\n        \"\"\"Match keywords in a spaCy Doc and return their context.\n\n        Args:\n            docs (list[Doc]): The spaCy Doc(s) to search within.\n            labels (list[str]): A list of labels for the documents.\n            window (int): The number of tokens to include before and after the match.\n            matcher (Matcher): The spaCy Matcher object with patterns added.\n\n        Returns:\n            list[tuple[str, str, str, str]]: A list of tuples, each containing the context before, the matched keyword, and the context after.\n        \"\"\"\n        hits = []  # List to store the hits\n        for i, doc in enumerate(docs):\n            matches = matcher(doc)\n            for _, start, end in matches:\n                span = doc[start:end]  # The matched span (keyword)\n                context_start = max(0, start - window)  #  Start of context window\n                context_end = min(len(doc), end + window)  # End of context window\n                context_before = doc[context_start : span.start]\n                context_after = doc[span.end : context_end]  # Fixed indentation\n                hits.append(\n                    (labels[i], context_before.text, span.text, context_after.text)\n                )  # Fixed indentation\n        return hits\n\n    def _validate_sorting_algorithm(self) -&gt; bool:\n        \"\"\"Ensure that the specified sorting algorithm is a valid natsort locale.\n\n        Returns:\n            bool: Whether the sorting algorithm is valid.\n        \"\"\"\n        if self.alg not in [e for e in ns]:\n            locales = \", \".join([f\"ns.{e.name}\" for e in ns])\n            err = (\n                f\"Invalid sorting algorithm: {self.alg}.\",\n                f\"Valid algorithms for `alg` are: {locales}.\",\n                \"See https://natsort.readthedocs.io/en/stable/api.html#natsort.ns.\",\n            )\n            raise LexosException(\" \".join(err))\n        return True\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic.alg","title":"<code>alg: Optional[ns] = ns.LOCALE</code>  <code>pydantic-field</code>","text":"<p>The sorting algorithm to use.</p>"},{"location":"api/kwic/#lexos.kwic.Kwic.__call__","title":"<code>__call__(docs: Optional[Doc | str | list[Doc | str]] = Field(default_factory=list, description='The spaCy Doc(s) or string(s) to search within.'), labels: Optional[str | list[str]] = Field(None, description='A list of labels for the documents. Defaults to None.'), patterns: list = Field(default_factory=list, description='A list of patterns to match. Can be regex strings or spaCy token patterns.'), window: Optional[int] = Field(50, description='The number of tokens or characters to include before and after the match.'), matcher: Optional[str] = Field('characters', description=\"The type of matcher to use. Can be 'rule' for spaCy Matcher, 'phrase' for PhraseMatcher, 'tokens' for token patterns, or 'characters' for string matching.\"), case_sensitive: Optional[bool] = Field(False, description='If True, the matching will be case-sensitive. Defaults to False.'), use_regex: Optional[bool] = Field(False, description=\"If True, use regex for matching with the 'tokens' setting. Defaults to False.\"), as_df: Optional[bool] = Field(True, description='If True, return results as a pandas DataFrame. Defaults to True.'), sort_by: Optional[str] = Field('keyword', description=\"The column to sort the results by if as_df is True. Defaults to 'keyword'.\"), ascending: Optional[bool] = Field(True, description='If True, sort in ascending order. Defaults to True.')) -&gt; list[tuple[str, str, str]] | pd.DataFrame</code>","text":"<p>Call the Kwic instance to find keywords in context.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[tuple[str, str, str]] | DataFrame</code> <p>A list of tuples, each containing the context before, the matched keyword, and the context after, or a DataFrame with the same content.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def __call__(\n    self,\n    docs: Optional[Doc | str | list[Doc | str]] = Field(\n        default_factory=list,\n        description=\"The spaCy Doc(s) or string(s) to search within.\",\n    ),\n    labels: Optional[str | list[str]] = Field(\n        None,\n        description=\"A list of labels for the documents. Defaults to None.\",\n    ),\n    patterns: list = Field(\n        default_factory=list,\n        description=\"A list of patterns to match. Can be regex strings or spaCy token patterns.\",\n    ),\n    window: Optional[int] = Field(\n        50,\n        description=\"The number of tokens or characters to include before and after the match.\",\n    ),\n    matcher: Optional[str] = Field(\n        \"characters\",\n        description=\"The type of matcher to use. Can be 'rule' for spaCy Matcher, 'phrase' for PhraseMatcher, 'tokens' for token patterns, or 'characters' for string matching.\",\n    ),\n    case_sensitive: Optional[bool] = Field(\n        False,\n        description=\"If True, the matching will be case-sensitive. Defaults to False.\",\n    ),\n    use_regex: Optional[bool] = Field(\n        False,\n        description=\"If True, use regex for matching with the 'tokens' setting. Defaults to False.\",\n    ),\n    as_df: Optional[bool] = Field(\n        True,\n        description=\"If True, return results as a pandas DataFrame. Defaults to True.\",\n    ),\n    sort_by: Optional[str] = Field(\n        \"keyword\",  # Make sure this matches the column name exactly\n        description=\"The column to sort the results by if as_df is True. Defaults to 'keyword'.\",\n    ),\n    ascending: Optional[bool] = Field(\n        True,\n        description=\"If True, sort in ascending order. Defaults to True.\",\n    ),\n) -&gt; list[tuple[str, str, str]] | pd.DataFrame:\n    \"\"\"Call the Kwic instance to find keywords in context.\n\n    Returns:\n        list: A list of tuples, each containing the context before, the matched keyword,\n            and the context after, or a DataFrame with the same content.\n    \"\"\"\n    # Validate input types\n    if matcher in [\"rule\", \"phrase\", \"tokens\"] and any(\n        isinstance(doc, str) for doc in docs\n    ):\n        raise LexosException(\n            \"Docs must be spaCy Doc objects when using 'rule', 'phrase', or 'tokens' matcher. To search raw text strings, use the 'characters' matcher type, setting `use_regex` if you wish to use regex patterns.\"\n        )\n\n    # Ensure that docs and labels are lists of equal length\n    docs = ensure_list(docs)\n    if isinstance(labels, list):\n        labels = ensure_list(labels)\n        if len(docs) != len(labels) and labels:\n            raise LexosException(\n                \"The number of documents and labels must match. If you do not want to label the documents, set `labels` to None.\"\n            )\n    else:\n        labels = [f\"Doc {i + 1}\" for i in range(len(docs))]\n\n    # Assign search parameters and call match method\n    match matcher:\n        case \"rule\":\n            matcher = Matcher(self.nlp.vocab)\n            matcher.add(\"KWIC_PATTERNS\", patterns)\n            hits = self._match_tokens(docs, labels, window, matcher)\n        case \"phrase\":\n            if case_sensitive:\n                matcher = PhraseMatcher(self.nlp.vocab)\n            else:\n                matcher = PhraseMatcher(self.nlp.vocab, attr=\"LOWER\")\n            patterns = [self.nlp.make_doc(phrase) for phrase in patterns]\n            matcher.add(\"KWIC_PATTERNS\", patterns)\n            hits = self._match_tokens(docs, labels, window, matcher)\n        case \"tokens\":\n            matcher = Matcher(self.nlp.vocab)\n            patterns = ensure_list(patterns)\n            patterns = self._convert_patterns_to_spacy(\n                patterns, case_sensitive, use_regex\n            )\n            matcher.add(\"KWIC_PATTERNS\", patterns)\n            hits = self._match_tokens(docs, labels, window, matcher)\n        case _:\n            docs = [doc.text if isinstance(doc, Doc) else doc for doc in docs]\n            patterns = ensure_list(patterns)\n            hits = list(\n                self._match_strings(\n                    docs, labels, patterns, window, case_sensitive=case_sensitive\n                )\n            )\n\n    # Convert hits to DataFrame for sorting\n    df = pd.DataFrame(\n        hits, columns=[\"doc\", \"context_before\", \"keyword\", \"context_after\"]\n    )\n\n    # Only sort if we have data and the sort_by column exists\n    if not df.empty and sort_by in df.columns:\n        df = df.sort_values(\n            by=sort_by, ascending=ascending, key=natsort_keygen(alg=self.alg)\n        )\n\n    # If as_df is False, convert DataFrame to list of dictionaries\n    if not as_df:\n        result = list(df.to_records(index=False))\n        return [tuple(item) for item in result]\n\n    return df\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the Kwic class with a spaCy model.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the Kwic class with a spaCy model.\"\"\"\n    super().__init__(**data)\n    self.nlp = spacy.load(self.nlp)\n\n    # Make sure the sorting algorithm is valid\n    self._validate_sorting_algorithm()\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic.nlp","title":"<code>nlp: Optional[str]</code>  <code>pydantic-field</code>","text":""},{"location":"api/kwic/#lexos.kwic.Kwic.alg","title":"<code>alg: Optional[ns] = ns.LOCALE</code>  <code>pydantic-field</code>","text":"<p>The sorting algorithm to use.</p>"},{"location":"api/kwic/#lexos.kwic.Kwic.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kwic/#lexos.kwic.Kwic.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the Kwic class with a spaCy model.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the Kwic class with a spaCy model.\"\"\"\n    super().__init__(**data)\n    self.nlp = spacy.load(self.nlp)\n\n    # Make sure the sorting algorithm is valid\n    self._validate_sorting_algorithm()\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic.__call__","title":"<code>__call__(docs: Optional[Doc | str | list[Doc | str]] = Field(default_factory=list, description='The spaCy Doc(s) or string(s) to search within.'), labels: Optional[str | list[str]] = Field(None, description='A list of labels for the documents. Defaults to None.'), patterns: list = Field(default_factory=list, description='A list of patterns to match. Can be regex strings or spaCy token patterns.'), window: Optional[int] = Field(50, description='The number of tokens or characters to include before and after the match.'), matcher: Optional[str] = Field('characters', description=\"The type of matcher to use. Can be 'rule' for spaCy Matcher, 'phrase' for PhraseMatcher, 'tokens' for token patterns, or 'characters' for string matching.\"), case_sensitive: Optional[bool] = Field(False, description='If True, the matching will be case-sensitive. Defaults to False.'), use_regex: Optional[bool] = Field(False, description=\"If True, use regex for matching with the 'tokens' setting. Defaults to False.\"), as_df: Optional[bool] = Field(True, description='If True, return results as a pandas DataFrame. Defaults to True.'), sort_by: Optional[str] = Field('keyword', description=\"The column to sort the results by if as_df is True. Defaults to 'keyword'.\"), ascending: Optional[bool] = Field(True, description='If True, sort in ascending order. Defaults to True.')) -&gt; list[tuple[str, str, str]] | pd.DataFrame</code>","text":"<p>Call the Kwic instance to find keywords in context.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[tuple[str, str, str]] | DataFrame</code> <p>A list of tuples, each containing the context before, the matched keyword, and the context after, or a DataFrame with the same content.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def __call__(\n    self,\n    docs: Optional[Doc | str | list[Doc | str]] = Field(\n        default_factory=list,\n        description=\"The spaCy Doc(s) or string(s) to search within.\",\n    ),\n    labels: Optional[str | list[str]] = Field(\n        None,\n        description=\"A list of labels for the documents. Defaults to None.\",\n    ),\n    patterns: list = Field(\n        default_factory=list,\n        description=\"A list of patterns to match. Can be regex strings or spaCy token patterns.\",\n    ),\n    window: Optional[int] = Field(\n        50,\n        description=\"The number of tokens or characters to include before and after the match.\",\n    ),\n    matcher: Optional[str] = Field(\n        \"characters\",\n        description=\"The type of matcher to use. Can be 'rule' for spaCy Matcher, 'phrase' for PhraseMatcher, 'tokens' for token patterns, or 'characters' for string matching.\",\n    ),\n    case_sensitive: Optional[bool] = Field(\n        False,\n        description=\"If True, the matching will be case-sensitive. Defaults to False.\",\n    ),\n    use_regex: Optional[bool] = Field(\n        False,\n        description=\"If True, use regex for matching with the 'tokens' setting. Defaults to False.\",\n    ),\n    as_df: Optional[bool] = Field(\n        True,\n        description=\"If True, return results as a pandas DataFrame. Defaults to True.\",\n    ),\n    sort_by: Optional[str] = Field(\n        \"keyword\",  # Make sure this matches the column name exactly\n        description=\"The column to sort the results by if as_df is True. Defaults to 'keyword'.\",\n    ),\n    ascending: Optional[bool] = Field(\n        True,\n        description=\"If True, sort in ascending order. Defaults to True.\",\n    ),\n) -&gt; list[tuple[str, str, str]] | pd.DataFrame:\n    \"\"\"Call the Kwic instance to find keywords in context.\n\n    Returns:\n        list: A list of tuples, each containing the context before, the matched keyword,\n            and the context after, or a DataFrame with the same content.\n    \"\"\"\n    # Validate input types\n    if matcher in [\"rule\", \"phrase\", \"tokens\"] and any(\n        isinstance(doc, str) for doc in docs\n    ):\n        raise LexosException(\n            \"Docs must be spaCy Doc objects when using 'rule', 'phrase', or 'tokens' matcher. To search raw text strings, use the 'characters' matcher type, setting `use_regex` if you wish to use regex patterns.\"\n        )\n\n    # Ensure that docs and labels are lists of equal length\n    docs = ensure_list(docs)\n    if isinstance(labels, list):\n        labels = ensure_list(labels)\n        if len(docs) != len(labels) and labels:\n            raise LexosException(\n                \"The number of documents and labels must match. If you do not want to label the documents, set `labels` to None.\"\n            )\n    else:\n        labels = [f\"Doc {i + 1}\" for i in range(len(docs))]\n\n    # Assign search parameters and call match method\n    match matcher:\n        case \"rule\":\n            matcher = Matcher(self.nlp.vocab)\n            matcher.add(\"KWIC_PATTERNS\", patterns)\n            hits = self._match_tokens(docs, labels, window, matcher)\n        case \"phrase\":\n            if case_sensitive:\n                matcher = PhraseMatcher(self.nlp.vocab)\n            else:\n                matcher = PhraseMatcher(self.nlp.vocab, attr=\"LOWER\")\n            patterns = [self.nlp.make_doc(phrase) for phrase in patterns]\n            matcher.add(\"KWIC_PATTERNS\", patterns)\n            hits = self._match_tokens(docs, labels, window, matcher)\n        case \"tokens\":\n            matcher = Matcher(self.nlp.vocab)\n            patterns = ensure_list(patterns)\n            patterns = self._convert_patterns_to_spacy(\n                patterns, case_sensitive, use_regex\n            )\n            matcher.add(\"KWIC_PATTERNS\", patterns)\n            hits = self._match_tokens(docs, labels, window, matcher)\n        case _:\n            docs = [doc.text if isinstance(doc, Doc) else doc for doc in docs]\n            patterns = ensure_list(patterns)\n            hits = list(\n                self._match_strings(\n                    docs, labels, patterns, window, case_sensitive=case_sensitive\n                )\n            )\n\n    # Convert hits to DataFrame for sorting\n    df = pd.DataFrame(\n        hits, columns=[\"doc\", \"context_before\", \"keyword\", \"context_after\"]\n    )\n\n    # Only sort if we have data and the sort_by column exists\n    if not df.empty and sort_by in df.columns:\n        df = df.sort_values(\n            by=sort_by, ascending=ascending, key=natsort_keygen(alg=self.alg)\n        )\n\n    # If as_df is False, convert DataFrame to list of dictionaries\n    if not as_df:\n        result = list(df.to_records(index=False))\n        return [tuple(item) for item in result]\n\n    return df\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic._convert_patterns_to_spacy","title":"<code>_convert_patterns_to_spacy(patterns: list, case_sensitive: bool, use_regex: bool) -&gt; list</code>","text":"<p>Convert a list of string patterns to spaCy token patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>list</code> <p>A list of string patterns to convert.</p> required <code>case_sensitive</code> <code>bool</code> <p>If True, the patterns will be case-sensitive.</p> required <code>use_regex</code> <code>bool</code> <p>If True, the patterns will be treated as regex patterns.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of spaCy token patterns.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def _convert_patterns_to_spacy(\n    self, patterns: list, case_sensitive: bool, use_regex: bool\n) -&gt; list:\n    \"\"\"Convert a list of string patterns to spaCy token patterns.\n\n    Args:\n        patterns (list): A list of string patterns to convert.\n        case_sensitive (bool): If True, the patterns will be case-sensitive.\n        use_regex (bool): If True, the patterns will be treated as regex patterns.\n\n    Returns:\n        list: A list of spaCy token patterns.\n    \"\"\"\n    if use_regex:\n        if case_sensitive:\n            return [[{\"TEXT\": {\"REGEX\": pattern}}] for pattern in patterns]\n        else:\n            return [[{\"LOWER\": {\"REGEX\": pattern.lower()}}] for pattern in patterns]\n    else:\n        if case_sensitive:\n            return [[{\"TEXT\": pattern}] for pattern in patterns]\n        else:\n            return [[{\"LOWER\": pattern}] for pattern in patterns]\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic._match_strings","title":"<code>_match_strings(docs: list[str], labels: list[str], patterns: list, window: int, case_sensitive: bool)</code>","text":"<p>Match keywords in a string and return their context.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list[str]</code> <p>The text to search within.</p> required <code>labels</code> <code>str</code> <p>A list of labels for the documents.</p> required <code>patterns</code> <code>list</code> <p>A list of regex patterns to match.</p> required <code>window</code> <code>int</code> <p>The number of characters to include before and after the match.</p> required <code>case_sensitive</code> <code>bool</code> <p>If True, the matching will be case-sensitive.</p> required <p>Yields:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the context before, the matched keyword, and the context after.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def _match_strings(\n    self,\n    docs: list[str],\n    labels: list[str],\n    patterns: list,\n    window: int,\n    case_sensitive: bool,\n):\n    \"\"\"Match keywords in a string and return their context.\n\n    Args:\n        docs (list[str]): The text to search within.\n        labels (str): A list of labels for the documents.\n        patterns (list): A list of regex patterns to match.\n        window (int): The number of characters to include before and after the match.\n        case_sensitive (bool): If True, the matching will be case-sensitive.\n\n    Yields:\n        tuple (tuple): A tuple containing the context before, the matched keyword, and the context after.\n    \"\"\"\n    flags = 0 if case_sensitive else re.IGNORECASE\n    for i, doc in enumerate(docs):\n        for pattern in patterns:\n            for match in re.finditer(pattern, doc, flags=flags):\n                start = match.start()\n                end = match.end()\n                context_start = max(0, start - window)\n                context_end = min(len(doc), end + window)\n                context_before = doc[context_start:start]\n                context_after = doc[end:context_end]\n                yield (labels[i], context_before, match.group(), context_after)\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic._match_tokens","title":"<code>_match_tokens(docs: list[Doc], labels: list[str], window: int, matcher: Matcher) -&gt; list[tuple[str, str, str, str]]</code>","text":"<p>Match keywords in a spaCy Doc and return their context.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list[Doc]</code> <p>The spaCy Doc(s) to search within.</p> required <code>labels</code> <code>list[str]</code> <p>A list of labels for the documents.</p> required <code>window</code> <code>int</code> <p>The number of tokens to include before and after the match.</p> required <code>matcher</code> <code>Matcher</code> <p>The spaCy Matcher object with patterns added.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, str, str]]</code> <p>list[tuple[str, str, str, str]]: A list of tuples, each containing the context before, the matched keyword, and the context after.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def _match_tokens(\n    self, docs: list[Doc], labels: list[str], window: int, matcher: Matcher\n) -&gt; list[tuple[str, str, str, str]]:\n    \"\"\"Match keywords in a spaCy Doc and return their context.\n\n    Args:\n        docs (list[Doc]): The spaCy Doc(s) to search within.\n        labels (list[str]): A list of labels for the documents.\n        window (int): The number of tokens to include before and after the match.\n        matcher (Matcher): The spaCy Matcher object with patterns added.\n\n    Returns:\n        list[tuple[str, str, str, str]]: A list of tuples, each containing the context before, the matched keyword, and the context after.\n    \"\"\"\n    hits = []  # List to store the hits\n    for i, doc in enumerate(docs):\n        matches = matcher(doc)\n        for _, start, end in matches:\n            span = doc[start:end]  # The matched span (keyword)\n            context_start = max(0, start - window)  #  Start of context window\n            context_end = min(len(doc), end + window)  # End of context window\n            context_before = doc[context_start : span.start]\n            context_after = doc[span.end : context_end]  # Fixed indentation\n            hits.append(\n                (labels[i], context_before.text, span.text, context_after.text)\n            )  # Fixed indentation\n    return hits\n</code></pre>"},{"location":"api/kwic/#lexos.kwic.Kwic._validate_sorting_algorithm","title":"<code>_validate_sorting_algorithm() -&gt; bool</code>","text":"<p>Ensure that the specified sorting algorithm is a valid natsort locale.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the sorting algorithm is valid.</p> Source code in <code>lexos/kwic/__init__.py</code> <pre><code>def _validate_sorting_algorithm(self) -&gt; bool:\n    \"\"\"Ensure that the specified sorting algorithm is a valid natsort locale.\n\n    Returns:\n        bool: Whether the sorting algorithm is valid.\n    \"\"\"\n    if self.alg not in [e for e in ns]:\n        locales = \", \".join([f\"ns.{e.name}\" for e in ns])\n        err = (\n            f\"Invalid sorting algorithm: {self.alg}.\",\n            f\"Valid algorithms for `alg` are: {locales}.\",\n            \"See https://natsort.readthedocs.io/en/stable/api.html#natsort.ns.\",\n        )\n        raise LexosException(\" \".join(err))\n    return True\n</code></pre>"},{"location":"api/milestones/","title":"Milestones","text":"<p>Milestones are specified locations in the text that designate structural or sectional divisions. A milestone can be either a designated unit within the text or a placemarker inserted between sections of text. The Lexos <code>milestones</code> module provides methods for identifying milestone locations by searching for patterns you designate. There three modules for different kinds of milestones:</p> <ul> <li><code>string_milestones</code></li> <li><code>span_milestones</code></li> <li><code>token_milestones</code></li> </ul>"},{"location":"api/milestones/span_milestones/","title":"Span Milestones","text":"<p>Class for handling multi-token milestones.</p>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones","title":"<code>SpanMilestones</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Span Milestones class.</p> <ul> <li>Referencing the Milestones instance yields an iterator of the spans in the Doc.</li> <li>Referencing Milestones.spans returns an indexed list of spans in the Doc.</li> <li>Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.</li> </ul> <p>Config:</p> <ul> <li><code>default</code>: <code>validation_config</code></li> </ul> <p>Fields:</p> <ul> <li> <code>doc</code>                 (<code>Doc | Span</code>)             </li> <li> <code>nlp</code>                 (<code>str</code>)             </li> </ul> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>class SpanMilestones(BaseModel):\n    \"\"\"Span Milestones class.\n\n    - Referencing the Milestones instance yields an iterator of the spans in the Doc.\n    - Referencing Milestones.spans returns an indexed list of spans in the Doc.\n    - Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.\n    \"\"\"\n\n    doc: Doc | Span = Field(json_schema_extra={\"description\": \"A spaCy Doc object.\"})\n    nlp: str = Field(\n        default=\"xx_sent_ud_sm\",\n        json_schema_extra={\"description\": \"The language model to use.\"},\n    )\n\n    model_config = validation_config\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n        super().__init__(**data)\n        self.doc.spans[\"milestones\"] = []\n        if not Token.has_extension(\"milestone_iob\"):\n            Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n        if not Token.has_extension(\"milestone_label\"):\n            Token.set_extension(\"milestone_label\", default=\"\", force=True)\n\n    @property\n    def spans(self) -&gt; list[Span]:\n        \"\"\"Return the milestone Spans.\n\n        Returns:\n            list[Span]: A list of spaCy Spans.\n        \"\"\"\n        if \"milestones\" in self.doc.spans:\n            return list(self.doc.spans[\"milestones\"])\n        else:\n            return []\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Make the class iterable.\n\n        Returns:\n            Iterator: A generator containing the object's spans.\n        \"\"\"\n        return (span for span in self.spans)\n\n    def _assign_token_attributes(\n        self, spans: list[Span], max_label_length: int = 20\n    ) -&gt; None:\n        \"\"\"Assign token attributes in the doc based on spans.\n\n        Args:\n            spans (list[Span]): A list of spaCy Spans.\n            max_label_length (int): The maximum number of characters to include in the label.\n        \"\"\"\n        # Early return if no spans\n        if not spans:\n            for token in self.doc:\n                self.doc[token.i]._.milestone_iob = \"O\"\n                self.doc[token.i]._.milestone_label = \"\"\n            return\n\n        # Pre-compute token positions and labels\n        milestone_starts = {span.start: span for span in spans}\n        milestone_ranges = {token.i for span in spans for token in span[1:]}\n\n        # Assign attributes in single pass\n        for token in self.doc:\n            if span := milestone_starts.get(token.i):\n                self.doc[token.i]._.milestone_iob = \"B\"\n                self.doc[\n                    token.i\n                ]._.milestone_label = f\"{span.text:.{max_label_length}}{'...' if len(span.text) &gt; max_label_length else ''}\"\n            elif token.i in milestone_ranges:\n                self.doc[token.i]._.milestone_iob = \"I\"\n                self.doc[token.i]._.milestone_label = \"\"\n            else:\n                self.doc[token.i]._.milestone_iob = \"O\"\n                self.doc[token.i]._.milestone_label = \"\"\n\n    def _get_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n        \"\"\"Get a list of milestone dicts.\n\n        Args:\n            strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n        \"\"\"\n        milestone_dicts = []\n        for span in self.doc.spans[\"milestones\"]:\n            start_char = self.doc[span.start].idx\n            end_char = start_char + len(span.text)\n            chars = self.doc.text[start_char:end_char]\n            if strip_punct:\n                chars = chars.rstrip(punctuation)\n                end_char -= 1\n            milestone_dicts.append(\n                {\n                    \"text\": span.text,\n                    \"characters\": chars,\n                    \"start_token\": span.start,\n                    \"end_token\": span.end,\n                    \"start_char\": start_char,\n                    \"end_char\": end_char,\n                }\n            )\n\n        return milestone_dicts\n\n    def _reset(self) -&gt; None:\n        \"\"\"Reset token attributes.\"\"\"\n        self.doc.spans[\"milestones\"] = []\n        for i, _ in enumerate(self.doc):\n            self.doc[i]._.milestone_iob = \"O\"\n            self.doc[i]._.milestone_label = \"\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones.spans","title":"<code>spans: list[Span]</code>  <code>property</code>","text":"<p>Return the milestone Spans.</p> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans.</p>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    self.doc.spans[\"milestones\"] = []\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    self.doc.spans[\"milestones\"] = []\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones.spans","title":"<code>spans: list[Span]</code>  <code>property</code>","text":"<p>Return the milestone Spans.</p> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans.</p>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones._assign_token_attributes","title":"<code>_assign_token_attributes(spans: list[Span], max_label_length: int = 20) -&gt; None</code>","text":"<p>Assign token attributes in the doc based on spans.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>A list of spaCy Spans.</p> required <code>max_label_length</code> <code>int</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def _assign_token_attributes(\n    self, spans: list[Span], max_label_length: int = 20\n) -&gt; None:\n    \"\"\"Assign token attributes in the doc based on spans.\n\n    Args:\n        spans (list[Span]): A list of spaCy Spans.\n        max_label_length (int): The maximum number of characters to include in the label.\n    \"\"\"\n    # Early return if no spans\n    if not spans:\n        for token in self.doc:\n            self.doc[token.i]._.milestone_iob = \"O\"\n            self.doc[token.i]._.milestone_label = \"\"\n        return\n\n    # Pre-compute token positions and labels\n    milestone_starts = {span.start: span for span in spans}\n    milestone_ranges = {token.i for span in spans for token in span[1:]}\n\n    # Assign attributes in single pass\n    for token in self.doc:\n        if span := milestone_starts.get(token.i):\n            self.doc[token.i]._.milestone_iob = \"B\"\n            self.doc[\n                token.i\n            ]._.milestone_label = f\"{span.text:.{max_label_length}}{'...' if len(span.text) &gt; max_label_length else ''}\"\n        elif token.i in milestone_ranges:\n            self.doc[token.i]._.milestone_iob = \"I\"\n            self.doc[token.i]._.milestone_label = \"\"\n        else:\n            self.doc[token.i]._.milestone_iob = \"O\"\n            self.doc[token.i]._.milestone_label = \"\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones._get_list","title":"<code>_get_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def _get_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n    \"\"\"\n    milestone_dicts = []\n    for span in self.doc.spans[\"milestones\"]:\n        start_char = self.doc[span.start].idx\n        end_char = start_char + len(span.text)\n        chars = self.doc.text[start_char:end_char]\n        if strip_punct:\n            chars = chars.rstrip(punctuation)\n            end_char -= 1\n        milestone_dicts.append(\n            {\n                \"text\": span.text,\n                \"characters\": chars,\n                \"start_token\": span.start,\n                \"end_token\": span.end,\n                \"start_char\": start_char,\n                \"end_char\": end_char,\n            }\n        )\n\n    return milestone_dicts\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SpanMilestones._reset","title":"<code>_reset() -&gt; None</code>","text":"<p>Reset token attributes.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def _reset(self) -&gt; None:\n    \"\"\"Reset token attributes.\"\"\"\n    self.doc.spans[\"milestones\"] = []\n    for i, _ in enumerate(self.doc):\n        self.doc[i]._.milestone_iob = \"O\"\n        self.doc[i]._.milestone_label = \"\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones","title":"<code>SentenceMilestones</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>SpanMilestones</code></p> <p>Sentence Milestones class.</p> <ul> <li>Referencing the Milestones instance yields an iterator of the spans in the Doc.</li> <li>Referencing Milestones.spans returns an indexed list of spans in the Doc.</li> <li>Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.</li> </ul> <p>Fields:</p> <ul> <li> <code>nlp</code>                 (<code>str</code>)             </li> <li> <code>doc</code>                 (<code>Doc | Span</code>)             </li> </ul> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>class SentenceMilestones(SpanMilestones):\n    \"\"\"Sentence Milestones class.\n\n    - Referencing the Milestones instance yields an iterator of the spans in the Doc.\n    - Referencing Milestones.spans returns an indexed list of spans in the Doc.\n    - Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.\n    \"\"\"\n\n    type: ClassVar[str] = \"sentences\"\n\n    doc: Doc | Span = Field(json_schema_extra={\"description\": \"A spaCy Doc object.\"})\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n        super().__init__(**data)\n        if not self.doc.has_annotation(\"SENT_START\"):\n            raise ValueError(\n                \"Either the document's model does not parse sentence boundaries or the sentence boundary component has been disabled in the pipeline.\"\n            )\n        if not Token.has_extension(\"milestone_iob\"):\n            Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n        if not Token.has_extension(\"milestone_label\"):\n            Token.set_extension(\"milestone_label\", default=\"\", force=True)\n\n    def reset(self):\n        \"\"\"Reset all `milestone` values to defaults.\"\"\"\n        self._reset()\n\n    @validate_call(config=validation_config)\n    def set(\n        self, *, step: Optional[int] = 1, max_label_length: Optional[int] = 20\n    ) -&gt; None:\n        \"\"\"Generate spans with n sentences per span.\n\n        Args:\n            step (Optional[int]): The number of sentences to group under a single milestone\n            max_label_length (Optional[int]): The maximum number of characters to include in the label.\n        \"\"\"\n        self.reset()\n        # Apply the step and set new milestone spans\n        sents = list(self.doc.sents)\n        segments = [sents[x : x + step] for x in range(0, len(sents), step)]\n        self.doc.spans[\"milestones\"] = [\n            self.doc[span[0].start : span[-1].end] for span in segments\n        ]\n        # Set the token attributes\n        for span in self.doc.spans[\"milestones\"]:\n            self.doc[span.start]._.milestone_iob = \"B\"\n            self.doc[\n                span.start\n            ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n\n    @validate_call(config=validation_config)\n    def to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n        \"\"\"Get a list of milestone dicts.\n\n        Args:\n            strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n        Returns:\n            list[dict]: A list of milestone dicts.\n        \"\"\"\n        return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.spans","title":"<code>spans: list[Span]</code>  <code>property</code>","text":"<p>Return the milestone Spans.</p> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans.</p>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not self.doc.has_annotation(\"SENT_START\"):\n        raise ValueError(\n            \"Either the document's model does not parse sentence boundaries or the sentence boundary component has been disabled in the pipeline.\"\n        )\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\"\"\"\n    self._reset()\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.set","title":"<code>set(*, step: Optional[int] = 1, max_label_length: Optional[int] = 20) -&gt; None</code>","text":"<p>Generate spans with n sentences per span.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>Optional[int]</code> <p>The number of sentences to group under a single milestone</p> <code>1</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set(\n    self, *, step: Optional[int] = 1, max_label_length: Optional[int] = 20\n) -&gt; None:\n    \"\"\"Generate spans with n sentences per span.\n\n    Args:\n        step (Optional[int]): The number of sentences to group under a single milestone\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n    \"\"\"\n    self.reset()\n    # Apply the step and set new milestone spans\n    sents = list(self.doc.sents)\n    segments = [sents[x : x + step] for x in range(0, len(sents), step)]\n    self.doc.spans[\"milestones\"] = [\n        self.doc[span[0].start : span[-1].end] for span in segments\n    ]\n    # Set the token attributes\n    for span in self.doc.spans[\"milestones\"]:\n        self.doc[span.start]._.milestone_iob = \"B\"\n        self.doc[\n            span.start\n        ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n    \"\"\"\n    return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not self.doc.has_annotation(\"SENT_START\"):\n        raise ValueError(\n            \"Either the document's model does not parse sentence boundaries or the sentence boundary component has been disabled in the pipeline.\"\n        )\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\"\"\"\n    self._reset()\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.set","title":"<code>set(*, step: Optional[int] = 1, max_label_length: Optional[int] = 20) -&gt; None</code>","text":"<p>Generate spans with n sentences per span.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>Optional[int]</code> <p>The number of sentences to group under a single milestone</p> <code>1</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set(\n    self, *, step: Optional[int] = 1, max_label_length: Optional[int] = 20\n) -&gt; None:\n    \"\"\"Generate spans with n sentences per span.\n\n    Args:\n        step (Optional[int]): The number of sentences to group under a single milestone\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n    \"\"\"\n    self.reset()\n    # Apply the step and set new milestone spans\n    sents = list(self.doc.sents)\n    segments = [sents[x : x + step] for x in range(0, len(sents), step)]\n    self.doc.spans[\"milestones\"] = [\n        self.doc[span[0].start : span[-1].end] for span in segments\n    ]\n    # Set the token attributes\n    for span in self.doc.spans[\"milestones\"]:\n        self.doc[span.start]._.milestone_iob = \"B\"\n        self.doc[\n            span.start\n        ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.SentenceMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n    \"\"\"\n    return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones","title":"<code>LineMilestones</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>SpanMilestones</code></p> <p>Line Milestones class.</p> <ul> <li>Referencing the Milestones instance yields an iterator of the spans in the Doc.</li> <li>Referencing Milestones.spans returns an indexed list of spans in the Doc.</li> <li>Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.</li> </ul> <p>Fields:</p> <ul> <li> <code>nlp</code>                 (<code>str</code>)             </li> <li> <code>doc</code>                 (<code>Doc | Span</code>)             </li> </ul> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>class LineMilestones(SpanMilestones):\n    \"\"\"Line Milestones class.\n\n    - Referencing the Milestones instance yields an iterator of the spans in the Doc.\n    - Referencing Milestones.spans returns an indexed list of spans in the Doc.\n    - Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.\n    \"\"\"\n\n    type: ClassVar[str] = \"lines\"\n\n    doc: Doc | Span = Field(json_schema_extra={\"description\": \"A spaCy Doc object.\"})\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n        super().__init__(**data)\n        if not Token.has_extension(\"milestone_iob\"):\n            Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n        if not Token.has_extension(\"milestone_label\"):\n            Token.set_extension(\"milestone_label\", default=\"\", force=True)\n\n    def reset(self):\n        \"\"\"Reset all `milestone` values to defaults.\"\"\"\n        self._reset()\n\n    @validate_call(config=validation_config)\n    def set(\n        self,\n        pattern: Optional[str] = \"\\n\",\n        *,\n        step: Optional[int] = 1,\n        remove_linebreak: Optional[bool] = True,\n        max_label_length: Optional[int] = 20,\n    ) -&gt; list[Span]:\n        \"\"\"Generate spans based on line breaks.\n\n        Args:\n            pattern (Optional[str]): The string or regex pattern to use to identify the milestone.\n            step (Optional[int]): The number of lines to include in the spans. By default, all lines are included.\n            remove_linebreak (Optional[bool]): Whether or not to remove the linebreak character.\n            max_label_length (Optional[int]): The maximum number of characters to include in the label.\n\n        Returns:\n            list[Span]: A list of spaCy spans.\n        \"\"\"\n        self.reset()\n        spans = []\n        start = 0\n        for token in self.doc:\n            if token.text == pattern:\n                if not remove_linebreak:\n                    new_span = self.doc[start : token.i + 1]\n                    spans.append(new_span)\n                    start = token.i + 1\n                else:\n                    spans.append(self.doc[start : token.i])\n                    start = token.i + 1\n        # Append any remaining span\n        if start &lt; len(self.doc):\n            spans.append(self.doc[start:])\n\n        if step:\n            steps = zip_longest(*[iter(spans)] * step, fillvalue=None)\n            new_spans = [\n                self.doc[\n                    min(span.start for span in group if span) : max(\n                        span.end for span in group if span\n                    )\n                ]\n                for group in steps\n            ]\n            group = SpanGroup(self.doc, name=\"milestones\", spans=new_spans)\n            self.doc.spans[\"milestones\"] = group\n        else:\n            self.doc.spans[\"milestones\"] = spans\n\n        # Set the token attributes\n        for span in self.doc.spans[\"milestones\"]:\n            self.doc[span.start]._.milestone_iob = \"B\"\n            self.doc[\n                span.start\n            ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n\n    @validate_call(config=validation_config)\n    def to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n        \"\"\"Get a list of milestone dicts.\n\n        Args:\n            strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n        Returns:\n            list[dict]: A list of milestone dicts.\n        \"\"\"\n        return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.spans","title":"<code>spans: list[Span]</code>  <code>property</code>","text":"<p>Return the milestone Spans.</p> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans.</p>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\"\"\"\n    self._reset()\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.set","title":"<code>set(pattern: Optional[str] = '\\n', *, step: Optional[int] = 1, remove_linebreak: Optional[bool] = True, max_label_length: Optional[int] = 20) -&gt; list[Span]</code>","text":"<p>Generate spans based on line breaks.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>Optional[str]</code> <p>The string or regex pattern to use to identify the milestone.</p> <code>'\\n'</code> <code>step</code> <code>Optional[int]</code> <p>The number of lines to include in the spans. By default, all lines are included.</p> <code>1</code> <code>remove_linebreak</code> <code>Optional[bool]</code> <p>Whether or not to remove the linebreak character.</p> <code>True</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set(\n    self,\n    pattern: Optional[str] = \"\\n\",\n    *,\n    step: Optional[int] = 1,\n    remove_linebreak: Optional[bool] = True,\n    max_label_length: Optional[int] = 20,\n) -&gt; list[Span]:\n    \"\"\"Generate spans based on line breaks.\n\n    Args:\n        pattern (Optional[str]): The string or regex pattern to use to identify the milestone.\n        step (Optional[int]): The number of lines to include in the spans. By default, all lines are included.\n        remove_linebreak (Optional[bool]): Whether or not to remove the linebreak character.\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n\n    Returns:\n        list[Span]: A list of spaCy spans.\n    \"\"\"\n    self.reset()\n    spans = []\n    start = 0\n    for token in self.doc:\n        if token.text == pattern:\n            if not remove_linebreak:\n                new_span = self.doc[start : token.i + 1]\n                spans.append(new_span)\n                start = token.i + 1\n            else:\n                spans.append(self.doc[start : token.i])\n                start = token.i + 1\n    # Append any remaining span\n    if start &lt; len(self.doc):\n        spans.append(self.doc[start:])\n\n    if step:\n        steps = zip_longest(*[iter(spans)] * step, fillvalue=None)\n        new_spans = [\n            self.doc[\n                min(span.start for span in group if span) : max(\n                    span.end for span in group if span\n                )\n            ]\n            for group in steps\n        ]\n        group = SpanGroup(self.doc, name=\"milestones\", spans=new_spans)\n        self.doc.spans[\"milestones\"] = group\n    else:\n        self.doc.spans[\"milestones\"] = spans\n\n    # Set the token attributes\n    for span in self.doc.spans[\"milestones\"]:\n        self.doc[span.start]._.milestone_iob = \"B\"\n        self.doc[\n            span.start\n        ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n    \"\"\"\n    return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\"\"\"\n    self._reset()\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.set","title":"<code>set(pattern: Optional[str] = '\\n', *, step: Optional[int] = 1, remove_linebreak: Optional[bool] = True, max_label_length: Optional[int] = 20) -&gt; list[Span]</code>","text":"<p>Generate spans based on line breaks.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>Optional[str]</code> <p>The string or regex pattern to use to identify the milestone.</p> <code>'\\n'</code> <code>step</code> <code>Optional[int]</code> <p>The number of lines to include in the spans. By default, all lines are included.</p> <code>1</code> <code>remove_linebreak</code> <code>Optional[bool]</code> <p>Whether or not to remove the linebreak character.</p> <code>True</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set(\n    self,\n    pattern: Optional[str] = \"\\n\",\n    *,\n    step: Optional[int] = 1,\n    remove_linebreak: Optional[bool] = True,\n    max_label_length: Optional[int] = 20,\n) -&gt; list[Span]:\n    \"\"\"Generate spans based on line breaks.\n\n    Args:\n        pattern (Optional[str]): The string or regex pattern to use to identify the milestone.\n        step (Optional[int]): The number of lines to include in the spans. By default, all lines are included.\n        remove_linebreak (Optional[bool]): Whether or not to remove the linebreak character.\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n\n    Returns:\n        list[Span]: A list of spaCy spans.\n    \"\"\"\n    self.reset()\n    spans = []\n    start = 0\n    for token in self.doc:\n        if token.text == pattern:\n            if not remove_linebreak:\n                new_span = self.doc[start : token.i + 1]\n                spans.append(new_span)\n                start = token.i + 1\n            else:\n                spans.append(self.doc[start : token.i])\n                start = token.i + 1\n    # Append any remaining span\n    if start &lt; len(self.doc):\n        spans.append(self.doc[start:])\n\n    if step:\n        steps = zip_longest(*[iter(spans)] * step, fillvalue=None)\n        new_spans = [\n            self.doc[\n                min(span.start for span in group if span) : max(\n                    span.end for span in group if span\n                )\n            ]\n            for group in steps\n        ]\n        group = SpanGroup(self.doc, name=\"milestones\", spans=new_spans)\n        self.doc.spans[\"milestones\"] = group\n    else:\n        self.doc.spans[\"milestones\"] = spans\n\n    # Set the token attributes\n    for span in self.doc.spans[\"milestones\"]:\n        self.doc[span.start]._.milestone_iob = \"B\"\n        self.doc[\n            span.start\n        ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.LineMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n    \"\"\"\n    return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones","title":"<code>CustomMilestones</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>SpanMilestones</code></p> <p>Custom Milestones class.</p> <ul> <li>Referencing the Milestones instance yields an iterator of the spans in the Doc.</li> <li>Referencing Milestones.spans returns an indexed list of spans in the Doc.</li> <li>Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.</li> </ul> <p>Fields:</p> <ul> <li> <code>nlp</code>                 (<code>str</code>)             </li> <li> <code>doc</code>                 (<code>Doc | Span</code>)             </li> </ul> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>class CustomMilestones(SpanMilestones):\n    \"\"\"Custom Milestones class.\n\n    - Referencing the Milestones instance yields an iterator of the spans in the Doc.\n    - Referencing Milestones.spans returns an indexed list of spans in the Doc.\n    - Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.\n    \"\"\"\n\n    type: ClassVar[str] = \"custom\"\n\n    doc: Doc | Span = Field(json_schema_extra={\"description\": \"A spaCy Doc object.\"})\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n        super().__init__(**data)\n        if not Token.has_extension(\"milestone_iob\"):\n            Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n        if not Token.has_extension(\"milestone_label\"):\n            Token.set_extension(\"milestone_label\", default=\"\", force=True)\n\n    def reset(self):\n        \"\"\"Reset all `milestone` values to defaults.\"\"\"\n        self._reset()\n\n    @validate_call(config=validation_config)\n    def set(\n        self,\n        spans: list[Span],\n        *,\n        step: Optional[int] = 1,\n        max_label_length: Optional[int] = 20,\n    ) -&gt; list[Span]:\n        \"\"\"Generate spans based on a custom list.\n\n        Args:\n            spans (list[Span]): A list of spaCy spans.\n            step (Optional[int]): The number of spans to group into each milestone span. By default, all spans are included.\n            max_label_length (Optional[int]): The maximum number of characters to include in the label.\n\n        Returns:\n            list[Span]: A list of spaCy spans.\n        \"\"\"\n        self.reset()\n\n        if step:\n            segments = [\n                (spans[i].start, spans[min(i + step, len(spans)) - 1].end)\n                for i in range(0, len(spans), step)\n            ]\n            # Use the segment start and end indexes to generate new spans\n            self.doc.spans[\"milestones\"] = [\n                self.doc[start:end] for start, end in segments\n            ]\n        else:\n            self.doc.spans[\"milestones\"] = spans\n\n        # Set the token attributes\n        for span in self.doc.spans[\"milestones\"]:\n            self.doc[span.start]._.milestone_iob = \"B\"\n            self.doc[\n                span.start\n            ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n\n    @validate_call(config=validation_config)\n    def to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n        \"\"\"Get a list of milestone dicts.\n\n        Args:\n            strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n        Returns:\n            list[dict]: A list of milestone dicts.\n        \"\"\"\n        return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.spans","title":"<code>spans: list[Span]</code>  <code>property</code>","text":"<p>Return the milestone Spans.</p> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans.</p>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\"\"\"\n    self._reset()\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.set","title":"<code>set(spans: list[Span], *, step: Optional[int] = 1, max_label_length: Optional[int] = 20) -&gt; list[Span]</code>","text":"<p>Generate spans based on a custom list.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>A list of spaCy spans.</p> required <code>step</code> <code>Optional[int]</code> <p>The number of spans to group into each milestone span. By default, all spans are included.</p> <code>1</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set(\n    self,\n    spans: list[Span],\n    *,\n    step: Optional[int] = 1,\n    max_label_length: Optional[int] = 20,\n) -&gt; list[Span]:\n    \"\"\"Generate spans based on a custom list.\n\n    Args:\n        spans (list[Span]): A list of spaCy spans.\n        step (Optional[int]): The number of spans to group into each milestone span. By default, all spans are included.\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n\n    Returns:\n        list[Span]: A list of spaCy spans.\n    \"\"\"\n    self.reset()\n\n    if step:\n        segments = [\n            (spans[i].start, spans[min(i + step, len(spans)) - 1].end)\n            for i in range(0, len(spans), step)\n        ]\n        # Use the segment start and end indexes to generate new spans\n        self.doc.spans[\"milestones\"] = [\n            self.doc[start:end] for start, end in segments\n        ]\n    else:\n        self.doc.spans[\"milestones\"] = spans\n\n    # Set the token attributes\n    for span in self.doc.spans[\"milestones\"]:\n        self.doc[span.start]._.milestone_iob = \"B\"\n        self.doc[\n            span.start\n        ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n    \"\"\"\n    return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\"\"\"\n    self._reset()\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.set","title":"<code>set(spans: list[Span], *, step: Optional[int] = 1, max_label_length: Optional[int] = 20) -&gt; list[Span]</code>","text":"<p>Generate spans based on a custom list.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>A list of spaCy spans.</p> required <code>step</code> <code>Optional[int]</code> <p>The number of spans to group into each milestone span. By default, all spans are included.</p> <code>1</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy spans.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set(\n    self,\n    spans: list[Span],\n    *,\n    step: Optional[int] = 1,\n    max_label_length: Optional[int] = 20,\n) -&gt; list[Span]:\n    \"\"\"Generate spans based on a custom list.\n\n    Args:\n        spans (list[Span]): A list of spaCy spans.\n        step (Optional[int]): The number of spans to group into each milestone span. By default, all spans are included.\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n\n    Returns:\n        list[Span]: A list of spaCy spans.\n    \"\"\"\n    self.reset()\n\n    if step:\n        segments = [\n            (spans[i].start, spans[min(i + step, len(spans)) - 1].end)\n            for i in range(0, len(spans), step)\n        ]\n        # Use the segment start and end indexes to generate new spans\n        self.doc.spans[\"milestones\"] = [\n            self.doc[start:end] for start, end in segments\n        ]\n    else:\n        self.doc.spans[\"milestones\"] = spans\n\n    # Set the token attributes\n    for span in self.doc.spans[\"milestones\"]:\n        self.doc[span.start]._.milestone_iob = \"B\"\n        self.doc[\n            span.start\n        ]._.milestone_label = f\"{span.text[:max_label_length]}{'...' if len(span.text) &gt; max_label_length else ''}\"\n</code></pre>"},{"location":"api/milestones/span_milestones/#lexos.milestones.span_milestones.CustomMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Source code in <code>lexos/milestones/span_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n    \"\"\"\n    return self._get_list(strip_punct=strip_punct)\n</code></pre>"},{"location":"api/milestones/string_milestones/","title":"String Milestones","text":"<p>Class for handling text string milestones.</p>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones","title":"<code>StringMilestones</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>String Milestones class.</p> <p>Milestones object for text strings or spaCy Doc objects to be treated as strings.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>doc</code>                 (<code>Doclike</code>)             </li> <li> <code>patterns</code>                 (<code>str | list[str]</code>)             </li> <li> <code>case_sensitive</code>                 (<code>bool</code>)             </li> <li> <code>flags</code>                 (<code>Enum</code>)             </li> <li> <code>_spans</code>                 (<code>list</code>)             </li> </ul> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>class StringMilestones(BaseModel):\n    \"\"\"String Milestones class.\n\n    Milestones object for text strings or spaCy Doc objects to\n    be treated as strings.\n    \"\"\"\n\n    doc: Doclike = Field(\n        json_schema_extra={\"description\": \"A string or spaCy Doc object.\"}\n    )\n\n    patterns: str | list[str] = Field(\n        default=None,\n        json_schema_extra={\"description\": \"The pattern(s) used to match milestones.\"},\n    )\n    case_sensitive: bool = Field(\n        default=True,\n        json_schema_extra={\n            \"description\": \"Whether to perform case-sensitive searches.\"\n        },\n    )\n    flags: Enum = Field(\n        default=case_sensitive_flags,\n        json_schema_extra={\"description\": \"The regex flags to use.\"},\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n        super().__init__(**data)\n        self._spans: list = []\n        if not self.case_sensitive:\n            self.flags = case_insensitive_flags\n        self.patterns = ensure_list(self.patterns)\n        if self.patterns != [None]:\n            self.set()\n\n    @property\n    def spans(self) -&gt; list[StringSpan]:\n        \"\"\"Return the Spans.\n\n        Returns:\n            list[StringSpan]: A list of StringSpans.\n        \"\"\"\n        return self._spans or []\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Make the class iterable.\n\n        Returns:\n            Iterator: A generator containing the object's spans.\n        \"\"\"\n        return (span for span in self.spans)\n\n    def _set_case_sensitivity(self, case_sensitive: Optional[bool] = None) -&gt; None:\n        \"\"\"Set the object's case sensitivity.\n\n        Args:\n            case_sensitive (optional, bool): Whether or not to use case-sensitive searching.\n        \"\"\"\n        if case_sensitive is not None:\n            self.case_sensitive = case_sensitive\n        if self.case_sensitive is True:\n            self.flags = case_sensitive_flags\n        else:\n            self.flags = case_insensitive_flags\n\n    @validate_call()\n    def set(\n        self,\n        patterns: Optional[str | list[str]] = None,\n        case_sensitive: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"Return the milestones.\n\n        Args:\n            patterns (Optional[str | list[str]]): The pattern(s) used to match milestones.\n            case_sensitive (bool, optional): Whether to perform case-sensitive searches. Defaults to True.\n\n        Note:\n            If no parameters are set, the method will use the object's current patterns and case sensitivity.\n        \"\"\"\n        if patterns:\n            self.patterns = ensure_list(patterns)\n        self._set_case_sensitivity(case_sensitive)\n        text = self.doc if isinstance(self.doc, str) else self.doc.text\n        all_matches = []\n        for pattern in self.patterns:\n            matches = re.finditer(pattern, text, self.flags)\n            all_matches.extend(\n                StringSpan(text=match.group(), start=match.start(), end=match.end())\n                for match in matches\n            )\n        all_matches.sort(key=lambda match: match.start)\n        self._spans = all_matches\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.spans","title":"<code>spans: list[StringSpan]</code>  <code>property</code>","text":"<p>Return the Spans.</p> <p>Returns:</p> Type Description <code>list[StringSpan]</code> <p>list[StringSpan]: A list of StringSpans.</p>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    self._spans: list = []\n    if not self.case_sensitive:\n        self.flags = case_insensitive_flags\n    self.patterns = ensure_list(self.patterns)\n    if self.patterns != [None]:\n        self.set()\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.set","title":"<code>set(patterns: Optional[str | list[str]] = None, case_sensitive: Optional[bool] = None) -&gt; None</code>","text":"<p>Return the milestones.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Optional[str | list[str]]</code> <p>The pattern(s) used to match milestones.</p> <code>None</code> <code>case_sensitive</code> <code>bool</code> <p>Whether to perform case-sensitive searches. Defaults to True.</p> <code>None</code> Note <p>If no parameters are set, the method will use the object's current patterns and case sensitivity.</p> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>@validate_call()\ndef set(\n    self,\n    patterns: Optional[str | list[str]] = None,\n    case_sensitive: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Return the milestones.\n\n    Args:\n        patterns (Optional[str | list[str]]): The pattern(s) used to match milestones.\n        case_sensitive (bool, optional): Whether to perform case-sensitive searches. Defaults to True.\n\n    Note:\n        If no parameters are set, the method will use the object's current patterns and case sensitivity.\n    \"\"\"\n    if patterns:\n        self.patterns = ensure_list(patterns)\n    self._set_case_sensitivity(case_sensitive)\n    text = self.doc if isinstance(self.doc, str) else self.doc.text\n    all_matches = []\n    for pattern in self.patterns:\n        matches = re.finditer(pattern, text, self.flags)\n        all_matches.extend(\n            StringSpan(text=match.group(), start=match.start(), end=match.end())\n            for match in matches\n        )\n    all_matches.sort(key=lambda match: match.start)\n    self._spans = all_matches\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    self._spans: list = []\n    if not self.case_sensitive:\n        self.flags = case_insensitive_flags\n    self.patterns = ensure_list(self.patterns)\n    if self.patterns != [None]:\n        self.set()\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.spans","title":"<code>spans: list[StringSpan]</code>  <code>property</code>","text":"<p>Return the Spans.</p> <p>Returns:</p> Type Description <code>list[StringSpan]</code> <p>list[StringSpan]: A list of StringSpans.</p>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones._set_case_sensitivity","title":"<code>_set_case_sensitivity(case_sensitive: Optional[bool] = None) -&gt; None</code>","text":"<p>Set the object's case sensitivity.</p> <p>Parameters:</p> Name Type Description Default <code>case_sensitive</code> <code>(optional, bool)</code> <p>Whether or not to use case-sensitive searching.</p> <code>None</code> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>def _set_case_sensitivity(self, case_sensitive: Optional[bool] = None) -&gt; None:\n    \"\"\"Set the object's case sensitivity.\n\n    Args:\n        case_sensitive (optional, bool): Whether or not to use case-sensitive searching.\n    \"\"\"\n    if case_sensitive is not None:\n        self.case_sensitive = case_sensitive\n    if self.case_sensitive is True:\n        self.flags = case_sensitive_flags\n    else:\n        self.flags = case_insensitive_flags\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringMilestones.set","title":"<code>set(patterns: Optional[str | list[str]] = None, case_sensitive: Optional[bool] = None) -&gt; None</code>","text":"<p>Return the milestones.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Optional[str | list[str]]</code> <p>The pattern(s) used to match milestones.</p> <code>None</code> <code>case_sensitive</code> <code>bool</code> <p>Whether to perform case-sensitive searches. Defaults to True.</p> <code>None</code> Note <p>If no parameters are set, the method will use the object's current patterns and case sensitivity.</p> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>@validate_call()\ndef set(\n    self,\n    patterns: Optional[str | list[str]] = None,\n    case_sensitive: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Return the milestones.\n\n    Args:\n        patterns (Optional[str | list[str]]): The pattern(s) used to match milestones.\n        case_sensitive (bool, optional): Whether to perform case-sensitive searches. Defaults to True.\n\n    Note:\n        If no parameters are set, the method will use the object's current patterns and case sensitivity.\n    \"\"\"\n    if patterns:\n        self.patterns = ensure_list(patterns)\n    self._set_case_sensitivity(case_sensitive)\n    text = self.doc if isinstance(self.doc, str) else self.doc.text\n    all_matches = []\n    for pattern in self.patterns:\n        matches = re.finditer(pattern, text, self.flags)\n        all_matches.extend(\n            StringSpan(text=match.group(), start=match.start(), end=match.end())\n            for match in matches\n        )\n    all_matches.sort(key=lambda match: match.start)\n    self._spans = all_matches\n</code></pre>"},{"location":"api/milestones/string_milestones/#lexos.milestones.string_milestones.StringSpan","title":"<code>StringSpan</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>StringSpan class.</p> <p>A Pydantic model containing the milestone text, and the start and character indices of the milestone in the original text.</p> <p>Fields:</p> <ul> <li> <code>text</code>                 (<code>str</code>)             </li> <li> <code>start</code>                 (<code>int</code>)             </li> <li> <code>end</code>                 (<code>int</code>)             </li> </ul> Source code in <code>lexos/milestones/string_milestones.py</code> <pre><code>class StringSpan(BaseModel):\n    \"\"\"StringSpan class.\n\n    A Pydantic model containing the milestone text, and the start\n    and character indices of the milestone in the original text.\n    \"\"\"\n\n    text: str\n    start: int\n    end: int\n</code></pre>"},{"location":"api/milestones/token_milestones/","title":"Token Milestones","text":"<p>Class for handling token milestones.</p>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones","title":"<code>TokenMilestones</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Milestones class.</p> <ul> <li>Referencing the Milestones instance yields an iterator of the spans in the Doc.</li> <li>Referencing Milestones.spans returns an indexed list of spans in the Doc.</li> <li>Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.</li> </ul> <p>Config:</p> <ul> <li><code>default</code>: <code>validation_config</code></li> </ul> <p>Fields:</p> <ul> <li> <code>doc</code>                 (<code>Doclike</code>)             </li> <li> <code>patterns</code>                 (<code>Optional[Any]</code>)             </li> <li> <code>case_sensitive</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>character_map</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>attr</code>                 (<code>Optional[str]</code>)             </li> <li> <code>flags</code>                 (<code>Optional[Enum]</code>)             </li> <li> <code>mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>nlp</code>                 (<code>Optional[str]</code>)             </li> <li> <code>type</code>                 (<code>Optional[str]</code>)             </li> </ul> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>class TokenMilestones(BaseModel):\n    \"\"\"Milestones class.\n\n    - Referencing the Milestones instance yields an iterator of the spans in the Doc.\n    - Referencing Milestones.spans returns an indexed list of spans in the Doc.\n    - Referencing milestones.doc.spans[\"milestones\"] returns a SpanGroup.\n    \"\"\"\n\n    doc: Doclike = Field(\n        json_schema_extra={\"description\": \"A string or spaCy Doc object.\"}\n    )\n\n    patterns: Optional[Any] = Field(\n        default=None,\n        json_schema_extra={\"description\": \"The pattern(s) used to match milestones.\"},\n    )\n    case_sensitive: Optional[bool] = Field(\n        default=True,\n        json_schema_extra={\n            \"description\": \"Whether to perform case-sensitive searches.\"\n        },\n    )\n    character_map: Optional[dict] = Field(\n        default=None,\n        json_schema_extra={\"description\": \"A map of characters to token indexes.\"},\n    )\n    attr: Optional[str] = Field(\n        default=\"ORTH\",\n        json_schema_extra={\n            \"description\": \"The spaCy token attribute to search ('ORTH' or 'LOWER').\"\n        },\n    )\n    flags: Optional[Enum] = Field(\n        default=case_sensitive_flags,\n        json_schema_extra={\"description\": \"The regex flags to use.\"},\n    )\n    mode: Optional[str] = Field(\n        default=\"string\",\n        json_schema_extra={\"description\": \"The mode used to match patterns.\"},\n    )\n    nlp: Optional[str] = Field(\n        default=\"xx_sent_ud_sm\",\n        json_schema_extra={\"description\": \"The language model to use.\"},\n    )\n    type: Optional[str] = Field(\n        default=None, json_schema_extra={\"description\": \"The type of milestone.\"}\n    )\n\n    model_config = validation_config\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n        super().__init__(**data)\n        if not self.case_sensitive:\n            self.flags = case_insensitive_flags\n            self.attr = \"LOWER\"\n        if not Token.has_extension(\"milestone_iob\"):\n            Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n        if not Token.has_extension(\"milestone_label\"):\n            Token.set_extension(\"milestone_label\", default=\"\", force=True)\n\n    @property\n    def spans(self) -&gt; list[Span]:\n        \"\"\"Return the Spans.\n\n        Returns:\n            list[Span]: A list of spaCy Spans.\n        \"\"\"\n        if \"milestones\" in self.doc.spans:\n            return list(self.doc.spans[\"milestones\"])\n        else:\n            return []\n\n    def __iter__(self) -&gt; Iterator:\n        \"\"\"Make the class iterable.\n\n        Returns:\n            Iterator: A generator containing the object's spans.\n        \"\"\"\n        return (span for span in self.spans)\n\n    def _assign_token_attributes(\n        self, spans: list[Span], max_label_length: int = 20\n    ) -&gt; None:\n        \"\"\"Assign token attributes in the doc based on spans.\n\n        Args:\n            spans (list[Span]): A list of spaCy Spans.\n            max_label_length (int): The maximum number of characters to include in the label.\n        \"\"\"\n        # Early return if no spans\n        if not spans:\n            for token in self.doc:\n                self.doc[token.i]._.milestone_iob = \"O\"\n                self.doc[token.i]._.milestone_label = \"\"\n            return\n\n        # Pre-compute token positions and labels\n        milestone_starts = {span.start: span for span in spans}\n        milestone_ranges = {token.i for span in spans for token in span[1:]}\n\n        # Assign attributes in single pass\n        for token in self.doc:\n            if span := milestone_starts.get(token.i):\n                self.doc[token.i]._.milestone_iob = \"B\"\n                self.doc[\n                    token.i\n                ]._.milestone_label = f\"{span.text:.{max_label_length}}{'...' if len(span.text) &gt; max_label_length else ''}\"\n            elif token.i in milestone_ranges:\n                self.doc[token.i]._.milestone_iob = \"I\"\n                self.doc[token.i]._.milestone_label = \"\"\n            else:\n                self.doc[token.i]._.milestone_iob = \"O\"\n                self.doc[token.i]._.milestone_label = \"\"\n\n    def _autodetect_mode(self, patterns: str | list) -&gt; str:\n        \"\"\"Autodetect mode for matching milestones if not supplied (experimental).\n\n        Args:\n            patterns (str | list): A pattern to match.\n\n        Returns:\n            str: A string to supply to the get_matches() mode argument.\n        \"\"\"\n        for pattern in patterns:\n            if not isinstance(pattern, (str, list)):\n                raise ValueError(\n                    f\"Pattern {pattern} must be a string or a spaCy Matcher rule.\"\n                )\n            if isinstance(pattern, str):\n                if re.search(r\"\\s\", pattern):\n                    self.mode = \"phrase\"\n                else:\n                    self.mode = \"string\"\n            else:\n                try:\n                    matcher = Matcher(self.doc.vocab, validate=True)\n                    matcher.add(\"Pattern\", [pattern])\n                    self.mode = \"rule\"\n                # Raise an error if the pattern is not a valid Matcher pattern\n                except BaseException:\n                    raise BaseException(\n                        f\"The pattern `{pattern}` could not be matched automatically. Check that the pattern is correct and try setting the `mode` argument in `get_matches()`.\"\n                    )\n        return self.mode\n\n    def _get_string_matches(self, patterns: Any, flags: Enum) -&gt; list[Span]:\n        \"\"\"Get matches to milestone patterns.\n\n        Args:\n            patterns (Any): A pattern to match.\n            flags (Enum): An enum of regex flags.\n\n        Returns:\n            list[Span]: A list of Spans matching the pattern.\n        \"\"\"\n        if patterns is None or patterns == []:\n            raise ValueError(\"Patterns cannot be empty\")\n        patterns = ensure_list(patterns)\n        if self.character_map is None:\n            self.character_map = chars_to_tokens(self.doc)\n        pattern_matches = []\n        for pattern in patterns:\n            matches = re.finditer(pattern, self.doc.text, flags=flags)\n            for match in matches:\n                pattern_matches.append(match)\n        return [self._to_spacy_span(match) for match in pattern_matches]\n\n    def _get_phrase_matches(self, patterns: Any, attr: str = \"ORTH\") -&gt; list[Span]:\n        \"\"\"Get matches to milestone patterns in phrases.\n\n        Args:\n            patterns (Any): A pattern to match.\n            attr (str): A spaCy Token attribute to search.\n\n        Returns:\n            list[Span]: A list of Spans matching the pattern.\n        \"\"\"\n        nlp = spacy.load(self.nlp)\n        matcher = PhraseMatcher(self.doc.vocab, attr=attr)\n        patterns = [nlp.make_doc(text) for text in patterns]\n        matcher.add(\"PatternList\", patterns)\n        matches = matcher(self.doc)\n        return [self.doc[start:end] for _, start, end in matches]\n\n    def _get_rule_matches(self, patterns: Any) -&gt; list[Span]:\n        \"\"\"Get matches to milestone patterns with spaCy rules.\n\n        Args:\n            patterns (Any): A pattern to match.\n\n        Returns:\n            list[Span]: A list of Spans matching the pattern.\n        \"\"\"\n        nlp = spacy.load(self.nlp)\n        spans = []\n        if not self.case_sensitive:\n            patterns = lowercase_spacy_rules(patterns)\n        for pattern in patterns:\n            matcher = Matcher(nlp.vocab, validate=True)\n            matcher.add(\"Pattern\", [pattern])\n            matches = matcher(self.doc)\n            spans.extend([self.doc[start:end] for _, start, end in matches])\n        return spans\n\n    def _remove_duplicate_spans(self, spans: list[Span]) -&gt; list[Span]:\n        \"\"\"Remove duplicate spans, generally created when a pattern is added.\n\n        Args:\n            spans (list[Span]): A list of Spans.\n\n        Returns:\n            list[Span]: A list of de-duplicated Spans.\n        \"\"\"\n        result = []\n        seen = set()\n        for span in spans:\n            key = (span.start, span.end)\n            if key not in seen:\n                result.append(span)\n                seen.add(key)\n        return result\n\n    def _set_case_sensitivity(self, case_sensitive: Optional[bool] = None) -&gt; None:\n        \"\"\"Set the object's case sensitivity.\n\n        Args:\n            case_sensitive (optional, bool): Whether or not to use case-sensitive searching.\n        \"\"\"\n        if case_sensitive is not None:\n            self.case_sensitive = case_sensitive\n        if self.case_sensitive is True:\n            self.flags: Enum = re.DOTALL | re.MULTILINE | re.UNICODE\n            self.attr = \"ORTH\"\n        else:\n            self.flags: Enum = re.DOTALL | re.IGNORECASE | re.MULTILINE | re.UNICODE\n            self.attr = \"LOWER\"\n\n    def _to_spacy_span(self, match: Match) -&gt; Span:\n        \"\"\"Convert a re.match object to a Span.\n\n        Args:\n            match (Match): A re.match object.\n\n        Returns:\n            Span: A spaCy Span.\n\n        Raises:\n            ValueError: If match is None or span cannot be created.\n        \"\"\"\n        if not match:\n            raise ValueError(\"Match object is None.\")\n\n        # Lazy load character map\n        if not self.character_map:\n            self.character_map = chars_to_tokens(self.doc)\n\n        # Get character positions\n        start_char, end_char = match.span()\n\n        # Try direct char_span first\n        if span := self.doc.char_span(start_char, end_char):\n            return span\n\n        # Fallback to character map\n        start_token = self.character_map.get(start_char)\n        end_token = self.character_map.get(end_char)\n\n        if start_token is not None and end_token is not None:\n            if span := self.doc[start_token : end_token + 1]:\n                return span\n\n        raise ValueError(\n            f\"Could not create span for match at positions {start_char}:{end_char}\"\n        )\n\n    @validate_call(config=validation_config)\n    def get_matches(\n        self,\n        patterns: Optional[Any] = None,\n        mode: Optional[str] = None,\n        case_sensitive: Optional[bool] = None,\n    ) -&gt; list[Span]:\n        \"\"\"Get matches to milestone patterns.\n\n        Args:\n            patterns (Optional[Any]): The pattern(s) to match.\n            mode (Optional[str]): The mode to use for matching ('string', 'phrase', 'rule').\n            case_sensitive (Optional[bool]): Whether to use case sensitive matching. Defaults to True.\n\n        Returns:\n            list[Span]: A list of spaCy Spans matching the pattern.\n\n        Raises:\n            ValueError: If patterns is None or empty.\n        \"\"\"\n        self._set_case_sensitivity(case_sensitive)\n\n        # Update patterns list\n        if patterns:\n            self.patterns = ensure_list(patterns)\n\n        # Define mode handlers\n        mode_handlers = {\n            \"string\": lambda: self._get_string_matches(patterns, self.flags),\n            \"phrase\": lambda: self._get_phrase_matches(patterns, self.attr),\n            \"rule\": lambda: self._get_rule_matches(patterns),\n        }\n\n        # If mode not provided or invalid, autodetect\n        if not mode or mode not in mode_handlers:\n            spans = self.get_matches(patterns, mode=self._autodetect_mode(patterns))\n        # Get spans using appropriate handler\n        else:\n            spans = mode_handlers[mode]()\n        return self._remove_duplicate_spans(spans)\n\n    @validate_call(config=validation_config)\n    def remove(self, patterns: Any, *, mode: Optional[str] = \"string\") -&gt; None:\n        \"\"\"Remove patterns.\n\n        Args:\n            patterns (Any): The pattern(s) to match.\n            mode (Optional[str]): The mode to use for matching.\n        \"\"\"\n        patterns = ensure_list(patterns)\n        spans = self.get_matches(patterns, mode=mode)\n\n        # Create a set of spans to remove for faster lookup\n        remove_spans = {f\"{span.start},{span.end}\" for span in spans}\n\n        # Filter out the spans to be removed\n        new_spans = [\n            span\n            for span in self.doc.spans[\"milestones\"]\n            if f\"{span.start},{span.end}\" not in remove_spans\n        ]\n\n        # Reset the token attributes for the spans to be removed\n        for span in spans:\n            for token in self.doc[span.start : span.end]:\n                token._.milestone_iob = \"O\"\n                token._.milestone_label = \"\"\n\n        # Re-set the milestones with the remaining spans\n        self.set_milestones(new_spans)\n\n        # Remove the patterns from the object's patterns list\n        self.patterns = [p for p in self.patterns if p not in patterns]\n\n    def reset(self):\n        \"\"\"Reset all `milestone` values to defaults.\n\n        Note: Does not modify patterns or any other settings.\n        \"\"\"\n        self.doc.spans[\"milestones\"] = []\n        for i, _ in enumerate(self.doc):\n            self.doc[i]._.milestone_iob = \"O\"\n            self.doc[i]._.milestone_label = \"\"\n\n    @validate_call(config=validation_config)\n    def set_milestones(\n        self,\n        spans: list[Span],\n        *,\n        start: Optional[str | None] = None,\n        remove: Optional[bool] = False,\n        max_label_length: Optional[int] = 20,\n    ) -&gt; None:\n        \"\"\"Commit milestones to the object instance.\n\n        Args:\n            spans (list[Span]): The span(s) to use for identifying token attributes.\n            start (Optional[str | None]): Set milestone start to the token before or after the milestone span. May be \"before\" or \"after\".\n            remove (Optional[bool]): Set milestone start to the token following the milestone span and\n                remove the milestone span tokens from the Doc.\n            max_label_length (Optional[int]): The maximum number of characters to include in the label.\n        \"\"\"\n        if start not in [None, \"before\", \"after\"]:\n            raise ValueError(\"Start must be None, 'before', or 'after'.\")\n        if remove:\n            self.doc = filter_doc(self.doc, spans)\n        elif start is not None:\n            # Update the doc's milestones\n            self.doc.spans[\"milestones\"] = move_milestone(self.doc, spans, start)\n        else:\n            self.doc.spans[\"milestones\"] = spans\n            self._assign_token_attributes(\n                self.doc.spans[\"milestones\"], max_label_length\n            )\n        self.type = \"tokens\"\n\n    @validate_call(config=validation_config)\n    def to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n        \"\"\"Get a list of milestone dicts.\n\n        Args:\n            strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n        Returns:\n            list[dict]: A list of milestone dicts.\n\n        Note:\n            Some language models include a final punctuation mark in the token string,\n            particularly at the end of a sentence. The strip_punct argument is a\n            somewhat hacky convenience method to remove it. However, the user may wish\n            instead to do some post-processing in order to use the output for their\n            own purposes.\n        \"\"\"\n        milestone_dicts = []\n        for span in self.doc.spans[\"milestones\"]:\n            start_char = self.doc[span.start].idx\n            end_char = start_char + len(span.text)\n            chars = self.doc.text[start_char:end_char]\n            if strip_punct:\n                chars = chars.rstrip(punctuation)\n                end_char -= 1\n            milestone_dicts.append(\n                {\n                    \"text\": span.text,\n                    \"characters\": chars,\n                    \"start_token\": span.start,\n                    \"end_token\": span.end,\n                    \"start_char\": start_char,\n                    \"end_char\": end_char,\n                }\n            )\n\n        return milestone_dicts\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.spans","title":"<code>spans: list[Span]</code>  <code>property</code>","text":"<p>Return the Spans.</p> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans.</p>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not self.case_sensitive:\n        self.flags = case_insensitive_flags\n        self.attr = \"LOWER\"\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.get_matches","title":"<code>get_matches(patterns: Optional[Any] = None, mode: Optional[str] = None, case_sensitive: Optional[bool] = None) -&gt; list[Span]</code>","text":"<p>Get matches to milestone patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Optional[Any]</code> <p>The pattern(s) to match.</p> <code>None</code> <code>mode</code> <code>Optional[str]</code> <p>The mode to use for matching ('string', 'phrase', 'rule').</p> <code>None</code> <code>case_sensitive</code> <code>Optional[bool]</code> <p>Whether to use case sensitive matching. Defaults to True.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans matching the pattern.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If patterns is None or empty.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef get_matches(\n    self,\n    patterns: Optional[Any] = None,\n    mode: Optional[str] = None,\n    case_sensitive: Optional[bool] = None,\n) -&gt; list[Span]:\n    \"\"\"Get matches to milestone patterns.\n\n    Args:\n        patterns (Optional[Any]): The pattern(s) to match.\n        mode (Optional[str]): The mode to use for matching ('string', 'phrase', 'rule').\n        case_sensitive (Optional[bool]): Whether to use case sensitive matching. Defaults to True.\n\n    Returns:\n        list[Span]: A list of spaCy Spans matching the pattern.\n\n    Raises:\n        ValueError: If patterns is None or empty.\n    \"\"\"\n    self._set_case_sensitivity(case_sensitive)\n\n    # Update patterns list\n    if patterns:\n        self.patterns = ensure_list(patterns)\n\n    # Define mode handlers\n    mode_handlers = {\n        \"string\": lambda: self._get_string_matches(patterns, self.flags),\n        \"phrase\": lambda: self._get_phrase_matches(patterns, self.attr),\n        \"rule\": lambda: self._get_rule_matches(patterns),\n    }\n\n    # If mode not provided or invalid, autodetect\n    if not mode or mode not in mode_handlers:\n        spans = self.get_matches(patterns, mode=self._autodetect_mode(patterns))\n    # Get spans using appropriate handler\n    else:\n        spans = mode_handlers[mode]()\n    return self._remove_duplicate_spans(spans)\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.remove","title":"<code>remove(patterns: Any, *, mode: Optional[str] = 'string') -&gt; None</code>","text":"<p>Remove patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Any</code> <p>The pattern(s) to match.</p> required <code>mode</code> <code>Optional[str]</code> <p>The mode to use for matching.</p> <code>'string'</code> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef remove(self, patterns: Any, *, mode: Optional[str] = \"string\") -&gt; None:\n    \"\"\"Remove patterns.\n\n    Args:\n        patterns (Any): The pattern(s) to match.\n        mode (Optional[str]): The mode to use for matching.\n    \"\"\"\n    patterns = ensure_list(patterns)\n    spans = self.get_matches(patterns, mode=mode)\n\n    # Create a set of spans to remove for faster lookup\n    remove_spans = {f\"{span.start},{span.end}\" for span in spans}\n\n    # Filter out the spans to be removed\n    new_spans = [\n        span\n        for span in self.doc.spans[\"milestones\"]\n        if f\"{span.start},{span.end}\" not in remove_spans\n    ]\n\n    # Reset the token attributes for the spans to be removed\n    for span in spans:\n        for token in self.doc[span.start : span.end]:\n            token._.milestone_iob = \"O\"\n            token._.milestone_label = \"\"\n\n    # Re-set the milestones with the remaining spans\n    self.set_milestones(new_spans)\n\n    # Remove the patterns from the object's patterns list\n    self.patterns = [p for p in self.patterns if p not in patterns]\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> <p>Note: Does not modify patterns or any other settings.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\n\n    Note: Does not modify patterns or any other settings.\n    \"\"\"\n    self.doc.spans[\"milestones\"] = []\n    for i, _ in enumerate(self.doc):\n        self.doc[i]._.milestone_iob = \"O\"\n        self.doc[i]._.milestone_label = \"\"\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.set_milestones","title":"<code>set_milestones(spans: list[Span], *, start: Optional[str | None] = None, remove: Optional[bool] = False, max_label_length: Optional[int] = 20) -&gt; None</code>","text":"<p>Commit milestones to the object instance.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>The span(s) to use for identifying token attributes.</p> required <code>start</code> <code>Optional[str | None]</code> <p>Set milestone start to the token before or after the milestone span. May be \"before\" or \"after\".</p> <code>None</code> <code>remove</code> <code>Optional[bool]</code> <p>Set milestone start to the token following the milestone span and remove the milestone span tokens from the Doc.</p> <code>False</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set_milestones(\n    self,\n    spans: list[Span],\n    *,\n    start: Optional[str | None] = None,\n    remove: Optional[bool] = False,\n    max_label_length: Optional[int] = 20,\n) -&gt; None:\n    \"\"\"Commit milestones to the object instance.\n\n    Args:\n        spans (list[Span]): The span(s) to use for identifying token attributes.\n        start (Optional[str | None]): Set milestone start to the token before or after the milestone span. May be \"before\" or \"after\".\n        remove (Optional[bool]): Set milestone start to the token following the milestone span and\n            remove the milestone span tokens from the Doc.\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n    \"\"\"\n    if start not in [None, \"before\", \"after\"]:\n        raise ValueError(\"Start must be None, 'before', or 'after'.\")\n    if remove:\n        self.doc = filter_doc(self.doc, spans)\n    elif start is not None:\n        # Update the doc's milestones\n        self.doc.spans[\"milestones\"] = move_milestone(self.doc, spans, start)\n    else:\n        self.doc.spans[\"milestones\"] = spans\n        self._assign_token_attributes(\n            self.doc.spans[\"milestones\"], max_label_length\n        )\n    self.type = \"tokens\"\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Note <p>Some language models include a final punctuation mark in the token string, particularly at the end of a sentence. The strip_punct argument is a somewhat hacky convenience method to remove it. However, the user may wish instead to do some post-processing in order to use the output for their own purposes.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n\n    Note:\n        Some language models include a final punctuation mark in the token string,\n        particularly at the end of a sentence. The strip_punct argument is a\n        somewhat hacky convenience method to remove it. However, the user may wish\n        instead to do some post-processing in order to use the output for their\n        own purposes.\n    \"\"\"\n    milestone_dicts = []\n    for span in self.doc.spans[\"milestones\"]:\n        start_char = self.doc[span.start].idx\n        end_char = start_char + len(span.text)\n        chars = self.doc.text[start_char:end_char]\n        if strip_punct:\n            chars = chars.rstrip(punctuation)\n            end_char -= 1\n        milestone_dicts.append(\n            {\n                \"text\": span.text,\n                \"characters\": chars,\n                \"start_token\": span.start,\n                \"end_token\": span.end,\n                \"start_char\": start_char,\n                \"end_char\": end_char,\n            }\n        )\n\n    return milestone_dicts\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Set regex flags and milestone IOB extensions after initialization.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Set regex flags and milestone IOB extensions after initialization.\"\"\"\n    super().__init__(**data)\n    if not self.case_sensitive:\n        self.flags = case_insensitive_flags\n        self.attr = \"LOWER\"\n    if not Token.has_extension(\"milestone_iob\"):\n        Token.set_extension(\"milestone_iob\", default=\"O\", force=True)\n    if not Token.has_extension(\"milestone_label\"):\n        Token.set_extension(\"milestone_label\", default=\"\", force=True)\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.__iter__","title":"<code>__iter__() -&gt; Iterator</code>","text":"<p>Make the class iterable.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>A generator containing the object's spans.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n    \"\"\"Make the class iterable.\n\n    Returns:\n        Iterator: A generator containing the object's spans.\n    \"\"\"\n    return (span for span in self.spans)\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.spans","title":"<code>spans: list[Span]</code>  <code>property</code>","text":"<p>Return the Spans.</p> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans.</p>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._assign_token_attributes","title":"<code>_assign_token_attributes(spans: list[Span], max_label_length: int = 20) -&gt; None</code>","text":"<p>Assign token attributes in the doc based on spans.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>A list of spaCy Spans.</p> required <code>max_label_length</code> <code>int</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _assign_token_attributes(\n    self, spans: list[Span], max_label_length: int = 20\n) -&gt; None:\n    \"\"\"Assign token attributes in the doc based on spans.\n\n    Args:\n        spans (list[Span]): A list of spaCy Spans.\n        max_label_length (int): The maximum number of characters to include in the label.\n    \"\"\"\n    # Early return if no spans\n    if not spans:\n        for token in self.doc:\n            self.doc[token.i]._.milestone_iob = \"O\"\n            self.doc[token.i]._.milestone_label = \"\"\n        return\n\n    # Pre-compute token positions and labels\n    milestone_starts = {span.start: span for span in spans}\n    milestone_ranges = {token.i for span in spans for token in span[1:]}\n\n    # Assign attributes in single pass\n    for token in self.doc:\n        if span := milestone_starts.get(token.i):\n            self.doc[token.i]._.milestone_iob = \"B\"\n            self.doc[\n                token.i\n            ]._.milestone_label = f\"{span.text:.{max_label_length}}{'...' if len(span.text) &gt; max_label_length else ''}\"\n        elif token.i in milestone_ranges:\n            self.doc[token.i]._.milestone_iob = \"I\"\n            self.doc[token.i]._.milestone_label = \"\"\n        else:\n            self.doc[token.i]._.milestone_iob = \"O\"\n            self.doc[token.i]._.milestone_label = \"\"\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._autodetect_mode","title":"<code>_autodetect_mode(patterns: str | list) -&gt; str</code>","text":"<p>Autodetect mode for matching milestones if not supplied (experimental).</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>str | list</code> <p>A pattern to match.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string to supply to the get_matches() mode argument.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _autodetect_mode(self, patterns: str | list) -&gt; str:\n    \"\"\"Autodetect mode for matching milestones if not supplied (experimental).\n\n    Args:\n        patterns (str | list): A pattern to match.\n\n    Returns:\n        str: A string to supply to the get_matches() mode argument.\n    \"\"\"\n    for pattern in patterns:\n        if not isinstance(pattern, (str, list)):\n            raise ValueError(\n                f\"Pattern {pattern} must be a string or a spaCy Matcher rule.\"\n            )\n        if isinstance(pattern, str):\n            if re.search(r\"\\s\", pattern):\n                self.mode = \"phrase\"\n            else:\n                self.mode = \"string\"\n        else:\n            try:\n                matcher = Matcher(self.doc.vocab, validate=True)\n                matcher.add(\"Pattern\", [pattern])\n                self.mode = \"rule\"\n            # Raise an error if the pattern is not a valid Matcher pattern\n            except BaseException:\n                raise BaseException(\n                    f\"The pattern `{pattern}` could not be matched automatically. Check that the pattern is correct and try setting the `mode` argument in `get_matches()`.\"\n                )\n    return self.mode\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._get_string_matches","title":"<code>_get_string_matches(patterns: Any, flags: Enum) -&gt; list[Span]</code>","text":"<p>Get matches to milestone patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Any</code> <p>A pattern to match.</p> required <code>flags</code> <code>Enum</code> <p>An enum of regex flags.</p> required <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of Spans matching the pattern.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _get_string_matches(self, patterns: Any, flags: Enum) -&gt; list[Span]:\n    \"\"\"Get matches to milestone patterns.\n\n    Args:\n        patterns (Any): A pattern to match.\n        flags (Enum): An enum of regex flags.\n\n    Returns:\n        list[Span]: A list of Spans matching the pattern.\n    \"\"\"\n    if patterns is None or patterns == []:\n        raise ValueError(\"Patterns cannot be empty\")\n    patterns = ensure_list(patterns)\n    if self.character_map is None:\n        self.character_map = chars_to_tokens(self.doc)\n    pattern_matches = []\n    for pattern in patterns:\n        matches = re.finditer(pattern, self.doc.text, flags=flags)\n        for match in matches:\n            pattern_matches.append(match)\n    return [self._to_spacy_span(match) for match in pattern_matches]\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._get_phrase_matches","title":"<code>_get_phrase_matches(patterns: Any, attr: str = 'ORTH') -&gt; list[Span]</code>","text":"<p>Get matches to milestone patterns in phrases.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Any</code> <p>A pattern to match.</p> required <code>attr</code> <code>str</code> <p>A spaCy Token attribute to search.</p> <code>'ORTH'</code> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of Spans matching the pattern.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _get_phrase_matches(self, patterns: Any, attr: str = \"ORTH\") -&gt; list[Span]:\n    \"\"\"Get matches to milestone patterns in phrases.\n\n    Args:\n        patterns (Any): A pattern to match.\n        attr (str): A spaCy Token attribute to search.\n\n    Returns:\n        list[Span]: A list of Spans matching the pattern.\n    \"\"\"\n    nlp = spacy.load(self.nlp)\n    matcher = PhraseMatcher(self.doc.vocab, attr=attr)\n    patterns = [nlp.make_doc(text) for text in patterns]\n    matcher.add(\"PatternList\", patterns)\n    matches = matcher(self.doc)\n    return [self.doc[start:end] for _, start, end in matches]\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._get_rule_matches","title":"<code>_get_rule_matches(patterns: Any) -&gt; list[Span]</code>","text":"<p>Get matches to milestone patterns with spaCy rules.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Any</code> <p>A pattern to match.</p> required <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of Spans matching the pattern.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _get_rule_matches(self, patterns: Any) -&gt; list[Span]:\n    \"\"\"Get matches to milestone patterns with spaCy rules.\n\n    Args:\n        patterns (Any): A pattern to match.\n\n    Returns:\n        list[Span]: A list of Spans matching the pattern.\n    \"\"\"\n    nlp = spacy.load(self.nlp)\n    spans = []\n    if not self.case_sensitive:\n        patterns = lowercase_spacy_rules(patterns)\n    for pattern in patterns:\n        matcher = Matcher(nlp.vocab, validate=True)\n        matcher.add(\"Pattern\", [pattern])\n        matches = matcher(self.doc)\n        spans.extend([self.doc[start:end] for _, start, end in matches])\n    return spans\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._remove_duplicate_spans","title":"<code>_remove_duplicate_spans(spans: list[Span]) -&gt; list[Span]</code>","text":"<p>Remove duplicate spans, generally created when a pattern is added.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>A list of Spans.</p> required <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of de-duplicated Spans.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _remove_duplicate_spans(self, spans: list[Span]) -&gt; list[Span]:\n    \"\"\"Remove duplicate spans, generally created when a pattern is added.\n\n    Args:\n        spans (list[Span]): A list of Spans.\n\n    Returns:\n        list[Span]: A list of de-duplicated Spans.\n    \"\"\"\n    result = []\n    seen = set()\n    for span in spans:\n        key = (span.start, span.end)\n        if key not in seen:\n            result.append(span)\n            seen.add(key)\n    return result\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._set_case_sensitivity","title":"<code>_set_case_sensitivity(case_sensitive: Optional[bool] = None) -&gt; None</code>","text":"<p>Set the object's case sensitivity.</p> <p>Parameters:</p> Name Type Description Default <code>case_sensitive</code> <code>(optional, bool)</code> <p>Whether or not to use case-sensitive searching.</p> <code>None</code> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _set_case_sensitivity(self, case_sensitive: Optional[bool] = None) -&gt; None:\n    \"\"\"Set the object's case sensitivity.\n\n    Args:\n        case_sensitive (optional, bool): Whether or not to use case-sensitive searching.\n    \"\"\"\n    if case_sensitive is not None:\n        self.case_sensitive = case_sensitive\n    if self.case_sensitive is True:\n        self.flags: Enum = re.DOTALL | re.MULTILINE | re.UNICODE\n        self.attr = \"ORTH\"\n    else:\n        self.flags: Enum = re.DOTALL | re.IGNORECASE | re.MULTILINE | re.UNICODE\n        self.attr = \"LOWER\"\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones._to_spacy_span","title":"<code>_to_spacy_span(match: Match) -&gt; Span</code>","text":"<p>Convert a re.match object to a Span.</p> <p>Parameters:</p> Name Type Description Default <code>match</code> <code>Match</code> <p>A re.match object.</p> required <p>Returns:</p> Name Type Description <code>Span</code> <code>Span</code> <p>A spaCy Span.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If match is None or span cannot be created.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def _to_spacy_span(self, match: Match) -&gt; Span:\n    \"\"\"Convert a re.match object to a Span.\n\n    Args:\n        match (Match): A re.match object.\n\n    Returns:\n        Span: A spaCy Span.\n\n    Raises:\n        ValueError: If match is None or span cannot be created.\n    \"\"\"\n    if not match:\n        raise ValueError(\"Match object is None.\")\n\n    # Lazy load character map\n    if not self.character_map:\n        self.character_map = chars_to_tokens(self.doc)\n\n    # Get character positions\n    start_char, end_char = match.span()\n\n    # Try direct char_span first\n    if span := self.doc.char_span(start_char, end_char):\n        return span\n\n    # Fallback to character map\n    start_token = self.character_map.get(start_char)\n    end_token = self.character_map.get(end_char)\n\n    if start_token is not None and end_token is not None:\n        if span := self.doc[start_token : end_token + 1]:\n            return span\n\n    raise ValueError(\n        f\"Could not create span for match at positions {start_char}:{end_char}\"\n    )\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.get_matches","title":"<code>get_matches(patterns: Optional[Any] = None, mode: Optional[str] = None, case_sensitive: Optional[bool] = None) -&gt; list[Span]</code>","text":"<p>Get matches to milestone patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Optional[Any]</code> <p>The pattern(s) to match.</p> <code>None</code> <code>mode</code> <code>Optional[str]</code> <p>The mode to use for matching ('string', 'phrase', 'rule').</p> <code>None</code> <code>case_sensitive</code> <code>Optional[bool]</code> <p>Whether to use case sensitive matching. Defaults to True.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Span]</code> <p>list[Span]: A list of spaCy Spans matching the pattern.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If patterns is None or empty.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef get_matches(\n    self,\n    patterns: Optional[Any] = None,\n    mode: Optional[str] = None,\n    case_sensitive: Optional[bool] = None,\n) -&gt; list[Span]:\n    \"\"\"Get matches to milestone patterns.\n\n    Args:\n        patterns (Optional[Any]): The pattern(s) to match.\n        mode (Optional[str]): The mode to use for matching ('string', 'phrase', 'rule').\n        case_sensitive (Optional[bool]): Whether to use case sensitive matching. Defaults to True.\n\n    Returns:\n        list[Span]: A list of spaCy Spans matching the pattern.\n\n    Raises:\n        ValueError: If patterns is None or empty.\n    \"\"\"\n    self._set_case_sensitivity(case_sensitive)\n\n    # Update patterns list\n    if patterns:\n        self.patterns = ensure_list(patterns)\n\n    # Define mode handlers\n    mode_handlers = {\n        \"string\": lambda: self._get_string_matches(patterns, self.flags),\n        \"phrase\": lambda: self._get_phrase_matches(patterns, self.attr),\n        \"rule\": lambda: self._get_rule_matches(patterns),\n    }\n\n    # If mode not provided or invalid, autodetect\n    if not mode or mode not in mode_handlers:\n        spans = self.get_matches(patterns, mode=self._autodetect_mode(patterns))\n    # Get spans using appropriate handler\n    else:\n        spans = mode_handlers[mode]()\n    return self._remove_duplicate_spans(spans)\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.remove","title":"<code>remove(patterns: Any, *, mode: Optional[str] = 'string') -&gt; None</code>","text":"<p>Remove patterns.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Any</code> <p>The pattern(s) to match.</p> required <code>mode</code> <code>Optional[str]</code> <p>The mode to use for matching.</p> <code>'string'</code> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef remove(self, patterns: Any, *, mode: Optional[str] = \"string\") -&gt; None:\n    \"\"\"Remove patterns.\n\n    Args:\n        patterns (Any): The pattern(s) to match.\n        mode (Optional[str]): The mode to use for matching.\n    \"\"\"\n    patterns = ensure_list(patterns)\n    spans = self.get_matches(patterns, mode=mode)\n\n    # Create a set of spans to remove for faster lookup\n    remove_spans = {f\"{span.start},{span.end}\" for span in spans}\n\n    # Filter out the spans to be removed\n    new_spans = [\n        span\n        for span in self.doc.spans[\"milestones\"]\n        if f\"{span.start},{span.end}\" not in remove_spans\n    ]\n\n    # Reset the token attributes for the spans to be removed\n    for span in spans:\n        for token in self.doc[span.start : span.end]:\n            token._.milestone_iob = \"O\"\n            token._.milestone_label = \"\"\n\n    # Re-set the milestones with the remaining spans\n    self.set_milestones(new_spans)\n\n    # Remove the patterns from the object's patterns list\n    self.patterns = [p for p in self.patterns if p not in patterns]\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.reset","title":"<code>reset()</code>","text":"<p>Reset all <code>milestone</code> values to defaults.</p> <p>Note: Does not modify patterns or any other settings.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all `milestone` values to defaults.\n\n    Note: Does not modify patterns or any other settings.\n    \"\"\"\n    self.doc.spans[\"milestones\"] = []\n    for i, _ in enumerate(self.doc):\n        self.doc[i]._.milestone_iob = \"O\"\n        self.doc[i]._.milestone_label = \"\"\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.set_milestones","title":"<code>set_milestones(spans: list[Span], *, start: Optional[str | None] = None, remove: Optional[bool] = False, max_label_length: Optional[int] = 20) -&gt; None</code>","text":"<p>Commit milestones to the object instance.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>The span(s) to use for identifying token attributes.</p> required <code>start</code> <code>Optional[str | None]</code> <p>Set milestone start to the token before or after the milestone span. May be \"before\" or \"after\".</p> <code>None</code> <code>remove</code> <code>Optional[bool]</code> <p>Set milestone start to the token following the milestone span and remove the milestone span tokens from the Doc.</p> <code>False</code> <code>max_label_length</code> <code>Optional[int]</code> <p>The maximum number of characters to include in the label.</p> <code>20</code> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef set_milestones(\n    self,\n    spans: list[Span],\n    *,\n    start: Optional[str | None] = None,\n    remove: Optional[bool] = False,\n    max_label_length: Optional[int] = 20,\n) -&gt; None:\n    \"\"\"Commit milestones to the object instance.\n\n    Args:\n        spans (list[Span]): The span(s) to use for identifying token attributes.\n        start (Optional[str | None]): Set milestone start to the token before or after the milestone span. May be \"before\" or \"after\".\n        remove (Optional[bool]): Set milestone start to the token following the milestone span and\n            remove the milestone span tokens from the Doc.\n        max_label_length (Optional[int]): The maximum number of characters to include in the label.\n    \"\"\"\n    if start not in [None, \"before\", \"after\"]:\n        raise ValueError(\"Start must be None, 'before', or 'after'.\")\n    if remove:\n        self.doc = filter_doc(self.doc, spans)\n    elif start is not None:\n        # Update the doc's milestones\n        self.doc.spans[\"milestones\"] = move_milestone(self.doc, spans, start)\n    else:\n        self.doc.spans[\"milestones\"] = spans\n        self._assign_token_attributes(\n            self.doc.spans[\"milestones\"], max_label_length\n        )\n    self.type = \"tokens\"\n</code></pre>"},{"location":"api/milestones/token_milestones/#lexos.milestones.token_milestones.TokenMilestones.to_list","title":"<code>to_list(*, strip_punct: Optional[bool] = True) -&gt; list[dict]</code>","text":"<p>Get a list of milestone dicts.</p> <p>Parameters:</p> Name Type Description Default <code>strip_punct</code> <code>Optional[bool]</code> <p>Strip single punctation mark at the end of the character string.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of milestone dicts.</p> Note <p>Some language models include a final punctuation mark in the token string, particularly at the end of a sentence. The strip_punct argument is a somewhat hacky convenience method to remove it. However, the user may wish instead to do some post-processing in order to use the output for their own purposes.</p> Source code in <code>lexos/milestones/token_milestones.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_list(self, *, strip_punct: Optional[bool] = True) -&gt; list[dict]:\n    \"\"\"Get a list of milestone dicts.\n\n    Args:\n        strip_punct (Optional[bool]): Strip single punctation mark at the end of the character string.\n\n    Returns:\n        list[dict]: A list of milestone dicts.\n\n    Note:\n        Some language models include a final punctuation mark in the token string,\n        particularly at the end of a sentence. The strip_punct argument is a\n        somewhat hacky convenience method to remove it. However, the user may wish\n        instead to do some post-processing in order to use the output for their\n        own purposes.\n    \"\"\"\n    milestone_dicts = []\n    for span in self.doc.spans[\"milestones\"]:\n        start_char = self.doc[span.start].idx\n        end_char = start_char + len(span.text)\n        chars = self.doc.text[start_char:end_char]\n        if strip_punct:\n            chars = chars.rstrip(punctuation)\n            end_char -= 1\n        milestone_dicts.append(\n            {\n                \"text\": span.text,\n                \"characters\": chars,\n                \"start_token\": span.start,\n                \"end_token\": span.end,\n                \"start_char\": start_char,\n                \"end_char\": end_char,\n            }\n        )\n\n    return milestone_dicts\n</code></pre>"},{"location":"api/milestones/util/","title":"Util","text":"<p>Utility functions for the <code>milestones</code> module.</p>"},{"location":"api/milestones/util/#api-documentation-utilpy","title":"API Documentation: <code>util.py</code>","text":""},{"location":"api/milestones/util/#lexos.milestones.util.LexosBaseModel","title":"<code>LexosBaseModel</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model inherits from Pydantic base model but validates spaCy objects.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> Source code in <code>lexos/milestones/util.py</code> <pre><code>class LexosBaseModel(BaseModel):\n    \"\"\"Base model inherits from Pydantic base model but validates spaCy objects.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n</code></pre>"},{"location":"api/milestones/util/#lexos.milestones.util.chars_to_tokens","title":"<code>chars_to_tokens(doc: Doc) -&gt; dict[int, int]</code>","text":"<p>Generate a characters to tokens mapping for _match_regex().</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>A spaCy doc.</p> required <p>Returns:</p> Type Description <code>dict[int, int]</code> <p>A dict mapping character indexes to token indexes.</p> Source code in <code>lexos/milestones/util.py</code> <pre><code>def chars_to_tokens(doc: Doc) -&gt; dict[int, int]:\n    \"\"\"Generate a characters to tokens mapping for _match_regex().\n\n    Args:\n        doc: A spaCy doc.\n\n    Returns:\n        A dict mapping character indexes to token indexes.\n    \"\"\"\n    chars_to_tokens = {}\n    for token in doc:\n        for i in range(token.idx, token.idx + len(token.text)):\n            chars_to_tokens[i] = token.i\n    return chars_to_tokens\n</code></pre>"},{"location":"api/milestones/util/#lexos.milestones.util.ensure_list","title":"<code>ensure_list(item: Any) -&gt; list[Any]</code>","text":"<p>Ensure that the input is a list.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The item to ensure is a list.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: The item as a list.</p> Source code in <code>lexos/milestones/util.py</code> <pre><code>def ensure_list(item: Any) -&gt; list[Any]:\n    \"\"\"Ensure that the input is a list.\n\n    Args:\n        item (Any): The item to ensure is a list.\n\n    Returns:\n        list[Any]: The item as a list.\n    \"\"\"\n    if not isinstance(item, list):\n        return [item]\n    return item\n</code></pre>"},{"location":"api/milestones/util/#lexos.milestones.util.filter_doc","title":"<code>filter_doc(doc: Doc, spans: list[Span]) -&gt; Doc</code>","text":"<p>Filter a doc to remove tokens by index, retaining custom extensions.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>A spaCy doc.</p> required <code>spans</code> <code>list[Span]</code> <p>The span(s) to remove from the doc.</p> required <p>Returns:</p> Type Description <code>Doc</code> <p>A new doc with the spans removed.</p> Source code in <code>lexos/milestones/util.py</code> <pre><code>def filter_doc(doc: Doc, spans: list[Span]) -&gt; Doc:\n    \"\"\"Filter a doc to remove tokens by index, retaining custom extensions.\n\n    Args:\n        doc: A spaCy doc.\n        spans (list[Span]): The span(s) to remove from the doc.\n\n    Returns:\n        A new doc with the spans removed.\n    \"\"\"\n    # Check if the milestone extensions have been set\n    if not Token.has_extension(\"milestone_iob\") or not Token.has_extension(\n        \"milestone_label\"\n    ):\n        raise ValueError(\"The milestone extensions have not been set.\")\n    # Get the start ids of the original milestones\n    remove_ids = [token.i for span in spans for token in span]\n    ms_start_ids = [span.start for span in spans]\n    # Get the user data from the original doc; remove milestones\n    new_user_data = {\n        key: value\n        for key, value in doc.user_data.items()\n        if not isinstance(key, tuple)\n        and key[1] not in [\"milestone_iob\", \"milestone_label\"]\n    }\n    # Make a new doc without the tokens in remove_ids\n    words = [token.text for token in doc if token.i not in remove_ids]\n    spaces = [token.whitespace_ == \" \" for token in doc if token.i not in remove_ids]\n    new_doc = Doc(doc.vocab, words=words, spaces=spaces)\n    # Replace the new doc's user data with the modified user data from the original doc\n    new_doc.user_data = new_user_data\n    # Set the milestone IOB to \"B\" for the tokens that were the start of milestones in the original doc\n    new_doc.spans[\"milestones\"] = []\n    for token in new_doc:\n        if token.i in ms_start_ids:\n            token._.milestone_iob = \"B\"\n            new_doc.spans[\"milestones\"].append(new_doc[token.i : token.i + 1])\n    return new_doc\n</code></pre>"},{"location":"api/milestones/util/#lexos.milestones.util.lowercase_spacy_rules","title":"<code>lowercase_spacy_rules(patterns: list[list[dict[str, Any]]], old_key: list[str] | str = ['TEXT', 'ORTH'], new_key: str = 'LOWER') -&gt; list</code>","text":"<p>Convert spaCy Rule Matcher patterns to lowercase.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>list[list[dict[str, Any]]]</code> <p>A list of spacy Rule Matcher patterns.</p> required <code>old_key</code> <code>list[str] | str</code> <p>A dictionary key or list of keys to rename.</p> <code>['TEXT', 'ORTH']</code> <code>new_key</code> <code>str</code> <p>The new key name.</p> <code>'LOWER'</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of spaCy Rule Matcher patterns.</p> Source code in <code>lexos/milestones/util.py</code> <pre><code>def lowercase_spacy_rules(\n    patterns: list[list[dict[str, Any]]],\n    old_key: list[str] | str = [\"TEXT\", \"ORTH\"],\n    new_key: str = \"LOWER\",\n) -&gt; list:\n    \"\"\"Convert spaCy Rule Matcher patterns to lowercase.\n\n    Args:\n        patterns: A list of spacy Rule Matcher patterns.\n        old_key: A dictionary key or list of keys to rename.\n        new_key: The new key name.\n\n    Returns:\n        A list of spaCy Rule Matcher patterns.\n    \"\"\"\n\n    def convert(key):\n        if key in old_key:\n            return new_key\n        else:\n            return key\n\n    if isinstance(patterns, dict):\n        new_dict = {}\n        for key, value in patterns.items():\n            value = lowercase_spacy_rules(value)\n            key = convert(key)\n            new_dict[key] = value\n        return new_dict\n    if isinstance(patterns, list):\n        new_list = []\n        for value in patterns:\n            new_list.append(lowercase_spacy_rules(value))\n        return new_list\n    return patterns\n</code></pre>"},{"location":"api/milestones/util/#lexos.milestones.util.move_milestone","title":"<code>move_milestone(doc: Doc, spans: list[Span], start: str) -&gt; list[Span]</code>","text":"<p>Move the milestone start to a new token index.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>A spaCy doc.</p> required <code>spans</code> <code>list[Span]</code> <p>The span(s) to use for identifying token attributes.</p> required <code>start</code> <code>str</code> <p>Set milestone start to the token before or after the milestone span. May be \"before\" or \"after\".</p> required <p>Returns:</p> Type Description <code>list[Span]</code> <p>A list of new milestone spans.</p> Source code in <code>lexos/milestones/util.py</code> <pre><code>def move_milestone(doc: Doc, spans: list[Span], start: str) -&gt; list[Span]:\n    \"\"\"Move the milestone start to a new token index.\n\n    Args:\n        doc: A spaCy doc.\n        spans (list[Span]): The span(s) to use for identifying token attributes.\n        start (str): Set milestone start to the token before or after the milestone span. May be \"before\" or \"after\".\n\n    Returns:\n        A list of new milestone spans.\n    \"\"\"\n    # Do not process spans at the beginning or end of the doc\n    if start == \"before\":\n        spans = [span for span in spans if span.start &gt; 0]\n    if start == \"after\":\n        spans = [span for span in spans if span.end &lt; len(doc)]\n    new_milestones = []\n    for span in spans:\n        # Reset the current milestone IOB and label\n        for token in span:\n            doc[token.i]._.milestone_iob = \"O\"\n            doc[token.i]._.milestone_label = \"\"\n        # Set the following token's IOB to \"B\" and add the following token to a list of new milestones\n        try:\n            if start == \"after\":\n                doc[span.end]._.milestone_iob = \"B\"\n                new_milestones.append(doc[span.end : span.end + 1])\n            elif start == \"before\" and span.start &gt; 0:\n                doc[span.start - 1]._.milestone_iob = \"B\"\n                new_milestones.append(doc[span.start - 1 : span.start])\n        except IndexError:\n            pass\n    return new_milestones\n</code></pre>"},{"location":"api/rolling_windows/","title":"Rolling Windows","text":"<p>The <code>rolling_windows</code> module provides classes for calculating and visualizing statistical frequencies of terms over sliding windows.</p> <p>The main module is rolling_windows.</p> <p>The <code>rolling_windows</code> module has three built-in calculator classes, counts, averages, and ratios. Custom calculators should inherit from base_plotter.</p> <p>The <code>rolling_windows</code> module has two built-in plotter classes, simple_plotter and plotly_plotter. Custom plotters should inherit from base_plotter.</p>"},{"location":"api/rolling_windows/rolling_windows/","title":"API Documentation: <code>rolling_windows/__init__.py</code>","text":""},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows","title":"<code>Windows</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Basic model for windows.</p> <p>Config:</p> <ul> <li><code>default</code>: <code>validation_config</code></li> </ul> <p>Fields:</p> <ul> <li> <code>input</code>                 (<code>Optional[str | list[str] | Tokenized]</code>)             </li> <li> <code>n</code>                 (<code>Optional[int]</code>)             </li> <li> <code>window_type</code>                 (<code>Optional[str]</code>)             </li> <li> <code>alignment_mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>output</code>                 (<code>Optional[str]</code>)             </li> <li> <code>windows</code>                 (<code>Optional[Iterator]</code>)             </li> </ul> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>class Windows(BaseModel):\n    \"\"\"Basic model for windows.\"\"\"\n\n    input: Optional[str | list[str] | Tokenized] = Field(\n        None, description=\"The input data to be windowed.\"\n    )\n    n: Optional[int] = Field(1000, gt=0, description=\"The size of the window.\")\n    window_type: Optional[str] = Field(\n        \"characters\",\n        description=\"The type of window to generate: `characters`, `spans`, or `tokens`.\",\n    )\n    alignment_mode: Optional[str] = Field(\n        \"strict\",\n        description=\"The alignment mode for the window.\",\n    )\n    output: Optional[str] = Field(\n        \"strings\", description=\"The output type for the windows.\"\n    )\n    windows: Optional[Iterator] = Field(\n        None, description=\"Container for the windows generator.\"\n    )\n\n    model_config = validation_config\n\n    def __init__(self, **data):\n        \"\"\"Create the Windows instance.\"\"\"\n        super().__init__(**data)\n        if self.window_type not in [None, \"characters\", \"spans\", \"tokens\"]:\n            raise LexosException(\"Window type must be 'characters' or 'tokens'.\")\n        if self.output not in [\"strings\", \"tokens\"]:\n            raise LexosException(\"Output must be 'strings' or 'tokens'.\")\n\n    def __iter__(self):\n        \"\"\"Iterate over the windows.\"\"\"\n        if self.windows is None:\n            return iter([])\n        return iter(self.windows)\n\n    @validate_call(config=validation_config)\n    def __call__(\n        self,\n        input: Optional[str | list[str] | Tokenized] = Field(\n            None, description=\"The input data to be windowed.\"\n        ),\n        n: Optional[int] = Field(None, gt=0, description=\"The size of the window.\"),\n        window_type: Optional[str] = Field(\n            None,\n            description=\"The type of window to generate: `characters`, `spans`, or `tokens`.\",\n        ),\n        alignment_mode: Optional[str] = Field(\n            None,\n            description=\"The alignment mode for the window.\",\n        ),\n        output: Optional[str] = Field(\n            None,\n            description=\"The output type for the windows.\",\n        ),\n    ) -&gt; Iterator:\n        \"\"\"Generate windows based on the input data type.\"\"\"\n        self._set_attrs(\n            input=input,\n            n=n,\n            window_type=window_type,\n            alignment_mode=alignment_mode,\n            output=output,\n            windows=None,\n        )\n        if self.window_type not in [\"characters\", \"spans\", \"tokens\"]:\n            raise LexosException(\"Window type must be 'characters' or 'tokens'.\")\n        if self.output not in [\"spans\", \"strings\", \"tokens\"]:\n            raise LexosException(\"Output must be 'spans', 'strings' or 'tokens'.\")\n        if isinstance(self.input, str):\n            self.windows = self._get_string_windows(self.input)\n        elif isinstance(self.input, list):\n            if isinstance(self.input[0], str):\n                self.windows = self._get_string_windows(self.input)\n            elif isinstance(self.input[0], Token):\n                self.windows = self._get_token_list_windows(self.input)\n            else:\n                self.windows = self._get_span_list_windows(self.input)\n        else:\n            self.windows = self._get_doc_windows(self.input)\n        return self\n\n    @property\n    def length(self):\n        \"\"\"Create a temporary copy of the windows and calculate the length.\"\"\"\n        if self.windows is None:\n            return 0\n\n        temp_windows, self.windows = itertools.tee(self.windows)\n        return sum(1 for _ in temp_windows)\n\n    def _get_doc_windows(self, input: Doc | Span) -&gt; Iterator[list[Span | str | Token]]:\n        \"\"\"Generate windows from a Doc or Span object with output as Spans, strings, or Tokens.\n\n        Args:\n            input (Doc | Span): A spaCy Doc or Span object.\n\n        Yields:\n            Iterator[list[Span | str | Token]]: A generator of windows.\n        \"\"\"\n        if self.output not in [\"spans\", \"strings\", \"tokens\"]:\n            raise LexosException(\"Output must be 'spans', 'strings', or 'tokens'.\")\n        # Process a Span as a Doc\n        if isinstance(input, Span):\n            input = input.as_doc()\n        if self.window_type == \"characters\":\n            length = len(input.text)\n        else:\n            length = len(input)\n        length = len(input)\n        boundaries = [(i, i + self.n) for i in range(length) if i + self.n &lt;= length]\n        for start, end in boundaries:\n            if self.alignment_mode == \"strict\":\n                span = input[start:end]\n            else:\n                span = input.char_span(start, end, self.alignment_mode)\n            if span is not None:\n                if self.output == \"strings\":\n                    if span.text != \"\":\n                        yield span.text\n                elif self.output == \"tokens\":\n                    yield [token for token in span if token.text != \"\"]\n                else:\n                    yield span\n\n    def _get_span_list_windows(\n        self, input: list[Span]\n    ) -&gt; Iterator[list[Span | str | Token]]:\n        \"\"\"Generate windows from a Doc or Span object with output as strings or Token objects.\n\n        Args:\n            input (list[Span]): A list of spaCy Span objects.\n\n        Yields:\n            Iterator[list[Span | str | Token]]: A generator of windows.\n        \"\"\"\n        if self.output not in [\"spans\", \"strings\", \"tokens\"]:\n            raise LexosException(\"Output must be 'strings', or 'tokens'.\")\n        if self.window_type != \"characters\":\n            length = len(input)\n            boundaries = [\n                (i, i + self.n) for i in range(length) if i + self.n &lt;= length\n            ]\n            for start, end in boundaries:\n                slice = input[start:end]\n                if slice is not None:\n                    if self.output == \"strings\":\n                        yield [token.text for token in slice]\n                    elif self.output == \"tokens\":  # assuming self.output == \"tokens\"\n                        yield [token for token in slice]\n                    # appears to be unreachable code as output must be 'strings' or 'tokens'\n                    else:\n                        yield slice\n        else:\n            # Merge spans into a single Doc object\n            input = Doc.from_docs([span.as_doc() for span in input])\n            yield from self._get_doc_windows(input)\n\n    def _get_string_windows(self, input: str | list[str]) -&gt; Iterator[list[str]]:\n        \"\"\"Generate windows from a string or list of strings.\n\n        Args:\n            input (str | list[str]): A string or list of strings.\n\n        Yields:\n            Iterator[list[str]]: A generator of windows.\n        \"\"\"\n        if self.output != \"strings\":\n            raise LexosException(\"Output must be 'strings'.\")\n        length = len(input)\n        boundaries = [(i, i + self.n) for i in range(length) if i + self.n &lt;= length]\n        for start_char, end_char in boundaries:\n            yield input[start_char:end_char]\n\n    def _get_token_list_windows(\n        self, input: list[Token]\n    ) -&gt; Iterator[list[str | Token]]:\n        \"\"\"Generate windows from a Doc or Span object with output as strings or Token objects.\n\n        Args:\n            input (list[Token]): A list of spaCy Token objects.\n\n        Yields:\n            Iterator[list[str | Token]]: A generator of windows.\n        \"\"\"\n        if self.output not in [\"strings\", \"tokens\"]:\n            raise LexosException(\"Output must be 'strings' or 'tokens'.\")\n        if self.window_type != \"characters\":\n            length = len(input)\n            boundaries = [\n                (i, i + self.n) for i in range(length) if i + self.n &lt;= length\n            ]\n            for start, end in boundaries:\n                slice = input[start:end]\n                if slice is not None:\n                    if self.output == \"strings\":\n                        yield [token.text for token in slice]\n                    else:\n                        yield [token for token in slice]\n        else:\n            # Merge tokens into a single Doc object\n            words = [token.text for token in input]\n            spaces = [True if token.whitespace_ else False for token in input]\n            input = Doc(input[0].vocab, words=words, spaces=spaces)\n            yield from self._get_doc_windows(input)\n\n    def _set_attrs(self, **kwargs) -&gt; None:\n        \"\"\"Set instance attributes when public method is called.\"\"\"\n        for key, value in kwargs.items():\n            if value is not None:\n                setattr(self, key, value)\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.alignment_mode","title":"<code>alignment_mode: Optional[str] = 'strict'</code>  <code>pydantic-field</code>","text":"<p>The alignment mode for the window.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.input","title":"<code>input: Optional[str | list[str] | Tokenized] = None</code>  <code>pydantic-field</code>","text":"<p>The input data to be windowed.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.length","title":"<code>length</code>  <code>property</code>","text":"<p>Create a temporary copy of the windows and calculate the length.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.n","title":"<code>n: Optional[int] = 1000</code>  <code>pydantic-field</code>","text":"<p>The size of the window.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.output","title":"<code>output: Optional[str] = 'strings'</code>  <code>pydantic-field</code>","text":"<p>The output type for the windows.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.window_type","title":"<code>window_type: Optional[str] = 'characters'</code>  <code>pydantic-field</code>","text":"<p>The type of window to generate: <code>characters</code>, <code>spans</code>, or <code>tokens</code>.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.windows","title":"<code>windows: Optional[Iterator] = None</code>  <code>pydantic-field</code>","text":"<p>Container for the windows generator.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.__call__","title":"<code>__call__(input: Optional[str | list[str] | Tokenized] = Field(None, description='The input data to be windowed.'), n: Optional[int] = Field(None, gt=0, description='The size of the window.'), window_type: Optional[str] = Field(None, description='The type of window to generate: `characters`, `spans`, or `tokens`.'), alignment_mode: Optional[str] = Field(None, description='The alignment mode for the window.'), output: Optional[str] = Field(None, description='The output type for the windows.')) -&gt; Iterator</code>","text":"<p>Generate windows based on the input data type.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>@validate_call(config=validation_config)\ndef __call__(\n    self,\n    input: Optional[str | list[str] | Tokenized] = Field(\n        None, description=\"The input data to be windowed.\"\n    ),\n    n: Optional[int] = Field(None, gt=0, description=\"The size of the window.\"),\n    window_type: Optional[str] = Field(\n        None,\n        description=\"The type of window to generate: `characters`, `spans`, or `tokens`.\",\n    ),\n    alignment_mode: Optional[str] = Field(\n        None,\n        description=\"The alignment mode for the window.\",\n    ),\n    output: Optional[str] = Field(\n        None,\n        description=\"The output type for the windows.\",\n    ),\n) -&gt; Iterator:\n    \"\"\"Generate windows based on the input data type.\"\"\"\n    self._set_attrs(\n        input=input,\n        n=n,\n        window_type=window_type,\n        alignment_mode=alignment_mode,\n        output=output,\n        windows=None,\n    )\n    if self.window_type not in [\"characters\", \"spans\", \"tokens\"]:\n        raise LexosException(\"Window type must be 'characters' or 'tokens'.\")\n    if self.output not in [\"spans\", \"strings\", \"tokens\"]:\n        raise LexosException(\"Output must be 'spans', 'strings' or 'tokens'.\")\n    if isinstance(self.input, str):\n        self.windows = self._get_string_windows(self.input)\n    elif isinstance(self.input, list):\n        if isinstance(self.input[0], str):\n            self.windows = self._get_string_windows(self.input)\n        elif isinstance(self.input[0], Token):\n            self.windows = self._get_token_list_windows(self.input)\n        else:\n            self.windows = self._get_span_list_windows(self.input)\n    else:\n        self.windows = self._get_doc_windows(self.input)\n    return self\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.__init__","title":"<code>__init__(**data)</code>","text":"<p>Create the Windows instance.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Create the Windows instance.\"\"\"\n    super().__init__(**data)\n    if self.window_type not in [None, \"characters\", \"spans\", \"tokens\"]:\n        raise LexosException(\"Window type must be 'characters' or 'tokens'.\")\n    if self.output not in [\"strings\", \"tokens\"]:\n        raise LexosException(\"Output must be 'strings' or 'tokens'.\")\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the windows.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over the windows.\"\"\"\n    if self.windows is None:\n        return iter([])\n    return iter(self.windows)\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.__init__","title":"<code>__init__(**data)</code>","text":"<p>Create the Windows instance.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Create the Windows instance.\"\"\"\n    super().__init__(**data)\n    if self.window_type not in [None, \"characters\", \"spans\", \"tokens\"]:\n        raise LexosException(\"Window type must be 'characters' or 'tokens'.\")\n    if self.output not in [\"strings\", \"tokens\"]:\n        raise LexosException(\"Output must be 'strings' or 'tokens'.\")\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the windows.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over the windows.\"\"\"\n    if self.windows is None:\n        return iter([])\n    return iter(self.windows)\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.__call__","title":"<code>__call__(input: Optional[str | list[str] | Tokenized] = Field(None, description='The input data to be windowed.'), n: Optional[int] = Field(None, gt=0, description='The size of the window.'), window_type: Optional[str] = Field(None, description='The type of window to generate: `characters`, `spans`, or `tokens`.'), alignment_mode: Optional[str] = Field(None, description='The alignment mode for the window.'), output: Optional[str] = Field(None, description='The output type for the windows.')) -&gt; Iterator</code>","text":"<p>Generate windows based on the input data type.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>@validate_call(config=validation_config)\ndef __call__(\n    self,\n    input: Optional[str | list[str] | Tokenized] = Field(\n        None, description=\"The input data to be windowed.\"\n    ),\n    n: Optional[int] = Field(None, gt=0, description=\"The size of the window.\"),\n    window_type: Optional[str] = Field(\n        None,\n        description=\"The type of window to generate: `characters`, `spans`, or `tokens`.\",\n    ),\n    alignment_mode: Optional[str] = Field(\n        None,\n        description=\"The alignment mode for the window.\",\n    ),\n    output: Optional[str] = Field(\n        None,\n        description=\"The output type for the windows.\",\n    ),\n) -&gt; Iterator:\n    \"\"\"Generate windows based on the input data type.\"\"\"\n    self._set_attrs(\n        input=input,\n        n=n,\n        window_type=window_type,\n        alignment_mode=alignment_mode,\n        output=output,\n        windows=None,\n    )\n    if self.window_type not in [\"characters\", \"spans\", \"tokens\"]:\n        raise LexosException(\"Window type must be 'characters' or 'tokens'.\")\n    if self.output not in [\"spans\", \"strings\", \"tokens\"]:\n        raise LexosException(\"Output must be 'spans', 'strings' or 'tokens'.\")\n    if isinstance(self.input, str):\n        self.windows = self._get_string_windows(self.input)\n    elif isinstance(self.input, list):\n        if isinstance(self.input[0], str):\n            self.windows = self._get_string_windows(self.input)\n        elif isinstance(self.input[0], Token):\n            self.windows = self._get_token_list_windows(self.input)\n        else:\n            self.windows = self._get_span_list_windows(self.input)\n    else:\n        self.windows = self._get_doc_windows(self.input)\n    return self\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows.length","title":"<code>length</code>  <code>property</code>","text":"<p>Create a temporary copy of the windows and calculate the length.</p>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows._get_doc_windows","title":"<code>_get_doc_windows(input: Doc | Span) -&gt; Iterator[list[Span | str | Token]]</code>","text":"<p>Generate windows from a Doc or Span object with output as Spans, strings, or Tokens.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Doc | Span</code> <p>A spaCy Doc or Span object.</p> required <p>Yields:</p> Type Description <code>list[Span | str | Token]</code> <p>Iterator[list[Span | str | Token]]: A generator of windows.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def _get_doc_windows(self, input: Doc | Span) -&gt; Iterator[list[Span | str | Token]]:\n    \"\"\"Generate windows from a Doc or Span object with output as Spans, strings, or Tokens.\n\n    Args:\n        input (Doc | Span): A spaCy Doc or Span object.\n\n    Yields:\n        Iterator[list[Span | str | Token]]: A generator of windows.\n    \"\"\"\n    if self.output not in [\"spans\", \"strings\", \"tokens\"]:\n        raise LexosException(\"Output must be 'spans', 'strings', or 'tokens'.\")\n    # Process a Span as a Doc\n    if isinstance(input, Span):\n        input = input.as_doc()\n    if self.window_type == \"characters\":\n        length = len(input.text)\n    else:\n        length = len(input)\n    length = len(input)\n    boundaries = [(i, i + self.n) for i in range(length) if i + self.n &lt;= length]\n    for start, end in boundaries:\n        if self.alignment_mode == \"strict\":\n            span = input[start:end]\n        else:\n            span = input.char_span(start, end, self.alignment_mode)\n        if span is not None:\n            if self.output == \"strings\":\n                if span.text != \"\":\n                    yield span.text\n            elif self.output == \"tokens\":\n                yield [token for token in span if token.text != \"\"]\n            else:\n                yield span\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows._get_span_list_windows","title":"<code>_get_span_list_windows(input: list[Span]) -&gt; Iterator[list[Span | str | Token]]</code>","text":"<p>Generate windows from a Doc or Span object with output as strings or Token objects.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>list[Span]</code> <p>A list of spaCy Span objects.</p> required <p>Yields:</p> Type Description <code>list[Span | str | Token]</code> <p>Iterator[list[Span | str | Token]]: A generator of windows.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def _get_span_list_windows(\n    self, input: list[Span]\n) -&gt; Iterator[list[Span | str | Token]]:\n    \"\"\"Generate windows from a Doc or Span object with output as strings or Token objects.\n\n    Args:\n        input (list[Span]): A list of spaCy Span objects.\n\n    Yields:\n        Iterator[list[Span | str | Token]]: A generator of windows.\n    \"\"\"\n    if self.output not in [\"spans\", \"strings\", \"tokens\"]:\n        raise LexosException(\"Output must be 'strings', or 'tokens'.\")\n    if self.window_type != \"characters\":\n        length = len(input)\n        boundaries = [\n            (i, i + self.n) for i in range(length) if i + self.n &lt;= length\n        ]\n        for start, end in boundaries:\n            slice = input[start:end]\n            if slice is not None:\n                if self.output == \"strings\":\n                    yield [token.text for token in slice]\n                elif self.output == \"tokens\":  # assuming self.output == \"tokens\"\n                    yield [token for token in slice]\n                # appears to be unreachable code as output must be 'strings' or 'tokens'\n                else:\n                    yield slice\n    else:\n        # Merge spans into a single Doc object\n        input = Doc.from_docs([span.as_doc() for span in input])\n        yield from self._get_doc_windows(input)\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows._get_string_windows","title":"<code>_get_string_windows(input: str | list[str]) -&gt; Iterator[list[str]]</code>","text":"<p>Generate windows from a string or list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str | list[str]</code> <p>A string or list of strings.</p> required <p>Yields:</p> Type Description <code>list[str]</code> <p>Iterator[list[str]]: A generator of windows.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def _get_string_windows(self, input: str | list[str]) -&gt; Iterator[list[str]]:\n    \"\"\"Generate windows from a string or list of strings.\n\n    Args:\n        input (str | list[str]): A string or list of strings.\n\n    Yields:\n        Iterator[list[str]]: A generator of windows.\n    \"\"\"\n    if self.output != \"strings\":\n        raise LexosException(\"Output must be 'strings'.\")\n    length = len(input)\n    boundaries = [(i, i + self.n) for i in range(length) if i + self.n &lt;= length]\n    for start_char, end_char in boundaries:\n        yield input[start_char:end_char]\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows._get_token_list_windows","title":"<code>_get_token_list_windows(input: list[Token]) -&gt; Iterator[list[str | Token]]</code>","text":"<p>Generate windows from a Doc or Span object with output as strings or Token objects.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>list[Token]</code> <p>A list of spaCy Token objects.</p> required <p>Yields:</p> Type Description <code>list[str | Token]</code> <p>Iterator[list[str | Token]]: A generator of windows.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def _get_token_list_windows(\n    self, input: list[Token]\n) -&gt; Iterator[list[str | Token]]:\n    \"\"\"Generate windows from a Doc or Span object with output as strings or Token objects.\n\n    Args:\n        input (list[Token]): A list of spaCy Token objects.\n\n    Yields:\n        Iterator[list[str | Token]]: A generator of windows.\n    \"\"\"\n    if self.output not in [\"strings\", \"tokens\"]:\n        raise LexosException(\"Output must be 'strings' or 'tokens'.\")\n    if self.window_type != \"characters\":\n        length = len(input)\n        boundaries = [\n            (i, i + self.n) for i in range(length) if i + self.n &lt;= length\n        ]\n        for start, end in boundaries:\n            slice = input[start:end]\n            if slice is not None:\n                if self.output == \"strings\":\n                    yield [token.text for token in slice]\n                else:\n                    yield [token for token in slice]\n    else:\n        # Merge tokens into a single Doc object\n        words = [token.text for token in input]\n        spaces = [True if token.whitespace_ else False for token in input]\n        input = Doc(input[0].vocab, words=words, spaces=spaces)\n        yield from self._get_doc_windows(input)\n</code></pre>"},{"location":"api/rolling_windows/rolling_windows/#lexos.rolling_windows.Windows._set_attrs","title":"<code>_set_attrs(**kwargs) -&gt; None</code>","text":"<p>Set instance attributes when public method is called.</p> Source code in <code>lexos/rolling_windows/__init__.py</code> <pre><code>def _set_attrs(self, **kwargs) -&gt; None:\n    \"\"\"Set instance attributes when public method is called.\"\"\"\n    for key, value in kwargs.items():\n        if value is not None:\n            setattr(self, key, value)\n</code></pre>"},{"location":"api/rolling_windows/calculators/","title":"Rolling Windows Calculators","text":"<p>The <code>rolling_windows</code> module has three built-in calculator classes, counts, averages, and ratios. Custom calculators should inherit from base_plotter.</p>"},{"location":"api/rolling_windows/calculators/averages/","title":"Averages","text":""},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages","title":"<code>Averages</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>Counts</code></p> <p>A calculator for calculating averages of counts in rolling windows.</p> <p>Fields:</p> <ul> <li> <code>patterns</code>                 (<code>Optional[list | str]</code>)             </li> <li> <code>windows</code>                 (<code>Optional[Windows]</code>)             </li> <li> <code>mode</code>                 (<code>Optional[bool | str]</code>)             </li> <li> <code>case_sensitive</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>alignment_mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>model</code>                 (<code>Optional[str]</code>)             </li> <li> <code>nlp</code>                 (<code>Optional[Language]</code>)             </li> <li> <code>data</code>                 (<code>Optional[list]</code>)             </li> </ul> Source code in <code>lexos/rolling_windows/calculators/averages.py</code> <pre><code>class Averages(Counts):\n    \"\"\"A calculator for calculating averages of counts in rolling windows.\"\"\"\n\n    _id: ClassVar[str] = \"averages\"\n\n    def __call__(\n        self,\n        patterns: Optional[list | str] = None,\n        windows: Optional[Windows] = None,\n        mode: Optional[bool] = None,\n        case_sensitive: Optional[bool] = None,\n        alignment_mode: Optional[str] = None,\n        model: Optional[str] = None,\n    ):\n        \"\"\"Call the calculator.\"\"\"\n        attrs = {\n            \"patterns\": patterns,\n            \"windows\": windows,\n            \"mode\": mode,\n            \"case_sensitive\": case_sensitive,\n            \"alignment_mode\": alignment_mode,\n            \"model\": model,\n        }\n        self._set_attrs(attrs)\n        if self.windows is not None:\n            self.data = [\n                [\n                    self._get_window_count(window, pattern) / self.n\n                    for pattern in self.patterns\n                ]\n                for window in self.windows\n            ]\n            return self.data\n        else:\n            raise LexosException(\"Calculator `windows` attribute is empty.\")\n\n    @validate_call(config=validation_config)\n    def to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n        \"\"\"Convert the data to a pandas dataframe.\n\n        Args:\n            show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n        Returns:\n            pd.DataFrame: A pandas DataFrame.\n        \"\"\"\n        if show_spacy_rules:\n            patterns = self.patterns\n        else:\n            patterns = []\n            # Extract strings from spaCy rules\n            for pattern in self.patterns:\n                if isinstance(pattern, list):\n                    patterns.append(self._extract_string_pattern(pattern))\n                else:\n                    patterns.append(pattern)\n        # Assign column labels\n        cols = []\n        for pattern in patterns:\n            if not self.case_sensitive and isinstance(pattern, str):\n                pattern = pattern.lower()\n            elif not self.case_sensitive and isinstance(pattern, list):\n                pattern = str(spacy_rule_to_lower(pattern))\n            cols.append(str(pattern))\n        # Generate dataframe\n        return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.alignment_mode","title":"<code>alignment_mode: Optional[str] = 'strict'</code>  <code>pydantic-field</code>","text":"<p>Whether to snap searches to token boundaries. Values are 'strict', 'contract', and 'expand'.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.case_sensitive","title":"<code>case_sensitive: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to make searches case-sensitive.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.data","title":"<code>data: Optional[list] = []</code>  <code>pydantic-field</code>","text":"<p>A container for the calculated data.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata for the calculator.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.mode","title":"<code>mode: Optional[bool | str] = 'exact'</code>  <code>pydantic-field</code>","text":"<p>The search method to use ('regex', 'spacy_rule', 'multi_token', 'multi_token_exact').</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The language model to be used for searching spaCy tokens/spans.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.n","title":"<code>n</code>  <code>property</code>","text":"<p>Get the number of units per window.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.nlp","title":"<code>nlp: Optional[Language] = None</code>  <code>pydantic-field</code>","text":"<p>The spaCy nlp object.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.patterns","title":"<code>patterns: Optional[list | str] = None</code>  <code>pydantic-field</code>","text":"<p>A pattern or list of patterns to search in windows.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.regex_flags","title":"<code>regex_flags</code>  <code>property</code>","text":"<p>Return regex flags based on case_sensitive setting.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.window_type","title":"<code>window_type</code>  <code>property</code>","text":"<p>Get the type of units in the windows.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.windows","title":"<code>windows: Optional[Windows] = None</code>  <code>pydantic-field</code>","text":"<p>A Windows object containing the windows to search.</p>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.__call__","title":"<code>__call__(patterns: Optional[list | str] = None, windows: Optional[Windows] = None, mode: Optional[bool] = None, case_sensitive: Optional[bool] = None, alignment_mode: Optional[str] = None, model: Optional[str] = None)</code>","text":"<p>Call the calculator.</p> Source code in <code>lexos/rolling_windows/calculators/averages.py</code> <pre><code>def __call__(\n    self,\n    patterns: Optional[list | str] = None,\n    windows: Optional[Windows] = None,\n    mode: Optional[bool] = None,\n    case_sensitive: Optional[bool] = None,\n    alignment_mode: Optional[str] = None,\n    model: Optional[str] = None,\n):\n    \"\"\"Call the calculator.\"\"\"\n    attrs = {\n        \"patterns\": patterns,\n        \"windows\": windows,\n        \"mode\": mode,\n        \"case_sensitive\": case_sensitive,\n        \"alignment_mode\": alignment_mode,\n        \"model\": model,\n    }\n    self._set_attrs(attrs)\n    if self.windows is not None:\n        self.data = [\n            [\n                self._get_window_count(window, pattern) / self.n\n                for pattern in self.patterns\n            ]\n            for window in self.windows\n        ]\n        return self.data\n    else:\n        raise LexosException(\"Calculator `windows` attribute is empty.\")\n</code></pre>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.to_df","title":"<code>to_df(show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Convert the data to a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>show_spacy_rules</code> <code>Optional[bool]</code> <p>If True, use full spaCy rules for labels; otherwise use only the string pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/averages.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"Convert the data to a pandas dataframe.\n\n    Args:\n        show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame.\n    \"\"\"\n    if show_spacy_rules:\n        patterns = self.patterns\n    else:\n        patterns = []\n        # Extract strings from spaCy rules\n        for pattern in self.patterns:\n            if isinstance(pattern, list):\n                patterns.append(self._extract_string_pattern(pattern))\n            else:\n                patterns.append(pattern)\n    # Assign column labels\n    cols = []\n    for pattern in patterns:\n        if not self.case_sensitive and isinstance(pattern, str):\n            pattern = pattern.lower()\n        elif not self.case_sensitive and isinstance(pattern, list):\n            pattern = str(spacy_rule_to_lower(pattern))\n        cols.append(str(pattern))\n    # Generate dataframe\n    return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.__call__","title":"<code>__call__(patterns: Optional[list | str] = None, windows: Optional[Windows] = None, mode: Optional[bool] = None, case_sensitive: Optional[bool] = None, alignment_mode: Optional[str] = None, model: Optional[str] = None)</code>","text":"<p>Call the calculator.</p> Source code in <code>lexos/rolling_windows/calculators/averages.py</code> <pre><code>def __call__(\n    self,\n    patterns: Optional[list | str] = None,\n    windows: Optional[Windows] = None,\n    mode: Optional[bool] = None,\n    case_sensitive: Optional[bool] = None,\n    alignment_mode: Optional[str] = None,\n    model: Optional[str] = None,\n):\n    \"\"\"Call the calculator.\"\"\"\n    attrs = {\n        \"patterns\": patterns,\n        \"windows\": windows,\n        \"mode\": mode,\n        \"case_sensitive\": case_sensitive,\n        \"alignment_mode\": alignment_mode,\n        \"model\": model,\n    }\n    self._set_attrs(attrs)\n    if self.windows is not None:\n        self.data = [\n            [\n                self._get_window_count(window, pattern) / self.n\n                for pattern in self.patterns\n            ]\n            for window in self.windows\n        ]\n        return self.data\n    else:\n        raise LexosException(\"Calculator `windows` attribute is empty.\")\n</code></pre>"},{"location":"api/rolling_windows/calculators/averages/#lexos.rolling_windows.calculators.averages.Averages.to_df","title":"<code>to_df(show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Convert the data to a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>show_spacy_rules</code> <code>Optional[bool]</code> <p>If True, use full spaCy rules for labels; otherwise use only the string pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/averages.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"Convert the data to a pandas dataframe.\n\n    Args:\n        show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame.\n    \"\"\"\n    if show_spacy_rules:\n        patterns = self.patterns\n    else:\n        patterns = []\n        # Extract strings from spaCy rules\n        for pattern in self.patterns:\n            if isinstance(pattern, list):\n                patterns.append(self._extract_string_pattern(pattern))\n            else:\n                patterns.append(pattern)\n    # Assign column labels\n    cols = []\n    for pattern in patterns:\n        if not self.case_sensitive and isinstance(pattern, str):\n            pattern = pattern.lower()\n        elif not self.case_sensitive and isinstance(pattern, list):\n            pattern = str(spacy_rule_to_lower(pattern))\n        cols.append(str(pattern))\n    # Generate dataframe\n    return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/","title":"Base Calculator","text":""},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.flatten","title":"<code>flatten(input: dict | list | str) -&gt; Iterable</code>","text":"<p>Yield items from any nested iterable.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>dict | list | str</code> <p>A list of lists or dicts.</p> required <p>Yields:</p> Name Type Description <code>Iterable</code> <code>Iterable</code> <p>Items from the nested iterable.</p> Notes <p>See https://stackoverflow.com/a/40857703.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def flatten(input: dict | list | str) -&gt; Iterable:\n    \"\"\"Yield items from any nested iterable.\n\n    Args:\n        input (dict | list | str): A list of lists or dicts.\n\n    Yields:\n        Iterable: Items from the nested iterable.\n\n    Notes:\n        See https://stackoverflow.com/a/40857703.\n    \"\"\"\n    for x in input:\n        if isinstance(x, Iterable) and not isinstance(x, str):\n            if isinstance(x, list):\n                for sub_x in flatten(x):\n                    yield sub_x\n            elif isinstance(x, dict):\n                yield list(x.values())[0]\n        else:\n            yield x\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.regex_escape","title":"<code>regex_escape(s: str) -&gt; str</code>","text":"<p>Escape only regex special characters.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>A string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>An escaped string.</p> Note <p>See https://stackoverflow.com/a/78136529/22853742.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def regex_escape(s: str) -&gt; str:\n    \"\"\"Escape only regex special characters.\n\n    Args:\n        s (str): A string.\n\n    Returns:\n        An escaped string.\n\n    Note:\n        See https://stackoverflow.com/a/78136529/22853742.\n    \"\"\"\n    if isinstance(s, bytes):\n        return re.sub(rb\"[][(){}?*+.^$]\", lambda m: b\"\\\\\" + m.group(), s)\n    return re.sub(r\"[][(){}?*+.^$]\", lambda m: \"\\\\\" + m.group(), s)\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.spacy_rule_to_lower","title":"<code>spacy_rule_to_lower(patterns: dict | list[dict], old_key: Optional[list[str] | str] = ['TEXT', 'ORTH'], new_key: Optional[str] = 'LOWER') -&gt; list</code>","text":"<p>Convert spacy Rule Matcher patterns to lowercase.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>dict | list[dict]</code> <p>A list of spacy Rule Matcher patterns.</p> required <code>old_key</code> <code>list[str] | str</code> <p>A dictionary key or list of keys to rename.</p> <code>['TEXT', 'ORTH']</code> <code>new_key</code> <code>Optional[str]</code> <p>The new key name.</p> <code>'LOWER'</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of spacy Rule Matcher patterns</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def spacy_rule_to_lower(\n    patterns: dict | list[dict],\n    old_key: Optional[list[str] | str] = [\"TEXT\", \"ORTH\"],\n    new_key: Optional[str] = \"LOWER\",\n) -&gt; list:\n    \"\"\"Convert spacy Rule Matcher patterns to lowercase.\n\n    Args:\n        patterns (dict | list[dict]): A list of spacy Rule Matcher patterns.\n        old_key (list[str] | str): A dictionary key or list of keys to rename.\n        new_key (Optional[str]): The new key name.\n\n    Returns:\n        A list of spacy Rule Matcher patterns\n    \"\"\"\n\n    def convert(key):\n        \"\"\"Converts the key to lowercase.\"\"\"\n        if key in old_key:\n            return new_key\n        else:\n            return key\n\n    if isinstance(patterns, dict):\n        new_dict = {}\n        for key, value in patterns.items():\n            key = convert(key)\n            new_dict[key] = value\n        return new_dict\n\n    if isinstance(patterns, list):\n        new_list = []\n        for value in patterns:\n            new_list.append(spacy_rule_to_lower(value))\n        return new_list\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator","title":"<code>BaseCalculator</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>An abstract base class for calculators.</p> <p>Fields:</p> <ul> <li> <code>patterns</code>                 (<code>Optional[list | str]</code>)             </li> <li> <code>windows</code>                 (<code>Optional[Windows]</code>)             </li> <li> <code>mode</code>                 (<code>Optional[bool | str]</code>)             </li> <li> <code>case_sensitive</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>alignment_mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>model</code>                 (<code>Optional[str]</code>)             </li> <li> <code>nlp</code>                 (<code>Optional[Language]</code>)             </li> <li> <code>data</code>                 (<code>Optional[list]</code>)             </li> </ul> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>class BaseCalculator(ABC, BaseModel):\n    \"\"\"An abstract base class for calculators.\"\"\"\n\n    id: ClassVar[str] = \"base_calculator\"\n\n    patterns: Optional[list | str] = Field(\n        default=None, description=\"A pattern or list of patterns to search in windows.\"\n    )\n    windows: Optional[Windows] = Field(\n        default=None, description=\"A Windows object containing the windows to search.\"\n    )\n    mode: Optional[bool | str] = Field(\n        default=\"exact\",\n        description=\"The search method to use ('regex', 'spacy_rule', 'multi_token', 'multi_token_exact').\",\n    )\n    case_sensitive: Optional[bool] = Field(\n        default=False, description=\"Whether to make searches case-sensitive.\"\n    )\n    alignment_mode: Optional[str] = Field(\n        default=\"strict\",\n        description=\"Whether to snap searches to token boundaries. Values are 'strict', 'contract', and 'expand'.\",\n    )\n    model: Optional[str] = Field(\n        default=\"xx_sent_ud_sm\",\n        description=\"The language model to be used for searching spaCy tokens/spans.\",\n    )\n    nlp: Optional[Language] = Field(default=None, description=\"The spaCy nlp object.\")\n    data: Optional[list] = Field(\n        default=[], description=\"A container for the calculated data.\"\n    )\n\n    model_config = validation_config\n\n    @property\n    def metadata(self) -&gt; dict:\n        \"\"\"Return metadata for the calculator.\"\"\"\n        # Note: model_dump() may evaluate computed fields on this model.\n        # If any computed properties rely on external state or are expensive\n        # to compute, accessing them via model_dump() could raise or cause\n        # performance issues. Subclasses should ensure their computed fields\n        # are safe to evaluate here or override this property to exclude\n        # such fields explicitly (e.g., model_dump(exclude=[...])).\n        #\n        # Historically, this property intentionally did not return the\n        # result of `model_dump()`; it was used to ensure that validators or\n        # other side effects ran without exposing the entire dict to the\n        # caller. Maintain that behavior by calling `model_dump()` but not\n        # returning the value. If you need the metadata dict, override this\n        # property in subclasses to return it explicitly.\n        self.model_dump()\n\n    @property\n    def n(self):\n        \"\"\"Get the number of units per window.\"\"\"\n        if self.windows.n is not None:\n            return self.windows.n\n        return None\n\n    @property\n    def regex_flags(self):\n        \"\"\"Return regex flags based on case_sensitive setting.\"\"\"\n        if not self.case_sensitive:\n            return re.IGNORECASE | re.UNICODE\n        else:\n            return re.UNICODE\n\n    @property\n    def window_type(self):\n        \"\"\"Get the type of units in the windows.\"\"\"\n        if self.windows.window_type is not None:\n            return self.windows.window_type\n        return None\n\n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        \"\"\"Call the instance.\"\"\"\n        ...\n\n    def _count_character_patterns_in_character_windows(\n        self, window: str, pattern: str\n    ) -&gt; int:\n        \"\"\"Use Python count() to count exact character matches in a character window.\n\n        Args:\n            window (str): A string window.\n            pattern (str): A string pattern to search for.\n\n        Returns:\n            The number of occurrences of the pattern in the window.\n        \"\"\"\n        if self.mode == \"regex\":\n            return len(re.findall(pattern, window, self.regex_flags))\n        else:\n            if not self.case_sensitive:\n                window = window.lower()\n                pattern = pattern.lower()\n            return window.count(pattern)\n\n    def _count_in_character_window(self, window: str, pattern: str) -&gt; int:\n        \"\"\"Choose function for counting matches in character windows.\n\n        Args:\n            window (str): A string window.\n            pattern (str): A string pattern to search for.\n\n        Returns:\n            The number of occurrences of the pattern in the window.\n        \"\"\"\n        if self.mode in [\"exact\", \"regex\"]:\n            return self._count_character_patterns_in_character_windows(window, pattern)\n        else:\n            raise LexosException(\"Invalid mode for character windows.\")\n\n    def _count_token_patterns_in_token_lists(\n        self, window: list[str], pattern: str\n    ) -&gt; int:\n        \"\"\"Count patterns in lists of token strings.\n\n        Args:\n            window (list[str]): A window consisting of a list of strings.\n            pattern (str): A string pattern to search for.\n\n        Returns:\n            The number of occurrences of the pattern in the window.\n        \"\"\"\n        if self.mode == \"regex\":\n            return sum(\n                [len(re.findall(pattern, token, self.regex_flags)) for token in window]\n            )\n        else:\n            if not self.case_sensitive:\n                window = [token.lower() for token in window]\n                pattern = pattern.lower()\n            return window.count(pattern)\n\n    def _count_token_patterns_in_span(self, window: Span, pattern: list | str) -&gt; int:\n        \"\"\"Count patterns in spans or docs.\n\n        Args:\n            window (Span): A window consisting of a list of spaCy spans or a spaCy doc.\n            pattern (list | str): A string pattern or spaCy rule to search for.\n\n        Returns:\n            The number of occurrences of the pattern in the window.\n        \"\"\"\n        if self.mode == \"exact\":\n            if not self.case_sensitive:\n                window = [token.lower_ for token in window]\n                pattern = pattern.lower()\n            else:\n                window = [token.text for token in window]\n            return window.count(pattern)\n        elif self.mode == \"regex\":\n            return sum(\n                [\n                    len(re.findall(pattern, token.text, self.regex_flags))\n                    for token in window\n                ]\n            )\n        elif self.mode == \"spacy_rule\":\n            if not self.case_sensitive:\n                pattern = spacy_rule_to_lower(pattern)\n            matcher = Matcher(self.nlp.vocab)\n            matcher.add(\"Pattern\", [pattern])\n            return len(matcher(window))\n\n    def _count_token_patterns_in_span_text(self, window: Span, pattern: str) -&gt; int:\n        \"\"\"Count patterns in span or doc text with token alignment.\n\n        Args:\n            window (Span): A Span window.\n            pattern (str): A string pattern to search for.\n\n        Returns:\n            The number of occurrences of the pattern in the window.\n        \"\"\"\n        count = 0\n        if self.mode == \"multi_token_exact\":\n            pattern = regex_escape(pattern)\n        for match in re.finditer(pattern, window.text, self.regex_flags):\n            start, end = match.span()\n            span = window.char_span(start, end, self.alignment_mode)\n            if span is not None:\n                count += 1\n        return count\n\n    def _count_in_token_window(\n        self, window: list[str] | list[Token] | Doc | Span, pattern: list | str\n    ) -&gt; int:\n        \"\"\"Choose function for counting matches in token windows.\n\n        Args:\n            window (list[str] | Span): A window consisting of a list of token strings, a list of spaCy spans, or a spaCy doc.\n            pattern (list | str): A string pattern or spaCy rule to search for.\n\n        Returns:\n            The number of occurrences of the pattern in the window.\n        \"\"\"\n        # Validate window type for multi_token and spacy_rule modes\n        if self.mode in [\"multi_token\", \"spacy_rule\"]:\n            if not isinstance(window, (Doc, Span)):\n                raise LexosException(\n                    \"You cannot use spaCy rules or perform multi-token searches with a string or list of token strings.\"\n                )\n        if isinstance(window, (list)) and self.mode in [\"multi_token\", \"spacy_rule\"]:\n            raise LexosException(\n                \"You cannot use spaCy rules or perform multi-token searches with a string or list of token strings.\"\n            )\n        elif isinstance(window, list) and all(isinstance(i, str) for i in window):\n            # Match in single tokens\n            return self._count_token_patterns_in_token_lists(window, pattern)\n        elif isinstance(window, Doc | Span):\n            # Iterate over the full text with token boundary alignment\n            if self.mode.startswith(\"multi_token\"):\n                return self._count_token_patterns_in_span_text(window, pattern)\n            # Match in single tokens\n            else:\n                return self._count_token_patterns_in_span(window, pattern)\n\n    def _extract_string_pattern(self, pattern: list[list[dict[str, Any]]]) -&gt; str:\n        \"\"\"Extract a string pattern from a spaCy rule.\n\n        Args:\n            pattern (list[list[dict[str, Any]]]): A list of spaCy rule patterns to search.\n\n        Returns:\n            str: A string pattern.\n        \"\"\"\n        return \"|\".join(\n            [\n                item if isinstance(item, str) else list(item.values())[0]\n                for item in list(flatten(pattern))\n            ]\n        )\n\n    def _get_window_count(\n        self, window: list[str] | Span | str, pattern: list | str\n    ) -&gt; int:\n        \"\"\"Call character or token window methods, as appropriate.\n\n        Args:\n            window (list[str] | Span | str]): A window consisting of a list of token strings, a list of spaCy spans, a spaCy doc, or a string.\n            pattern (list | str): A string pattern or spaCy rule to search for.\n\n        Returns:\n            The number of occurrences of the pattern in the window.\n        \"\"\"\n        if self.window_type == \"characters\":\n            return self._count_in_character_window(window, pattern)\n        else:\n            return self._count_in_token_window(window, pattern)\n\n    def _set_attrs(self, attrs: dict) -&gt; None:\n        \"\"\"Set instance attributes when public method is called.\n\n        Args:\n            attrs (dict): A dict of keyword arguments and their values.\n        \"\"\"\n        for key, value in attrs.items():\n            if value is not None:\n                setattr(self, key, value)\n            if key == \"model\" and value is not None:\n                self.nlp = spacy.load(self.model)\n\n    @abstractmethod\n    def to_df(self, *args, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Output the calcualtions as a pandas DataFrame.\"\"\"\n        ...\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.alignment_mode","title":"<code>alignment_mode: Optional[str] = 'strict'</code>  <code>pydantic-field</code>","text":"<p>Whether to snap searches to token boundaries. Values are 'strict', 'contract', and 'expand'.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.case_sensitive","title":"<code>case_sensitive: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to make searches case-sensitive.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.data","title":"<code>data: Optional[list] = []</code>  <code>pydantic-field</code>","text":"<p>A container for the calculated data.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata for the calculator.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.mode","title":"<code>mode: Optional[bool | str] = 'exact'</code>  <code>pydantic-field</code>","text":"<p>The search method to use ('regex', 'spacy_rule', 'multi_token', 'multi_token_exact').</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The language model to be used for searching spaCy tokens/spans.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.n","title":"<code>n</code>  <code>property</code>","text":"<p>Get the number of units per window.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.nlp","title":"<code>nlp: Optional[Language] = None</code>  <code>pydantic-field</code>","text":"<p>The spaCy nlp object.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.patterns","title":"<code>patterns: Optional[list | str] = None</code>  <code>pydantic-field</code>","text":"<p>A pattern or list of patterns to search in windows.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.regex_flags","title":"<code>regex_flags</code>  <code>property</code>","text":"<p>Return regex flags based on case_sensitive setting.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.window_type","title":"<code>window_type</code>  <code>property</code>","text":"<p>Get the type of units in the windows.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.windows","title":"<code>windows: Optional[Windows] = None</code>  <code>pydantic-field</code>","text":"<p>A Windows object containing the windows to search.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.__call__","title":"<code>__call__(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Call the instance.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>@abstractmethod\ndef __call__(self, *args, **kwargs):\n    \"\"\"Call the instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.to_df","title":"<code>to_df(*args, **kwargs) -&gt; pd.DataFrame</code>  <code>abstractmethod</code>","text":"<p>Output the calcualtions as a pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>@abstractmethod\ndef to_df(self, *args, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Output the calcualtions as a pandas DataFrame.\"\"\"\n    ...\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata for the calculator.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.n","title":"<code>n</code>  <code>property</code>","text":"<p>Get the number of units per window.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.regex_flags","title":"<code>regex_flags</code>  <code>property</code>","text":"<p>Return regex flags based on case_sensitive setting.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.window_type","title":"<code>window_type</code>  <code>property</code>","text":"<p>Get the type of units in the windows.</p>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.__call__","title":"<code>__call__(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Call the instance.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>@abstractmethod\ndef __call__(self, *args, **kwargs):\n    \"\"\"Call the instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._count_character_patterns_in_character_windows","title":"<code>_count_character_patterns_in_character_windows(window: str, pattern: str) -&gt; int</code>","text":"<p>Use Python count() to count exact character matches in a character window.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>str</code> <p>A string window.</p> required <code>pattern</code> <code>str</code> <p>A string pattern to search for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of occurrences of the pattern in the window.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _count_character_patterns_in_character_windows(\n    self, window: str, pattern: str\n) -&gt; int:\n    \"\"\"Use Python count() to count exact character matches in a character window.\n\n    Args:\n        window (str): A string window.\n        pattern (str): A string pattern to search for.\n\n    Returns:\n        The number of occurrences of the pattern in the window.\n    \"\"\"\n    if self.mode == \"regex\":\n        return len(re.findall(pattern, window, self.regex_flags))\n    else:\n        if not self.case_sensitive:\n            window = window.lower()\n            pattern = pattern.lower()\n        return window.count(pattern)\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._count_in_character_window","title":"<code>_count_in_character_window(window: str, pattern: str) -&gt; int</code>","text":"<p>Choose function for counting matches in character windows.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>str</code> <p>A string window.</p> required <code>pattern</code> <code>str</code> <p>A string pattern to search for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of occurrences of the pattern in the window.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _count_in_character_window(self, window: str, pattern: str) -&gt; int:\n    \"\"\"Choose function for counting matches in character windows.\n\n    Args:\n        window (str): A string window.\n        pattern (str): A string pattern to search for.\n\n    Returns:\n        The number of occurrences of the pattern in the window.\n    \"\"\"\n    if self.mode in [\"exact\", \"regex\"]:\n        return self._count_character_patterns_in_character_windows(window, pattern)\n    else:\n        raise LexosException(\"Invalid mode for character windows.\")\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._count_token_patterns_in_token_lists","title":"<code>_count_token_patterns_in_token_lists(window: list[str], pattern: str) -&gt; int</code>","text":"<p>Count patterns in lists of token strings.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>list[str]</code> <p>A window consisting of a list of strings.</p> required <code>pattern</code> <code>str</code> <p>A string pattern to search for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of occurrences of the pattern in the window.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _count_token_patterns_in_token_lists(\n    self, window: list[str], pattern: str\n) -&gt; int:\n    \"\"\"Count patterns in lists of token strings.\n\n    Args:\n        window (list[str]): A window consisting of a list of strings.\n        pattern (str): A string pattern to search for.\n\n    Returns:\n        The number of occurrences of the pattern in the window.\n    \"\"\"\n    if self.mode == \"regex\":\n        return sum(\n            [len(re.findall(pattern, token, self.regex_flags)) for token in window]\n        )\n    else:\n        if not self.case_sensitive:\n            window = [token.lower() for token in window]\n            pattern = pattern.lower()\n        return window.count(pattern)\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._count_token_patterns_in_span","title":"<code>_count_token_patterns_in_span(window: Span, pattern: list | str) -&gt; int</code>","text":"<p>Count patterns in spans or docs.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>Span</code> <p>A window consisting of a list of spaCy spans or a spaCy doc.</p> required <code>pattern</code> <code>list | str</code> <p>A string pattern or spaCy rule to search for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of occurrences of the pattern in the window.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _count_token_patterns_in_span(self, window: Span, pattern: list | str) -&gt; int:\n    \"\"\"Count patterns in spans or docs.\n\n    Args:\n        window (Span): A window consisting of a list of spaCy spans or a spaCy doc.\n        pattern (list | str): A string pattern or spaCy rule to search for.\n\n    Returns:\n        The number of occurrences of the pattern in the window.\n    \"\"\"\n    if self.mode == \"exact\":\n        if not self.case_sensitive:\n            window = [token.lower_ for token in window]\n            pattern = pattern.lower()\n        else:\n            window = [token.text for token in window]\n        return window.count(pattern)\n    elif self.mode == \"regex\":\n        return sum(\n            [\n                len(re.findall(pattern, token.text, self.regex_flags))\n                for token in window\n            ]\n        )\n    elif self.mode == \"spacy_rule\":\n        if not self.case_sensitive:\n            pattern = spacy_rule_to_lower(pattern)\n        matcher = Matcher(self.nlp.vocab)\n        matcher.add(\"Pattern\", [pattern])\n        return len(matcher(window))\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._count_token_patterns_in_span_text","title":"<code>_count_token_patterns_in_span_text(window: Span, pattern: str) -&gt; int</code>","text":"<p>Count patterns in span or doc text with token alignment.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>Span</code> <p>A Span window.</p> required <code>pattern</code> <code>str</code> <p>A string pattern to search for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of occurrences of the pattern in the window.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _count_token_patterns_in_span_text(self, window: Span, pattern: str) -&gt; int:\n    \"\"\"Count patterns in span or doc text with token alignment.\n\n    Args:\n        window (Span): A Span window.\n        pattern (str): A string pattern to search for.\n\n    Returns:\n        The number of occurrences of the pattern in the window.\n    \"\"\"\n    count = 0\n    if self.mode == \"multi_token_exact\":\n        pattern = regex_escape(pattern)\n    for match in re.finditer(pattern, window.text, self.regex_flags):\n        start, end = match.span()\n        span = window.char_span(start, end, self.alignment_mode)\n        if span is not None:\n            count += 1\n    return count\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._count_in_token_window","title":"<code>_count_in_token_window(window: list[str] | list[Token] | Doc | Span, pattern: list | str) -&gt; int</code>","text":"<p>Choose function for counting matches in token windows.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>list[str] | Span</code> <p>A window consisting of a list of token strings, a list of spaCy spans, or a spaCy doc.</p> required <code>pattern</code> <code>list | str</code> <p>A string pattern or spaCy rule to search for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of occurrences of the pattern in the window.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _count_in_token_window(\n    self, window: list[str] | list[Token] | Doc | Span, pattern: list | str\n) -&gt; int:\n    \"\"\"Choose function for counting matches in token windows.\n\n    Args:\n        window (list[str] | Span): A window consisting of a list of token strings, a list of spaCy spans, or a spaCy doc.\n        pattern (list | str): A string pattern or spaCy rule to search for.\n\n    Returns:\n        The number of occurrences of the pattern in the window.\n    \"\"\"\n    # Validate window type for multi_token and spacy_rule modes\n    if self.mode in [\"multi_token\", \"spacy_rule\"]:\n        if not isinstance(window, (Doc, Span)):\n            raise LexosException(\n                \"You cannot use spaCy rules or perform multi-token searches with a string or list of token strings.\"\n            )\n    if isinstance(window, (list)) and self.mode in [\"multi_token\", \"spacy_rule\"]:\n        raise LexosException(\n            \"You cannot use spaCy rules or perform multi-token searches with a string or list of token strings.\"\n        )\n    elif isinstance(window, list) and all(isinstance(i, str) for i in window):\n        # Match in single tokens\n        return self._count_token_patterns_in_token_lists(window, pattern)\n    elif isinstance(window, Doc | Span):\n        # Iterate over the full text with token boundary alignment\n        if self.mode.startswith(\"multi_token\"):\n            return self._count_token_patterns_in_span_text(window, pattern)\n        # Match in single tokens\n        else:\n            return self._count_token_patterns_in_span(window, pattern)\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._extract_string_pattern","title":"<code>_extract_string_pattern(pattern: list[list[dict[str, Any]]]) -&gt; str</code>","text":"<p>Extract a string pattern from a spaCy rule.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>list[list[dict[str, Any]]]</code> <p>A list of spaCy rule patterns to search.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string pattern.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _extract_string_pattern(self, pattern: list[list[dict[str, Any]]]) -&gt; str:\n    \"\"\"Extract a string pattern from a spaCy rule.\n\n    Args:\n        pattern (list[list[dict[str, Any]]]): A list of spaCy rule patterns to search.\n\n    Returns:\n        str: A string pattern.\n    \"\"\"\n    return \"|\".join(\n        [\n            item if isinstance(item, str) else list(item.values())[0]\n            for item in list(flatten(pattern))\n        ]\n    )\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._get_window_count","title":"<code>_get_window_count(window: list[str] | Span | str, pattern: list | str) -&gt; int</code>","text":"<p>Call character or token window methods, as appropriate.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>list[str] | Span | str]</code> <p>A window consisting of a list of token strings, a list of spaCy spans, a spaCy doc, or a string.</p> required <code>pattern</code> <code>list | str</code> <p>A string pattern or spaCy rule to search for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of occurrences of the pattern in the window.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _get_window_count(\n    self, window: list[str] | Span | str, pattern: list | str\n) -&gt; int:\n    \"\"\"Call character or token window methods, as appropriate.\n\n    Args:\n        window (list[str] | Span | str]): A window consisting of a list of token strings, a list of spaCy spans, a spaCy doc, or a string.\n        pattern (list | str): A string pattern or spaCy rule to search for.\n\n    Returns:\n        The number of occurrences of the pattern in the window.\n    \"\"\"\n    if self.window_type == \"characters\":\n        return self._count_in_character_window(window, pattern)\n    else:\n        return self._count_in_token_window(window, pattern)\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator._set_attrs","title":"<code>_set_attrs(attrs: dict) -&gt; None</code>","text":"<p>Set instance attributes when public method is called.</p> <p>Parameters:</p> Name Type Description Default <code>attrs</code> <code>dict</code> <p>A dict of keyword arguments and their values.</p> required Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>def _set_attrs(self, attrs: dict) -&gt; None:\n    \"\"\"Set instance attributes when public method is called.\n\n    Args:\n        attrs (dict): A dict of keyword arguments and their values.\n    \"\"\"\n    for key, value in attrs.items():\n        if value is not None:\n            setattr(self, key, value)\n        if key == \"model\" and value is not None:\n            self.nlp = spacy.load(self.model)\n</code></pre>"},{"location":"api/rolling_windows/calculators/base_calculator/#lexos.rolling_windows.calculators.base_calculator.BaseCalculator.to_df","title":"<code>to_df(*args, **kwargs) -&gt; pd.DataFrame</code>  <code>abstractmethod</code>","text":"<p>Output the calcualtions as a pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/base_calculator.py</code> <pre><code>@abstractmethod\ndef to_df(self, *args, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Output the calcualtions as a pandas DataFrame.\"\"\"\n    ...\n</code></pre>"},{"location":"api/rolling_windows/calculators/counts/","title":"Counts","text":""},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts","title":"<code>Counts</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseCalculator</code></p> <p>A calculator for counting patterns in rolling windows.</p> <p>Config:</p> <ul> <li><code>default</code>: <code>validation_config</code></li> </ul> <p>Fields:</p> <ul> <li> <code>patterns</code>                 (<code>Optional[list | str]</code>)             </li> <li> <code>windows</code>                 (<code>Optional[Windows]</code>)             </li> <li> <code>mode</code>                 (<code>Optional[bool | str]</code>)             </li> <li> <code>case_sensitive</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>alignment_mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>model</code>                 (<code>Optional[str]</code>)             </li> <li> <code>nlp</code>                 (<code>Optional[Language]</code>)             </li> <li> <code>data</code>                 (<code>Optional[list]</code>)             </li> </ul> Source code in <code>lexos/rolling_windows/calculators/counts.py</code> <pre><code>class Counts(BaseCalculator):\n    \"\"\"A calculator for counting patterns in rolling windows.\"\"\"\n\n    _id: ClassVar[str] = \"counts\"\n\n    @validate_call(config=validation_config)\n    def __call__(\n        self,\n        patterns: Optional[list | str] = None,\n        windows: Optional[Windows] = None,\n        mode: Optional[bool | str] = None,\n        case_sensitive: Optional[bool] = None,\n        alignment_mode: Optional[str] = None,\n        model: Optional[str] = None,\n    ):\n        \"\"\"Call the calculator.\"\"\"\n        attrs = {\n            \"patterns\": patterns,\n            \"windows\": windows,\n            \"mode\": mode,\n            \"case_sensitive\": case_sensitive,\n            \"alignment_mode\": alignment_mode,\n            \"model\": model,\n        }\n        self._set_attrs(attrs)\n        if self.windows is not None:\n            self.data = [\n                [self._get_window_count(window, pattern) for pattern in self.patterns]\n                for window in self.windows\n            ]\n            return self.data\n        else:\n            raise LexosException(\"Calculator `windows` attribute is empty.\")\n\n    @validate_call(config=validation_config)\n    def to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n        \"\"\"Convert the data to a pandas dataframe.\n\n        Args:\n            show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n        Returns:\n            pd.DataFrame: A pandas DataFrame.\n        \"\"\"\n        if show_spacy_rules:\n            patterns = self.patterns\n        else:\n            patterns = []\n            # Extract strings from spaCy rules\n            for pattern in self.patterns:\n                if isinstance(pattern, list):\n                    patterns.append(self._extract_string_pattern(pattern))\n                else:\n                    patterns.append(pattern)\n        # Assign column labels\n        cols = []\n        for pattern in patterns:\n            if not self.case_sensitive and isinstance(pattern, str):\n                pattern = pattern.lower()\n            elif not self.case_sensitive and isinstance(pattern, list):\n                pattern = str(spacy_rule_to_lower(pattern))\n            cols.append(str(pattern))\n        # Generate dataframe\n        return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.alignment_mode","title":"<code>alignment_mode: Optional[str] = 'strict'</code>  <code>pydantic-field</code>","text":"<p>Whether to snap searches to token boundaries. Values are 'strict', 'contract', and 'expand'.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.case_sensitive","title":"<code>case_sensitive: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to make searches case-sensitive.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.data","title":"<code>data: Optional[list] = []</code>  <code>pydantic-field</code>","text":"<p>A container for the calculated data.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata for the calculator.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.mode","title":"<code>mode: Optional[bool | str] = 'exact'</code>  <code>pydantic-field</code>","text":"<p>The search method to use ('regex', 'spacy_rule', 'multi_token', 'multi_token_exact').</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The language model to be used for searching spaCy tokens/spans.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.n","title":"<code>n</code>  <code>property</code>","text":"<p>Get the number of units per window.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.nlp","title":"<code>nlp: Optional[Language] = None</code>  <code>pydantic-field</code>","text":"<p>The spaCy nlp object.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.patterns","title":"<code>patterns: Optional[list | str] = None</code>  <code>pydantic-field</code>","text":"<p>A pattern or list of patterns to search in windows.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.regex_flags","title":"<code>regex_flags</code>  <code>property</code>","text":"<p>Return regex flags based on case_sensitive setting.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.window_type","title":"<code>window_type</code>  <code>property</code>","text":"<p>Get the type of units in the windows.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.windows","title":"<code>windows: Optional[Windows] = None</code>  <code>pydantic-field</code>","text":"<p>A Windows object containing the windows to search.</p>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.__call__","title":"<code>__call__(patterns: Optional[list | str] = None, windows: Optional[Windows] = None, mode: Optional[bool | str] = None, case_sensitive: Optional[bool] = None, alignment_mode: Optional[str] = None, model: Optional[str] = None)</code>","text":"<p>Call the calculator.</p> Source code in <code>lexos/rolling_windows/calculators/counts.py</code> <pre><code>@validate_call(config=validation_config)\ndef __call__(\n    self,\n    patterns: Optional[list | str] = None,\n    windows: Optional[Windows] = None,\n    mode: Optional[bool | str] = None,\n    case_sensitive: Optional[bool] = None,\n    alignment_mode: Optional[str] = None,\n    model: Optional[str] = None,\n):\n    \"\"\"Call the calculator.\"\"\"\n    attrs = {\n        \"patterns\": patterns,\n        \"windows\": windows,\n        \"mode\": mode,\n        \"case_sensitive\": case_sensitive,\n        \"alignment_mode\": alignment_mode,\n        \"model\": model,\n    }\n    self._set_attrs(attrs)\n    if self.windows is not None:\n        self.data = [\n            [self._get_window_count(window, pattern) for pattern in self.patterns]\n            for window in self.windows\n        ]\n        return self.data\n    else:\n        raise LexosException(\"Calculator `windows` attribute is empty.\")\n</code></pre>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.to_df","title":"<code>to_df(show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Convert the data to a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>show_spacy_rules</code> <code>Optional[bool]</code> <p>If True, use full spaCy rules for labels; otherwise use only the string pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/counts.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"Convert the data to a pandas dataframe.\n\n    Args:\n        show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame.\n    \"\"\"\n    if show_spacy_rules:\n        patterns = self.patterns\n    else:\n        patterns = []\n        # Extract strings from spaCy rules\n        for pattern in self.patterns:\n            if isinstance(pattern, list):\n                patterns.append(self._extract_string_pattern(pattern))\n            else:\n                patterns.append(pattern)\n    # Assign column labels\n    cols = []\n    for pattern in patterns:\n        if not self.case_sensitive and isinstance(pattern, str):\n            pattern = pattern.lower()\n        elif not self.case_sensitive and isinstance(pattern, list):\n            pattern = str(spacy_rule_to_lower(pattern))\n        cols.append(str(pattern))\n    # Generate dataframe\n    return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.__call__","title":"<code>__call__(patterns: Optional[list | str] = None, windows: Optional[Windows] = None, mode: Optional[bool | str] = None, case_sensitive: Optional[bool] = None, alignment_mode: Optional[str] = None, model: Optional[str] = None)</code>","text":"<p>Call the calculator.</p> Source code in <code>lexos/rolling_windows/calculators/counts.py</code> <pre><code>@validate_call(config=validation_config)\ndef __call__(\n    self,\n    patterns: Optional[list | str] = None,\n    windows: Optional[Windows] = None,\n    mode: Optional[bool | str] = None,\n    case_sensitive: Optional[bool] = None,\n    alignment_mode: Optional[str] = None,\n    model: Optional[str] = None,\n):\n    \"\"\"Call the calculator.\"\"\"\n    attrs = {\n        \"patterns\": patterns,\n        \"windows\": windows,\n        \"mode\": mode,\n        \"case_sensitive\": case_sensitive,\n        \"alignment_mode\": alignment_mode,\n        \"model\": model,\n    }\n    self._set_attrs(attrs)\n    if self.windows is not None:\n        self.data = [\n            [self._get_window_count(window, pattern) for pattern in self.patterns]\n            for window in self.windows\n        ]\n        return self.data\n    else:\n        raise LexosException(\"Calculator `windows` attribute is empty.\")\n</code></pre>"},{"location":"api/rolling_windows/calculators/counts/#lexos.rolling_windows.calculators.counts.Counts.to_df","title":"<code>to_df(show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Convert the data to a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>show_spacy_rules</code> <code>Optional[bool]</code> <p>If True, use full spaCy rules for labels; otherwise use only the string pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/counts.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"Convert the data to a pandas dataframe.\n\n    Args:\n        show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame.\n    \"\"\"\n    if show_spacy_rules:\n        patterns = self.patterns\n    else:\n        patterns = []\n        # Extract strings from spaCy rules\n        for pattern in self.patterns:\n            if isinstance(pattern, list):\n                patterns.append(self._extract_string_pattern(pattern))\n            else:\n                patterns.append(pattern)\n    # Assign column labels\n    cols = []\n    for pattern in patterns:\n        if not self.case_sensitive and isinstance(pattern, str):\n            pattern = pattern.lower()\n        elif not self.case_sensitive and isinstance(pattern, list):\n            pattern = str(spacy_rule_to_lower(pattern))\n        cols.append(str(pattern))\n    # Generate dataframe\n    return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/ratios/","title":"Ratios","text":""},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios","title":"<code>Ratios</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>Counts</code></p> <p>A calculator for calculating ratios of counts in rolling windows.</p> <p>Fields:</p> <ul> <li> <code>patterns</code>                 (<code>Optional[list | str]</code>)             </li> <li> <code>windows</code>                 (<code>Optional[Windows]</code>)             </li> <li> <code>mode</code>                 (<code>Optional[bool | str]</code>)             </li> <li> <code>case_sensitive</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>alignment_mode</code>                 (<code>Optional[str]</code>)             </li> <li> <code>model</code>                 (<code>Optional[str]</code>)             </li> <li> <code>nlp</code>                 (<code>Optional[Language]</code>)             </li> <li> <code>data</code>                 (<code>Optional[list]</code>)             </li> </ul> Source code in <code>lexos/rolling_windows/calculators/ratios.py</code> <pre><code>class Ratios(Counts):\n    \"\"\"A calculator for calculating ratios of counts in rolling windows.\"\"\"\n\n    _id: ClassVar[str] = \"ratios\"\n\n    def __call__(\n        self,\n        patterns: Optional[list | str] = None,\n        windows: Optional[Windows] = None,\n        mode: Optional[bool] = None,\n        case_sensitive: Optional[bool] = None,\n        alignment_mode: Optional[str] = None,\n        model: Optional[str] = None,\n    ):\n        \"\"\"Call the calculator.\"\"\"\n        attrs = {\n            \"patterns\": patterns,\n            \"windows\": windows,\n            \"mode\": mode,\n            \"case_sensitive\": case_sensitive,\n            \"alignment_mode\": alignment_mode,\n            \"model\": model,\n        }\n        self._set_attrs(attrs)\n        if not isinstance(self.patterns, list):\n            raise LexosException(\n                \"You must supply a list of two patterns to calculate ratios.\"\n            )\n        if len(self.patterns) != 2:\n            raise LexosException(\"You can only calculate ratios for two patterns.\")\n        if self.windows is not None:\n            self.data = [\n                self._get_ratio(\n                    [\n                        self._get_window_count(window, pattern)\n                        for pattern in self.patterns\n                    ]\n                )\n                for window in self.windows\n            ]\n            return self.data\n        else:\n            raise LexosException(\"Calculator `windows` attribute is empty.\")\n\n    def _get_ratio(self, counts: list[int]) -&gt; float:\n        \"\"\"Calculate the ratio between two counts.\n\n        Args:\n            counts (List[int]): A list of two counts.\n\n        Returns:\n            The calculated ratio.\n        \"\"\"\n        numerator = counts[0]\n        denominator = counts[1]\n        # Handle division by 0\n        if denominator + numerator == 0:\n            return 0.0\n        else:\n            return numerator / (denominator + numerator)\n\n    @validate_call(config=validation_config)\n    def to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n        \"\"\"Convert the data to a pandas dataframe.\n\n        Args:\n            show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n        Returns:\n                pd.DataFrame: A pandas DataFrame.\n        \"\"\"\n        if show_spacy_rules:\n            patterns = self.patterns\n        else:\n            patterns = []\n            # Extract strings from spaCy rules\n            for pattern in self.patterns:\n                if isinstance(pattern, list):\n                    patterns.append(self._extract_string_pattern(pattern))\n                else:\n                    patterns.append(pattern)\n        # Assign column labels\n        cols = []\n        for pattern in patterns:\n            if not self.case_sensitive and isinstance(pattern, str):\n                pattern = pattern.lower()\n            elif not self.case_sensitive and isinstance(pattern, list):\n                pattern = str(spacy_rule_to_lower(pattern))\n            cols.append(str(pattern))\n        # Merge columns for ratios\n        cols = [\":\".join(cols)]\n        # Generate dataframe\n        return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.alignment_mode","title":"<code>alignment_mode: Optional[str] = 'strict'</code>  <code>pydantic-field</code>","text":"<p>Whether to snap searches to token boundaries. Values are 'strict', 'contract', and 'expand'.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.case_sensitive","title":"<code>case_sensitive: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to make searches case-sensitive.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.data","title":"<code>data: Optional[list] = []</code>  <code>pydantic-field</code>","text":"<p>A container for the calculated data.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata for the calculator.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.mode","title":"<code>mode: Optional[bool | str] = 'exact'</code>  <code>pydantic-field</code>","text":"<p>The search method to use ('regex', 'spacy_rule', 'multi_token', 'multi_token_exact').</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The language model to be used for searching spaCy tokens/spans.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.n","title":"<code>n</code>  <code>property</code>","text":"<p>Get the number of units per window.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.nlp","title":"<code>nlp: Optional[Language] = None</code>  <code>pydantic-field</code>","text":"<p>The spaCy nlp object.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.patterns","title":"<code>patterns: Optional[list | str] = None</code>  <code>pydantic-field</code>","text":"<p>A pattern or list of patterns to search in windows.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.regex_flags","title":"<code>regex_flags</code>  <code>property</code>","text":"<p>Return regex flags based on case_sensitive setting.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.window_type","title":"<code>window_type</code>  <code>property</code>","text":"<p>Get the type of units in the windows.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.windows","title":"<code>windows: Optional[Windows] = None</code>  <code>pydantic-field</code>","text":"<p>A Windows object containing the windows to search.</p>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.__call__","title":"<code>__call__(patterns: Optional[list | str] = None, windows: Optional[Windows] = None, mode: Optional[bool] = None, case_sensitive: Optional[bool] = None, alignment_mode: Optional[str] = None, model: Optional[str] = None)</code>","text":"<p>Call the calculator.</p> Source code in <code>lexos/rolling_windows/calculators/ratios.py</code> <pre><code>def __call__(\n    self,\n    patterns: Optional[list | str] = None,\n    windows: Optional[Windows] = None,\n    mode: Optional[bool] = None,\n    case_sensitive: Optional[bool] = None,\n    alignment_mode: Optional[str] = None,\n    model: Optional[str] = None,\n):\n    \"\"\"Call the calculator.\"\"\"\n    attrs = {\n        \"patterns\": patterns,\n        \"windows\": windows,\n        \"mode\": mode,\n        \"case_sensitive\": case_sensitive,\n        \"alignment_mode\": alignment_mode,\n        \"model\": model,\n    }\n    self._set_attrs(attrs)\n    if not isinstance(self.patterns, list):\n        raise LexosException(\n            \"You must supply a list of two patterns to calculate ratios.\"\n        )\n    if len(self.patterns) != 2:\n        raise LexosException(\"You can only calculate ratios for two patterns.\")\n    if self.windows is not None:\n        self.data = [\n            self._get_ratio(\n                [\n                    self._get_window_count(window, pattern)\n                    for pattern in self.patterns\n                ]\n            )\n            for window in self.windows\n        ]\n        return self.data\n    else:\n        raise LexosException(\"Calculator `windows` attribute is empty.\")\n</code></pre>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.to_df","title":"<code>to_df(show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Convert the data to a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>show_spacy_rules</code> <code>Optional[bool]</code> <p>If True, use full spaCy rules for labels; otherwise use only the string pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/ratios.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"Convert the data to a pandas dataframe.\n\n    Args:\n        show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n    Returns:\n            pd.DataFrame: A pandas DataFrame.\n    \"\"\"\n    if show_spacy_rules:\n        patterns = self.patterns\n    else:\n        patterns = []\n        # Extract strings from spaCy rules\n        for pattern in self.patterns:\n            if isinstance(pattern, list):\n                patterns.append(self._extract_string_pattern(pattern))\n            else:\n                patterns.append(pattern)\n    # Assign column labels\n    cols = []\n    for pattern in patterns:\n        if not self.case_sensitive and isinstance(pattern, str):\n            pattern = pattern.lower()\n        elif not self.case_sensitive and isinstance(pattern, list):\n            pattern = str(spacy_rule_to_lower(pattern))\n        cols.append(str(pattern))\n    # Merge columns for ratios\n    cols = [\":\".join(cols)]\n    # Generate dataframe\n    return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.__call__","title":"<code>__call__(patterns: Optional[list | str] = None, windows: Optional[Windows] = None, mode: Optional[bool] = None, case_sensitive: Optional[bool] = None, alignment_mode: Optional[str] = None, model: Optional[str] = None)</code>","text":"<p>Call the calculator.</p> Source code in <code>lexos/rolling_windows/calculators/ratios.py</code> <pre><code>def __call__(\n    self,\n    patterns: Optional[list | str] = None,\n    windows: Optional[Windows] = None,\n    mode: Optional[bool] = None,\n    case_sensitive: Optional[bool] = None,\n    alignment_mode: Optional[str] = None,\n    model: Optional[str] = None,\n):\n    \"\"\"Call the calculator.\"\"\"\n    attrs = {\n        \"patterns\": patterns,\n        \"windows\": windows,\n        \"mode\": mode,\n        \"case_sensitive\": case_sensitive,\n        \"alignment_mode\": alignment_mode,\n        \"model\": model,\n    }\n    self._set_attrs(attrs)\n    if not isinstance(self.patterns, list):\n        raise LexosException(\n            \"You must supply a list of two patterns to calculate ratios.\"\n        )\n    if len(self.patterns) != 2:\n        raise LexosException(\"You can only calculate ratios for two patterns.\")\n    if self.windows is not None:\n        self.data = [\n            self._get_ratio(\n                [\n                    self._get_window_count(window, pattern)\n                    for pattern in self.patterns\n                ]\n            )\n            for window in self.windows\n        ]\n        return self.data\n    else:\n        raise LexosException(\"Calculator `windows` attribute is empty.\")\n</code></pre>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios._get_ratio","title":"<code>_get_ratio(counts: list[int]) -&gt; float</code>","text":"<p>Calculate the ratio between two counts.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>List[int]</code> <p>A list of two counts.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The calculated ratio.</p> Source code in <code>lexos/rolling_windows/calculators/ratios.py</code> <pre><code>def _get_ratio(self, counts: list[int]) -&gt; float:\n    \"\"\"Calculate the ratio between two counts.\n\n    Args:\n        counts (List[int]): A list of two counts.\n\n    Returns:\n        The calculated ratio.\n    \"\"\"\n    numerator = counts[0]\n    denominator = counts[1]\n    # Handle division by 0\n    if denominator + numerator == 0:\n        return 0.0\n    else:\n        return numerator / (denominator + numerator)\n</code></pre>"},{"location":"api/rolling_windows/calculators/ratios/#lexos.rolling_windows.calculators.ratios.Ratios.to_df","title":"<code>to_df(show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame</code>","text":"<p>Convert the data to a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>show_spacy_rules</code> <code>Optional[bool]</code> <p>If True, use full spaCy rules for labels; otherwise use only the string pattern.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame.</p> Source code in <code>lexos/rolling_windows/calculators/ratios.py</code> <pre><code>@validate_call(config=validation_config)\ndef to_df(self, show_spacy_rules: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"Convert the data to a pandas dataframe.\n\n    Args:\n        show_spacy_rules (Optional[bool]): If True, use full spaCy rules for labels; otherwise use only the string pattern.\n\n    Returns:\n            pd.DataFrame: A pandas DataFrame.\n    \"\"\"\n    if show_spacy_rules:\n        patterns = self.patterns\n    else:\n        patterns = []\n        # Extract strings from spaCy rules\n        for pattern in self.patterns:\n            if isinstance(pattern, list):\n                patterns.append(self._extract_string_pattern(pattern))\n            else:\n                patterns.append(pattern)\n    # Assign column labels\n    cols = []\n    for pattern in patterns:\n        if not self.case_sensitive and isinstance(pattern, str):\n            pattern = pattern.lower()\n        elif not self.case_sensitive and isinstance(pattern, list):\n            pattern = str(spacy_rule_to_lower(pattern))\n        cols.append(str(pattern))\n    # Merge columns for ratios\n    cols = [\":\".join(cols)]\n    # Generate dataframe\n    return pd.DataFrame(self.data, columns=cols)\n</code></pre>"},{"location":"api/rolling_windows/plotters/","title":"Rolling Windows Plotters","text":"<p>The <code>rolling_windows</code> module has two built-in plotter classes, simple_plotter and plotly_plotter. Custom plotters should inherit from base_plotter.</p>"},{"location":"api/rolling_windows/plotters/base_plotter/","title":"Base Plotter","text":""},{"location":"api/rolling_windows/plotters/base_plotter/#lexos.rolling_windows.plotters.base_plotter.BasePlotter","title":"<code>BasePlotter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>BasePlotter class to enable type hinting and validation.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>extra</code>: <code>allow</code></li> </ul> Source code in <code>lexos/rolling_windows/plotters/base_plotter.py</code> <pre><code>class BasePlotter(BaseModel):\n    \"\"\"BasePlotter class to enable type hinting and validation.\"\"\"\n\n    id: ClassVar[str] = \"base_plotter\"\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"allow\")\n\n    @property\n    def metadata(self) -&gt; dict:\n        \"\"\"Return metadata about the object.\n\n        Returns:\n            dict: A dictionary containing metadata about the object.\n        \"\"\"\n        # Note: model_dump() may evaluate computed fields on this model.\n        # If computed properties rely on external state or are expensive to\n        # compute, calling model_dump() may cause unexpected exceptions or\n        # performance issues. Subclasses should exclude costly computed\n        # fields by overriding this property or explicitly excluding fields\n        # when calling model_dump().\n        return self.model_dump()\n\n    def _set_attrs(self, **kwargs) -&gt; None:\n        \"\"\"Set instance attributes when public method is called.\n\n        Args:\n            kwargs (dict): A dict of keyword arguments and their values.\n        \"\"\"\n        for key, value in kwargs.items():\n            if value is not None:\n                setattr(self, key, value)\n</code></pre>"},{"location":"api/rolling_windows/plotters/base_plotter/#lexos.rolling_windows.plotters.base_plotter.BasePlotter.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata about the object.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing metadata about the object.</p>"},{"location":"api/rolling_windows/plotters/base_plotter/#lexos.rolling_windows.plotters.base_plotter.BasePlotter.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata about the object.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing metadata about the object.</p>"},{"location":"api/rolling_windows/plotters/base_plotter/#lexos.rolling_windows.plotters.base_plotter.BasePlotter._set_attrs","title":"<code>_set_attrs(**kwargs) -&gt; None</code>","text":"<p>Set instance attributes when public method is called.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>A dict of keyword arguments and their values.</p> <code>{}</code> Source code in <code>lexos/rolling_windows/plotters/base_plotter.py</code> <pre><code>def _set_attrs(self, **kwargs) -&gt; None:\n    \"\"\"Set instance attributes when public method is called.\n\n    Args:\n        kwargs (dict): A dict of keyword arguments and their values.\n    \"\"\"\n    for key, value in kwargs.items():\n        if value is not None:\n            setattr(self, key, value)\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/","title":"Plotly Plotter","text":""},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.MilestonesModel","title":"<code>MilestonesModel</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for the milestone labels and their positions on the x axis.</p> <p>Ensures that milestone labels exist, are properly structured, and valid.</p> <p>Fields:</p> <ul> <li> <code>milestone_labels</code>                 (<code>dict[str, int]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>check_empty_dict</code> </li> </ul> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>class MilestonesModel(BaseModel):\n    \"\"\"Model for the milestone labels and their positions on the x axis.\n\n    Ensures that milestone labels exist, are properly structured, and valid.\n    \"\"\"\n\n    milestone_labels: dict[str, int]\n\n    @model_validator(mode=\"after\")\n    def check_empty_dict(self):\n        \"\"\"Check that the milestone_labels dict is not empty.\"\"\"\n        if not self.milestone_labels or len(self.milestone_labels) == 0:\n            raise ValueError(\"`milestone_labels` dictionary is empty.\")\n        return self\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.MilestonesModel.check_empty_dict","title":"<code>check_empty_dict()</code>  <code>pydantic-validator</code>","text":"<p>Check that the milestone_labels dict is not empty.</p> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_empty_dict(self):\n    \"\"\"Check that the milestone_labels dict is not empty.\"\"\"\n    if not self.milestone_labels or len(self.milestone_labels) == 0:\n        raise ValueError(\"`milestone_labels` dictionary is empty.\")\n    return self\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter","title":"<code>PlotlyPlotter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BasePlotter</code></p> <p>Plotter using Plotly.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>validate_assignment</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>df</code>                 (<code>DataFrame</code>)             </li> <li> <code>width</code>                 (<code>Optional[int]</code>)             </li> <li> <code>height</code>                 (<code>Optional[int]</code>)             </li> <li> <code>title</code>                 (<code>Optional[dict | str]</code>)             </li> <li> <code>xlabel</code>                 (<code>Optional[str]</code>)             </li> <li> <code>ylabel</code>                 (<code>Optional[str]</code>)             </li> <li> <code>line_color</code>                 (<code>Optional[list[str] | str]</code>)             </li> <li> <code>showlegend</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>titlepad</code>                 (<code>Optional[float]</code>)             </li> <li> <code>show_milestones</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>milestone_marker_style</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>show_milestone_labels</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>milestone_labels</code>                 (<code>Optional[dict[str, int]]</code>)             </li> <li> <code>milestone_label_rotation</code>                 (<code>Optional[float]</code>)             </li> <li> <code>milestone_label_style</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>is_valid_rotation</code>                 \u2192                   <code>milestone_label_rotation</code> </li> </ul> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>class PlotlyPlotter(BasePlotter):\n    \"\"\"Plotter using Plotly.\"\"\"\n\n    id: ClassVar[str] = \"rw_plotly_plotter\"\n\n    df: pd.DataFrame = Field(\n        ..., description=\"A dataframe containing the data to plot.\"\n    )\n    width: Optional[int] = Field(\n        default=700, description=\"The width of the plot in pixels.\"\n    )\n    height: Optional[int] = Field(\n        default=450,\n        description=\"The height of the plot in pixels. Note that if you change the height, you will need to adjust the `titelpad` manually to show the title above milestone labels.\",\n    )\n    title: Optional[dict | str] = Field(\n        default=\"Rolling Windows Plot\",\n        description=\"The title to use for the plot. It can be styled with a dict containing any of the keywords listed at https://plotly.com/python/reference/layout/#layout-title.\",\n    )\n    xlabel: Optional[str] = Field(\n        default=\"Token Count\", description=\"The text to display along the x axis.\"\n    )\n    ylabel: Optional[str] = Field(\n        default=\"Average Frequency\", description=\"The text to display along the y axis.\"\n    )\n    line_color: Optional[list[str] | str] = Field(\n        default=px.colors.qualitative.Plotly,\n        description=\"The colour pattern to use for lines on the plot.\",\n    )\n    showlegend: Optional[bool] = Field(default=True, description=\"Show the legend.\")\n    titlepad: Optional[float] = Field(\n        default=None,\n        description=\"The margin in pixels between the title and the top of the plot.\",\n    )\n    show_milestones: Optional[bool] = Field(\n        default=False, description=\"Whether to show the milestone markers.\"\n    )\n    milestone_marker_style: Optional[dict] = Field(\n        default={\"width\": 1, \"color\": \"teal\"},\n        description=\"A dict containing the styles to apply to the milestone marker. For valid properties, see https://plotly.com/python-api-reference/generated/plotly.graph_objects.layout.shape.html#plotly.graph_objects.layout.shape.Line.\",\n    )\n    show_milestone_labels: Optional[bool] = Field(\n        default=False, description=\"Whether to show the milestone labels.\"\n    )\n    milestone_labels: Optional[dict[str, int]] = Field(\n        default=None,\n        description=\"A dict containing the milestone labels and their values on the x-axis.\",\n    )\n    milestone_label_rotation: Optional[float] = Field(\n        default=0.0,\n        description=\"The number of degrees clockwise to rotate the milestone labels (maximum 90).\",\n    )\n    milestone_label_style: Optional[dict] = Field(\n        default={\n            \"size\": 10.0,\n            \"family\": \"Open Sans, verdana, arial, sans-serif\",\n            \"color\": \"teal\",\n        },\n        description=\"A dict containing the styling information for the milestone labels. For valid properties, see https://plotly.com/python/reference/layout/annotations/#layout-annotations-items-annotation-font.\",\n    )\n    fig: Optional[Figure] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, validate_assignment=True)\n\n    @field_validator(\"milestone_label_rotation\", mode=\"after\")\n    @classmethod\n    def is_valid_rotation(cls, value: float) -&gt; float:\n        \"\"\"Ensure that the milestone label rotation is between 0 and 90 degrees.\"\"\"\n        if value &gt; 90:\n            raise LexosException(\n                \"Milestone labels can only be rotated clockwise a maximum of 90 degrees.\"\n            )\n        return value\n\n    def _validate_edge_cases(self) -&gt; None:\n        \"\"\"Validate edge cases for the PlotlyPlotter.\"\"\"\n        if self.show_milestones is True or self.show_milestone_labels is True:\n            try:\n                MilestonesModel(milestone_labels=self.milestone_labels)\n            except ValidationError:\n                if not self.milestone_labels or len(self.milestone_labels) == 0:\n                    raise LexosException(\"`milestone_labels` dictionary is empty.\")\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"Initialise the instance with arbitrary keywords.\"\"\"\n        super().__init__(**kwargs)\n        self._validate_edge_cases()\n\n        # Massage the DataFrame for Plotly Express\n        self.df[\"id\"] = self.df.index\n\n    @validate_call(config=model_config)\n    def plot(\n        self, show: Optional[bool] = True, config: Optional[dict] = None, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Initialise object.\n\n        Args:\n            show (Optional[bool]): Whether to display the plot immediately.\n            config (Optional[dict]): A dictionary supply Plotly configuration values.\n            **kwargs (Any): Additional keyword arguments accepted by plotly.express.line.\n\n        \"\"\"\n        # Massage the DataFrame for Plotly Express\n        df = self.df.copy()\n        df[\"id\"] = df.index\n        df = pd.melt(df, id_vars=\"id\", value_vars=df.columns[:-1])\n\n        # Create plotly line figure\n        self.fig = px.line(\n            df,\n            x=\"id\",\n            y=\"value\",\n            color=\"variable\",\n            color_discrete_sequence=ensure_list(self.line_color),\n            width=self.width,\n            height=self.height,\n        )\n\n        title_dict, xlabel_dict, ylabel_dict = self._get_axis_and_title_labels()\n        self.fig.update_layout(\n            title=title_dict,\n            xaxis=xlabel_dict,\n            yaxis=ylabel_dict,\n            showlegend=self.showlegend,\n            **kwargs,\n        )\n\n        # Show milestones\n        if self.show_milestones:\n            # Add milestones using absolute references\n            for label, x in self.milestone_labels.items():\n                df_val_min = df[\"value\"].min() * 1.2\n                df_val_max = df[\"value\"].max() * 1.2\n                self._plot_milestone_marker(x, df_val_min, df_val_max)\n                if self.show_milestone_labels:\n                    self._plot_milestone_label(label, x)\n\n        # Increase the margin from the top to accommodate the milestone labels\n        if self.show_milestone_labels:\n            titlepad = self._get_titlepad(self.milestone_labels)\n            self.fig.update_layout(margin=dict(t=titlepad))\n\n        if show:\n            if not config:\n                config = {\"displaylogo\": False}\n            self.fig.show(config=config)\n\n    def _get_axis_and_title_labels(self) -&gt; tuple[bool, str]:\n        \"\"\"Ensure that the title, xlabel, and ylabel values are dicts.\"\"\"\n        if isinstance(self.title, str):\n            title_dict = dict(\n                text=self.title, y=0.9, x=0.5, xanchor=\"center\", yanchor=\"top\"\n            )\n        else:\n            title_dict = self.title\n        if isinstance(self.xlabel, str):\n            xlabel_dict = dict(title=self.xlabel)\n        else:\n            xlabel_dict = self.xlabel\n        if isinstance(self.ylabel, str):\n            ylabel_dict = dict(title=self.ylabel)\n        else:\n            ylabel_dict = self.ylabel\n        return title_dict, xlabel_dict, ylabel_dict\n\n    def _get_titlepad(self, labels: dict[str, int]) -&gt; float:\n        \"\"\"Get a titlepad value based on the height of the longest milestone label.\n\n        Args:\n            labels (dict[str, int]): A dict with the labels as keys.\n\n        Returns:\n            A float.\n        \"\"\"\n        if self.titlepad:\n            return self.titlepad\n        fontfamily = self.milestone_label_style[\"family\"]\n        if \",\" in self.milestone_label_style[\"family\"]:\n            fontfamily = self.milestone_label_style[\"family\"].split(\",\")\n            fontfamily = [x.strip() for x in fontfamily]\n        tmp_fig, tmp_ax = plt.subplots()\n        r = tmp_fig.canvas.get_renderer()\n        heights = []\n        for x in list(labels.keys()):\n            t = tmp_ax.annotate(\n                x,\n                xy=(0, 0),\n                xytext=(0, 0),\n                textcoords=\"offset points\",\n                rotation=self.milestone_label_rotation,\n                fontfamily=fontfamily,\n                fontsize=self.milestone_label_style[\"size\"],\n            )\n            bb = t.get_window_extent(renderer=r)\n            heights.append(bb.height)\n        plt.close()\n        if max(heights) &lt; 50:\n            return max(heights) + 75\n        else:\n            return max(heights) + 50\n\n    def _plot_milestone_label(self, label: str, x: int) -&gt; None:\n        \"\"\"Add a milestone label to the Plotly figure.\n\n        Args:\n            label (str): The label text.\n            x (int): The location on the x axis.\n        \"\"\"\n        self.fig.add_annotation(\n            x=x,\n            y=1,\n            xanchor=\"left\",\n            yanchor=\"bottom\",\n            xshift=-10,\n            yref=\"paper\",\n            showarrow=False,\n            text=label,\n            textangle=-self.milestone_label_rotation,\n            font=self.milestone_label_style,\n        )\n\n    def _plot_milestone_marker(\n        self, x: int, df_val_min: float | int, df_val_max: float | int\n    ) -&gt; None:\n        \"\"\"Add a milestone marker to the Plotly figure.\n\n        Args:\n            x (int): The location on the x axis.\n            df_val_min (float | int): The minimum value in the pandas DataFrame.\n            df_val_max (float | int): The maximum value in the pandas DataFrame.\n        \"\"\"\n        self.fig.add_shape(\n            type=\"line\",\n            yref=\"y\",\n            xref=\"x\",\n            x0=x,\n            y0=0,  # df_val_min,\n            x1=x,\n            y1=df_val_max,\n            line=self.milestone_marker_style,\n        )\n\n    @validate_call(config=model_config)\n    def save(self, path: Path | str, **kwargs: Any) -&gt; None:\n        \"\"\"Save the plot to a file.\n\n        Args:\n            path (Path | str): The path to the file to save.\n            **kwargs (Any): Additional keyword arguments accepted by plotly.io.write_html or plotly.io.write_image.\n        \"\"\"\n        if not self.fig:\n            raise LexosException(\n                \"There is no plot to save, try calling `plotter(data)`.\"\n            )\n        # Try first to save as HTML; if that doesn't work, try to save as a static image\n        if Path(path).suffix == \".html\":\n            self.fig.write_html(path, **kwargs)\n        else:\n            pio.write_image(self.fig, path)\n\n    @validate_call(config=model_config)\n    def show(self, config: Optional[dict] = None) -&gt; None:\n        \"\"\"Display a plot.\n\n        Args:\n            config (Optional[dict]): A dictionary supply Plotly configuration values.\n        \"\"\"\n        if not config:\n            config = {\"displaylogo\": False}\n        self.fig.show(config=config)\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.df","title":"<code>df: pd.DataFrame</code>  <code>pydantic-field</code>","text":"<p>A dataframe containing the data to plot.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.height","title":"<code>height: Optional[int] = 450</code>  <code>pydantic-field</code>","text":"<p>The height of the plot in pixels. Note that if you change the height, you will need to adjust the <code>titelpad</code> manually to show the title above milestone labels.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.line_color","title":"<code>line_color: Optional[list[str] | str] = px.colors.qualitative.Plotly</code>  <code>pydantic-field</code>","text":"<p>The colour pattern to use for lines on the plot.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata about the object.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing metadata about the object.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.milestone_label_rotation","title":"<code>milestone_label_rotation: Optional[float] = 0.0</code>  <code>pydantic-field</code>","text":"<p>The number of degrees clockwise to rotate the milestone labels (maximum 90).</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.milestone_label_style","title":"<code>milestone_label_style: Optional[dict] = {'size': 10.0, 'family': 'Open Sans, verdana, arial, sans-serif', 'color': 'teal'}</code>  <code>pydantic-field</code>","text":"<p>A dict containing the styling information for the milestone labels. For valid properties, see https://plotly.com/python/reference/layout/annotations/#layout-annotations-items-annotation-font.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.milestone_labels","title":"<code>milestone_labels: Optional[dict[str, int]] = None</code>  <code>pydantic-field</code>","text":"<p>A dict containing the milestone labels and their values on the x-axis.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.milestone_marker_style","title":"<code>milestone_marker_style: Optional[dict] = {'width': 1, 'color': 'teal'}</code>  <code>pydantic-field</code>","text":"<p>A dict containing the styles to apply to the milestone marker. For valid properties, see https://plotly.com/python-api-reference/generated/plotly.graph_objects.layout.shape.html#plotly.graph_objects.layout.shape.Line.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.show_milestone_labels","title":"<code>show_milestone_labels: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to show the milestone labels.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.show_milestones","title":"<code>show_milestones: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to show the milestone markers.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.showlegend","title":"<code>showlegend: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Show the legend.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.title","title":"<code>title: Optional[dict | str] = 'Rolling Windows Plot'</code>  <code>pydantic-field</code>","text":"<p>The title to use for the plot. It can be styled with a dict containing any of the keywords listed at https://plotly.com/python/reference/layout/#layout-title.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.titlepad","title":"<code>titlepad: Optional[float] = None</code>  <code>pydantic-field</code>","text":"<p>The margin in pixels between the title and the top of the plot.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.width","title":"<code>width: Optional[int] = 700</code>  <code>pydantic-field</code>","text":"<p>The width of the plot in pixels.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.xlabel","title":"<code>xlabel: Optional[str] = 'Token Count'</code>  <code>pydantic-field</code>","text":"<p>The text to display along the x axis.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.ylabel","title":"<code>ylabel: Optional[str] = 'Average Frequency'</code>  <code>pydantic-field</code>","text":"<p>The text to display along the y axis.</p>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.__init__","title":"<code>__init__(**kwargs) -&gt; None</code>","text":"<p>Initialise the instance with arbitrary keywords.</p> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"Initialise the instance with arbitrary keywords.\"\"\"\n    super().__init__(**kwargs)\n    self._validate_edge_cases()\n\n    # Massage the DataFrame for Plotly Express\n    self.df[\"id\"] = self.df.index\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.is_valid_rotation","title":"<code>is_valid_rotation(value: float) -&gt; float</code>  <code>pydantic-validator</code>","text":"<p>Ensure that the milestone label rotation is between 0 and 90 degrees.</p> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>@field_validator(\"milestone_label_rotation\", mode=\"after\")\n@classmethod\ndef is_valid_rotation(cls, value: float) -&gt; float:\n    \"\"\"Ensure that the milestone label rotation is between 0 and 90 degrees.\"\"\"\n    if value &gt; 90:\n        raise LexosException(\n            \"Milestone labels can only be rotated clockwise a maximum of 90 degrees.\"\n        )\n    return value\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.plot","title":"<code>plot(show: Optional[bool] = True, config: Optional[dict] = None, **kwargs: Any) -&gt; None</code>","text":"<p>Initialise object.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>Optional[bool]</code> <p>Whether to display the plot immediately.</p> <code>True</code> <code>config</code> <code>Optional[dict]</code> <p>A dictionary supply Plotly configuration values.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments accepted by plotly.express.line.</p> <code>{}</code> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>@validate_call(config=model_config)\ndef plot(\n    self, show: Optional[bool] = True, config: Optional[dict] = None, **kwargs: Any\n) -&gt; None:\n    \"\"\"Initialise object.\n\n    Args:\n        show (Optional[bool]): Whether to display the plot immediately.\n        config (Optional[dict]): A dictionary supply Plotly configuration values.\n        **kwargs (Any): Additional keyword arguments accepted by plotly.express.line.\n\n    \"\"\"\n    # Massage the DataFrame for Plotly Express\n    df = self.df.copy()\n    df[\"id\"] = df.index\n    df = pd.melt(df, id_vars=\"id\", value_vars=df.columns[:-1])\n\n    # Create plotly line figure\n    self.fig = px.line(\n        df,\n        x=\"id\",\n        y=\"value\",\n        color=\"variable\",\n        color_discrete_sequence=ensure_list(self.line_color),\n        width=self.width,\n        height=self.height,\n    )\n\n    title_dict, xlabel_dict, ylabel_dict = self._get_axis_and_title_labels()\n    self.fig.update_layout(\n        title=title_dict,\n        xaxis=xlabel_dict,\n        yaxis=ylabel_dict,\n        showlegend=self.showlegend,\n        **kwargs,\n    )\n\n    # Show milestones\n    if self.show_milestones:\n        # Add milestones using absolute references\n        for label, x in self.milestone_labels.items():\n            df_val_min = df[\"value\"].min() * 1.2\n            df_val_max = df[\"value\"].max() * 1.2\n            self._plot_milestone_marker(x, df_val_min, df_val_max)\n            if self.show_milestone_labels:\n                self._plot_milestone_label(label, x)\n\n    # Increase the margin from the top to accommodate the milestone labels\n    if self.show_milestone_labels:\n        titlepad = self._get_titlepad(self.milestone_labels)\n        self.fig.update_layout(margin=dict(t=titlepad))\n\n    if show:\n        if not config:\n            config = {\"displaylogo\": False}\n        self.fig.show(config=config)\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.save","title":"<code>save(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save the plot to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file to save.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments accepted by plotly.io.write_html or plotly.io.write_image.</p> <code>{}</code> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save the plot to a file.\n\n    Args:\n        path (Path | str): The path to the file to save.\n        **kwargs (Any): Additional keyword arguments accepted by plotly.io.write_html or plotly.io.write_image.\n    \"\"\"\n    if not self.fig:\n        raise LexosException(\n            \"There is no plot to save, try calling `plotter(data)`.\"\n        )\n    # Try first to save as HTML; if that doesn't work, try to save as a static image\n    if Path(path).suffix == \".html\":\n        self.fig.write_html(path, **kwargs)\n    else:\n        pio.write_image(self.fig, path)\n</code></pre>"},{"location":"api/rolling_windows/plotters/plotly_plotter/#lexos.rolling_windows.plotters.plotly_plotter.PlotlyPlotter.show","title":"<code>show(config: Optional[dict] = None) -&gt; None</code>","text":"<p>Display a plot.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[dict]</code> <p>A dictionary supply Plotly configuration values.</p> <code>None</code> Source code in <code>lexos/rolling_windows/plotters/plotly_plotter.py</code> <pre><code>@validate_call(config=model_config)\ndef show(self, config: Optional[dict] = None) -&gt; None:\n    \"\"\"Display a plot.\n\n    Args:\n        config (Optional[dict]): A dictionary supply Plotly configuration values.\n    \"\"\"\n    if not config:\n        config = {\"displaylogo\": False}\n    self.fig.show(config=config)\n</code></pre>"},{"location":"api/rolling_windows/plotters/simple_plotter/","title":"Simple Plotter","text":""},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.interpolate","title":"<code>interpolate(x: np.ndarray, y: np.ndarray, xx: np.ndarray, interpolation_kind: str = None) -&gt; np.ndarray</code>","text":"<p>Get interpolated points for plotting.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The x values</p> required <code>y</code> <code>ndarray</code> <p>The y values</p> required <code>xx</code> <code>ndarray</code> <p>The projected interpolation range</p> required <code>interpolation_kind</code> <code>str</code> <p>The interpolation function to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The interpolated points.</p> Note <p>The interpolation function may be either scipy.interpolate.pchip_interpolate, numpy.interp, or one of the options for scipy.interpolate.interp1d. Note however, that <code>scipy.interpolate.interp1d</code> is deprecated.</p> Source code in <code>lexos/rolling_windows/plotters/simple_plotter.py</code> <pre><code>def interpolate(\n    x: np.ndarray, y: np.ndarray, xx: np.ndarray, interpolation_kind: str = None\n) -&gt; np.ndarray:\n    \"\"\"Get interpolated points for plotting.\n\n    Args:\n        x (np.ndarray): The x values\n        y (np.ndarray): The y values\n        xx (np.ndarray): The projected interpolation range\n        interpolation_kind (str): The interpolation function to use.\n\n    Returns:\n        The interpolated points.\n\n    Note:\n        The interpolation function may be either\n        [scipy.interpolate.pchip_interpolate](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.pchip_interpolate.html#scipy.interpolate.pchip_interpolate),\n        [numpy.interp](https://numpy.org/devdocs/reference/generated/numpy.interp.html#numpy.interp),\n        or one of the options for [scipy.interpolate.interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html).\n        Note however, that `scipy.interpolate.interp1d` is [deprecated](https://docs.scipy.org/doc/scipy/tutorial/interpolate/1D.html#piecewise-linear-interpolation).\n    \"\"\"\n    legacy_interp1d = [\n        \"linear\",\n        \"nearest\",\n        \"nearest-up\",\n        \"zero\",\n        \"slinear\",\n        \"quadratic\",\n        \"cubic\",\n        \"previous\",\n        \"next\",\n    ]\n    # Return the values interpolated with the specified function\n    if interpolation_kind == \"pchip\":\n        interpolator = pchip(x, y)\n        return interpolator(xx)\n    elif interpolation_kind in legacy_interp1d:\n        interpolator = interp1d(x, y, kind=interpolation_kind)\n        return interpolator(xx)\n    else:\n        return np.interp(xx, x, y)\n</code></pre>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.MilestonesModel","title":"<code>MilestonesModel</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for the milestone labels and their positions on the x axis.</p> <p>Ensures that milestone labels exist, are properly structured, and valid.</p> <p>Fields:</p> <ul> <li> <code>milestone_labels</code>                 (<code>dict[str, int]</code>)             </li> </ul> Source code in <code>lexos/rolling_windows/plotters/simple_plotter.py</code> <pre><code>class MilestonesModel(BaseModel):\n    \"\"\"Model for the milestone labels and their positions on the x axis.\n\n    Ensures that milestone labels exist, are properly structured, and valid.\n    \"\"\"\n\n    milestone_labels: dict[str, int]\n</code></pre>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter","title":"<code>SimplePlotter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BasePlotter</code></p> <p>Simple plotter using pyplot.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>df</code>                 (<code>DataFrame</code>)             </li> <li> <code>width</code>                 (<code>Optional[float | int]</code>)             </li> <li> <code>height</code>                 (<code>Optional[float | int]</code>)             </li> <li> <code>figsize</code>                 (<code>Optional[tuple[float | int, float | int]]</code>)             </li> <li> <code>hide_spines</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>titlepad</code>                 (<code>Optional[float | int]</code>)             </li> <li> <code>title_position</code>                 (<code>Optional[str]</code>)             </li> <li> <code>show_legend</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>show_grid</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>xlabel</code>                 (<code>Optional[str]</code>)             </li> <li> <code>ylabel</code>                 (<code>Optional[str]</code>)             </li> <li> <code>show_milestones</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>milestone_colors</code>                 (<code>Optional[list[str] | str]</code>)             </li> <li> <code>milestone_style</code>                 (<code>Optional[str]</code>)             </li> <li> <code>milestone_width</code>                 (<code>Optional[int]</code>)             </li> <li> <code>show_milestone_labels</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>milestone_labels</code>                 (<code>Optional[dict]</code>)             </li> <li> <code>milestone_labels_ha</code>                 (<code>Optional[str]</code>)             </li> <li> <code>milestone_labels_va</code>                 (<code>Optional[str]</code>)             </li> <li> <code>milestone_label_rotation</code>                 (<code>Optional[int]</code>)             </li> <li> <code>milestone_labels_offset</code>                 (<code>Optional[tuple]</code>)             </li> <li> <code>milestone_labels_textcoords</code>                 (<code>Optional[str]</code>)             </li> <li> <code>use_interpolation</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>interpolation_num</code>                 (<code>Optional[int]</code>)             </li> <li> <code>interpolation_kind</code>                 (<code>Optional[str]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> <li> <code>ax</code>                 (<code>Optional[Axes]</code>)             </li> </ul> Source code in <code>lexos/rolling_windows/plotters/simple_plotter.py</code> <pre><code>class SimplePlotter(BasePlotter):\n    \"\"\"Simple plotter using pyplot.\"\"\"\n\n    id: ClassVar[str] = \"rw_simple_plotter\"\n    df: pd.DataFrame = Field(\n        ..., description=\"A dataframe containing the data to plot.\"\n    )\n    width: Optional[float | int] = Field(\n        default=6.4, description=\"The width in inches.\"\n    )\n    height: Optional[float | int] = Field(\n        default=4.8, description=\"The height in inches.\"\n    )\n    figsize: Optional[tuple[float | int, float | int]] = Field(\n        default=None,\n        description=\"A tuple containing the width and height in inches (overrides the previous keywords).\",\n    )\n    hide_spines: Optional[list[str]] = Field(\n        default=[\"top\", \"right\"],\n        description=\"A list of ['top', 'right', 'bottom', 'left'] indicating which spines to hide.\",\n    )\n    title: Optional[str] = Field(\n        default=\"Rolling Windows Plot\",\n        description=\"The title to use for the plot.\",\n    )\n    titlepad: Optional[float | int] = Field(\n        default=6.0,\n        description=\"The padding in points to place between the title and the plot. May need to be increased if you are showing milestone labels.\",\n    )\n    title_position: Optional[str] = Field(\n        default=\"top\",\n        description=\"Show the title on the 'bottom' or the 'top' of the figure.\",\n    )\n    show_legend: Optional[bool] = Field(\n        default=True, description=\"Whether to show the legend.\"\n    )\n    show_grid: Optional[bool] = Field(\n        default=False, description=\"Whether to show the grid.\"\n    )\n    xlabel: Optional[str] = Field(\n        default=\"Token Count\",\n        description=\"The text to display along the x axis.\",\n    )\n    ylabel: Optional[str] = Field(\n        default=\"Average Frequency\",\n        description=\"The text to display along the y axis.\",\n    )\n    show_milestones: Optional[bool] = Field(\n        default=False,\n        description=\"Whether to show the milestone markers.\",\n    )\n    milestone_colors: Optional[list[str] | str] = Field(\n        default=\"teal\",\n        description=\"The colour or colours to use for milestone markers. See pyplot.vlines().\",\n    )\n    milestone_style: Optional[str] = Field(\n        default=\"--\",\n        description=\"The style of the milestone markers. See pyplot.vlines().\",\n    )\n    milestone_width: Optional[int] = Field(\n        default=1,\n        description=\"The width of the milestone markers. See pyplot.vlines().\",\n    )\n    show_milestone_labels: Optional[bool] = Field(\n        default=False, description=\"Whether to show the milestone labels.\"\n    )\n    milestone_labels: Optional[dict] = Field(\n        default=None,\n        description=\"A dict with keys as milestone labels and values as token indexes.\",\n    )\n    milestone_labels_ha: Optional[str] = Field(\n        default=\"left\",\n        description=\"The horizontal alignment of the milestone labels. See pyplot.annotate().\",\n    )\n    milestone_labels_va: Optional[str] = Field(\n        default=\"baseline\",\n        description=\"The vertical alignment of the milestone labels. See pyplot.annotate().\",\n    )\n    milestone_label_rotation: Optional[int] = Field(\n        default=45,\n        description=\"The rotation of the milestone labels. See pyplot.annotate().\",\n    )\n    milestone_labels_offset: Optional[tuple] = Field(\n        default=(-8, 4),\n        description=\"A tuple containing the number of pixels along the x and y axes to offset the milestone labels. See pyplot.annotate().\",\n    )\n    milestone_labels_textcoords: Optional[str] = Field(\n        default=\"offset pixels\",\n        description=\"Whether to offset milestone labels by pixels or points. See pyplot.annotate(str).\",\n    )\n    use_interpolation: Optional[bool] = Field(\n        default=False, description=\"Whether to use interpolation on values.\"\n    )\n    interpolation_num: Optional[int] = Field(\n        default=500, description=\"Number of values to add between points.\"\n    )\n    interpolation_kind: Optional[str] = Field(\n        default=\"pchip\", description=\"Algorithm to use for interpolation.\"\n    )\n    fig: Optional[plt.Figure] = None\n    ax: Optional[plt.Axes] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _validate_edge_cases(self) -&gt; None:\n        \"\"\"Validate edge cases for the PlotlyPlotter.\"\"\"\n        if self.show_milestones or self.show_milestone_labels:\n            try:\n                MilestonesModel(milestone_labels=self.milestone_labels)\n            except ValueError:\n                raise LexosException(\n                    \"The `show_milestones` and `show_milestone_labels` parameters require a value for `milestone_labels`. It should be a list of dicts where the keys are labels and the values are points on the x axis.\"\n                )\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"Initialise the instance with arbitrary keywords.\"\"\"\n        super().__init__(**kwargs)\n        self._validate_edge_cases()\n\n        # Drop the id column if it exists\n        self.df.drop(\"id\", axis=1, inplace=True, errors=\"ignore\")\n\n        # Get the plot dimensions and title position\n        width, height = self._get_width_height()\n        titlepad = self.titlepad\n        titlepad = self._adjust_titlepad(titlepad, width, height)\n\n        # Generate the plot\n        self.fig, self.ax = plt.subplots(figsize=(width, height))\n\n        # Set the spines\n        for spine in self.hide_spines:\n            self.ax.spines[spine].set_visible(False)\n\n        # Labels and title\n        plt.margins(x=0, y=0)\n        plt.ticklabel_format(axis=\"both\", style=\"plain\")\n        if self.title_position == \"bottom\":\n            plt.title(self.title, y=-0.25)\n        else:\n            plt.title(self.title, pad=titlepad)\n        # TODO: plt.xlabel(self.xlabel, fontsize=10)\n        plt.xlabel(self.xlabel)\n        plt.ylabel(self.ylabel)\n\n    def _adjust_titlepad(self, titlepad: float, width: float, height: float) -&gt; None:\n        \"\"\"Hack to move the title above the labels.\n\n        Args:\n            titlepad (float): The padding in points to place between the title and the plot.\n            width (float): The width of the plot.\n            height (float): The height of the plot.\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(width, height))\n        plt.close()\n        if self.show_milestone_labels and self.title_position == \"top\":\n            # Only override self.titlepad if it is the default value\n            if self.titlepad == 6.0:\n                titlepad = self._get_label_height(\n                    self.milestone_labels, self.milestone_label_rotation\n                )\n        return titlepad\n\n    def _get_label_height(\n        self, milestone_labels: dict, milestone_label_rotation: int\n    ) -&gt; float:\n        \"\"\"Calculate the height of the longest milestone label.\n\n        Args:\n            milestone_labels (dict): A dict containing milestone labels and x-axis positions.\n            milestone_label_rotation (int): The rotation in degrees of the labels\n\n        Returns:\n            float: The height of the longest label.\n\n        Note:\n            This method is a hack to calculate the label height using a separate plot.\n        \"\"\"\n        tmp_fig, tmp_ax = plt.subplots()\n        r = tmp_fig.canvas.get_renderer()\n        heights = set()\n        for x in list(milestone_labels.keys()):\n            t = tmp_ax.annotate(\n                x,\n                xy=(0, 0),\n                xytext=(0, 0),\n                textcoords=\"offset points\",\n                rotation=milestone_label_rotation,\n            )\n            bb = t.get_window_extent(renderer=r)\n            heights.add(bb.height)\n        plt.close()\n        return max(list(heights))\n\n    def _get_width_height(self) -&gt; tuple[float, float]:\n        \"\"\"Set the figure size for the plot.\n\n        Returns:\n            tuple[float, float]: A tuple containing the width and height in inches.\n        \"\"\"\n        if self.figsize:\n            width = self.figsize[0]\n            height = self.figsize[1]\n        else:\n            width = self.width\n            height = self.height\n        return (width, height)\n\n    def _plot_interpolated(self, df: pd.DataFrame, **kwargs) -&gt; None:\n        \"\"\"Plot with interpolate dvalues between points.\n\n        Args:\n            df (pd.DataFrame): A dataframe containing the data to plot.\n        \"\"\"\n        x = np.arange(df.shape[0])\n        xx = np.linspace(x[0], x[-1], self.interpolation_num)\n        for term in df.columns:\n            y = np.array(df[term].values.tolist())\n            interpolated = interpolate(x, y, xx, self.interpolation_kind)\n            plt.plot(xx, interpolated, label=term, **kwargs)\n\n    def _show_milestones(self, df: pd.DataFrame, ax: Axes) -&gt; None:\n        \"\"\"Plot the milestone markers and labels.\n\n        Args:\n            df (pd.DataFrame): A dataframe containing the data to plot.\n            ax (Axes): The axes object to plot on.\n        \"\"\"\n        # Plot the milestones with adjustments to the margin and spines\n        # This looks like it is the highest value\n        ymax = df.to_numpy().max()\n        for k, v in self.milestone_labels.items():\n            if self.show_milestones:\n                plt.vlines(\n                    x=v,\n                    ymin=0,\n                    ymax=ymax,\n                    colors=self.milestone_colors,\n                    ls=self.milestone_style,\n                    lw=self.milestone_width,\n                )\n            if self.show_milestone_labels:\n                ax.annotate(\n                    k,\n                    xy=(v, ymax),\n                    ha=self.milestone_labels_ha,\n                    va=self.milestone_labels_va,\n                    rotation=self.milestone_label_rotation,\n                    xytext=self.milestone_labels_offset,\n                    textcoords=self.milestone_labels_textcoords,\n                )\n\n    @validate_call(config=model_config)\n    def plot(self, show: Optional[bool] = True, **kwargs: Any) -&gt; None:\n        \"\"\"Call the plotter.\n\n        Args:\n            show (Optional[bool]): Whether to show the plot after generating it.\n            **kwargs (Any): Additional keyword arguments accepted by matplotlib.pyplot.plot().\n        \"\"\"\n        # Grid\n        if self.show_grid:\n            plt.grid(visible=True)\n\n        # Interpolation\n        if self.use_interpolation:\n            self._plot_interpolated(self.df, **kwargs)\n        else:\n            for term in self.df.columns:\n                plt.plot(self.df[term].values.tolist(), label=term, **kwargs)  # self.ax\n        if self.show_legend:\n            plt.legend()\n\n        # If milestones have been set, plot them\n        if self.show_milestones or self.show_milestone_labels:\n            self._show_milestones(self.df, self.ax)\n\n        if not show:\n            plt.close()\n\n    @validate_call\n    def save(self, path: Path | str, **kwargs) -&gt; None:\n        \"\"\"Save the plot to a file (wrapper for `pyplot.savefig()`).\n\n        Args:\n            path (Path | str): The path to the file to save.\n\n        Returns:\n            None\n        \"\"\"\n        if not self.fig:\n            raise LexosException(\n                \"There is no plot to save. You must first calling `plotter(data)`.\"\n            )\n        self.fig.savefig(path, **kwargs)\n\n    def show(self) -&gt; None:\n        \"\"\"Display a plot.\n\n        Note:\n            Calling pyplot.show() doesn't work with an inline backend like Jupyter notebooks, so we need to detect this via a UserWarning and then call the `fig` attribute.\n        \"\"\"\n        if not self.fig:\n            raise LexosException(\n                \"There is no plot to show. You must first call `plotter(data)`.\"\n            )\n        return self.fig\n</code></pre>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.df","title":"<code>df: pd.DataFrame</code>  <code>pydantic-field</code>","text":"<p>A dataframe containing the data to plot.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.figsize","title":"<code>figsize: Optional[tuple[float | int, float | int]] = None</code>  <code>pydantic-field</code>","text":"<p>A tuple containing the width and height in inches (overrides the previous keywords).</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.height","title":"<code>height: Optional[float | int] = 4.8</code>  <code>pydantic-field</code>","text":"<p>The height in inches.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.hide_spines","title":"<code>hide_spines: Optional[list[str]] = ['top', 'right']</code>  <code>pydantic-field</code>","text":"<p>A list of ['top', 'right', 'bottom', 'left'] indicating which spines to hide.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.interpolation_kind","title":"<code>interpolation_kind: Optional[str] = 'pchip'</code>  <code>pydantic-field</code>","text":"<p>Algorithm to use for interpolation.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.interpolation_num","title":"<code>interpolation_num: Optional[int] = 500</code>  <code>pydantic-field</code>","text":"<p>Number of values to add between points.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.metadata","title":"<code>metadata: dict</code>  <code>property</code>","text":"<p>Return metadata about the object.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing metadata about the object.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_colors","title":"<code>milestone_colors: Optional[list[str] | str] = 'teal'</code>  <code>pydantic-field</code>","text":"<p>The colour or colours to use for milestone markers. See pyplot.vlines().</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_label_rotation","title":"<code>milestone_label_rotation: Optional[int] = 45</code>  <code>pydantic-field</code>","text":"<p>The rotation of the milestone labels. See pyplot.annotate().</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_labels","title":"<code>milestone_labels: Optional[dict] = None</code>  <code>pydantic-field</code>","text":"<p>A dict with keys as milestone labels and values as token indexes.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_labels_ha","title":"<code>milestone_labels_ha: Optional[str] = 'left'</code>  <code>pydantic-field</code>","text":"<p>The horizontal alignment of the milestone labels. See pyplot.annotate().</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_labels_offset","title":"<code>milestone_labels_offset: Optional[tuple] = (-8, 4)</code>  <code>pydantic-field</code>","text":"<p>A tuple containing the number of pixels along the x and y axes to offset the milestone labels. See pyplot.annotate().</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_labels_textcoords","title":"<code>milestone_labels_textcoords: Optional[str] = 'offset pixels'</code>  <code>pydantic-field</code>","text":"<p>Whether to offset milestone labels by pixels or points. See pyplot.annotate(str).</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_labels_va","title":"<code>milestone_labels_va: Optional[str] = 'baseline'</code>  <code>pydantic-field</code>","text":"<p>The vertical alignment of the milestone labels. See pyplot.annotate().</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_style","title":"<code>milestone_style: Optional[str] = '--'</code>  <code>pydantic-field</code>","text":"<p>The style of the milestone markers. See pyplot.vlines().</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.milestone_width","title":"<code>milestone_width: Optional[int] = 1</code>  <code>pydantic-field</code>","text":"<p>The width of the milestone markers. See pyplot.vlines().</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.show_grid","title":"<code>show_grid: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to show the grid.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.show_legend","title":"<code>show_legend: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to show the legend.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.show_milestone_labels","title":"<code>show_milestone_labels: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to show the milestone labels.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.show_milestones","title":"<code>show_milestones: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to show the milestone markers.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.title","title":"<code>title: Optional[str] = 'Rolling Windows Plot'</code>  <code>pydantic-field</code>","text":"<p>The title to use for the plot.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.title_position","title":"<code>title_position: Optional[str] = 'top'</code>  <code>pydantic-field</code>","text":"<p>Show the title on the 'bottom' or the 'top' of the figure.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.titlepad","title":"<code>titlepad: Optional[float | int] = 6.0</code>  <code>pydantic-field</code>","text":"<p>The padding in points to place between the title and the plot. May need to be increased if you are showing milestone labels.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.use_interpolation","title":"<code>use_interpolation: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>Whether to use interpolation on values.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.width","title":"<code>width: Optional[float | int] = 6.4</code>  <code>pydantic-field</code>","text":"<p>The width in inches.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.xlabel","title":"<code>xlabel: Optional[str] = 'Token Count'</code>  <code>pydantic-field</code>","text":"<p>The text to display along the x axis.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.ylabel","title":"<code>ylabel: Optional[str] = 'Average Frequency'</code>  <code>pydantic-field</code>","text":"<p>The text to display along the y axis.</p>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.__init__","title":"<code>__init__(**kwargs) -&gt; None</code>","text":"<p>Initialise the instance with arbitrary keywords.</p> Source code in <code>lexos/rolling_windows/plotters/simple_plotter.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"Initialise the instance with arbitrary keywords.\"\"\"\n    super().__init__(**kwargs)\n    self._validate_edge_cases()\n\n    # Drop the id column if it exists\n    self.df.drop(\"id\", axis=1, inplace=True, errors=\"ignore\")\n\n    # Get the plot dimensions and title position\n    width, height = self._get_width_height()\n    titlepad = self.titlepad\n    titlepad = self._adjust_titlepad(titlepad, width, height)\n\n    # Generate the plot\n    self.fig, self.ax = plt.subplots(figsize=(width, height))\n\n    # Set the spines\n    for spine in self.hide_spines:\n        self.ax.spines[spine].set_visible(False)\n\n    # Labels and title\n    plt.margins(x=0, y=0)\n    plt.ticklabel_format(axis=\"both\", style=\"plain\")\n    if self.title_position == \"bottom\":\n        plt.title(self.title, y=-0.25)\n    else:\n        plt.title(self.title, pad=titlepad)\n    # TODO: plt.xlabel(self.xlabel, fontsize=10)\n    plt.xlabel(self.xlabel)\n    plt.ylabel(self.ylabel)\n</code></pre>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.plot","title":"<code>plot(show: Optional[bool] = True, **kwargs: Any) -&gt; None</code>","text":"<p>Call the plotter.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>Optional[bool]</code> <p>Whether to show the plot after generating it.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments accepted by matplotlib.pyplot.plot().</p> <code>{}</code> Source code in <code>lexos/rolling_windows/plotters/simple_plotter.py</code> <pre><code>@validate_call(config=model_config)\ndef plot(self, show: Optional[bool] = True, **kwargs: Any) -&gt; None:\n    \"\"\"Call the plotter.\n\n    Args:\n        show (Optional[bool]): Whether to show the plot after generating it.\n        **kwargs (Any): Additional keyword arguments accepted by matplotlib.pyplot.plot().\n    \"\"\"\n    # Grid\n    if self.show_grid:\n        plt.grid(visible=True)\n\n    # Interpolation\n    if self.use_interpolation:\n        self._plot_interpolated(self.df, **kwargs)\n    else:\n        for term in self.df.columns:\n            plt.plot(self.df[term].values.tolist(), label=term, **kwargs)  # self.ax\n    if self.show_legend:\n        plt.legend()\n\n    # If milestones have been set, plot them\n    if self.show_milestones or self.show_milestone_labels:\n        self._show_milestones(self.df, self.ax)\n\n    if not show:\n        plt.close()\n</code></pre>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.save","title":"<code>save(path: Path | str, **kwargs) -&gt; None</code>","text":"<p>Save the plot to a file (wrapper for <code>pyplot.savefig()</code>).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file to save.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>lexos/rolling_windows/plotters/simple_plotter.py</code> <pre><code>@validate_call\ndef save(self, path: Path | str, **kwargs) -&gt; None:\n    \"\"\"Save the plot to a file (wrapper for `pyplot.savefig()`).\n\n    Args:\n        path (Path | str): The path to the file to save.\n\n    Returns:\n        None\n    \"\"\"\n    if not self.fig:\n        raise LexosException(\n            \"There is no plot to save. You must first calling `plotter(data)`.\"\n        )\n    self.fig.savefig(path, **kwargs)\n</code></pre>"},{"location":"api/rolling_windows/plotters/simple_plotter/#lexos.rolling_windows.plotters.simple_plotter.SimplePlotter.show","title":"<code>show() -&gt; None</code>","text":"<p>Display a plot.</p> Note <p>Calling pyplot.show() doesn't work with an inline backend like Jupyter notebooks, so we need to detect this via a UserWarning and then call the <code>fig</code> attribute.</p> Source code in <code>lexos/rolling_windows/plotters/simple_plotter.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Display a plot.\n\n    Note:\n        Calling pyplot.show() doesn't work with an inline backend like Jupyter notebooks, so we need to detect this via a UserWarning and then call the `fig` attribute.\n    \"\"\"\n    if not self.fig:\n        raise LexosException(\n            \"There is no plot to show. You must first call `plotter(data)`.\"\n        )\n    return self.fig\n</code></pre>"},{"location":"api/scrubber/","title":"Scrubber","text":""},{"location":"api/scrubber/#overview","title":"Overview","text":"<p>The Scrubber module provides a flexible, pipeline-based system for text cleaning and normalization as part of the Lexos project. It enables users to preprocess text by applying a customizable sequence of \"scrubber components\" (pipes) to remove, replace, or normalize elements such as punctuation, digits, whitespace, and more.</p>"},{"location":"api/scrubber/#features","title":"Features","text":"<ul> <li>Modular pipeline for text scrubbing</li> <li>Built-in registry of reusable scrubber components</li> <li>Easy addition and removal of pipeline components</li> <li>Support for custom components and configuration</li> <li>Integration with other Lexos modules</li> <li>Batch processing of texts via generator interface</li> <li>Robust error handling</li> </ul>"},{"location":"api/scrubber/#submodules","title":"Submodules","text":""},{"location":"api/scrubber/#normalize","title":"Normalize","text":"<p>The normalize submodule contains functions to normalize all bullet points, hyphenated words, letters (to lowercase), quotation marks, repeating characters, unicode, and whitespace by replacing them with more standardized characters.</p>"},{"location":"api/scrubber/#pipeline","title":"Pipeline","text":"<p>The pipeline submodule allows the user to create a pipeline which calls functions from the other submodules in a specific order.</p>"},{"location":"api/scrubber/#registry","title":"Registry","text":"<p>The registry submodule contains functions <code>get_component</code> and <code>get_components</code> to get one component from a string, or multiple from a tuple, respectively.</p>"},{"location":"api/scrubber/#remove","title":"Remove","text":"<p>The remove submodule contains functions to remove accents, all brackets ( ) [ ] { } and the text within them, digits, new_lines, given regex a pattern, Project Gutenberg headers, punctuation, tabs, and tags.</p>"},{"location":"api/scrubber/#replace","title":"Replace","text":"<p>The replace submodule contains functions which replace currency symbols, digits, emails, emojis, hashtags, given a regex pattern, phone numbers, punctuation, special characters, urls, and user handles with a string of the form <code>_TYPE_</code>.</p>"},{"location":"api/scrubber/#resources","title":"Resources","text":"<p>The resources submodule contains the HTMLTextExtractor class, a subclass of html.parser.HTMLParser.</p>"},{"location":"api/scrubber/#scrubber_1","title":"Scrubber","text":"<p>The scrubber submodule contains the main logic for the Scrubber module. It contains the <code>Pipe</code> dataclass and the <code>Scrubber</code> class. The Pipe class contains only a call method and the Scrubber class contains an initialization method along with methods <code>add_pipe</code>, <code>pipe</code>, <code>remove_pipe</code>, <code>reset</code>, and <code>scrub</code>. The Scrubber class also contains the attribute <code>pipes</code> which returns a list of the pipeline components. The submodule also includes the function <code>scrub</code> which takes in the text to scrub, the pipeline, and the optional factory and returns the scrubbed text</p>"},{"location":"api/scrubber/#tags","title":"Tags","text":"<p>The tags submodule uses Beautiful Soup for several functions to remove attributes, remove comments, remove doctypes, remove elements, remove tags, replace attributes, and replace tags in HTML and XML files.</p>"},{"location":"api/scrubber/#utils","title":"Utils","text":"<p>The utils submodule contains the function <code>get_tags</code>.</p>"},{"location":"api/scrubber/normalize/","title":"Normalize","text":"<p>The collection of \"Normalize\" functions take notations which are not standardized (such as - or * or ~ for a bullet point) and replaces them all with the same, normalized notation.</p>"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.bullet_points","title":"<code>bullet_points(text: str) -&gt; str</code>","text":"<p>Normalize bullet points.</p> <p>Normalises all \"fancy\" bullet point symbols in <code>text</code> to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's <code>utils.normalize_bullets</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to normalize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized text.</p> Source code in <code>lexos/scrubber/normalize.py</code> <pre><code>@validate_call(config=validation_config)\ndef bullet_points(text: str) -&gt; str:\n    \"\"\"Normalize bullet points.\n\n    Normalises all \"fancy\" bullet point symbols in `text` to just the basic\n    ASCII \"-\", provided they are the first non-whitespace characters on a new\n    line (like a list of items). Duplicates Textacy's `utils.normalize_bullets`.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        str: The normalized text.\n    \"\"\"\n    return resources.RE_BULLET_POINTS.sub(r\"\\1-\", text)\n</code></pre>"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.hyphenated_words","title":"<code>hyphenated_words(text: str) -&gt; str</code>","text":"<p>Normalize hyphenated words.</p> <p>Normalize words in <code>text</code> that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's <code>utils.normalize_hyphens</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to normalize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized text.</p> Source code in <code>lexos/scrubber/normalize.py</code> <pre><code>@validate_call(config=validation_config)\ndef hyphenated_words(text: str) -&gt; str:\n    \"\"\"Normalize hyphenated words.\n\n    Normalize words in `text` that have been split across lines by a hyphen\n    for visual consistency (aka hyphenated) by joining the pieces back together,\n    sans hyphen and whitespace. Duplicates Textacy's `utils.normalize_hyphens`.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        str: The normalized text.\n    \"\"\"\n    return resources.RE_HYPHENATED_WORD.sub(r\"\\1\\2\", text)\n</code></pre>"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.lower_case","title":"<code>lower_case(text: str) -&gt; str</code>","text":"<p>Convert <code>text</code> to lower case.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to convert to lower case.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The converted text.</p> Source code in <code>lexos/scrubber/normalize.py</code> <pre><code>@validate_call(config=validation_config)\ndef lower_case(text: str) -&gt; str:\n    \"\"\"Convert `text` to lower case.\n\n    Args:\n        text (str): The text to convert to lower case.\n\n    Returns:\n        str: The converted text.\n    \"\"\"\n    return text.lower()\n</code></pre>"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.quotation_marks","title":"<code>quotation_marks(text: str) -&gt; str</code>","text":"<p>Normalize quotation marks.</p> <p>Normalize all \"fancy\" single- and double-quotation marks in <code>text</code> to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's <code>utils.normalize_quotation_marks</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to normalize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized text.</p> Source code in <code>lexos/scrubber/normalize.py</code> <pre><code>@validate_call(config=validation_config)\ndef quotation_marks(text: str) -&gt; str:\n    \"\"\"Normalize quotation marks.\n\n    Normalize all \"fancy\" single- and double-quotation marks in `text`\n    to just the basic ASCII equivalents. Note that this will also normalize fancy\n    apostrophes, which are typically represented as single quotation marks.\n    Duplicates Textacy's `utils.normalize_quotation_marks`.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        str: The normalized text.\n    \"\"\"\n    return text.translate(resources.QUOTE_TRANSLATION_TABLE)\n</code></pre>"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.repeating_chars","title":"<code>repeating_chars(text: str, *, chars: Optional[str], maxn: Optional[int] = 1) -&gt; str</code>","text":"<p>Normalize repeating characters in <code>text</code>.</p> <p>Truncating their number of consecutive repetitions to <code>maxn</code>. Duplicates Textacy's <code>utils.normalize_repeating_chars</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to normalize.</p> required <code>chars</code> <code>Optional[str]</code> <p>One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\".</p> required <code>maxn</code> <code>Optional[int]</code> <p>Maximum number of consecutive repetitions of <code>chars</code> to which longer repetitions will be truncated.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>str</p> Source code in <code>lexos/scrubber/normalize.py</code> <pre><code>@validate_call(config=validation_config)\ndef repeating_chars(text: str, *, chars: Optional[str], maxn: Optional[int] = 1) -&gt; str:\n    \"\"\"Normalize repeating characters in `text`.\n\n    Truncating their number of consecutive repetitions to `maxn`.\n    Duplicates Textacy's `utils.normalize_repeating_chars`.\n\n    Args:\n        text (str): The text to normalize.\n        chars (Optional[str]): One or more characters whose consecutive repetitions are to be\n            normalized, e.g. \".\" or \"?!\".\n        maxn (Optional[int]): Maximum number of consecutive repetitions of `chars` to which\n            longer repetitions will be truncated.\n\n    Returns:\n        str: str\n    \"\"\"\n    return re.sub(r\"({}){{{},}}\".format(re.escape(chars), maxn + 1), chars * maxn, text)\n</code></pre>"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.unicode","title":"<code>unicode(text: str, *, form: Optional[Literal['NFC', 'NFD', 'NFKC', 'NFKD']] = 'NFC') -&gt; str</code>","text":"<p>Normalize unicode characters in <code>text</code> into canonical forms.</p> <p>Duplicates Textacy's <code>utils.normalize_unicode</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to normalize.</p> required <code>form</code> <code>Optional[Literal['NFC', 'NFD', 'NFKC', 'NFKD']]</code> <p>Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods.</p> <code>'NFC'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized text.</p> See Also <p>https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize</p> Source code in <code>lexos/scrubber/normalize.py</code> <pre><code>@validate_call(config=validation_config)\ndef unicode(\n    text: str, *, form: Optional[Literal[\"NFC\", \"NFD\", \"NFKC\", \"NFKD\"]] = \"NFC\"\n) -&gt; str:\n    \"\"\"Normalize unicode characters in `text` into canonical forms.\n\n    Duplicates Textacy's `utils.normalize_unicode`.\n\n    Args:\n        text (str): The text to normalize.\n        form (Optional[Literal[\"NFC\", \"NFD\", \"NFKC\", \"NFKD\"]]): Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods.\n\n    Returns:\n        str: The normalized text.\n\n    See Also:\n        https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize\n    \"\"\"\n    return unicodedata.normalize(form, text)\n</code></pre>"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.whitespace","title":"<code>whitespace(text: str) -&gt; str</code>","text":"<p>Normalize whitespace.</p> <p>Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to normalize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized text.</p> Source code in <code>lexos/scrubber/normalize.py</code> <pre><code>@validate_call(config=validation_config)\ndef whitespace(text: str) -&gt; str:\n    \"\"\"Normalize whitespace.\n\n    Replace all contiguous zero-width spaces with an empty string,\n    line-breaking spaces with a single newline, and non-breaking spaces\n    with a single space, then strip any leading/trailing whitespace.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        str: The normalized text.\n    \"\"\"\n    text = resources.RE_ZWSP.sub(\"\", text)\n    text = resources.RE_LINEBREAK.sub(r\"\\n\", text)\n    text = resources.RE_NONBREAKING_SPACE.sub(\" \", text)\n    return text.strip()\n</code></pre>"},{"location":"api/scrubber/pipeline/","title":"Pipeline","text":"<p>Allows the user to customize a \"pipeline,\" an order in which to perform different scrubbing operations. For example: removing all digits before replacing all phone numbers would have a very different effect than replacing all phone numbers before removing all digits.</p>"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.pipe","title":"<code>pipe(func: Callable, *args, **kwargs) -&gt; Callable</code>","text":"<p>Apply functool.partial and add <code>__name__</code> to the partial function.</p> <p>This allows the function to be passed to the pipeline along with keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A callable.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A partial function with <code>__name__</code> set to the name of the function.</p> Source code in <code>lexos/scrubber/pipeline.py</code> <pre><code>@validate_call\ndef pipe(func: Callable, *args, **kwargs) -&gt; Callable:\n    \"\"\"Apply functool.partial and add `__name__` to the partial function.\n\n    This allows the function to be passed to the pipeline along with\n    keyword arguments.\n\n    Args:\n        func (Callable): A callable.\n\n    Returns:\n        Callable: A partial function with `__name__` set to the name of the function.\n    \"\"\"\n    if not args and not kwargs:\n        return func\n    else:\n        partial_func = partial(func, *args, **kwargs)\n        update_wrapper(partial_func, func)\n        return partial_func\n</code></pre>"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline","title":"<code>make_pipeline(*funcs: Callable[[str], str]) -&gt; Callable[[str], str]</code>","text":"<p>Make a callable pipeline.</p> <p>Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string.</p> <p>This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable.</p> <p><code>python -m pip install cytoolz</code> is required for this function to work.</p> <p>Use <code>pipe</code> (an alias for <code>functools.partial</code>) to pass arguments to preprocessors.</p> <pre><code>from lexos import scrubber\nscrubber = Scrubber.pipeline.make_pipeline(\n    scrubber.replace.hashtags,\n    scrubber.replace.emojis,\n    pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"])\n)\nscrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\")\n'_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*funcs</code> <code>Callable[[str], str</code> <p>A series of functions to be applied to the text.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable[[str], str]</code> <p>Callable[[str], str]: Pipeline composed of <code>*funcs</code> that applies each in sequential order.</p> Source code in <code>lexos/scrubber/pipeline.py</code> <pre><code>@validate_call\ndef make_pipeline(*funcs: Callable[[str], str]) -&gt; Callable[[str], str]:\n    \"\"\"Make a callable pipeline.\n\n    Make a callable pipeline that passes a text through a series of\n    functions in sequential order, then outputs a (scrubbed) text string.\n\n    This function is intended as a lightweight convenience for users,\n    allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing\n    treating the whole thing as a single callable.\n\n    `python -m pip install cytoolz` is required for this function to work.\n\n    Use `pipe` (an alias for `functools.partial`) to pass arguments to preprocessors.\n\n    ```python\n    from lexos import scrubber\n    scrubber = Scrubber.pipeline.make_pipeline(\n        scrubber.replace.hashtags,\n        scrubber.replace.emojis,\n        pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"])\n    )\n    scrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\")\n    '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_'\n    ```\n\n    Args:\n        *funcs (Callable[[str], str): A series of functions to be applied to the text.\n\n    Returns:\n        Callable[[str], str]: Pipeline composed of ``*funcs`` that applies each in sequential order.\n    \"\"\"\n    return functoolz.compose_left(*funcs)\n</code></pre>"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline_from_tuple","title":"<code>make_pipeline_from_tuple(funcs: tuple) -&gt; tuple</code>","text":"<p>Return a pipeline from a tuple.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>tuple</code> <p>A tuple containing callables or string names of functions.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A pipeline composed of the functions in <code>funcs</code>.</p> Source code in <code>lexos/scrubber/pipeline.py</code> <pre><code>@validate_call\ndef make_pipeline_from_tuple(funcs: tuple) -&gt; tuple:\n    \"\"\"Return a pipeline from a tuple.\n\n    Args:\n        funcs (tuple): A tuple containing callables or string names of functions.\n\n    Returns:\n        tuple: A pipeline composed of the functions in `funcs`.\n    \"\"\"\n    return make_pipeline(*[eval(x) if isinstance(x, str) else x for x in funcs])\n</code></pre>"},{"location":"api/scrubber/registry/","title":"Registry","text":"<p>The registry component of Scrubber maintains a catalogue of registered functions that can be imported individually as needed. The registry enables the functions to be referenced by name using string values. The code registry is created and accessed using the catalogue library by Explosion.</p>"},{"location":"api/scrubber/registry/#lexos.scrubber.registry.get_component","title":"<code>get_component(s: str) -&gt; Callable</code>","text":"<p>Get a single component from a string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The name of the function.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The function.</p> Source code in <code>lexos/scrubber/registry.py</code> <pre><code>@validate_call\ndef get_component(s: str) -&gt; Callable:\n    \"\"\"Get a single component from a string.\n\n    Args:\n        s: The name of the function.\n\n    Returns:\n        Callable: The function.\n    \"\"\"\n    return scrubber_components.get(s)\n</code></pre>"},{"location":"api/scrubber/registry/#lexos.scrubber.registry.get_components","title":"<code>get_components(t: tuple[str, ...]) -&gt; Generator</code>","text":"<p>Get components from a tuple.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>tuple[str, ...]</code> <p>A tuple containing string names of functions.</p> required <p>Yields:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator containing the functions.</p> Source code in <code>lexos/scrubber/registry.py</code> <pre><code>@validate_call\ndef get_components(t: tuple[str, ...]) -&gt; Generator:\n    \"\"\"Get components from a tuple.\n\n    Args:\n        t (tuple[str, ...]): A tuple containing string names of functions.\n\n    Yields:\n        Generator: A generator containing the functions.\n    \"\"\"\n    for item in t:\n        yield scrubber_components.get(item)\n</code></pre>"},{"location":"api/scrubber/remove/","title":"Remove","text":"<p>A set of functions for removing strings and patterns from text.</p>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.accents","title":"<code>accents(text: str, *, fast: Optional[bool] = False, accents: Optional[str | tuple[str, ...]] = None) -&gt; str</code>","text":"<p>Remove accents from any accented unicode characters in <code>text</code>, either by replacing them with ASCII equivalents or removing them entirely.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which accents will be removed.</p> required <code>fast</code> <code>Optional[bool]</code> <p>If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless.</p> <code>False</code> <code>accents</code> <code>Optional[str | tuple[str, ...]]</code> <p>An optional string or tuple of strings indicating the names of diacritics to be stripped.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with accents removed.</p> <code>fast=True</code> can be significantly faster than <code>fast=False</code>, <p>but its transformation of <code>text</code> is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc.</p> See Also <ul> <li>For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table</li> <li>For a more powerful (but slower) alternative, check out <code>unidecode</code>: https://github.com/avian2/unidecode</li> </ul> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>@validate_call(config=validation_config)\ndef accents(\n    text: str,\n    *,\n    fast: Optional[bool] = False,\n    accents: Optional[str | tuple[str, ...]] = None,\n) -&gt; str:\n    \"\"\"Remove accents from any accented unicode characters in `text`, either by replacing them with ASCII equivalents or removing them entirely.\n\n    Args:\n        text (str): The text from which accents will be removed.\n        fast (Optional[bool]): If False, accents are removed from any unicode symbol\n            with a direct ASCII equivalent; if True, accented chars\n            for all unicode symbols are removed, regardless.\n        accents (Optional[str | tuple[str, ...]]): An optional string or tuple of strings indicating the\n            names of diacritics to be stripped.\n\n    Returns:\n        str: The text with accents removed.\n\n    Note: `fast=True` can be significantly faster than `fast=False`,\n        but its transformation of `text` is less \"safe\" and more likely\n        to result in changes of meaning, spelling errors, etc.\n\n    See Also:\n        - For a chart containing Unicode standard names of diacritics, see\n        https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table\n        - For a more powerful (but slower) alternative, check out `unidecode`:\n        https://github.com/avian2/unidecode\n    \"\"\"\n    if fast is False:\n        if accents:\n            if isinstance(accents, str):\n                accents = set(unicodedata.lookup(accents))\n            elif len(accents) == 1:\n                accents = set(unicodedata.lookup(accents[0]))\n            else:\n                accents = set(map(unicodedata.lookup, accents))\n            return \"\".join(\n                char\n                for char in unicodedata.normalize(\"NFKD\", text)\n                if char not in accents\n            )\n        else:\n            return \"\".join(\n                char\n                for char in unicodedata.normalize(\"NFKD\", text)\n                if not unicodedata.combining(char)\n            )\n    else:\n        return (\n            unicodedata.normalize(\"NFKD\", text)\n            .encode(\"ascii\", errors=\"ignore\")\n            .decode(\"ascii\")\n        )\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.brackets","title":"<code>brackets(text: str, *, only: Optional[str | Collection[str]] = ['curly', 'square', 'round']) -&gt; str</code>","text":"<p>Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which brackets will be removed.</p> required <code>only</code> <code>Optional[str | Collection[str]]</code> <p>Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, <code>\"square\"</code> removes only those contents found between square brackets, while <code>[\"round\", \"square\"]</code>  removes those contents found between square or round brackets, but not curly.</p> <code>['curly', 'square', 'round']</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with brackets removed.</p> Note <p>This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations.</p> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def brackets(\n    text: str,\n    *,\n    only: Optional[str | Collection[str]] = [\"curly\", \"square\", \"round\"],\n) -&gt; str:\n    \"\"\"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves.\n\n    Args:\n        text (str): The text from which brackets will be removed.\n        only (Optional[str | Collection[str]]): Remove only those bracketed contents\n            as specified here: \"curly\", \"square\", and/or \"round\". For example,\n            `\"square\"` removes only those contents found between square brackets,\n            while `[\"round\", \"square\"]`  removes those contents found between square\n            or round brackets, but not curly.\n\n    Returns:\n        str: The text with brackets removed.\n\n    Note:\n        This function relies on regular expressions, applied sequentially for curly,\n        square, then round brackets; as such, it doesn't handle nested brackets of the\n        same type and may behave unexpectedly on text with \"wild\" use of brackets.\n        It should be fine removing structured bracketed contents, as is often used,\n        for instance, to denote in-text citations.\n    \"\"\"\n    only = to_collection(only, val_type=str, col_type=set)\n    if only is None or \"curly\" in only:\n        text = resources.RE_BRACKETS_CURLY.sub(\"\", text)\n    if only is None or \"square\" in only:\n        text = resources.RE_BRACKETS_SQUARE.sub(\"\", text)\n    if only is None or \"round\" in only:\n        text = resources.RE_BRACKETS_ROUND.sub(\"\", text)\n    return text\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.digits","title":"<code>digits(text: str, *, only: Optional[str | Collection[str]] = None) -&gt; str</code>","text":"<p>Remove digits.</p> <p>Remove digits from <code>text</code> by replacing all instances of digits (or a subset thereof specified by <code>only</code>) with whitespace.</p> <p>Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which digits will be removed.</p> required <code>only</code> <code>Optional[str | Collection[str]]</code> <p>Remove only those digits specified here. For example, <code>\"9\"</code> removes only 9, while <code>[\"1\", \"2\", \"3\"]</code> removes 1, 2, 3; if None, all unicode digits marks are removed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with digits removed.</p> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def digits(text: str, *, only: Optional[str | Collection[str]] = None) -&gt; str:\n    \"\"\"Remove digits.\n\n    Remove digits from `text` by replacing all instances of digits\n    (or a subset thereof specified by `only`) with whitespace.\n\n    Removes signed/unsigned numbers and decimal/delimiter-separated\n    numbers. Does not remove currency symbols. Some tokens containing\n    digits will be modified.\n\n    Args:\n        text (str): The text from which digits will be removed.\n        only (Optional[str | Collection[str]]): Remove only those digits specified here. For example,\n            `\"9\"` removes only 9, while `[\"1\", \"2\", \"3\"]` removes 1, 2, 3;\n            if None, all unicode digits marks are removed.\n\n    Returns:\n        str: The text with digits removed.\n    \"\"\"\n    if only:\n        if isinstance(only, list):\n            pattern = re.compile(f\"[{''.join(only)}]\")\n        else:\n            pattern = re.compile(only)\n    else:\n        # Using \".\" to represent any unicode character used to indicate\n        # a decimal number, and \"***\" to represent any sequence of\n        # unicode digits, this pattern will match:\n        # 1) ***\n        # 2) ***.***\n        unicode_digits = \"\"\n        for i in range(sys.maxunicode):\n            if unicodedata.category(chr(i)).startswith(\"N\"):\n                unicode_digits = unicode_digits + chr(i)\n        pattern = re.compile(\n            r\"([+-]?[\"\n            + re.escape(unicode_digits)\n            + r\"])|((?&lt;=\"\n            + re.escape(unicode_digits)\n            + r\")[\\u0027|\\u002C|\\u002E|\\u00B7|\"\n            r\"\\u02D9|\\u066B|\\u066C|\\u2396][\" + re.escape(unicode_digits) + r\"]+)\",\n            re.UNICODE,\n        )\n    return str(re.sub(pattern, r\"\", text))\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.new_lines","title":"<code>new_lines(text: str) -&gt; str</code>","text":"<p>Remove new lines.</p> <p>Remove all line-breaking spaces.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which new lines will be removed.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with line-breaking spaces removed.</p> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def new_lines(text: str) -&gt; str:\n    \"\"\"Remove new lines.\n\n    Remove all line-breaking spaces.\n\n    Args:\n        text (str): The text from which new lines will be removed.\n\n    Returns:\n        str: The text with line-breaking spaces removed.\n    \"\"\"\n    return resources.RE_LINEBREAK.sub(\"\", text).strip()\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.pattern","title":"<code>pattern(text: str, *, pattern: Optional[str | Collection[str]]) -&gt; str</code>","text":"<p>Remove strings from <code>text</code> using a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which patterns will be removed.</p> required <code>pattern</code> <code>Optional[str | Collection[str]]</code> <p>The pattern to match.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with the pattern removed.</p> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def pattern(text: str, *, pattern: Optional[str | Collection[str]]) -&gt; str:\n    \"\"\"Remove strings from `text` using a regex pattern.\n\n    Args:\n        text (str): The text from which patterns will be removed.\n        pattern (Optional[str | Collection[str]]): The pattern to match.\n\n    Returns:\n        str: The text with the pattern removed.\n    \"\"\"\n    if isinstance(pattern, list):\n        pattern = \"|\".join(pattern)\n    pat = re.compile(pattern)\n    return re.sub(pat, \"\", text)\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.project_gutenberg_headers","title":"<code>project_gutenberg_headers(text: str) -&gt; str</code>","text":"<p>Remove Project Gutenberg headers and footers.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which headers and footers will be removed.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with Project Gutenberg boilerplate removed.</p> Notes <p>This function is reproduced from Gutenberg package's <code>strip_headers()</code> function (https://github.com/c-w/gutenberg), itself a port of the C++ utility by Johannes Krugel.</p> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def project_gutenberg_headers(text: str) -&gt; str:\n    \"\"\"Remove Project Gutenberg headers and footers.\n\n    Args:\n        text (str): The text from which headers and footers will be removed.\n\n    Returns:\n        str: The text with Project Gutenberg boilerplate removed.\n\n    Notes:\n        This function is reproduced from Gutenberg package's `strip_headers()`\n        function (https://github.com/c-w/gutenberg), itself a port of the C++ utility\n        by Johannes Krugel.\n    \"\"\"\n    lines = text.splitlines()\n    sep = str(os.linesep)\n\n    out = []\n    i = 0\n    footer_found = False\n    ignore_section = False\n\n    for line in lines:\n        reset = False\n\n        if i &lt;= 600:\n            # Check if the header ends here\n            if any(line.startswith(token) for token in resources.TEXT_START_MARKERS):\n                reset = True\n\n            # If it's the end of the header, delete the output produced so far.\n            # May be done several times, if multiple lines occur indicating the\n            # end of the header\n            if reset:\n                out = []\n                continue\n\n        if i &gt;= 100:\n            # Check if the footer begins here\n            if any(line.startswith(token) for token in resources.TEXT_END_MARKERS):\n                footer_found = True\n\n            # If it's the beginning of the footer, stop output\n            if footer_found:\n                break\n\n        if any(line.startswith(token) for token in resources.LEGALESE_START_MARKERS):\n            ignore_section = True\n            continue\n        elif any(line.startswith(token) for token in resources.LEGALESE_END_MARKERS):\n            ignore_section = False\n            continue\n\n        if not ignore_section:\n            out.append(line.rstrip(sep))\n            i += 1\n\n    return sep.join(out).strip()\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.punctuation","title":"<code>punctuation(text: str, *, exclude: Optional[str | Collection[str]] = None, only: Optional[str | Collection[str]] = None) -&gt; str</code>","text":"<p>Remove punctuation from <code>text</code>.</p> <p>Removes all instances of punctuation (or a subset thereof specified by <code>only</code>).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which punctuation will be removed.</p> required <code>exclude</code> <code>Optional[str | Collection[str]]</code> <p>Remove all punctuation except designated characters.</p> <code>None</code> <code>only</code> <code>Optional[str | Collection[str]]</code> <p>Remove only those punctuation marks specified here. For example, <code>\".\"</code> removes only periods, while <code>[\",\", \";\", \":\"]</code> removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with punctuation removed.</p> Note <p>When <code>only=None</code>, Python's built-in <code>str.translate()</code> is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster.</p> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def punctuation(\n    text: str,\n    *,\n    exclude: Optional[str | Collection[str]] = None,\n    only: Optional[str | Collection[str]] = None,\n) -&gt; str:\n    \"\"\"Remove punctuation from `text`.\n\n    Removes all instances of punctuation (or a subset thereof specified by `only`).\n\n    Args:\n        text (str): The text from which punctuation will be removed.\n        exclude (Optional[str | Collection[str]]): Remove all punctuation except designated characters.\n        only (Optional[str | Collection[str]]): Remove only those punctuation marks specified here.\n            For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas,\n            semicolons, and colons; if None, all unicode punctuation marks are removed.\n\n    Returns:\n        str: The text with punctuation removed.\n\n    Note:\n        When `only=None`, Python's built-in `str.translate()` is used;\n        otherwise, a regular expression is used. The former's performance\n        can be up to an order of magnitude faster.\n    \"\"\"\n    if only is not None:\n        only = to_collection(only, val_type=str, col_type=set)\n        return re.sub(\"[{}]+\".format(re.escape(\"\".join(only))), \"\", text)\n    else:\n        if exclude:\n            exclude = ensure_list(exclude)\n        else:\n            exclude = []\n        # Note: We can't use the cached translation table because it replaces\n        # the punctuation with whitespace, so we have to build a new one.\n        translation_table = dict.fromkeys(\n            (\n                i\n                for i in range(sys.maxunicode)\n                if unicodedata.category(chr(i)).startswith(\"P\")\n                and chr(i) not in exclude\n            ),\n            \"\",\n        )\n        return text.translate(translation_table)\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tabs","title":"<code>tabs(text: str) -&gt; str</code>","text":"<p>Remove tabs.</p> <p>If you want to replace tabs with a single space, use <code>normalize.whitespace()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which tabs will be removed.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with tabs removed.</p> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def tabs(text: str) -&gt; str:\n    \"\"\"Remove tabs.\n\n    If you want to replace tabs with a single space, use\n    `normalize.whitespace()` instead.\n\n    Args:\n        text (str): The text from which tabs will be removed.\n\n    Returns:\n        str: The text with tabs removed.\n    \"\"\"\n    return resources.RE_TAB.sub(\"\", text)\n</code></pre>"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tags","title":"<code>tags(text: str, sep: Optional[str] = ' ', remove_whitespace: Optional[bool] = True) -&gt; str</code>","text":"<p>Remove tags from <code>text</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which tags will be removed.</p> required <code>sep</code> <code>Optional[str]</code> <p>A string to insert between tags and text found between them.</p> <code>' '</code> <code>remove_whitespace</code> <code>Optional[bool]</code> <p>If True, remove extra whitespace between text after tags are removed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing just the text found between tags and other non-data elements.</p> Note <ul> <li>If you want to perfom selective removal of tags,     use <code>replace.tag_map</code> instead.</li> <li>This function relies on the stdlib <code>html.parser.HTMLParser</code>.     It appears to work for stripping tags from both html and xml.     Using <code>lxml</code> or BeautifulSoup might be faster, but this is untested.</li> <li>This function preserves text in comments, as well as tags</li> </ul> Source code in <code>lexos/scrubber/remove.py</code> <pre><code>def tags(\n    text: str, sep: Optional[str] = \" \", remove_whitespace: Optional[bool] = True\n) -&gt; str:\n    \"\"\"Remove tags from `text`.\n\n    Args:\n        text (str): The text from which tags will be removed.\n        sep (Optional[str]): A string to insert between tags and text found between them.\n        remove_whitespace (Optional[bool]): If True, remove extra whitespace between text\n            after tags are removed.\n\n    Returns:\n        str: A string containing just the text found between tags and other non-data elements.\n\n    Note:\n        - If you want to perfom selective removal of tags,\n            use `replace.tag_map` instead.\n        - This function relies on the stdlib `html.parser.HTMLParser`.\n            It appears to work for stripping tags from both html and xml.\n            Using `lxml` or BeautifulSoup might be faster, but this is untested.\n        - This function preserves text in comments, as well as tags\n    \"\"\"\n    parser = resources.HTMLTextExtractor()\n    parser.feed(text)\n    text = parser.get_text(sep=sep)\n    if remove_whitespace:\n        text = re.sub(r\"[\\n\\s\\t\\v ]+\", sep, text, re.UNICODE)\n    return text\n</code></pre>"},{"location":"api/scrubber/replace/","title":"Replace","text":"<p>A set of functions for replacing strings and patterns in text.</p>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.currency_symbols","title":"<code>currency_symbols(text: str, repl: str = '_CUR_') -&gt; str</code>","text":"<p>Replace all currency symbols in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which currency symbols will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for currency symbols.</p> <code>'_CUR_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with currency symbols replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef currency_symbols(text: str, repl: str = \"_CUR_\") -&gt; str:\n    \"\"\"Replace all currency symbols in `text` with `repl`.\n\n    Args:\n        text (str): The text in which currency symbols will be replaced.\n        repl (str): The replacement value for currency symbols.\n\n    Returns:\n        str: The text with currency symbols replaced.\n    \"\"\"\n    return resources.RE_CURRENCY_SYMBOL.sub(repl, text)\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.digits","title":"<code>digits(text: str, repl: str = '_DIGIT_') -&gt; str</code>","text":"<p>Replace all digits in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which digits will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for digits.</p> <code>'_DIGIT_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with digits replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef digits(text: str, repl: str = \"_DIGIT_\") -&gt; str:\n    \"\"\"Replace all digits in `text` with `repl`.\n\n    Args:\n        text (str): The text in which digits will be replaced.\n        repl (str): The replacement value for digits.\n\n    Returns:\n        str: The text with digits replaced.\n    \"\"\"\n    return resources.RE_NUMBER.sub(repl, text)\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emails","title":"<code>emails(text: str, repl: str = '_EMAIL_') -&gt; str</code>","text":"<p>Replace all email addresses in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which emails will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for emails.</p> <code>'_EMAIL_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with emails replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef emails(text: str, repl: str = \"_EMAIL_\") -&gt; str:\n    \"\"\"Replace all email addresses in `text` with `repl`.\n\n    Args:\n        text (str): The text in which emails will be replaced.\n        repl (str): The replacement value for emails.\n\n    Returns:\n        str: The text with emails replaced.\n    \"\"\"\n    return resources.RE_EMAIL.sub(repl, text)\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emojis","title":"<code>emojis(text: str, repl: str = '_EMOJI_') -&gt; str</code>","text":"<p>Replace all emoji and pictographs in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which emojis will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for emojis.</p> <code>'_EMOJI_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with emojis replaced.</p> Note <p>If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry!</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef emojis(text: str, repl: str = \"_EMOJI_\") -&gt; str:\n    \"\"\"Replace all emoji and pictographs in `text` with `repl`.\n\n    Args:\n        text (str): The text in which emojis will be replaced.\n        repl (str): The replacement value for emojis.\n\n    Returns:\n        str: The text with emojis replaced.\n\n    Note:\n        If your Python has a narrow unicode build (\"USC-2\"), only dingbats\n        and miscellaneous symbols are replaced because Python isn't able\n        to represent the unicode data for things like emoticons. Sorry!\n    \"\"\"\n    return resources.RE_EMOJI.sub(repl, text)\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.hashtags","title":"<code>hashtags(text: str, repl: str = '_HASHTAG_') -&gt; str</code>","text":"<p>Replace all hashtags in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which hashtags will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for hashtags.</p> <code>'_HASHTAG_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with currency hashtags replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef hashtags(text: str, repl: str = \"_HASHTAG_\") -&gt; str:\n    \"\"\"Replace all hashtags in `text` with `repl`.\n\n    Args:\n        text (str): The text in which hashtags will be replaced.\n        repl (str): The replacement value for hashtags.\n\n    Returns:\n        str: The text with currency hashtags replaced.\n    \"\"\"\n    return resources.RE_HASHTAG.sub(repl, text)\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.pattern","title":"<code>pattern(text: str, *, pattern: Optional[dict | Collection[dict]]) -&gt; str</code>","text":"<p>Replace strings from <code>text</code> using a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which a pattern or pattern will be replaced.</p> required <code>pattern</code> <code>Optional[dict | Collection[dict]]</code> <p>(Optional[dict | Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with pattern(s) replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef pattern(text: str, *, pattern: Optional[dict | Collection[dict]]) -&gt; str:\n    \"\"\"Replace strings from `text` using a regex pattern.\n\n    Args:\n        text (str): The text in which a pattern or pattern will be replaced.\n        pattern: (Optional[dict | Collection[dict]]): A dictionary or list of dictionaries\n            containing the pattern(s) and replacement(s).\n\n    Returns:\n        str: The text with pattern(s) replaced.\n    \"\"\"\n    pattern = ensure_list(pattern)\n    for pat in pattern:\n        k = str(*pat)\n        match = re.compile(k)\n        text = re.sub(match, pat[k], text)\n    return text\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.phone_numbers","title":"<code>phone_numbers(text: str, repl: str = '_PHONE_') -&gt; str</code>","text":"<p>Replace all phone numbers in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which phone numbers will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for phone numbers.</p> <code>'_PHONE_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with phone numbers replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef phone_numbers(text: str, repl: str = \"_PHONE_\") -&gt; str:\n    \"\"\"Replace all phone numbers in `text` with `repl`.\n\n    Args:\n        text (str): The text in which phone numbers will be replaced.\n        repl (str): The replacement value for phone numbers.\n\n    Returns:\n        str: The text with phone numbers replaced.\n    \"\"\"\n    return resources.RE_PHONE_NUMBER.sub(repl, text)\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.punctuation","title":"<code>punctuation(text: str, *, exclude: Optional[str | Collection[str]] = None, only: Optional[str | Collection[str]] = None) -&gt; str</code>","text":"<p>Replace punctuation from <code>text</code>.</p> <p>Replaces all instances of punctuation (or a subset thereof specified by <code>only</code>) with whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which punctuation will be replaced.</p> required <code>exclude</code> <code>Optional[str | Collection[str]]</code> <p>Remove all punctuation except designated characters.</p> <code>None</code> <code>only</code> <code>Optional[str | Collection[str]]</code> <p>Remove only those punctuation marks specified here. For example, <code>\".\"</code> removes only periods, while <code>[\",\", \";\", \":\"]</code> removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with punctuation replaced.</p> Note <p>When <code>only=None</code>, Python's built-in <code>str.translate()</code> is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef punctuation(\n    text: str,\n    *,\n    exclude: Optional[str | Collection[str]] = None,\n    only: Optional[str | Collection[str]] = None,\n) -&gt; str:\n    \"\"\"Replace punctuation from `text`.\n\n    Replaces all instances of punctuation (or a subset thereof specified by `only`)\n    with whitespace.\n\n    Args:\n        text (str): The text in which punctuation will be replaced.\n        exclude (Optional[str | Collection[str]]): Remove all punctuation except designated characters.\n        only (Optional[str | Collection[str]]): Remove only those punctuation marks specified here.\n            For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas,\n            semicolons, and colons; if None, all unicode punctuation marks are removed.\n\n    Returns:\n        str: The text with punctuation replaced.\n\n    Note:\n        When `only=None`, Python's built-in `str.translate()` is used;\n        otherwise, a regular expression is used. The former's performance\n        can be up to an order of magnitude faster.\n    \"\"\"\n    if only is not None:\n        only = to_collection(only, val_type=str, col_type=set)\n        return re.sub(\"[{}]+\".format(re.escape(\"\".join(only))), \" \", text)\n    else:\n        if exclude:\n            exclude = ensure_list(exclude)\n            translation_table = dict.fromkeys(\n                (\n                    i\n                    for i in range(sys.maxunicode)\n                    if unicodedata.category(chr(i)).startswith(\"P\")\n                    and chr(i) not in exclude\n                ),\n                \" \",\n            )\n        else:\n            translation_table = resources.PUNCT_TRANSLATION_TABLE\n        return text.translate(translation_table)\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.special_characters","title":"<code>special_characters(text: str, *, is_html: Optional[bool] = False, ruleset: Optional[dict] = None) -&gt; str</code>","text":"<p>Replace strings from <code>text</code> using a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which special characters will be replaced.</p> required <code>is_html</code> <code>Optional[bool]</code> <p>Whether to replace HTML entities.</p> <code>False</code> <code>ruleset</code> <code>Optional[dict]</code> <p>A dict containing the special characters to match and their replacements.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with special characters replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef special_characters(\n    text: str,\n    *,\n    is_html: Optional[bool] = False,\n    ruleset: Optional[dict] = None,\n) -&gt; str:\n    \"\"\"Replace strings from `text` using a regex pattern.\n\n    Args:\n        text (str): The text in which special characters will be replaced.\n        is_html (Optional[bool]): Whether to replace HTML entities.\n        ruleset (Optional[dict]): A dict containing the special characters to match and their replacements.\n\n    Returns:\n        str: The text with special characters replaced.\n    \"\"\"\n    if is_html:\n        text = html.unescape(text)\n    else:\n        for k, v in ruleset.items():\n            match = re.compile(k)\n            text = re.sub(match, v, text)\n    return text\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.urls","title":"<code>urls(text: str, repl: str = '_URL_') -&gt; str</code>","text":"<p>Replace all URLs in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which urls will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for urls.</p> <code>'_URL_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with urls replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef urls(text: str, repl: str = \"_URL_\") -&gt; str:\n    \"\"\"Replace all URLs in `text` with `repl`.\n\n    Args:\n        text (str): The text in which urls will be replaced.\n        repl (str): The replacement value for urls.\n\n    Returns:\n        str: The text with urls replaced.\n    \"\"\"\n    return resources.RE_SHORT_URL.sub(repl, resources.RE_URL.sub(repl, text))\n</code></pre>"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.user_handles","title":"<code>user_handles(text: str, repl: str = '_USER_') -&gt; str</code>","text":"<p>Replace all (Twitter-style) user handles in <code>text</code> with <code>repl</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text in which user handles will be replaced.</p> required <code>repl</code> <code>str</code> <p>The replacement value for user handles.</p> <code>'_USER_'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with user handles replaced.</p> Source code in <code>lexos/scrubber/replace.py</code> <pre><code>@validate_call(config=validation_config)\ndef user_handles(text: str, repl: str = \"_USER_\") -&gt; str:\n    \"\"\"Replace all (Twitter-style) user handles in `text` with `repl`.\n\n    Args:\n        text (str): The text in which user handles will be replaced.\n        repl (str): The replacement value for user handles.\n\n    Returns:\n        str: The text with user handles replaced.\n    \"\"\"\n    return resources.RE_USER_HANDLE.sub(repl, text)\n</code></pre>"},{"location":"api/scrubber/resources/","title":"Resources","text":"<p>Mappings for removing or transforming character patterns.</p>"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor","title":"<code>HTMLTextExtractor</code>","text":"<p>               Bases: <code>HTMLParser</code></p> <p>Simple subclass of :class:<code>html.parser.HTMLParser</code>.</p> <p>Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via <code>HTMLTextExtractor.get_text()</code>.</p> Note <p>Users probably shouldn't deal with this class directly; instead, use <code>:func:</code>remove.remove_html_tags()`.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the parser.</p> <code>get_text</code> <p>Return the collected text.</p> <code>handle_data</code> <p>Handle data elements.</p> Source code in <code>lexos/scrubber/resources.py</code> <pre><code>class HTMLTextExtractor(html.parser.HTMLParser):\n    \"\"\"Simple subclass of :class:`html.parser.HTMLParser`.\n\n    Collects data elements (non-tag, -comment, -pi, etc. elements)\n    fed to the parser, then make them available as stripped, concatenated\n    text via `HTMLTextExtractor.get_text()`.\n\n    Note:\n        Users probably shouldn't deal with this class directly;\n        instead, use `:func:`remove.remove_html_tags()`.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the parser.\"\"\"\n        super().__init__()\n        self.data = []\n\n    def handle_data(self, data: Any) -&gt; None:\n        \"\"\"Handle data elements.\n\n        Args:\n            data (Any): The data element(s) to handle.\n        \"\"\"\n        self.data.append(data)\n\n    def get_text(self, sep: Optional[str] = \"\") -&gt; str:\n        \"\"\"Return the collected text.\n\n        Args:\n            sep (Optional[str]): The separator to join the collected text with.\n\n        Returns:\n            str: The collected text.\n        \"\"\"\n        return sep.join(self.data).strip()\n</code></pre>"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the parser.</p> Source code in <code>lexos/scrubber/resources.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the parser.\"\"\"\n    super().__init__()\n    self.data = []\n</code></pre>"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.get_text","title":"<code>get_text(sep: Optional[str] = '') -&gt; str</code>","text":"<p>Return the collected text.</p> <p>Parameters:</p> Name Type Description Default <code>sep</code> <code>Optional[str]</code> <p>The separator to join the collected text with.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The collected text.</p> Source code in <code>lexos/scrubber/resources.py</code> <pre><code>def get_text(self, sep: Optional[str] = \"\") -&gt; str:\n    \"\"\"Return the collected text.\n\n    Args:\n        sep (Optional[str]): The separator to join the collected text with.\n\n    Returns:\n        str: The collected text.\n    \"\"\"\n    return sep.join(self.data).strip()\n</code></pre>"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.handle_data","title":"<code>handle_data(data: Any) -&gt; None</code>","text":"<p>Handle data elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data element(s) to handle.</p> required Source code in <code>lexos/scrubber/resources.py</code> <pre><code>def handle_data(self, data: Any) -&gt; None:\n    \"\"\"Handle data elements.\n\n    Args:\n        data (Any): The data element(s) to handle.\n    \"\"\"\n    self.data.append(data)\n</code></pre>"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_LINEBREAK","title":"<code>RE_LINEBREAK: Pattern = re.compile('(\\\\r\\\\n|[\\\\n\\\\v])+')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_NONBREAKING_SPACE","title":"<code>RE_NONBREAKING_SPACE: Pattern = re.compile('[^\\\\S\\\\n\\\\v]+')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_ZWSP","title":"<code>RE_ZWSP: Pattern = re.compile('[\\\\u200B\\\\u2060\\\\uFEFF]+')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_TAB","title":"<code>RE_TAB: Pattern = re.compile('[\\\\t\\\\v]+')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_BRACKETS_CURLY","title":"<code>RE_BRACKETS_CURLY = re.compile('\\\\{[^{}]*?\\\\}')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_BRACKETS_ROUND","title":"<code>RE_BRACKETS_ROUND = re.compile('\\\\([^()]*?\\\\)')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_BRACKETS_SQUARE","title":"<code>RE_BRACKETS_SQUARE = re.compile('\\\\[[^\\\\[\\\\]]*?\\\\]')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_BULLET_POINTS","title":"<code>RE_BULLET_POINTS = re.compile('((^|\\\\n)\\\\s*?)([\\\\u2022\\\\u2023\\\\u2043\\\\u204C\\\\u204D\\\\u2219\\\\u25aa\\\\u25CF\\\\u25E6\\\\u29BE\\\\u29BF\\\\u30fb])')</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/resources/#lexos.scrubber.resources.RE_URL","title":"<code>RE_URL: Pattern = re.compile('(?:^|(?&lt;![\\\\w/.]))(?:(?:https?://|ftp://|www\\\\d{0,3}\\\\.))(?:\\\\S+(?::\\\\S*)?@)?(?:(?!(?:10|127)(?:\\\\.\\\\d{1,3}){3})(?!(?:169\\\\.254|192\\\\.168)(?:\\\\.\\\\d{1,3}){2})(?!172\\\\.(?:1[6-9]|2\\\\d|3[0-1])(?:\\\\.\\\\d{1,3}){2})(?:[1-9]\\\\d?|1\\\\d\\\\d|2[01]\\\\d|22[0-3])(?:\\\\.(?:1?\\\\d{1,2}|2[0-4]\\\\d|25[0-5])){2}(?:\\\\.(?:[1-9]\\\\d?|1\\\\d\\\\d|2[0-4]\\\\d|25[0-4]))|(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,})))(?::\\\\d{2,5})?(?:/\\\\S*)?(?:$|(?![\\\\w?!+&amp;/]))', flags=(re.IGNORECASE))</code>  <code>module-attribute</code>","text":""},{"location":"api/scrubber/scrubber/","title":"Scrubber","text":"<p>The main class of the submodule, containing the main functions users will use.</p>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Pipe","title":"<code>Pipe</code>","text":"<p>A Pydantic dataclass containing a pipeline component.</p> Calls <p>The class is callable and returns a function that takes a string and returns a string.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Call the pipeline component on the text.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass Pipe:\n    \"\"\"A Pydantic dataclass containing a pipeline component.\n\n    Calls:\n        The class is callable and returns a function that takes a string and returns a string.\n    \"\"\"\n\n    name: str = Field(..., description=\"The name of the component.\")\n    opts: Optional[dict[str, Any]] = Field(\n        default={}, description=\"Options to pass to the component.\"\n    )\n    factory: Optional[catalogue.Registry] = Field(\n        default_factory=lambda: scrubber_components,\n        description=\"The factory to use to get the component.\",\n    )\n\n    def __call__(self, text: str) -&gt; Callable:\n        \"\"\"Call the pipeline component on the text.\n\n        Args:\n            text (str): The text to scrub.\n\n        Returns:\n            Callable: A function that takes a string and returns a string.\n        \"\"\"\n        try:\n            func = self.factory.get(self.name)\n            return func(text, **self.opts)\n        except NameError as e:\n            raise LexosException(e)\n        except catalogue.RegistryError as e:\n            raise LexosException(e)\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Pipe.__call__","title":"<code>__call__(text: str) -&gt; Callable</code>","text":"<p>Call the pipeline component on the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to scrub.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A function that takes a string and returns a string.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def __call__(self, text: str) -&gt; Callable:\n    \"\"\"Call the pipeline component on the text.\n\n    Args:\n        text (str): The text to scrub.\n\n    Returns:\n        Callable: A function that takes a string and returns a string.\n    \"\"\"\n    try:\n        func = self.factory.get(self.name)\n        return func(text, **self.opts)\n    except NameError as e:\n        raise LexosException(e)\n    except catalogue.RegistryError as e:\n        raise LexosException(e)\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Pipe.name","title":"<code>name: str = Field(..., description='The name of the component.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Pipe.opts","title":"<code>opts: Optional[dict[str, Any]] = Field(default={}, description='Options to pass to the component.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Pipe.factory","title":"<code>factory: Optional[catalogue.Registry] = Field(default_factory=(lambda: scrubber_components), description='The factory to use to get the component.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Pipe.__call__","title":"<code>__call__(text: str) -&gt; Callable</code>","text":"<p>Call the pipeline component on the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to scrub.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A function that takes a string and returns a string.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def __call__(self, text: str) -&gt; Callable:\n    \"\"\"Call the pipeline component on the text.\n\n    Args:\n        text (str): The text to scrub.\n\n    Returns:\n        Callable: A function that takes a string and returns a string.\n    \"\"\"\n    try:\n        func = self.factory.get(self.name)\n        return func(text, **self.opts)\n    except NameError as e:\n        raise LexosException(e)\n    except catalogue.RegistryError as e:\n        raise LexosException(e)\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber","title":"<code>Scrubber</code>","text":"<p>A class to scrub text using a pipeline of components.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the Scrubber object.</p> <code>add_pipe</code> <p>Add a component to the scrubber pipeline.</p> <code>pipe</code> <p>Scrub a list of texts with the current pipeline.</p> <code>remove_pipe</code> <p>Remove a component from the scrubber pipeline.</p> <code>reset</code> <p>Remove all components from the pipeline.</p> <code>scrub</code> <p>Run a text through the scrubber pipeline.</p> <p>Attributes:</p> Name Type Description <code>pipes</code> <code>list[Pipe]</code> <p>Return a list of the names of the pipeline components.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>class Scrubber:\n    \"\"\"A class to scrub text using a pipeline of components.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the Scrubber object.\"\"\"\n        self._components: list[Pipe] = []\n\n    @property\n    def pipes(self) -&gt; list[Pipe]:\n        \"\"\"Return a list of the names of the pipeline components.\"\"\"\n        return [component.name for component in self._components]\n\n    def _get_pipe_index(\n        self,\n        before: Optional[str | int] = None,\n        after: Optional[str | int] = None,\n        first: Optional[bool] = None,\n        last: Optional[bool] = None,\n    ) -&gt; int:\n        \"\"\"Determine where to insert a pipeline component based on the before/after/first/last values.\n\n        Args:\n            before (str): Name or index of the component to insert directly before.\n            after (str): Name or index of component to insert directly after.\n            first (bool): If True, insert component first in the pipeline.\n            last (bool): If True, insert component last in the pipeline.\n\n        Returns:\n            (int): The index of the new pipeline component.\n        \"\"\"\n        if sum(arg is not None for arg in [before, after, first, last]) &gt;= 2:\n            raise LexosException(\"Only one of before, after, first, last can be set.\")\n        if last or not any(value is not None for value in [first, before, after]):\n            return len(self._components)\n        elif first:\n            return 0\n        elif isinstance(before, str):\n            if before not in self.pipes:\n                raise LexosException(\n                    f\"The component name {before} is not in the pipeline.\"\n                )\n            return self.pipes.index(before)\n        elif isinstance(after, str):\n            if after not in self.pipes:\n                raise LexosException(\n                    f\"The component name {after} is not in the pipeline.\"\n                )\n            return self.pipes.index(after) + 1\n        # We only accept indices referring to components that exist.\n        # We can't use isinstance here because bools are instance of int.\n        elif type(before) is int:\n            if before &gt;= len(self._components) or before &lt; 0:\n                raise ValueError(f\"Index {before} out of range.\")\n            return before\n        elif type(after) is int:\n            if after &gt;= len(self._components) or after &lt; 0:\n                raise ValueError(f\"Index {after} out of range.\")\n            return after + 1\n        raise ValueError(\"Invalid combination of before, after, first, last.\")\n\n    def add_pipe(\n        self,\n        components: ScrubberComponent,\n        *,\n        before: Optional[str | int] = None,\n        after: Optional[str | int] = None,\n        first: Optional[bool] = None,\n        last: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"Add a component to the scrubber pipeline.\n\n        Args:\n            components (ScrubberComponent):\n                The component to add to the pipeline. If a string is passed, it is assumed\n                to be the name of the component. If a tuple is passed, the first element\n                is assumed to be the name of the component and the second element is\n                assumed to be a dictionary of options to pass to the component. The method\n                also accepts Pipe objects.\n            before (str | int): Name or index of the component to insert new\n                component directly before.\n            after (str | int): Name or index of the component to insert new\n                component directly after.\n            first (bool): If True, insert component first in the pipeline.\n            last (bool): If True, insert component last in the pipeline.\n        \"\"\"\n        # Ensure a list of tuples\n        components = ensure_list(components)\n        pipes = []\n        for component in components:\n            if isinstance(component, partial):\n                name = component.func.__name__\n                opts = component.keywords\n                pipes.append(Pipe(name=name, opts=opts))\n            elif isinstance(component, Pipe):\n                pipes.append(component)\n            elif isinstance(component, str):\n                pipes.append(Pipe(name=component, opts={}))\n            elif isinstance(component, tuple):\n                name, opts = component\n                pipes.append(Pipe(name=name, opts=opts))\n            else:\n                raise LexosException(\n                    \"Components must be strings, tuples, functools.partial, or Pipe objects.\"\n                )\n        # Create and Pipe objects and insert them into the pipeline\n        pipe_index = self._get_pipe_index(before, after, first, last)\n        for component in pipes:\n            # If component exists, merge options\n            if component.name in self.pipes:\n                # Find the index of the existing component\n                idx = self.pipes.index(component.name)\n                instance_opts = self._components[idx].opts\n                component.opts = {**instance_opts, **component.opts}\n            # Insert the component\n            self._components.insert(pipe_index, component)\n            pipe_index += 1\n\n    def pipe(\n        self,\n        texts: Iterable[str],\n        *,\n        disable: Optional[list[str]] = [],\n        component_cfg: Optional[dict[str, dict[str, Any]]] = {},\n    ) -&gt; Iterable[str]:\n        \"\"\"Scrub a list of texts with the current pipeline.\n\n        Args:\n            texts (Iterable[str]): The text(s) to scrub.\n            disable\t(Optional[list[str]]): Names of pipeline components to disable.\n            component_cfg (Optional[dict[str, dict[str, Any]]]): Optional dictionary of keyword arguments for components, keyed by component names. Defaults to None.\n\n        Yields:\n            Iterable: An iterator of scrubbed texts.\n        \"\"\"\n        pipes = []\n        for pipe in self._components:\n            if pipe.name in disable:\n                continue\n            kwargs = component_cfg.get(pipe.name, None)\n            # Allow component_cfg to overwrite the pipe options.\n            if kwargs is not None:\n                pipe.opts = kwargs\n            pipes.append(pipe)\n        for text in texts:\n            for pipe in pipes:\n                text = pipe(text)\n            yield text\n\n    def remove_pipe(self, components: str | Iterable[str]) -&gt; None:\n        \"\"\"Remove a component from the scrubber pipeline.\n\n        Args:\n            components (str | Iterable[str]): The name of the component to remove from the pipeline.\n        \"\"\"\n        self._components = [\n            pipe\n            for pipe in self._components\n            if pipe.name not in ensure_list(components)\n        ]\n\n    def reset(self) -&gt; None:\n        \"\"\"Remove all components from the pipeline.\"\"\"\n        self._components = []\n\n    def scrub(self, text: str) -&gt; str:\n        \"\"\"Run a text through the scrubber pipeline.\n\n        Args:\n            text (str): The text to scrub.\n\n        Returns:\n            str: The scrubbed text.\n        \"\"\"\n        for pipe in self._components:\n            text = pipe(text)\n        return text\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.pipes","title":"<code>pipes: list[Pipe]</code>  <code>property</code>","text":"<p>Return a list of the names of the pipeline components.</p>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.__init__","title":"<code>__init__() -&gt; None</code>","text":"<p>Initialize the Scrubber object.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Scrubber object.\"\"\"\n    self._components: list[Pipe] = []\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.add_pipe","title":"<code>add_pipe(components: ScrubberComponent, *, before: Optional[str | int] = None, after: Optional[str | int] = None, first: Optional[bool] = None, last: Optional[bool] = None) -&gt; None</code>","text":"<p>Add a component to the scrubber pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>ScrubberComponent</code> <p>The component to add to the pipeline. If a string is passed, it is assumed to be the name of the component. If a tuple is passed, the first element is assumed to be the name of the component and the second element is assumed to be a dictionary of options to pass to the component. The method also accepts Pipe objects.</p> required <code>before</code> <code>str | int</code> <p>Name or index of the component to insert new component directly before.</p> <code>None</code> <code>after</code> <code>str | int</code> <p>Name or index of the component to insert new component directly after.</p> <code>None</code> <code>first</code> <code>bool</code> <p>If True, insert component first in the pipeline.</p> <code>None</code> <code>last</code> <code>bool</code> <p>If True, insert component last in the pipeline.</p> <code>None</code> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def add_pipe(\n    self,\n    components: ScrubberComponent,\n    *,\n    before: Optional[str | int] = None,\n    after: Optional[str | int] = None,\n    first: Optional[bool] = None,\n    last: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Add a component to the scrubber pipeline.\n\n    Args:\n        components (ScrubberComponent):\n            The component to add to the pipeline. If a string is passed, it is assumed\n            to be the name of the component. If a tuple is passed, the first element\n            is assumed to be the name of the component and the second element is\n            assumed to be a dictionary of options to pass to the component. The method\n            also accepts Pipe objects.\n        before (str | int): Name or index of the component to insert new\n            component directly before.\n        after (str | int): Name or index of the component to insert new\n            component directly after.\n        first (bool): If True, insert component first in the pipeline.\n        last (bool): If True, insert component last in the pipeline.\n    \"\"\"\n    # Ensure a list of tuples\n    components = ensure_list(components)\n    pipes = []\n    for component in components:\n        if isinstance(component, partial):\n            name = component.func.__name__\n            opts = component.keywords\n            pipes.append(Pipe(name=name, opts=opts))\n        elif isinstance(component, Pipe):\n            pipes.append(component)\n        elif isinstance(component, str):\n            pipes.append(Pipe(name=component, opts={}))\n        elif isinstance(component, tuple):\n            name, opts = component\n            pipes.append(Pipe(name=name, opts=opts))\n        else:\n            raise LexosException(\n                \"Components must be strings, tuples, functools.partial, or Pipe objects.\"\n            )\n    # Create and Pipe objects and insert them into the pipeline\n    pipe_index = self._get_pipe_index(before, after, first, last)\n    for component in pipes:\n        # If component exists, merge options\n        if component.name in self.pipes:\n            # Find the index of the existing component\n            idx = self.pipes.index(component.name)\n            instance_opts = self._components[idx].opts\n            component.opts = {**instance_opts, **component.opts}\n        # Insert the component\n        self._components.insert(pipe_index, component)\n        pipe_index += 1\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.pipe","title":"<code>pipe(texts: Iterable[str], *, disable: Optional[list[str]] = [], component_cfg: Optional[dict[str, dict[str, Any]]] = {}) -&gt; Iterable[str]</code>","text":"<p>Scrub a list of texts with the current pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>The text(s) to scrub.</p> required <code>disable</code> <code>Optional[list[str]]</code> <p>Names of pipeline components to disable.</p> <code>[]</code> <code>component_cfg</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Optional dictionary of keyword arguments for components, keyed by component names. Defaults to None.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>Iterable</code> <code>Iterable[str]</code> <p>An iterator of scrubbed texts.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def pipe(\n    self,\n    texts: Iterable[str],\n    *,\n    disable: Optional[list[str]] = [],\n    component_cfg: Optional[dict[str, dict[str, Any]]] = {},\n) -&gt; Iterable[str]:\n    \"\"\"Scrub a list of texts with the current pipeline.\n\n    Args:\n        texts (Iterable[str]): The text(s) to scrub.\n        disable\t(Optional[list[str]]): Names of pipeline components to disable.\n        component_cfg (Optional[dict[str, dict[str, Any]]]): Optional dictionary of keyword arguments for components, keyed by component names. Defaults to None.\n\n    Yields:\n        Iterable: An iterator of scrubbed texts.\n    \"\"\"\n    pipes = []\n    for pipe in self._components:\n        if pipe.name in disable:\n            continue\n        kwargs = component_cfg.get(pipe.name, None)\n        # Allow component_cfg to overwrite the pipe options.\n        if kwargs is not None:\n            pipe.opts = kwargs\n        pipes.append(pipe)\n    for text in texts:\n        for pipe in pipes:\n            text = pipe(text)\n        yield text\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.remove_pipe","title":"<code>remove_pipe(components: str | Iterable[str]) -&gt; None</code>","text":"<p>Remove a component from the scrubber pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>str | Iterable[str]</code> <p>The name of the component to remove from the pipeline.</p> required Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def remove_pipe(self, components: str | Iterable[str]) -&gt; None:\n    \"\"\"Remove a component from the scrubber pipeline.\n\n    Args:\n        components (str | Iterable[str]): The name of the component to remove from the pipeline.\n    \"\"\"\n    self._components = [\n        pipe\n        for pipe in self._components\n        if pipe.name not in ensure_list(components)\n    ]\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Remove all components from the pipeline.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Remove all components from the pipeline.\"\"\"\n    self._components = []\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.scrub","title":"<code>scrub(text: str) -&gt; str</code>","text":"<p>Run a text through the scrubber pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to scrub.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The scrubbed text.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def scrub(self, text: str) -&gt; str:\n    \"\"\"Run a text through the scrubber pipeline.\n\n    Args:\n        text (str): The text to scrub.\n\n    Returns:\n        str: The scrubbed text.\n    \"\"\"\n    for pipe in self._components:\n        text = pipe(text)\n    return text\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.__init__","title":"<code>__init__() -&gt; None</code>","text":"<p>Initialize the Scrubber object.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Scrubber object.\"\"\"\n    self._components: list[Pipe] = []\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.pipes","title":"<code>pipes: list[Pipe]</code>  <code>property</code>","text":"<p>Return a list of the names of the pipeline components.</p>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber._get_pipe_index","title":"<code>_get_pipe_index(before: Optional[str | int] = None, after: Optional[str | int] = None, first: Optional[bool] = None, last: Optional[bool] = None) -&gt; int</code>","text":"<p>Determine where to insert a pipeline component based on the before/after/first/last values.</p> <p>Parameters:</p> Name Type Description Default <code>before</code> <code>str</code> <p>Name or index of the component to insert directly before.</p> <code>None</code> <code>after</code> <code>str</code> <p>Name or index of component to insert directly after.</p> <code>None</code> <code>first</code> <code>bool</code> <p>If True, insert component first in the pipeline.</p> <code>None</code> <code>last</code> <code>bool</code> <p>If True, insert component last in the pipeline.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The index of the new pipeline component.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def _get_pipe_index(\n    self,\n    before: Optional[str | int] = None,\n    after: Optional[str | int] = None,\n    first: Optional[bool] = None,\n    last: Optional[bool] = None,\n) -&gt; int:\n    \"\"\"Determine where to insert a pipeline component based on the before/after/first/last values.\n\n    Args:\n        before (str): Name or index of the component to insert directly before.\n        after (str): Name or index of component to insert directly after.\n        first (bool): If True, insert component first in the pipeline.\n        last (bool): If True, insert component last in the pipeline.\n\n    Returns:\n        (int): The index of the new pipeline component.\n    \"\"\"\n    if sum(arg is not None for arg in [before, after, first, last]) &gt;= 2:\n        raise LexosException(\"Only one of before, after, first, last can be set.\")\n    if last or not any(value is not None for value in [first, before, after]):\n        return len(self._components)\n    elif first:\n        return 0\n    elif isinstance(before, str):\n        if before not in self.pipes:\n            raise LexosException(\n                f\"The component name {before} is not in the pipeline.\"\n            )\n        return self.pipes.index(before)\n    elif isinstance(after, str):\n        if after not in self.pipes:\n            raise LexosException(\n                f\"The component name {after} is not in the pipeline.\"\n            )\n        return self.pipes.index(after) + 1\n    # We only accept indices referring to components that exist.\n    # We can't use isinstance here because bools are instance of int.\n    elif type(before) is int:\n        if before &gt;= len(self._components) or before &lt; 0:\n            raise ValueError(f\"Index {before} out of range.\")\n        return before\n    elif type(after) is int:\n        if after &gt;= len(self._components) or after &lt; 0:\n            raise ValueError(f\"Index {after} out of range.\")\n        return after + 1\n    raise ValueError(\"Invalid combination of before, after, first, last.\")\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.add_pipe","title":"<code>add_pipe(components: ScrubberComponent, *, before: Optional[str | int] = None, after: Optional[str | int] = None, first: Optional[bool] = None, last: Optional[bool] = None) -&gt; None</code>","text":"<p>Add a component to the scrubber pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>ScrubberComponent</code> <p>The component to add to the pipeline. If a string is passed, it is assumed to be the name of the component. If a tuple is passed, the first element is assumed to be the name of the component and the second element is assumed to be a dictionary of options to pass to the component. The method also accepts Pipe objects.</p> required <code>before</code> <code>str | int</code> <p>Name or index of the component to insert new component directly before.</p> <code>None</code> <code>after</code> <code>str | int</code> <p>Name or index of the component to insert new component directly after.</p> <code>None</code> <code>first</code> <code>bool</code> <p>If True, insert component first in the pipeline.</p> <code>None</code> <code>last</code> <code>bool</code> <p>If True, insert component last in the pipeline.</p> <code>None</code> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def add_pipe(\n    self,\n    components: ScrubberComponent,\n    *,\n    before: Optional[str | int] = None,\n    after: Optional[str | int] = None,\n    first: Optional[bool] = None,\n    last: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"Add a component to the scrubber pipeline.\n\n    Args:\n        components (ScrubberComponent):\n            The component to add to the pipeline. If a string is passed, it is assumed\n            to be the name of the component. If a tuple is passed, the first element\n            is assumed to be the name of the component and the second element is\n            assumed to be a dictionary of options to pass to the component. The method\n            also accepts Pipe objects.\n        before (str | int): Name or index of the component to insert new\n            component directly before.\n        after (str | int): Name or index of the component to insert new\n            component directly after.\n        first (bool): If True, insert component first in the pipeline.\n        last (bool): If True, insert component last in the pipeline.\n    \"\"\"\n    # Ensure a list of tuples\n    components = ensure_list(components)\n    pipes = []\n    for component in components:\n        if isinstance(component, partial):\n            name = component.func.__name__\n            opts = component.keywords\n            pipes.append(Pipe(name=name, opts=opts))\n        elif isinstance(component, Pipe):\n            pipes.append(component)\n        elif isinstance(component, str):\n            pipes.append(Pipe(name=component, opts={}))\n        elif isinstance(component, tuple):\n            name, opts = component\n            pipes.append(Pipe(name=name, opts=opts))\n        else:\n            raise LexosException(\n                \"Components must be strings, tuples, functools.partial, or Pipe objects.\"\n            )\n    # Create and Pipe objects and insert them into the pipeline\n    pipe_index = self._get_pipe_index(before, after, first, last)\n    for component in pipes:\n        # If component exists, merge options\n        if component.name in self.pipes:\n            # Find the index of the existing component\n            idx = self.pipes.index(component.name)\n            instance_opts = self._components[idx].opts\n            component.opts = {**instance_opts, **component.opts}\n        # Insert the component\n        self._components.insert(pipe_index, component)\n        pipe_index += 1\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.pipe","title":"<code>pipe(texts: Iterable[str], *, disable: Optional[list[str]] = [], component_cfg: Optional[dict[str, dict[str, Any]]] = {}) -&gt; Iterable[str]</code>","text":"<p>Scrub a list of texts with the current pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>The text(s) to scrub.</p> required <code>disable</code> <code>Optional[list[str]]</code> <p>Names of pipeline components to disable.</p> <code>[]</code> <code>component_cfg</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Optional dictionary of keyword arguments for components, keyed by component names. Defaults to None.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>Iterable</code> <code>Iterable[str]</code> <p>An iterator of scrubbed texts.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def pipe(\n    self,\n    texts: Iterable[str],\n    *,\n    disable: Optional[list[str]] = [],\n    component_cfg: Optional[dict[str, dict[str, Any]]] = {},\n) -&gt; Iterable[str]:\n    \"\"\"Scrub a list of texts with the current pipeline.\n\n    Args:\n        texts (Iterable[str]): The text(s) to scrub.\n        disable\t(Optional[list[str]]): Names of pipeline components to disable.\n        component_cfg (Optional[dict[str, dict[str, Any]]]): Optional dictionary of keyword arguments for components, keyed by component names. Defaults to None.\n\n    Yields:\n        Iterable: An iterator of scrubbed texts.\n    \"\"\"\n    pipes = []\n    for pipe in self._components:\n        if pipe.name in disable:\n            continue\n        kwargs = component_cfg.get(pipe.name, None)\n        # Allow component_cfg to overwrite the pipe options.\n        if kwargs is not None:\n            pipe.opts = kwargs\n        pipes.append(pipe)\n    for text in texts:\n        for pipe in pipes:\n            text = pipe(text)\n        yield text\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.remove_pipe","title":"<code>remove_pipe(components: str | Iterable[str]) -&gt; None</code>","text":"<p>Remove a component from the scrubber pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>str | Iterable[str]</code> <p>The name of the component to remove from the pipeline.</p> required Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def remove_pipe(self, components: str | Iterable[str]) -&gt; None:\n    \"\"\"Remove a component from the scrubber pipeline.\n\n    Args:\n        components (str | Iterable[str]): The name of the component to remove from the pipeline.\n    \"\"\"\n    self._components = [\n        pipe\n        for pipe in self._components\n        if pipe.name not in ensure_list(components)\n    ]\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Remove all components from the pipeline.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Remove all components from the pipeline.\"\"\"\n    self._components = []\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.scrub","title":"<code>scrub(text: str) -&gt; str</code>","text":"<p>Run a text through the scrubber pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to scrub.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The scrubbed text.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def scrub(self, text: str) -&gt; str:\n    \"\"\"Run a text through the scrubber pipeline.\n\n    Args:\n        text (str): The text to scrub.\n\n    Returns:\n        str: The scrubbed text.\n    \"\"\"\n    for pipe in self._components:\n        text = pipe(text)\n    return text\n</code></pre>"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.scrub","title":"<code>scrub(text: str, pipeline: PipelineComponents, factory: Optional[catalogue.Registry] = scrubber_components) -&gt; str</code>","text":"<p>Scrub a text with a pipeline of components.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to scrub.</p> required <code>pipeline</code> <code>PipelineComponents</code> <p>An iterable of components. These can be functions, partial functions, Pipe objects, tuple, or strings. If a string is passed, it is assumed to be the name of the component. If a tuple is passed, the first element is assumed to be the name of the component and the second element is assumed to be a dictionary of options to pass.</p> required <code>factory</code> <code>Optional[Registry]</code> <p>The factory to use to get the components. Defaults to scrubber_components.</p> <code>scrubber_components</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The scrubbed text.</p> Source code in <code>lexos/scrubber/scrubber.py</code> <pre><code>def scrub(\n    text: str,\n    pipeline: PipelineComponents,\n    factory: Optional[catalogue.Registry] = scrubber_components,\n) -&gt; str:\n    \"\"\"Scrub a text with a pipeline of components.\n\n    Args:\n        text (str): The text to scrub.\n        pipeline (PipelineComponents): An iterable of components. These can be\n            functions, partial functions, Pipe objects, tuple, or strings. If a\n            string is passed, it is assumed to be the name of the component.\n            If a tuple is passed, the first element is assumed to be the name of\n            the component and the second element is assumed to be a dictionary\n            of options to pass.\n        factory (Optional[catalogue.Registry], optional): The factory to use to get the components. Defaults to scrubber_components.\n\n    Returns:\n        str: The scrubbed text.\n    \"\"\"\n    for pipe in pipeline:\n        if isinstance(pipe, (Callable, partial)):\n            text = pipe(text)\n        elif isinstance(pipe, tuple):\n            func, opts = pipe\n            text = func(text, **opts)\n        else:\n            try:\n                func = factory.get(pipe)\n                text = func(text)\n            except AttributeError as e:\n                raise LexosException(e)\n            except catalogue.RegistryError as e:\n                raise LexosException(e)\n    return text\n</code></pre>"},{"location":"api/scrubber/tags/","title":"Tags","text":"<p>A set of functions to replace or remove HTML/XML tags using Beautiful Soup.</p>"},{"location":"api/scrubber/tags/#lexos.scrubber.tags.remove_attribute","title":"<code>remove_attribute(text: str, selector: str, attribute: str = None, mode: str = 'html', matcher_type: str = 'exact', attribute_value: Optional[str] = None, attribute_filter: Optional[str] = None) -&gt; str</code>","text":"<p>Removes attributes from HTML/XML elements.</p> <p>Removes specified attributes from elements matching the selector. Can filter elements by specific attribute or attribute value.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>HTML or XML text to process</p> required <code>selector</code> <code>str</code> <p>Tag name or CSS selector to match elements</p> required <code>attribute</code> <code>str</code> <p>Attribute name to remove.</p> <code>None</code> <code>mode</code> <code>str</code> <p>Parser mode, either \"html\" or \"xml\"</p> <code>'html'</code> <code>matcher_type</code> <code>str</code> <p>Type of match to perform, either \"exact\", \"contains\", or \"regex\"</p> <code>'exact'</code> <code>attribute_value</code> <code>Optional[str]</code> <p>Optional value for the attribute filter</p> <code>None</code> <code>attribute_filter</code> <code>Optional[str]</code> <p>Optional attribute name to filter elements</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Processed text with attributes removed from matching elements</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If mode is not \"html\" or \"xml\"</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = '&lt;div class=\"main\" id=\"content\"&gt;Text&lt;/div&gt;'\n&gt;&gt;&gt; remove_attributes(text, \"div\", \"class\")\n'&lt;div id=\"content\"&gt;Text&lt;/div&gt;'\n</code></pre> <pre><code>&gt;&gt;&gt; text = '&lt;p class=\"a\"&gt;Keep&lt;/p&gt;&lt;p class=\"b\" id=\"x\"&gt;Remove attrs&lt;/p&gt;'\n&gt;&gt;&gt; remove_attributes(text, \"p\", attribute_filter=\"class\", attribute_value=\"b\")\n'&lt;p class=\"a\"&gt;Keep&lt;/p&gt;&lt;p&gt;Remove attrs&lt;/p&gt;'\n</code></pre> Source code in <code>lexos/scrubber/tags.py</code> <pre><code>def remove_attribute(\n    text: str,\n    selector: str,\n    attribute: str = None,\n    mode: str = \"html\",\n    matcher_type: str = \"exact\",\n    attribute_value: Optional[str] = None,\n    attribute_filter: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Removes attributes from HTML/XML elements.\n\n    Removes specified attributes from elements matching the selector.\n    Can filter elements by specific attribute or attribute value.\n\n    Args:\n        text: HTML or XML text to process\n        selector: Tag name or CSS selector to match elements\n        attribute: Attribute name to remove.\n        mode: Parser mode, either \"html\" or \"xml\"\n        matcher_type: Type of match to perform, either \"exact\", \"contains\", or \"regex\"\n        attribute_value: Optional value for the attribute filter\n        attribute_filter: Optional attribute name to filter elements\n\n    Returns:\n        Processed text with attributes removed from matching elements\n\n    Raises:\n        LexosException: If mode is not \"html\" or \"xml\"\n\n    Examples:\n        &gt;&gt;&gt; text = '&lt;div class=\"main\" id=\"content\"&gt;Text&lt;/div&gt;'\n        &gt;&gt;&gt; remove_attributes(text, \"div\", \"class\")\n        '&lt;div id=\"content\"&gt;Text&lt;/div&gt;'\n\n        &gt;&gt;&gt; text = '&lt;p class=\"a\"&gt;Keep&lt;/p&gt;&lt;p class=\"b\" id=\"x\"&gt;Remove attrs&lt;/p&gt;'\n        &gt;&gt;&gt; remove_attributes(text, \"p\", attribute_filter=\"class\", attribute_value=\"b\")\n        '&lt;p class=\"a\"&gt;Keep&lt;/p&gt;&lt;p&gt;Remove attrs&lt;/p&gt;'\n    \"\"\"\n    # Get matching elements\n    soup, elements = _match_elements(\n        selector, text, mode, matcher_type, attribute, attribute_value, attribute_filter\n    )\n\n    # Filter by attribute if specified\n    # if attribute_filter:\n    #     if attribute_value:\n    #         elements = [\n    #             el\n    #             for el in elements\n    #             if el.has_attr(attribute_filter)\n    #             and _match_value(el[attribute_filter], attribute_value, matcher_type)\n    #         ]\n    #     else:\n    #         # Filter elements that have the attribute regardless of value\n    #         elements = [el for el in elements if el.has_attr(attribute_filter)]\n\n    # Remove specified attributes from matching elements\n    for element in elements:\n        if attribute:\n            # Remove only the specified attribute\n            if element.has_attr(attribute):\n                del element[attribute]\n        else:\n            # Remove all attributes\n            element.attrs = {}\n\n    # Return the processed document\n    return str(soup)\n</code></pre>"},{"location":"api/scrubber/tags/#lexos.scrubber.tags.remove_comments","title":"<code>remove_comments(text: str, mode: str = 'html') -&gt; str</code>","text":"<p>Removes comments from HTML or XML text.</p> <p>Uses BeautifulSoup to find and remove all comments from HTML or XML content.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>HTML or XML text to process</p> required <code>mode</code> <code>str</code> <p>Parser mode, either \"html\" or \"xml\"</p> <code>'html'</code> <p>Returns:</p> Type Description <code>str</code> <p>String containing the HTML/XML content with all comments removed</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If mode is not \"html\" or \"xml\"</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; html = '&lt;!-- Header comment --&gt;&lt;div&gt;Content&lt;/div&gt;&lt;!-- Footer --&gt;'\n&gt;&gt;&gt; remove_comments(html)\n'&lt;div&gt;Content&lt;/div&gt;'\n</code></pre> <pre><code>&gt;&gt;&gt; xml = '&lt;?xml version=\"1.0\"?&gt;&lt;!-- Config --&gt;&lt;root&gt;Data&lt;/root&gt;'\n&gt;&gt;&gt; remove_comments(xml, mode=\"xml\")\n'&lt;?xml version=\"1.0\"?&gt;&lt;root&gt;Data&lt;/root&gt;'\n</code></pre> Source code in <code>lexos/scrubber/tags.py</code> <pre><code>def remove_comments(text: str, mode: str = \"html\") -&gt; str:\n    \"\"\"Removes comments from HTML or XML text.\n\n    Uses BeautifulSoup to find and remove all comments from HTML or XML content.\n\n    Args:\n        text: HTML or XML text to process\n        mode: Parser mode, either \"html\" or \"xml\"\n\n    Returns:\n        String containing the HTML/XML content with all comments removed\n\n    Raises:\n        LexosException: If mode is not \"html\" or \"xml\"\n\n    Examples:\n        &gt;&gt;&gt; html = '&lt;!-- Header comment --&gt;&lt;div&gt;Content&lt;/div&gt;&lt;!-- Footer --&gt;'\n        &gt;&gt;&gt; remove_comments(html)\n        '&lt;div&gt;Content&lt;/div&gt;'\n\n        &gt;&gt;&gt; xml = '&lt;?xml version=\"1.0\"?&gt;&lt;!-- Config --&gt;&lt;root&gt;Data&lt;/root&gt;'\n        &gt;&gt;&gt; remove_comments(xml, mode=\"xml\")\n        '&lt;?xml version=\"1.0\"?&gt;&lt;root&gt;Data&lt;/root&gt;'\n    \"\"\"\n    # Validate mode\n    if mode not in [\"html\", \"xml\"]:\n        raise LexosException(\"Mode must be either 'html' or 'xml'.\")\n\n    # Parse the document\n    parser = \"lxml-xml\" if mode == \"xml\" else \"html.parser\"\n    soup = BeautifulSoup(text, parser)\n\n    # Find all comment nodes\n    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n\n    # Remove each comment\n    for comment in comments:\n        comment.extract()\n\n    # Return the processed document\n    return str(soup)\n</code></pre>"},{"location":"api/scrubber/tags/#lexos.scrubber.tags.remove_doctype","title":"<code>remove_doctype(text: str) -&gt; str</code>","text":"<p>Removes a document type declaration from HTML or XML text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>HTML or XML text to process</p> required <p>Returns:</p> Type Description <code>str</code> <p>String containing the HTML/XML content with document type declaration removed</p> Source code in <code>lexos/scrubber/tags.py</code> <pre><code>def remove_doctype(text: str) -&gt; str:\n    \"\"\"Removes a document type declaration from HTML or XML text.\n\n    Args:\n        text: HTML or XML text to process\n\n    Returns:\n        String containing the HTML/XML content with document type declaration removed\n    \"\"\"\n    # Remove HTML and XML doctype declarations\n    html_doctype_pattern = re.compile(r\"&lt;!DOCTYPE[^&gt;]*&gt;\", re.IGNORECASE | re.DOTALL)\n    text = re.sub(html_doctype_pattern, \"\", text)\n\n    xml_doctype_pattern = re.compile(r\"&lt;?xml[^&gt;]*&gt;\", re.IGNORECASE | re.DOTALL)\n    text = re.sub(xml_doctype_pattern, \"\", text)\n\n    # Return the processed document\n    return text\n</code></pre>"},{"location":"api/scrubber/tags/#lexos.scrubber.tags.remove_element","title":"<code>remove_element(text: str, selector: str, mode: str = 'html', matcher_type: str = 'exact', attribute: str = None, attribute_value: str = None) -&gt; str</code>","text":"<p>Removes HTML/XML elements using BeautifulSoup.</p> <p>Removes elements that match the given selector from HTML or XML text. Can further filter elements by specific attribute or attribute value.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>HTML or XML text to process</p> required <code>selector</code> <code>str</code> <p>Tag name or CSS selector to match elements for removal</p> required <code>mode</code> <code>str</code> <p>Parser mode, either \"html\" or \"xml\"</p> <code>'html'</code> <code>matcher_type</code> <code>str</code> <p>Type of match to perform, either \"exact\", \"contains\", or \"regex\"</p> <code>'exact'</code> <code>attribute</code> <code>str</code> <p>Optional attribute name to filter elements</p> <code>None</code> <code>attribute_value</code> <code>str</code> <p>Optional value for the attribute filter</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Processed text with matching elements removed</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If mode is not \"html\" or \"xml\"</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;p class='b'&gt;Remove&lt;/p&gt;&lt;div&gt;Remove&lt;/div&gt;\"\n&gt;&gt;&gt; remove_element(text, \"div\")\n'&lt;p&gt;Keep&lt;/p&gt;'\n&gt;&gt;&gt; remove_element(\"text\", \"p\", attribute=\"class\", attribute_value=\"b\")\n\"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;div&gt;Remove&lt;/div&gt;\"\n</code></pre> Source code in <code>lexos/scrubber/tags.py</code> <pre><code>def remove_element(\n    text: str,\n    selector: str,\n    mode: str = \"html\",\n    matcher_type: str = \"exact\",\n    attribute: str = None,\n    attribute_value: str = None,\n) -&gt; str:\n    \"\"\"Removes HTML/XML elements using BeautifulSoup.\n\n    Removes elements that match the given selector from HTML or XML text.\n    Can further filter elements by specific attribute or attribute value.\n\n    Args:\n        text: HTML or XML text to process\n        selector: Tag name or CSS selector to match elements for removal\n        mode: Parser mode, either \"html\" or \"xml\"\n        matcher_type: Type of match to perform, either \"exact\", \"contains\", or \"regex\"\n        attribute: Optional attribute name to filter elements\n        attribute_value: Optional value for the attribute filter\n\n    Returns:\n        Processed text with matching elements removed\n\n    Raises:\n        LexosException: If mode is not \"html\" or \"xml\"\n\n    Examples:\n        &gt;&gt;&gt; text = \"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;p class='b'&gt;Remove&lt;/p&gt;&lt;div&gt;Remove&lt;/div&gt;\"\n        &gt;&gt;&gt; remove_element(text, \"div\")\n        '&lt;p&gt;Keep&lt;/p&gt;'\n        &gt;&gt;&gt; remove_element(\"text\", \"p\", attribute=\"class\", attribute_value=\"b\")\n        \"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;div&gt;Remove&lt;/div&gt;\"\n    \"\"\"\n    # Get matching elements\n    soup, elements = _match_elements(\n        selector, text, mode, matcher_type, attribute, attribute_value\n    )\n\n    # Remove matching elements\n    for element in elements:\n        element.decompose()\n\n    # Return the processed document\n    return str(soup)\n</code></pre>"},{"location":"api/scrubber/tags/#lexos.scrubber.tags.remove_tag","title":"<code>remove_tag(text: str, selector: str, mode: str = 'html', matcher_type: str = 'exact', attribute: str = None, attribute_value: str = None) -&gt; str</code>","text":"<p>Removes HTML/XML tags but keeps their inner content.</p> <p>Removes tags matching the selector while preserving their inner content. Can filter elements by specific attribute or attribute value.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>HTML or XML text to process</p> required <code>selector</code> <code>str</code> <p>Tag name or CSS selector to match elements for unwrapping</p> required <code>mode</code> <code>str</code> <p>Parser mode, either \"html\" or \"xml\"</p> <code>'html'</code> <code>matcher_type</code> <code>str</code> <p>Type of match to perform, either \"exact\", \"contains\", or \"regex\"</p> <code>'exact'</code> <code>attribute</code> <code>str</code> <p>Optional attribute name to filter elements</p> <code>None</code> <code>attribute_value</code> <code>str</code> <p>Optional value for the attribute filter</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Processed text with matching tags unwrapped but content preserved</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If mode is not \"html\" or \"xml\"</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"&lt;div&gt;&lt;p&gt;Keep this&lt;/p&gt;&lt;/div&gt;&lt;span&gt;And this&lt;/span&gt;\"\n&gt;&gt;&gt; remove_tag(text, \"div\")\n'&lt;p&gt;Keep this&lt;/p&gt;&lt;span&gt;And this&lt;/span&gt;'\n&gt;&gt;&gt; text = \"&lt;p class='a'&gt;Keep tag&lt;/p&gt;&lt;p class='b'&gt;Remove tag only&lt;/p&gt;\"\n&gt;&gt;&gt; remove_tag(text, \"p\", attribute=\"class\", attribute_value=\"b\")\n\"&lt;p class='a'&gt;Keep tag&lt;/p&gt;Remove tag only\"\n</code></pre> Source code in <code>lexos/scrubber/tags.py</code> <pre><code>def remove_tag(\n    text: str,\n    selector: str,\n    mode: str = \"html\",\n    matcher_type: str = \"exact\",\n    attribute: str = None,\n    attribute_value: str = None,\n) -&gt; str:\n    \"\"\"Removes HTML/XML tags but keeps their inner content.\n\n    Removes tags matching the selector while preserving their inner content.\n    Can filter elements by specific attribute or attribute value.\n\n    Args:\n        text: HTML or XML text to process\n        selector: Tag name or CSS selector to match elements for unwrapping\n        mode: Parser mode, either \"html\" or \"xml\"\n        matcher_type: Type of match to perform, either \"exact\", \"contains\", or \"regex\"\n        attribute: Optional attribute name to filter elements\n        attribute_value: Optional value for the attribute filter\n\n    Returns:\n        Processed text with matching tags unwrapped but content preserved\n\n    Raises:\n        LexosException: If mode is not \"html\" or \"xml\"\n\n    Examples:\n        &gt;&gt;&gt; text = \"&lt;div&gt;&lt;p&gt;Keep this&lt;/p&gt;&lt;/div&gt;&lt;span&gt;And this&lt;/span&gt;\"\n        &gt;&gt;&gt; remove_tag(text, \"div\")\n        '&lt;p&gt;Keep this&lt;/p&gt;&lt;span&gt;And this&lt;/span&gt;'\n        &gt;&gt;&gt; text = \"&lt;p class='a'&gt;Keep tag&lt;/p&gt;&lt;p class='b'&gt;Remove tag only&lt;/p&gt;\"\n        &gt;&gt;&gt; remove_tag(text, \"p\", attribute=\"class\", attribute_value=\"b\")\n        \"&lt;p class='a'&gt;Keep tag&lt;/p&gt;Remove tag only\"\n    \"\"\"\n    # Get matching elements\n    soup, elements = _match_elements(\n        selector, text, mode, matcher_type, attribute, attribute_value\n    )\n\n    # Unwrap matching elements (remove tag but keep content)\n    for element in elements:\n        element.unwrap()\n\n    # Return the processed document\n    return str(soup)\n</code></pre>"},{"location":"api/scrubber/tags/#lexos.scrubber.tags.replace_attribute","title":"<code>replace_attribute(text: str, selector: str, old_attribute: str, new_attribute: str, mode: str = 'html', matcher_type: str = 'exact', attribute_value: Optional[str] = None, replace_value: Optional[str] = None, attribute_filter: Optional[str] = None, filter_value: Optional[str] = None) -&gt; str</code>","text":"<p>Replaces HTML/XML element attributes or their values.</p> <p>This function finds elements matching the selector and replaces attribute names or attribute values. It can filter elements by a specific attribute/value.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>HTML or XML text to process</p> required <code>selector</code> <code>str</code> <p>Tag name or CSS selector to match elements</p> required <code>old_attribute</code> <code>str</code> <p>Name of the attribute to replace</p> required <code>new_attribute</code> <code>str</code> <p>Name of the new attribute (or same name if only changing value)</p> required <code>mode</code> <code>str</code> <p>Parser mode, either \"html\" or \"xml\"</p> <code>'html'</code> <code>matcher_type</code> <code>str</code> <p>Type of match to perform, either \"exact\", \"contains\", or \"regex\"</p> <code>'exact'</code> <code>attribute_value</code> <code>Optional[str]</code> <p>Only replace attributes with this specific value</p> <code>None</code> <code>replace_value</code> <code>Optional[str]</code> <p>New value to use (keeps original value if None)</p> <code>None</code> <code>attribute_filter</code> <code>Optional[str]</code> <p>Optional attribute name to filter elements</p> <code>None</code> <code>filter_value</code> <code>Optional[str]</code> <p>Optional value for the attribute filter</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Processed text with attributes replaced in matching elements</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If mode is not \"html\" or \"xml\"</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Replace class attribute with data-type, keeping the value\n&gt;&gt;&gt; text = '&lt;div class=\"main\"&gt;Text&lt;/div&gt;'\n&gt;&gt;&gt; replace_attribute(text, \"div\", \"class\", \"data-type\")\n'&lt;div data-type=\"main\"&gt;Text&lt;/div&gt;'\n</code></pre> <pre><code>&gt;&gt;&gt; # Replace class=\"info\" with class=\"highlight\"\n&gt;&gt;&gt; text = '&lt;p class=\"info\"&gt;Text&lt;/p&gt;&lt;p class=\"data\"&gt;More&lt;/p&gt;'\n&gt;&gt;&gt; replace_attribute(text, \"p\", \"class\", \"class\", filter_value=\"info\", replace_value=\"highlight\")\n'&lt;p class=\"highlight\"&gt;Text&lt;/p&gt;&lt;p class=\"data\"&gt;More&lt;/p&gt;'\n</code></pre> <pre><code>&gt;&gt;&gt; # Only replace attributes on elements with a specific attribute value\n&gt;&gt;&gt; text = '&lt;div class=\"main\" id=\"content\"&gt;Text&lt;/div&gt;&lt;div class=\"sidebar\"&gt;Side&lt;/div&gt;'\n&gt;&gt;&gt; replace_attribute(text, \"div\", \"class\", \"role\", attribute_filter=\"id\", filter_value=\"content\")\n'&lt;div role=\"main\" id=\"content\"&gt;Text&lt;/div&gt;&lt;div class=\"sidebar\"&gt;Side&lt;/div&gt;'\n</code></pre> Source code in <code>lexos/scrubber/tags.py</code> <pre><code>def replace_attribute(\n    text: str,\n    selector: str,\n    old_attribute: str,\n    new_attribute: str,\n    mode: str = \"html\",\n    matcher_type: str = \"exact\",\n    attribute_value: Optional[str] = None,\n    replace_value: Optional[str] = None,\n    attribute_filter: Optional[str] = None,\n    filter_value: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Replaces HTML/XML element attributes or their values.\n\n    This function finds elements matching the selector and replaces attribute names\n    or attribute values. It can filter elements by a specific attribute/value.\n\n    Args:\n        text: HTML or XML text to process\n        selector: Tag name or CSS selector to match elements\n        old_attribute: Name of the attribute to replace\n        new_attribute: Name of the new attribute (or same name if only changing value)\n        mode: Parser mode, either \"html\" or \"xml\"\n        matcher_type: Type of match to perform, either \"exact\", \"contains\", or \"regex\"\n        attribute_value: Only replace attributes with this specific value\n        replace_value: New value to use (keeps original value if None)\n        attribute_filter: Optional attribute name to filter elements\n        filter_value: Optional value for the attribute filter\n\n    Returns:\n        Processed text with attributes replaced in matching elements\n\n    Raises:\n        LexosException: If mode is not \"html\" or \"xml\"\n\n    Examples:\n        &gt;&gt;&gt; # Replace class attribute with data-type, keeping the value\n        &gt;&gt;&gt; text = '&lt;div class=\"main\"&gt;Text&lt;/div&gt;'\n        &gt;&gt;&gt; replace_attribute(text, \"div\", \"class\", \"data-type\")\n        '&lt;div data-type=\"main\"&gt;Text&lt;/div&gt;'\n\n        &gt;&gt;&gt; # Replace class=\"info\" with class=\"highlight\"\n        &gt;&gt;&gt; text = '&lt;p class=\"info\"&gt;Text&lt;/p&gt;&lt;p class=\"data\"&gt;More&lt;/p&gt;'\n        &gt;&gt;&gt; replace_attribute(text, \"p\", \"class\", \"class\", filter_value=\"info\", replace_value=\"highlight\")\n        '&lt;p class=\"highlight\"&gt;Text&lt;/p&gt;&lt;p class=\"data\"&gt;More&lt;/p&gt;'\n\n        &gt;&gt;&gt; # Only replace attributes on elements with a specific attribute value\n        &gt;&gt;&gt; text = '&lt;div class=\"main\" id=\"content\"&gt;Text&lt;/div&gt;&lt;div class=\"sidebar\"&gt;Side&lt;/div&gt;'\n        &gt;&gt;&gt; replace_attribute(text, \"div\", \"class\", \"role\", attribute_filter=\"id\", filter_value=\"content\")\n        '&lt;div role=\"main\" id=\"content\"&gt;Text&lt;/div&gt;&lt;div class=\"sidebar\"&gt;Side&lt;/div&gt;'\n    \"\"\"\n    # Get matching elements\n    soup, elements = _match_elements(\n        selector, text, mode, matcher_type, old_attribute, attribute_value\n    )\n\n    # Filter by attribute if specified\n    if attribute_filter:\n        if filter_value:\n            # Filter elements that have the attribute with the specific value\n            elements = [\n                el\n                for el in elements\n                if el.has_attr(attribute_filter)\n                and el[attribute_filter] == filter_value\n            ]\n        else:\n            # Filter elements that have the attribute regardless of value\n            elements = [el for el in elements if el.has_attr(attribute_filter)]\n\n    result = []\n\n    # Replace attributes in matching elements\n    for element in elements:\n        result.append(element)\n        if element.has_attr(old_attribute):\n            # NOTE: It appears that this block is not needed\n            # Only process attributes with the specific value if provided\n            # if matcher_type == \"regex\":\n            #     check_match = re.search(\n            #         attribute_value, \" \".join(element[old_attribute])\n            #     )\n            # else:\n            #     check_match = \" \".join(element[old_attribute])\n            # if attribute_value is not None and check_match is None:\n            #     continue # Never reached because check_match is always a string\n\n            # Keep original value unless a replacement is specified\n            if replace_value:\n                # For debugging\n                # msg.text(\n                #     f\"Detected attribute value '{attribute_value}' in '{element.name}'.\"\n                # )\n                # msg.text(f\"Replaced '{old_attribute}' with '{new_attribute}'.\")\n                # msg.text(f\"Replaced '{attribute_value}' with '{replace_value}'.\")\n                # If the old attribute is a string, split it into a list\n                old_attribute_str = \" \".join(element[old_attribute])\n                if matcher_type == \"regex\":\n                    new_values = []\n                    for value in element[old_attribute]:\n                        if re.search(attribute_value, value):\n                            new_values.append(replace_value)\n                        else:\n                            new_values.append(value)\n                    replace_value = new_values\n                else:\n                    if len(replace_value) == 1:\n                        replace_value = replace_value[0]\n                    # Use string replacement (current logic)\n                    replace_value = old_attribute_str.replace(\n                        attribute_value, replace_value\n                    ).split(\" \")\n\n            value = (\n                replace_value if replace_value is not None else element[old_attribute]\n            )\n            if isinstance(value, list):\n                value = [str(v) for v in value if v]  # Remove empty strings\n                value = \" \".join(value)\n\n            # Remove old attribute if the names are different\n            if old_attribute != new_attribute:\n                del element[old_attribute]\n\n            # Set the new attribute with the appropriate value\n            element[new_attribute] = value\n\n    # Return the processed document\n    return str(soup)\n</code></pre>"},{"location":"api/scrubber/tags/#lexos.scrubber.tags.replace_tag","title":"<code>replace_tag(text: str, selector: str, replacement: str, mode: str = 'html', matcher_type: str = 'exact', attribute: str = None, attribute_value: str = None, preserve_attributes: bool = True) -&gt; str</code>","text":"<p>Replaces HTML/XML tags with another tag while preserving content.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>HTML or XML text to process</p> required <code>selector</code> <code>str</code> <p>Tag name or CSS selector to match elements for replacement</p> required <code>replacement</code> <code>str</code> <p>New tag name to replace the matched elements with</p> required <code>mode</code> <code>str</code> <p>Parser mode, either \"html\" or \"xml\"</p> <code>'html'</code> <code>matcher_type</code> <code>str</code> <p>Type of match to perform, either \"exact\", \"contains\", or \"regex\"</p> <code>'exact'</code> <code>attribute</code> <code>str</code> <p>Optional attribute name to filter elements</p> <code>None</code> <code>attribute_value</code> <code>str</code> <p>Optional value for the attribute filter</p> <code>None</code> <code>preserve_attributes</code> <code>bool</code> <p>Whether to preserve original tag attributes</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Processed text with matching tags replaced but content preserved</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If mode is not \"html\" or \"xml\"</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text = \"&lt;div&gt;&lt;p&gt;Keep this&lt;/p&gt;&lt;/div&gt;\"\n&gt;&gt;&gt; replace_tag(text, \"div\", \"section\")\n'&lt;section&gt;&lt;p&gt;Keep this&lt;/p&gt;&lt;/section&gt;'\n</code></pre> <pre><code>&gt;&gt;&gt; text = \"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;p class='b' id='x'&gt;Replace tag&lt;/p&gt;\"\n&gt;&gt;&gt; replace_tag(text, \"p\", \"span\", attribute=\"class\", attribute_value=\"b\")\n\"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;span class='b' id='x'&gt;Replace tag&lt;/span&gt;\"\n</code></pre> Source code in <code>lexos/scrubber/tags.py</code> <pre><code>def replace_tag(\n    text: str,\n    selector: str,\n    replacement: str,\n    mode: str = \"html\",\n    matcher_type: str = \"exact\",\n    attribute: str = None,\n    attribute_value: str = None,\n    preserve_attributes: bool = True,\n) -&gt; str:\n    \"\"\"Replaces HTML/XML tags with another tag while preserving content.\n\n    Args:\n        text: HTML or XML text to process\n        selector: Tag name or CSS selector to match elements for replacement\n        replacement: New tag name to replace the matched elements with\n        mode: Parser mode, either \"html\" or \"xml\"\n        matcher_type: Type of match to perform, either \"exact\", \"contains\", or \"regex\"\n        attribute: Optional attribute name to filter elements\n        attribute_value: Optional value for the attribute filter\n        preserve_attributes: Whether to preserve original tag attributes\n\n    Returns:\n        Processed text with matching tags replaced but content preserved\n\n    Raises:\n        LexosException: If mode is not \"html\" or \"xml\"\n\n    Examples:\n        &gt;&gt;&gt; text = \"&lt;div&gt;&lt;p&gt;Keep this&lt;/p&gt;&lt;/div&gt;\"\n        &gt;&gt;&gt; replace_tag(text, \"div\", \"section\")\n        '&lt;section&gt;&lt;p&gt;Keep this&lt;/p&gt;&lt;/section&gt;'\n\n        &gt;&gt;&gt; text = \"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;p class='b' id='x'&gt;Replace tag&lt;/p&gt;\"\n        &gt;&gt;&gt; replace_tag(text, \"p\", \"span\", attribute=\"class\", attribute_value=\"b\")\n        \"&lt;p class='a'&gt;Keep&lt;/p&gt;&lt;span class='b' id='x'&gt;Replace tag&lt;/span&gt;\"\n    \"\"\"\n    # Get matching elements\n    soup, elements = _match_elements(\n        selector, text, mode, matcher_type, attribute, attribute_value\n    )\n\n    # Replace matching elements with the new tag\n    for element in elements:\n        # Create a new tag with the same content\n        new_element = soup.new_tag(replacement)\n\n        # Copy all attributes if requested\n        if preserve_attributes:\n            for attr_name, attr_value in element.attrs.items():\n                new_element[attr_name] = attr_value\n\n        # Copy all child nodes\n        for child in list(element.children):\n            new_element.append(child)\n\n        # Replace the old element with the new one\n        element.replace_with(new_element)\n\n    # Return the processed document\n    return str(soup)\n</code></pre>"},{"location":"api/scrubber/utils/","title":"Utilities","text":"<p>Contains a function to get HTML/XML tags assuming well-formed XML using ETree with Beautiful Soup as a back-up.</p>"},{"location":"api/scrubber/utils/#lexos.scrubber.utils.get_tags","title":"<code>get_tags(text: str) -&gt; dict</code>","text":"<p>Get information about the tags in a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be analyzed.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names</p> <code>dict</code> <p>in the data and \"attributes\" is a list of dicts containing the attributes and values</p> <code>dict</code> <p>for those tags that have attributes.</p> Note <p>The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser.</p> Source code in <code>lexos/scrubber/utils.py</code> <pre><code>def get_tags(text: str) -&gt; dict:\n    \"\"\"Get information about the tags in a text.\n\n    Args:\n        text (str): The text to be analyzed.\n\n    Returns:\n        dict: A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names\n        in the data and \"attributes\" is a list of dicts containing the attributes and values\n        for those tags that have attributes.\n\n    Note:\n        The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls\n        back to BeautifulSoup's parser.\n    \"\"\"\n    tags = []\n    attributes = []\n\n    try:\n        root = ElementTree.fromstring(text)\n        for element in root.iter():\n            if re.sub(\"{.+}\", \"\", element.tag) not in tags:\n                tags.append(re.sub(\"{.+}\", \"\", element.tag))\n            if element.attrib != {}:\n                attributes.append({re.sub(\"{.+}\", \"\", element.tag): element.attrib})\n        tags = humansorted(tags)\n        attributes = json.loads(json.dumps(attributes, sort_keys=True))\n    except ElementTree.ParseError:\n        import bs4\n        from bs4 import BeautifulSoup\n\n        soup = BeautifulSoup(text, \"xml\")\n        for e in soup:\n            if isinstance(e, bs4.element.ProcessingInstruction):\n                e.extract()\n        [tags.append(tag.name) for tag in soup.find_all()]\n        [attributes.append({tag.name: tag.attrs}) for tag in soup.find_all()]\n        tags = humansorted(tags)\n        attributes = json.loads(json.dumps(attributes, sort_keys=True))\n    return {\"tags\": tags, \"attributes\": attributes}\n</code></pre>"},{"location":"api/tokenizer/","title":"Tokenizer","text":""},{"location":"api/tokenizer/#overview","title":"Overview","text":"<p>This module is designed to provide tools for tokenizing texts using natural language processing (NLP) models. There are four tokenizer classes, two of which use spaCy NLP models to tokenize input texts. These modules must be installed locally in order for the tokenizer module to run correctly. By default, the <code>xx_sent_ud_sm</code> model is used. If another model is desired, those models must also be installed.</p>"},{"location":"api/tokenizer/#tokenizers","title":"Tokenizers","text":"<p>The tokenizer module includes 4 classes across 2 files.</p> <ul> <li>lexos.tokenizer</li> <li><code>SliceTokenizer</code><ul> <li>Simple slice tokenizer</li> <li>Can be used to generate character tokens/ngrams</li> </ul> </li> <li><code>Tokenizer</code><ul> <li>Tokenizes texts using spaCy NLPs</li> <li>Takes in raw text as input</li> <li>Returns spaCy docs that contains the tokens of the given input text</li> <li>Includes filtering of digits, punctuation, stopwords, etc.</li> <li>Supports all spaCy NLP models</li> </ul> </li> <li><code>WhitespaceTokenizer</code><ul> <li>Tokenizes on whitespace</li> </ul> </li> <li>lexos.tokenizer.ngrams</li> <li><code>Ngrams</code><ul> <li>Returns ngrams from a text, spaCy doc, or list of tokens</li> <li>Includes filtering of digits, punctuation, stopwords, whitespace, etc.</li> <li>User selected size of ngrams</li> </ul> </li> </ul>"},{"location":"api/tokenizer/ngrams/","title":"Ngrams","text":"<p>Tokenizes such that each token is an \"ngram,\" or a set of n words in a row.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams","title":"<code>Ngrams</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generate ngrams from a text.</p> <p>Config:</p> <ul> <li><code>default</code>: <code>validation_config</code></li> </ul> <p>Fields:</p> <ul> <li> <code>n</code>                 (<code>int</code>)             </li> <li> <code>drop_ws</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>filter_digits</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>filter_nums</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>filter_punct</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>filter_stops</code>                 (<code>Optional[bool | list[str]]</code>)             </li> <li> <code>min_freq</code>                 (<code>Optional[int]</code>)             </li> <li> <code>output</code>                 (<code>Optional[str]</code>)             </li> <li> <code>tokenizer</code>                 (<code>Optional[Callable]</code>)             </li> </ul> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>class Ngrams(BaseModel):\n    \"\"\"Generate ngrams from a text.\"\"\"\n\n    n: int = Field(\n        default=2,\n        description=\"The size of the ngrams.\",\n    )\n    drop_ws: Optional[bool] = Field(\n        default=True,\n        description=\"Whether to drop whitespace from the ngrams.\",\n    )\n    filter_digits: Optional[bool] = Field(\n        default=False,\n        description=\"If True, remove ngrams that contain any digits. Automatically sets filter_nums to False.\",\n    )\n    filter_nums: Optional[bool] = Field(\n        default=False,\n        description=\"If True, remove ngrams that contain any numbers or number-like tokens.\",\n    )\n    filter_punct: Optional[bool] = Field(\n        default=True,\n        description=\"Remove ngrams that contain any punctuation-only tokens.\",\n    )\n    filter_stops: Optional[bool | list[str]] = Field(\n        default=[],\n        description=\"Remove ngrams that start or end with a stop word in the provided list.\",\n    )\n    min_freq: Optional[int] = Field(\n        default=1,\n        description=\"Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.\",\n    )\n    output: Optional[str] = Field(\n        default=\"text\",\n        description=\"The output format. Can be 'text', 'spans', or 'tuples'.\",\n    )\n    tokenizer: Optional[Callable] = Field(\n        default=WhitespaceTokenizer,\n        description=\"The tokenizer to use.\",\n    )\n\n    model_config = validation_config\n\n    @property\n    def stopwords(self) -&gt; bool | list[str] | None:\n        \"\"\"Get the list of stopwords.\"\"\"\n        return self.filter_stops\n\n    @validate_call(config=validation_config)\n    def _filter_tokens(\n        self,\n        tokens: list[str],\n        drop_ws: bool = True,\n        filter_digits: bool = False,\n        filter_punct: bool = True,\n        filter_stops: list[str] = [],\n    ) -&gt; Generator:\n        \"\"\"Apply filters to a list of tokens.\n\n        Args:\n            tokens (list[str]): The list of tokens.\n            drop_ws (bool): Whether to drop whitespace from the ngrams.\n            filter_digits (bool): If True, remove ngrams that contain any digits.\n            filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n            filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n\n        Returns:\n            Generator: A generator of filtered tokens.\n        \"\"\"\n        if drop_ws:\n            tokens = (t.strip() for t in tokens)\n        if len(filter_stops) &gt; 0:\n            tokens = (t for t in tokens if t not in filter_stops)\n        if filter_punct:\n            tokens = (t for t in tokens if not re.match(\"\\\\W\", t))\n        if filter_digits:\n            tokens = (t for t in tokens if not re.match(\"\\\\d\", t))\n        yield from tokens\n\n    def _set_attributes(\n        self, skip_set_attrs: bool = False, **kwargs: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"Set the instance attributes based on keyword arguments.\n\n        Args:\n            skip_set_attrs (bool): Whether to skip setting the attributes.\n            **kwargs (dict[str, Any]): The keyword arguments to set the attributes.\n        \"\"\"\n        if not skip_set_attrs:\n            for key, value in kwargs.items():\n                if hasattr(self, key):\n                    setattr(self, key, value)\n\n    @validate_call(config=validation_config)\n    def from_doc(\n        self,\n        doc: Doc,\n        n: int = 2,\n        filter_digits: Optional[bool] = False,\n        filter_nums: Optional[bool] = False,\n        filter_punct: Optional[bool] = True,\n        filter_stops: Optional[bool] = False,\n        output: Optional[str] = \"text\",\n        min_freq: Optional[int] = 1,\n        skip_set_attrs: Optional[bool] = False,\n        **kwargs: Any,\n    ) -&gt; Generator:\n        \"\"\"Generate a list of ngrams from a Doc.\n\n        Args:\n            doc (Doc): The source Doc.\n            n (int): The size of the ngrams.\n            filter_digits (bool): If True, remove ngrams that contain any digits.\n            filter_nums (bool): If True, remove ngrams that contain any numbers or number-like tokens.\n            filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n            filter_stops (bool): Remove ngrams that start or end with a stop word in the provided list.\n            output (str): The output format. Can be 'text', 'spans', or 'tuples'.\n            min_freq (int): Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.\n            skip_set_attrs (bool): Whether to skip setting the attributes.\n            **kwargs (Any): Extra keyword arguments to pass to textacy.extract.basics.ngrams.\n\n        Returns:\n            Generator: A generator of ngrams.\n        \"\"\"\n        attrs = {\n            \"n\": n,\n            \"filter_digits\": filter_digits,\n            \"filter_nums\": filter_nums,\n            \"filter_punct\": filter_punct,\n            \"filter_stops\": filter_stops,\n            \"min_freq\": min_freq,\n            \"output\": output,\n            \"skip_set_attrs\": skip_set_attrs,\n        }\n        attrs = {**attrs, **kwargs}\n        self._set_attributes(**attrs)\n        # Set filter_nums to false; we'll filter digits separately\n        if filter_digits:\n            self.filter_nums = False\n        # Get the ngrams\n        ngram_spans = textacy_ngrams(\n            doc,\n            n=self.n,\n            filter_nums=self.filter_nums,\n            filter_punct=self.filter_punct,\n            filter_stops=self.filter_stops,\n            **kwargs,\n        )\n        # Filter digits\n        if filter_digits:\n            ngram_spans = (\n                ng for ng in ngram_spans if not any(token.is_digit for token in ng)\n            )\n        # Apply min_freq (for some reason, it doesn't work if passed to Textacy)\n        if min_freq &gt; 1:\n            freqs = frequencies(ng.text.lower() for ng in ngram_spans)\n            ngram_spans = (\n                ng for ng in ngram_spans if freqs[ng.text.lower()] &gt;= min_freq\n            )\n        # Yield the desired output\n        if self.output == \"text\":\n            for span in ngram_spans:\n                yield span.text\n        elif self.output == \"spans\":\n            yield from ngram_spans\n        elif self.output == \"tuples\":\n            for span in ngram_spans:\n                yield tuple([token.text for token in span])\n        else:\n            raise LexosException(\"Invalid output type.\")\n\n    @validate_call(config=validation_config)\n    def from_docs(\n        self,\n        docs: Iterable[Doc],\n        n: int = 2,\n        filter_digits: Optional[bool] = False,\n        filter_nums: Optional[bool] = False,\n        filter_punct: Optional[bool] = True,\n        filter_stops: Optional[bool] = False,\n        min_freq: Optional[int] = 1,\n        output: Optional[str] = \"text\",\n        **kwargs: Any,\n    ) -&gt; list[Generator]:\n        \"\"\"Generate a list of ngrams from a Doc.\n\n        Args:\n            docs (Iterable[Doc]): An iterable of Docs.\n            n (int): The size of the ngrams.\n            filter_digits (bool): If True, remove ngrams that contain any digits.\n            filter_nums (bool): If True, remove ngrams that contain any numbers or number-like tokens.\n            filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n            filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n            min_freq (int): Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.\n            output (str): The output format. Can be 'text', 'spans', or 'tuples'.\n            **kwargs (Any): Extra keyword arguments to pass to textacy.extract.basics.ngrams.\n\n        Returns:\n            list[Generator]: A list of ngram generators.\n        \"\"\"\n        attrs = {\n            \"n\": n,\n            \"filter_digits\": filter_digits,\n            \"filter_nums\": filter_nums,\n            \"filter_punct\": filter_punct,\n            \"filter_stops\": filter_stops,\n            \"min_freq\": min_freq,\n            \"output\": output,\n        }\n        attrs = {**attrs, **kwargs}\n        self._set_attributes(**attrs)\n        ngram_list = []\n        for doc in docs:\n            ngram_list.append(self.from_doc(doc, skip_set_attrs=True))\n        return ngram_list\n\n    @validate_call(config=validation_config)\n    def from_text(\n        self,\n        text: str,\n        n: int = 2,\n        drop_ws: Optional[bool] = True,\n        filter_digits: Optional[bool] = False,\n        filter_punct: Optional[bool] = True,\n        filter_stops: Optional[Iterable[str]] = [],\n        min_freq: Optional[int] = 1,\n        output: Optional[str] = \"text\",\n        skip_set_attrs: Optional[bool] = False,\n        tokenizer: Optional[Callable] = WhitespaceTokenizer(),\n    ) -&gt; Generator:\n        \"\"\"Generate a list of ngrams from a list of tokens.\n\n        Args:\n            text (str): The text to generate ngrams from.\n            n (int): The size of the ngrams.\n            drop_ws (bool): Whether to drop whitespace from the ngrams.\n            filter_digits (bool): If True, remove ngrams that contain any digits.\n            filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n            filter_stops (Iterable[str]): Remove ngrams that start or end with a stop word in the provided list.\n            min_freq (Optional[int]): Remove ngrams that occur in text fewer than min_freq times.\n            output (str): The output format. Can be 'text' or 'tuples'.\n            skip_set_attrs (bool): Whether to skip setting the attributes.\n            tokenizer (Callable): The tokenizer to use.\n\n        Returns:\n            Generator: A generator of ngrams.\n        \"\"\"\n        self._set_attributes(\n            n=n,\n            drop_ws=drop_ws,\n            filter_digits=filter_digits,\n            filter_punct=filter_punct,\n            filter_stops=filter_stops,\n            min_freq=min_freq,\n            output=output,\n            skip_set_attrs=skip_set_attrs,\n        )\n        tokens = tokenizer(text)\n        # If the user tokenises with a spaCy pipeline, we need to extract the text\n        if isinstance(tokens[0], Token):\n            tokens = [token.text for token in tokens]\n        tokens = list(\n            self._filter_tokens(\n                tokens,\n                self.drop_ws,\n                self.filter_digits,\n                self.filter_punct,\n                self.filter_stops,\n            )\n        )\n        ngrams = zip(*[tokens[i:] for i in range(self.n)])\n        if min_freq &gt; 1:\n            ngrams = list(ngrams)\n            freqs = frequencies(\"\".join(ng).lower() for ng in ngrams)\n            ngrams = (ng for ng in ngrams if freqs[\"\".join(ng).lower()] &gt;= min_freq)\n        if self.output == \"text\":\n            for ngram in ngrams:\n                yield \" \".join(ngram)\n        elif self.output == \"tuples\":\n            yield from ngrams\n        else:\n            raise LexosException(\"Invalid output type.\")\n\n    @validate_call(config=validation_config)\n    def from_texts(\n        self,\n        texts: Iterable[str],\n        n: int = 2,\n        drop_ws: Optional[bool] = True,\n        filter_digits: Optional[bool] = False,\n        filter_punct: Optional[bool] = True,\n        filter_stops: Optional[Iterable[str]] = [],\n        min_freq: Optional[int] = 1,\n        output: Optional[str] = \"text\",\n        tokenizer: Optional[Callable] = WhitespaceTokenizer,\n    ) -&gt; list[Generator]:\n        \"\"\"Generate a list of ngrams from a list of tokens.\n\n        Args:\n            texts (Iterable[str]): An iterable of texts.\n            n (int): The size of the ngrams.\n            drop_ws (bool): Whether to drop whitespace from the ngrams.\n            filter_digits (bool): If True, remove ngrams that contain any digits.\n            filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n            filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n            min_freq (Optional[int]): Remove ngrams that occur in text fewer than min_freq times.\n            output (str): The output format. Can be 'text' or 'tuples'.\n            tokenizer (Callable): The tokenizer to use.\n\n        Returns:\n            list[Generator]: A list of ngram generators.\n        \"\"\"\n        self._set_attributes(\n            n=n,\n            drop_ws=drop_ws,\n            filter_punct=filter_punct,\n            filter_stops=filter_stops,\n            filter_digits=filter_digits,\n            min_freq=min_freq,\n            output=output,\n            tokenizer=tokenizer,\n        )\n        ngram_list = []\n        for text in texts:\n            ngram_list.append(self.from_text(text, skip_set_attrs=True))\n        return ngram_list\n\n    @validate_call(config=validation_config)\n    def from_tokens(\n        self,\n        tokens: Iterable[str],\n        n: int = 2,\n        drop_ws: Optional[bool] = True,\n        filter_digits: Optional[bool] = False,\n        filter_punct: Optional[bool] = True,\n        filter_stops: Optional[Iterable[str]] = [],\n        min_freq: Optional[int] = 1,\n        output: Optional[str] = \"text\",\n        skip_set_attrs: Optional[bool] = False,\n    ) -&gt; Generator:\n        \"\"\"Generate a ngrams from an iterable of tokens.\n\n        Args:\n            tokens (Iterable[str]): An iterable of tokens.\n            n (int): The size of the ngrams.\n            drop_ws (bool): Whether to drop whitespace from the ngrams.\n            filter_digits (bool): If True, remove ngrams that contain any digits.\n            filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n            filter_stops (Iterable[str]): Remove ngrams that start or end with a stop word in the provided list.\n            min_freq (int): Remove ngrams that occur in tokens fewer than min_freq times.\n            output (str): The output format. Can be 'text' or 'tuples'.\n            skip_set_attrs (bool): Whether to skip setting the attributes.\n\n        Returns:\n            Generator: A generator of ngrams.\n        \"\"\"\n        self._set_attributes(\n            n=n,\n            drop_ws=drop_ws,\n            filter_digits=filter_digits,\n            filter_punct=filter_punct,\n            filter_stops=filter_stops,\n            min_freq=min_freq,\n            output=output,\n            skip_set_attrs=skip_set_attrs,\n        )\n\n        tokens = list(\n            self._filter_tokens(\n                tokens,\n                self.drop_ws,\n                self.filter_digits,\n                self.filter_punct,\n                self.filter_stops,\n            )\n        )\n        ngrams = zip(*[tokens[i:] for i in range(self.n)])\n        if min_freq &gt; 1:\n            ngrams = list(ngrams)\n            freqs = frequencies(\"\".join(ng).lower() for ng in ngrams)\n            ngrams = (ng for ng in ngrams if freqs[\"\".join(ng).lower()] &gt;= min_freq)\n        if self.output == \"text\":\n            ngrams = zip(*[tokens[i:] for i in range(self.n)])\n            for ngram in ngrams:\n                yield \" \".join(ngram)\n        elif self.output == \"tuples\":\n            yield from ngrams\n        else:\n            raise LexosException(\"Invalid output type.\")\n\n    @validate_call(config=validation_config)\n    def from_token_lists(\n        self,\n        token_lists: Iterable[Iterable[str]],\n        n: int = 2,\n        drop_ws: Optional[bool] = True,\n        filter_digits: Optional[bool] = False,\n        filter_punct: Optional[bool] = True,\n        filter_stops: Optional[Iterable[str]] = [],\n        min_freq: Optional[int] = 1,\n        output: Optional[str] = \"text\",\n    ) -&gt; list[Generator]:\n        \"\"\"Generate a ngrams from an iterable of tokens.\n\n        Args:\n            token_lists (Iterable[Iterable[str]]): An iterable of token lists.\n            n (int): The size of the ngrams.\n            drop_ws (bool): Whether to drop whitespace from the ngrams.\n            filter_digits (bool): If True, remove ngrams that contain any digits.\n            filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n            filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n            min_freq (int): Remove ngrams that occur in tokens fewer than min_freq times.\n            output (str): The output format. Can be 'text' or 'tuples'.\n\n        Returns:\n            list[Generator]: A list of ngram generators.\n        \"\"\"\n        self._set_attributes(\n            n=n,\n            drop_ws=drop_ws,\n            filter_digits=filter_digits,\n            filter_punct=filter_punct,\n            filter_stops=filter_stops,\n            min_freq=min_freq,\n            output=output,\n        )\n        ngram_list = []\n        for token_list in token_lists:\n            ngram_list.append(self.from_tokens(token_list, skip_set_attrs=True))\n        return ngram_list\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.drop_ws","title":"<code>drop_ws: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to drop whitespace from the ngrams.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_digits","title":"<code>filter_digits: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>If True, remove ngrams that contain any digits. Automatically sets filter_nums to False.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_nums","title":"<code>filter_nums: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>If True, remove ngrams that contain any numbers or number-like tokens.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_punct","title":"<code>filter_punct: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Remove ngrams that contain any punctuation-only tokens.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_stops","title":"<code>filter_stops: Optional[bool | list[str]] = []</code>  <code>pydantic-field</code>","text":"<p>Remove ngrams that start or end with a stop word in the provided list.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.min_freq","title":"<code>min_freq: Optional[int] = 1</code>  <code>pydantic-field</code>","text":"<p>Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.n","title":"<code>n: int = 2</code>  <code>pydantic-field</code>","text":"<p>The size of the ngrams.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.output","title":"<code>output: Optional[str] = 'text'</code>  <code>pydantic-field</code>","text":"<p>The output format. Can be 'text', 'spans', or 'tuples'.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.stopwords","title":"<code>stopwords: bool | list[str] | None</code>  <code>property</code>","text":"<p>Get the list of stopwords.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.tokenizer","title":"<code>tokenizer: Optional[Callable] = WhitespaceTokenizer</code>  <code>pydantic-field</code>","text":"<p>The tokenizer to use.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_doc","title":"<code>from_doc(doc: Doc, n: int = 2, filter_digits: Optional[bool] = False, filter_nums: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[bool] = False, output: Optional[str] = 'text', min_freq: Optional[int] = 1, skip_set_attrs: Optional[bool] = False, **kwargs: Any) -&gt; Generator</code>","text":"<p>Generate a list of ngrams from a Doc.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>The source Doc.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_nums</code> <code>bool</code> <p>If True, remove ngrams that contain any numbers or number-like tokens.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>bool</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>False</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text', 'spans', or 'tuples'.</p> <code>'text'</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.</p> <code>1</code> <code>skip_set_attrs</code> <code>bool</code> <p>Whether to skip setting the attributes.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Extra keyword arguments to pass to textacy.extract.basics.ngrams.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator of ngrams.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_doc(\n    self,\n    doc: Doc,\n    n: int = 2,\n    filter_digits: Optional[bool] = False,\n    filter_nums: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[bool] = False,\n    output: Optional[str] = \"text\",\n    min_freq: Optional[int] = 1,\n    skip_set_attrs: Optional[bool] = False,\n    **kwargs: Any,\n) -&gt; Generator:\n    \"\"\"Generate a list of ngrams from a Doc.\n\n    Args:\n        doc (Doc): The source Doc.\n        n (int): The size of the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_nums (bool): If True, remove ngrams that contain any numbers or number-like tokens.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (bool): Remove ngrams that start or end with a stop word in the provided list.\n        output (str): The output format. Can be 'text', 'spans', or 'tuples'.\n        min_freq (int): Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.\n        skip_set_attrs (bool): Whether to skip setting the attributes.\n        **kwargs (Any): Extra keyword arguments to pass to textacy.extract.basics.ngrams.\n\n    Returns:\n        Generator: A generator of ngrams.\n    \"\"\"\n    attrs = {\n        \"n\": n,\n        \"filter_digits\": filter_digits,\n        \"filter_nums\": filter_nums,\n        \"filter_punct\": filter_punct,\n        \"filter_stops\": filter_stops,\n        \"min_freq\": min_freq,\n        \"output\": output,\n        \"skip_set_attrs\": skip_set_attrs,\n    }\n    attrs = {**attrs, **kwargs}\n    self._set_attributes(**attrs)\n    # Set filter_nums to false; we'll filter digits separately\n    if filter_digits:\n        self.filter_nums = False\n    # Get the ngrams\n    ngram_spans = textacy_ngrams(\n        doc,\n        n=self.n,\n        filter_nums=self.filter_nums,\n        filter_punct=self.filter_punct,\n        filter_stops=self.filter_stops,\n        **kwargs,\n    )\n    # Filter digits\n    if filter_digits:\n        ngram_spans = (\n            ng for ng in ngram_spans if not any(token.is_digit for token in ng)\n        )\n    # Apply min_freq (for some reason, it doesn't work if passed to Textacy)\n    if min_freq &gt; 1:\n        freqs = frequencies(ng.text.lower() for ng in ngram_spans)\n        ngram_spans = (\n            ng for ng in ngram_spans if freqs[ng.text.lower()] &gt;= min_freq\n        )\n    # Yield the desired output\n    if self.output == \"text\":\n        for span in ngram_spans:\n            yield span.text\n    elif self.output == \"spans\":\n        yield from ngram_spans\n    elif self.output == \"tuples\":\n        for span in ngram_spans:\n            yield tuple([token.text for token in span])\n    else:\n        raise LexosException(\"Invalid output type.\")\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_docs","title":"<code>from_docs(docs: Iterable[Doc], n: int = 2, filter_digits: Optional[bool] = False, filter_nums: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[bool] = False, min_freq: Optional[int] = 1, output: Optional[str] = 'text', **kwargs: Any) -&gt; list[Generator]</code>","text":"<p>Generate a list of ngrams from a Doc.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterable[Doc]</code> <p>An iterable of Docs.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_nums</code> <code>bool</code> <p>If True, remove ngrams that contain any numbers or number-like tokens.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>list[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>False</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text', 'spans', or 'tuples'.</p> <code>'text'</code> <code>**kwargs</code> <code>Any</code> <p>Extra keyword arguments to pass to textacy.extract.basics.ngrams.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Generator]</code> <p>list[Generator]: A list of ngram generators.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_docs(\n    self,\n    docs: Iterable[Doc],\n    n: int = 2,\n    filter_digits: Optional[bool] = False,\n    filter_nums: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[bool] = False,\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    **kwargs: Any,\n) -&gt; list[Generator]:\n    \"\"\"Generate a list of ngrams from a Doc.\n\n    Args:\n        docs (Iterable[Doc]): An iterable of Docs.\n        n (int): The size of the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_nums (bool): If True, remove ngrams that contain any numbers or number-like tokens.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (int): Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.\n        output (str): The output format. Can be 'text', 'spans', or 'tuples'.\n        **kwargs (Any): Extra keyword arguments to pass to textacy.extract.basics.ngrams.\n\n    Returns:\n        list[Generator]: A list of ngram generators.\n    \"\"\"\n    attrs = {\n        \"n\": n,\n        \"filter_digits\": filter_digits,\n        \"filter_nums\": filter_nums,\n        \"filter_punct\": filter_punct,\n        \"filter_stops\": filter_stops,\n        \"min_freq\": min_freq,\n        \"output\": output,\n    }\n    attrs = {**attrs, **kwargs}\n    self._set_attributes(**attrs)\n    ngram_list = []\n    for doc in docs:\n        ngram_list.append(self.from_doc(doc, skip_set_attrs=True))\n    return ngram_list\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_text","title":"<code>from_text(text: str, n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text', skip_set_attrs: Optional[bool] = False, tokenizer: Optional[Callable] = WhitespaceTokenizer()) -&gt; Generator</code>","text":"<p>Generate a list of ngrams from a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate ngrams from.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>Iterable[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>Optional[int]</code> <p>Remove ngrams that occur in text fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <code>skip_set_attrs</code> <code>bool</code> <p>Whether to skip setting the attributes.</p> <code>False</code> <code>tokenizer</code> <code>Callable</code> <p>The tokenizer to use.</p> <code>WhitespaceTokenizer()</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator of ngrams.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_text(\n    self,\n    text: str,\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    skip_set_attrs: Optional[bool] = False,\n    tokenizer: Optional[Callable] = WhitespaceTokenizer(),\n) -&gt; Generator:\n    \"\"\"Generate a list of ngrams from a list of tokens.\n\n    Args:\n        text (str): The text to generate ngrams from.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (Iterable[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (Optional[int]): Remove ngrams that occur in text fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n        skip_set_attrs (bool): Whether to skip setting the attributes.\n        tokenizer (Callable): The tokenizer to use.\n\n    Returns:\n        Generator: A generator of ngrams.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_digits=filter_digits,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        min_freq=min_freq,\n        output=output,\n        skip_set_attrs=skip_set_attrs,\n    )\n    tokens = tokenizer(text)\n    # If the user tokenises with a spaCy pipeline, we need to extract the text\n    if isinstance(tokens[0], Token):\n        tokens = [token.text for token in tokens]\n    tokens = list(\n        self._filter_tokens(\n            tokens,\n            self.drop_ws,\n            self.filter_digits,\n            self.filter_punct,\n            self.filter_stops,\n        )\n    )\n    ngrams = zip(*[tokens[i:] for i in range(self.n)])\n    if min_freq &gt; 1:\n        ngrams = list(ngrams)\n        freqs = frequencies(\"\".join(ng).lower() for ng in ngrams)\n        ngrams = (ng for ng in ngrams if freqs[\"\".join(ng).lower()] &gt;= min_freq)\n    if self.output == \"text\":\n        for ngram in ngrams:\n            yield \" \".join(ngram)\n    elif self.output == \"tuples\":\n        yield from ngrams\n    else:\n        raise LexosException(\"Invalid output type.\")\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_texts","title":"<code>from_texts(texts: Iterable[str], n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text', tokenizer: Optional[Callable] = WhitespaceTokenizer) -&gt; list[Generator]</code>","text":"<p>Generate a list of ngrams from a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>An iterable of texts.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>list[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>Optional[int]</code> <p>Remove ngrams that occur in text fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <code>tokenizer</code> <code>Callable</code> <p>The tokenizer to use.</p> <code>WhitespaceTokenizer</code> <p>Returns:</p> Type Description <code>list[Generator]</code> <p>list[Generator]: A list of ngram generators.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_texts(\n    self,\n    texts: Iterable[str],\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    tokenizer: Optional[Callable] = WhitespaceTokenizer,\n) -&gt; list[Generator]:\n    \"\"\"Generate a list of ngrams from a list of tokens.\n\n    Args:\n        texts (Iterable[str]): An iterable of texts.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (Optional[int]): Remove ngrams that occur in text fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n        tokenizer (Callable): The tokenizer to use.\n\n    Returns:\n        list[Generator]: A list of ngram generators.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        filter_digits=filter_digits,\n        min_freq=min_freq,\n        output=output,\n        tokenizer=tokenizer,\n    )\n    ngram_list = []\n    for text in texts:\n        ngram_list.append(self.from_text(text, skip_set_attrs=True))\n    return ngram_list\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_token_lists","title":"<code>from_token_lists(token_lists: Iterable[Iterable[str]], n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text') -&gt; list[Generator]</code>","text":"<p>Generate a ngrams from an iterable of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_lists</code> <code>Iterable[Iterable[str]]</code> <p>An iterable of token lists.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>list[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in tokens fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <p>Returns:</p> Type Description <code>list[Generator]</code> <p>list[Generator]: A list of ngram generators.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_token_lists(\n    self,\n    token_lists: Iterable[Iterable[str]],\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n) -&gt; list[Generator]:\n    \"\"\"Generate a ngrams from an iterable of tokens.\n\n    Args:\n        token_lists (Iterable[Iterable[str]]): An iterable of token lists.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (int): Remove ngrams that occur in tokens fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n\n    Returns:\n        list[Generator]: A list of ngram generators.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_digits=filter_digits,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        min_freq=min_freq,\n        output=output,\n    )\n    ngram_list = []\n    for token_list in token_lists:\n        ngram_list.append(self.from_tokens(token_list, skip_set_attrs=True))\n    return ngram_list\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_tokens","title":"<code>from_tokens(tokens: Iterable[str], n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text', skip_set_attrs: Optional[bool] = False) -&gt; Generator</code>","text":"<p>Generate a ngrams from an iterable of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Iterable[str]</code> <p>An iterable of tokens.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>Iterable[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in tokens fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <code>skip_set_attrs</code> <code>bool</code> <p>Whether to skip setting the attributes.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator of ngrams.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_tokens(\n    self,\n    tokens: Iterable[str],\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    skip_set_attrs: Optional[bool] = False,\n) -&gt; Generator:\n    \"\"\"Generate a ngrams from an iterable of tokens.\n\n    Args:\n        tokens (Iterable[str]): An iterable of tokens.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (Iterable[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (int): Remove ngrams that occur in tokens fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n        skip_set_attrs (bool): Whether to skip setting the attributes.\n\n    Returns:\n        Generator: A generator of ngrams.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_digits=filter_digits,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        min_freq=min_freq,\n        output=output,\n        skip_set_attrs=skip_set_attrs,\n    )\n\n    tokens = list(\n        self._filter_tokens(\n            tokens,\n            self.drop_ws,\n            self.filter_digits,\n            self.filter_punct,\n            self.filter_stops,\n        )\n    )\n    ngrams = zip(*[tokens[i:] for i in range(self.n)])\n    if min_freq &gt; 1:\n        ngrams = list(ngrams)\n        freqs = frequencies(\"\".join(ng).lower() for ng in ngrams)\n        ngrams = (ng for ng in ngrams if freqs[\"\".join(ng).lower()] &gt;= min_freq)\n    if self.output == \"text\":\n        ngrams = zip(*[tokens[i:] for i in range(self.n)])\n        for ngram in ngrams:\n            yield \" \".join(ngram)\n    elif self.output == \"tuples\":\n        yield from ngrams\n    else:\n        raise LexosException(\"Invalid output type.\")\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.n","title":"<code>n: int = 2</code>  <code>pydantic-field</code>","text":"<p>The size of the ngrams.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.drop_ws","title":"<code>drop_ws: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to drop whitespace from the ngrams.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_digits","title":"<code>filter_digits: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>If True, remove ngrams that contain any digits. Automatically sets filter_nums to False.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_nums","title":"<code>filter_nums: Optional[bool] = False</code>  <code>pydantic-field</code>","text":"<p>If True, remove ngrams that contain any numbers or number-like tokens.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_punct","title":"<code>filter_punct: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Remove ngrams that contain any punctuation-only tokens.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.filter_stops","title":"<code>filter_stops: Optional[bool | list[str]] = []</code>  <code>pydantic-field</code>","text":"<p>Remove ngrams that start or end with a stop word in the provided list.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.min_freq","title":"<code>min_freq: Optional[int] = 1</code>  <code>pydantic-field</code>","text":"<p>Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.output","title":"<code>output: Optional[str] = 'text'</code>  <code>pydantic-field</code>","text":"<p>The output format. Can be 'text', 'spans', or 'tuples'.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.tokenizer","title":"<code>tokenizer: Optional[Callable] = WhitespaceTokenizer</code>  <code>pydantic-field</code>","text":"<p>The tokenizer to use.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.model_config","title":"<code>model_config = validation_config</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.stopwords","title":"<code>stopwords: bool | list[str] | None</code>  <code>property</code>","text":"<p>Get the list of stopwords.</p>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams._filter_tokens","title":"<code>_filter_tokens(tokens: list[str], drop_ws: bool = True, filter_digits: bool = False, filter_punct: bool = True, filter_stops: list[str] = []) -&gt; Generator</code>","text":"<p>Apply filters to a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[str]</code> <p>The list of tokens.</p> required <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>list[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator of filtered tokens.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef _filter_tokens(\n    self,\n    tokens: list[str],\n    drop_ws: bool = True,\n    filter_digits: bool = False,\n    filter_punct: bool = True,\n    filter_stops: list[str] = [],\n) -&gt; Generator:\n    \"\"\"Apply filters to a list of tokens.\n\n    Args:\n        tokens (list[str]): The list of tokens.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n\n    Returns:\n        Generator: A generator of filtered tokens.\n    \"\"\"\n    if drop_ws:\n        tokens = (t.strip() for t in tokens)\n    if len(filter_stops) &gt; 0:\n        tokens = (t for t in tokens if t not in filter_stops)\n    if filter_punct:\n        tokens = (t for t in tokens if not re.match(\"\\\\W\", t))\n    if filter_digits:\n        tokens = (t for t in tokens if not re.match(\"\\\\d\", t))\n    yield from tokens\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams._set_attributes","title":"<code>_set_attributes(skip_set_attrs: bool = False, **kwargs: dict[str, Any]) -&gt; None</code>","text":"<p>Set the instance attributes based on keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>skip_set_attrs</code> <code>bool</code> <p>Whether to skip setting the attributes.</p> <code>False</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>The keyword arguments to set the attributes.</p> <code>{}</code> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>def _set_attributes(\n    self, skip_set_attrs: bool = False, **kwargs: dict[str, Any]\n) -&gt; None:\n    \"\"\"Set the instance attributes based on keyword arguments.\n\n    Args:\n        skip_set_attrs (bool): Whether to skip setting the attributes.\n        **kwargs (dict[str, Any]): The keyword arguments to set the attributes.\n    \"\"\"\n    if not skip_set_attrs:\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_doc","title":"<code>from_doc(doc: Doc, n: int = 2, filter_digits: Optional[bool] = False, filter_nums: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[bool] = False, output: Optional[str] = 'text', min_freq: Optional[int] = 1, skip_set_attrs: Optional[bool] = False, **kwargs: Any) -&gt; Generator</code>","text":"<p>Generate a list of ngrams from a Doc.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Doc</code> <p>The source Doc.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_nums</code> <code>bool</code> <p>If True, remove ngrams that contain any numbers or number-like tokens.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>bool</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>False</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text', 'spans', or 'tuples'.</p> <code>'text'</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.</p> <code>1</code> <code>skip_set_attrs</code> <code>bool</code> <p>Whether to skip setting the attributes.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Extra keyword arguments to pass to textacy.extract.basics.ngrams.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator of ngrams.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_doc(\n    self,\n    doc: Doc,\n    n: int = 2,\n    filter_digits: Optional[bool] = False,\n    filter_nums: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[bool] = False,\n    output: Optional[str] = \"text\",\n    min_freq: Optional[int] = 1,\n    skip_set_attrs: Optional[bool] = False,\n    **kwargs: Any,\n) -&gt; Generator:\n    \"\"\"Generate a list of ngrams from a Doc.\n\n    Args:\n        doc (Doc): The source Doc.\n        n (int): The size of the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_nums (bool): If True, remove ngrams that contain any numbers or number-like tokens.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (bool): Remove ngrams that start or end with a stop word in the provided list.\n        output (str): The output format. Can be 'text', 'spans', or 'tuples'.\n        min_freq (int): Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.\n        skip_set_attrs (bool): Whether to skip setting the attributes.\n        **kwargs (Any): Extra keyword arguments to pass to textacy.extract.basics.ngrams.\n\n    Returns:\n        Generator: A generator of ngrams.\n    \"\"\"\n    attrs = {\n        \"n\": n,\n        \"filter_digits\": filter_digits,\n        \"filter_nums\": filter_nums,\n        \"filter_punct\": filter_punct,\n        \"filter_stops\": filter_stops,\n        \"min_freq\": min_freq,\n        \"output\": output,\n        \"skip_set_attrs\": skip_set_attrs,\n    }\n    attrs = {**attrs, **kwargs}\n    self._set_attributes(**attrs)\n    # Set filter_nums to false; we'll filter digits separately\n    if filter_digits:\n        self.filter_nums = False\n    # Get the ngrams\n    ngram_spans = textacy_ngrams(\n        doc,\n        n=self.n,\n        filter_nums=self.filter_nums,\n        filter_punct=self.filter_punct,\n        filter_stops=self.filter_stops,\n        **kwargs,\n    )\n    # Filter digits\n    if filter_digits:\n        ngram_spans = (\n            ng for ng in ngram_spans if not any(token.is_digit for token in ng)\n        )\n    # Apply min_freq (for some reason, it doesn't work if passed to Textacy)\n    if min_freq &gt; 1:\n        freqs = frequencies(ng.text.lower() for ng in ngram_spans)\n        ngram_spans = (\n            ng for ng in ngram_spans if freqs[ng.text.lower()] &gt;= min_freq\n        )\n    # Yield the desired output\n    if self.output == \"text\":\n        for span in ngram_spans:\n            yield span.text\n    elif self.output == \"spans\":\n        yield from ngram_spans\n    elif self.output == \"tuples\":\n        for span in ngram_spans:\n            yield tuple([token.text for token in span])\n    else:\n        raise LexosException(\"Invalid output type.\")\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_docs","title":"<code>from_docs(docs: Iterable[Doc], n: int = 2, filter_digits: Optional[bool] = False, filter_nums: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[bool] = False, min_freq: Optional[int] = 1, output: Optional[str] = 'text', **kwargs: Any) -&gt; list[Generator]</code>","text":"<p>Generate a list of ngrams from a Doc.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterable[Doc]</code> <p>An iterable of Docs.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_nums</code> <code>bool</code> <p>If True, remove ngrams that contain any numbers or number-like tokens.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>list[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>False</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text', 'spans', or 'tuples'.</p> <code>'text'</code> <code>**kwargs</code> <code>Any</code> <p>Extra keyword arguments to pass to textacy.extract.basics.ngrams.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Generator]</code> <p>list[Generator]: A list of ngram generators.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_docs(\n    self,\n    docs: Iterable[Doc],\n    n: int = 2,\n    filter_digits: Optional[bool] = False,\n    filter_nums: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[bool] = False,\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    **kwargs: Any,\n) -&gt; list[Generator]:\n    \"\"\"Generate a list of ngrams from a Doc.\n\n    Args:\n        docs (Iterable[Doc]): An iterable of Docs.\n        n (int): The size of the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_nums (bool): If True, remove ngrams that contain any numbers or number-like tokens.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (int): Remove ngrams that occur in text, doc, or tokens fewer than min_freq times.\n        output (str): The output format. Can be 'text', 'spans', or 'tuples'.\n        **kwargs (Any): Extra keyword arguments to pass to textacy.extract.basics.ngrams.\n\n    Returns:\n        list[Generator]: A list of ngram generators.\n    \"\"\"\n    attrs = {\n        \"n\": n,\n        \"filter_digits\": filter_digits,\n        \"filter_nums\": filter_nums,\n        \"filter_punct\": filter_punct,\n        \"filter_stops\": filter_stops,\n        \"min_freq\": min_freq,\n        \"output\": output,\n    }\n    attrs = {**attrs, **kwargs}\n    self._set_attributes(**attrs)\n    ngram_list = []\n    for doc in docs:\n        ngram_list.append(self.from_doc(doc, skip_set_attrs=True))\n    return ngram_list\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_text","title":"<code>from_text(text: str, n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text', skip_set_attrs: Optional[bool] = False, tokenizer: Optional[Callable] = WhitespaceTokenizer()) -&gt; Generator</code>","text":"<p>Generate a list of ngrams from a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate ngrams from.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>Iterable[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>Optional[int]</code> <p>Remove ngrams that occur in text fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <code>skip_set_attrs</code> <code>bool</code> <p>Whether to skip setting the attributes.</p> <code>False</code> <code>tokenizer</code> <code>Callable</code> <p>The tokenizer to use.</p> <code>WhitespaceTokenizer()</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator of ngrams.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_text(\n    self,\n    text: str,\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    skip_set_attrs: Optional[bool] = False,\n    tokenizer: Optional[Callable] = WhitespaceTokenizer(),\n) -&gt; Generator:\n    \"\"\"Generate a list of ngrams from a list of tokens.\n\n    Args:\n        text (str): The text to generate ngrams from.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (Iterable[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (Optional[int]): Remove ngrams that occur in text fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n        skip_set_attrs (bool): Whether to skip setting the attributes.\n        tokenizer (Callable): The tokenizer to use.\n\n    Returns:\n        Generator: A generator of ngrams.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_digits=filter_digits,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        min_freq=min_freq,\n        output=output,\n        skip_set_attrs=skip_set_attrs,\n    )\n    tokens = tokenizer(text)\n    # If the user tokenises with a spaCy pipeline, we need to extract the text\n    if isinstance(tokens[0], Token):\n        tokens = [token.text for token in tokens]\n    tokens = list(\n        self._filter_tokens(\n            tokens,\n            self.drop_ws,\n            self.filter_digits,\n            self.filter_punct,\n            self.filter_stops,\n        )\n    )\n    ngrams = zip(*[tokens[i:] for i in range(self.n)])\n    if min_freq &gt; 1:\n        ngrams = list(ngrams)\n        freqs = frequencies(\"\".join(ng).lower() for ng in ngrams)\n        ngrams = (ng for ng in ngrams if freqs[\"\".join(ng).lower()] &gt;= min_freq)\n    if self.output == \"text\":\n        for ngram in ngrams:\n            yield \" \".join(ngram)\n    elif self.output == \"tuples\":\n        yield from ngrams\n    else:\n        raise LexosException(\"Invalid output type.\")\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_texts","title":"<code>from_texts(texts: Iterable[str], n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text', tokenizer: Optional[Callable] = WhitespaceTokenizer) -&gt; list[Generator]</code>","text":"<p>Generate a list of ngrams from a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>An iterable of texts.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>list[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>Optional[int]</code> <p>Remove ngrams that occur in text fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <code>tokenizer</code> <code>Callable</code> <p>The tokenizer to use.</p> <code>WhitespaceTokenizer</code> <p>Returns:</p> Type Description <code>list[Generator]</code> <p>list[Generator]: A list of ngram generators.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_texts(\n    self,\n    texts: Iterable[str],\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    tokenizer: Optional[Callable] = WhitespaceTokenizer,\n) -&gt; list[Generator]:\n    \"\"\"Generate a list of ngrams from a list of tokens.\n\n    Args:\n        texts (Iterable[str]): An iterable of texts.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (Optional[int]): Remove ngrams that occur in text fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n        tokenizer (Callable): The tokenizer to use.\n\n    Returns:\n        list[Generator]: A list of ngram generators.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        filter_digits=filter_digits,\n        min_freq=min_freq,\n        output=output,\n        tokenizer=tokenizer,\n    )\n    ngram_list = []\n    for text in texts:\n        ngram_list.append(self.from_text(text, skip_set_attrs=True))\n    return ngram_list\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_tokens","title":"<code>from_tokens(tokens: Iterable[str], n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text', skip_set_attrs: Optional[bool] = False) -&gt; Generator</code>","text":"<p>Generate a ngrams from an iterable of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Iterable[str]</code> <p>An iterable of tokens.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>Iterable[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in tokens fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <code>skip_set_attrs</code> <code>bool</code> <p>Whether to skip setting the attributes.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>A generator of ngrams.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_tokens(\n    self,\n    tokens: Iterable[str],\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n    skip_set_attrs: Optional[bool] = False,\n) -&gt; Generator:\n    \"\"\"Generate a ngrams from an iterable of tokens.\n\n    Args:\n        tokens (Iterable[str]): An iterable of tokens.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (Iterable[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (int): Remove ngrams that occur in tokens fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n        skip_set_attrs (bool): Whether to skip setting the attributes.\n\n    Returns:\n        Generator: A generator of ngrams.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_digits=filter_digits,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        min_freq=min_freq,\n        output=output,\n        skip_set_attrs=skip_set_attrs,\n    )\n\n    tokens = list(\n        self._filter_tokens(\n            tokens,\n            self.drop_ws,\n            self.filter_digits,\n            self.filter_punct,\n            self.filter_stops,\n        )\n    )\n    ngrams = zip(*[tokens[i:] for i in range(self.n)])\n    if min_freq &gt; 1:\n        ngrams = list(ngrams)\n        freqs = frequencies(\"\".join(ng).lower() for ng in ngrams)\n        ngrams = (ng for ng in ngrams if freqs[\"\".join(ng).lower()] &gt;= min_freq)\n    if self.output == \"text\":\n        ngrams = zip(*[tokens[i:] for i in range(self.n)])\n        for ngram in ngrams:\n            yield \" \".join(ngram)\n    elif self.output == \"tuples\":\n        yield from ngrams\n    else:\n        raise LexosException(\"Invalid output type.\")\n</code></pre>"},{"location":"api/tokenizer/ngrams/#lexos.tokenizer.ngrams.Ngrams.from_token_lists","title":"<code>from_token_lists(token_lists: Iterable[Iterable[str]], n: int = 2, drop_ws: Optional[bool] = True, filter_digits: Optional[bool] = False, filter_punct: Optional[bool] = True, filter_stops: Optional[Iterable[str]] = [], min_freq: Optional[int] = 1, output: Optional[str] = 'text') -&gt; list[Generator]</code>","text":"<p>Generate a ngrams from an iterable of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_lists</code> <code>Iterable[Iterable[str]]</code> <p>An iterable of token lists.</p> required <code>n</code> <code>int</code> <p>The size of the ngrams.</p> <code>2</code> <code>drop_ws</code> <code>bool</code> <p>Whether to drop whitespace from the ngrams.</p> <code>True</code> <code>filter_digits</code> <code>bool</code> <p>If True, remove ngrams that contain any digits.</p> <code>False</code> <code>filter_punct</code> <code>bool</code> <p>Remove ngrams that contain any punctuation-only tokens.</p> <code>True</code> <code>filter_stops</code> <code>list[str]</code> <p>Remove ngrams that start or end with a stop word in the provided list.</p> <code>[]</code> <code>min_freq</code> <code>int</code> <p>Remove ngrams that occur in tokens fewer than min_freq times.</p> <code>1</code> <code>output</code> <code>str</code> <p>The output format. Can be 'text' or 'tuples'.</p> <code>'text'</code> <p>Returns:</p> Type Description <code>list[Generator]</code> <p>list[Generator]: A list of ngram generators.</p> Source code in <code>lexos/tokenizer/ngrams.py</code> <pre><code>@validate_call(config=validation_config)\ndef from_token_lists(\n    self,\n    token_lists: Iterable[Iterable[str]],\n    n: int = 2,\n    drop_ws: Optional[bool] = True,\n    filter_digits: Optional[bool] = False,\n    filter_punct: Optional[bool] = True,\n    filter_stops: Optional[Iterable[str]] = [],\n    min_freq: Optional[int] = 1,\n    output: Optional[str] = \"text\",\n) -&gt; list[Generator]:\n    \"\"\"Generate a ngrams from an iterable of tokens.\n\n    Args:\n        token_lists (Iterable[Iterable[str]]): An iterable of token lists.\n        n (int): The size of the ngrams.\n        drop_ws (bool): Whether to drop whitespace from the ngrams.\n        filter_digits (bool): If True, remove ngrams that contain any digits.\n        filter_punct (bool): Remove ngrams that contain any punctuation-only tokens.\n        filter_stops (list[str]): Remove ngrams that start or end with a stop word in the provided list.\n        min_freq (int): Remove ngrams that occur in tokens fewer than min_freq times.\n        output (str): The output format. Can be 'text' or 'tuples'.\n\n    Returns:\n        list[Generator]: A list of ngram generators.\n    \"\"\"\n    self._set_attributes(\n        n=n,\n        drop_ws=drop_ws,\n        filter_digits=filter_digits,\n        filter_punct=filter_punct,\n        filter_stops=filter_stops,\n        min_freq=min_freq,\n        output=output,\n    )\n    ngram_list = []\n    for token_list in token_lists:\n        ngram_list.append(self.from_tokens(token_list, skip_set_attrs=True))\n    return ngram_list\n</code></pre>"},{"location":"api/tokenizer/tokenizer/","title":"Tokenizer Classes","text":"<p>Three tokenizers, each with different procedures for dividing the text into tokens.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer","title":"<code>Tokenizer</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class for tokenizing text using spaCy.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> <li><code>validate_assignment</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>model</code>                 (<code>Optional[str]</code>)             </li> <li> <code>max_length</code>                 (<code>Optional[int]</code>)             </li> <li> <code>disable</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>stopwords</code>                 (<code>Optional[list[str] | str]</code>)             </li> <li> <code>nlp</code>                 (<code>Optional[Language]</code>)             </li> </ul> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>class Tokenizer(BaseModel):\n    \"\"\"A class for tokenizing text using spaCy.\"\"\"\n\n    model: Optional[str] = Field(\n        default=\"xx_sent_ud_sm\",\n        description=\"The name of the spaCy model to be used for tokenization.\",\n    )\n    max_length: Optional[int] = Field(\n        default=2000000,\n        description=\"The maximum length of the doc.\",\n    )\n    disable: Optional[list[str]] = Field(\n        default=[],\n        description=\"A list of spaCy pipeline components to disable.\",\n    )\n    stopwords: Optional[list[str] | str] = Field(\n        default=[],\n        description=\"A list of stop words to apply to docs.\",\n    )\n    nlp: Optional[Language] = Field(\n        default=default_model,\n        description=\"The spaCy language object.\",\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        json_schema_extra=DocJSONSchema.schema(),\n        validate_assignment=True,\n    )\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Initialise the Tokenizer class.\"\"\"\n        super().__init__(**data)\n        try:\n            self.nlp = spacy.load(self.model)\n            self.nlp.max_length = self.max_length\n        except OSError:\n            raise LexosException(\n                f\"Error loading model {self.model}. Please check the name and try again. You may need to install the model on your system.\"\n            )\n\n    @validate_call\n    def __call__(self, texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]:\n        \"\"\"Tokenize a string or an iterable of strings.\n\n        Args:\n            texts (str | Iterable[str]): The text(s) to be tokenized.\n\n        Returns:\n            Doc | Iterable[Doc]: The tokenized doc(s).\n        \"\"\"\n        if isinstance(texts, str):\n            return self.make_doc(texts)\n        elif isinstance(texts, Iterable):\n            return self.make_docs(texts)\n\n    @property\n    def pipeline(self) -&gt; list[str]:\n        \"\"\"Return the spaCy pipeline components.\"\"\"\n        return self.nlp.pipe_names\n\n    @property\n    def components(self) -&gt; list[str]:\n        \"\"\"Return the spaCy pipeline components.\"\"\"\n        return self.nlp.components\n\n    @property\n    def disabled(self) -&gt; list[str]:\n        \"\"\"Return the disabled spaCy pipeline components.\"\"\"\n        return self.nlp.disabled\n\n    @validate_call\n    def add_extension(self, name: str, default: str) -&gt; None:\n        \"\"\"Add an extension to the spaCy Token class.\n\n        Args:\n            name (str): The name of the extension.\n            default (str): The default value of the extension.\n        \"\"\"\n        if not Token.has_extension(name):\n            Token.set_extension(name, default=default, force=True)\n\n    @validate_call\n    def add_stopwords(self, stopwords: str | list[str]) -&gt; None:\n        \"\"\"Add stopwords to the tokenizer.\n\n        Args:\n            stopwords (str | Iterable[str]): A list of stopwords to add to the model.\n        \"\"\"\n        stopwords = ensure_list(stopwords)\n        for term in stopwords:\n            self.nlp.vocab[term].is_stop = True\n        self.stopwords.extend(stopwords)\n\n    @validate_call\n    def make_doc(\n        self, text: str, max_length: int = None, disable: list[str] = [], **kwargs: Any\n    ) -&gt; Doc:\n        \"\"\"Return a doc from a text.\n\n        Args:\n            text (str): The text to be parsed.\n            max_length (int): The maximum length of the doc.\n            disable (list[str]): A list of spaCy pipeline components to disable.\n            kwargs (Any): Additional keyword arguments. Accepts any keyword arguments that\n                can be passed to spaCy's `Language.pipe` method, such as `batch_size`.\n\n        Returns:\n            Doc: A spaCy doc object.\n        \"\"\"\n        # Override instance settings with keyword arguments\n        if max_length:\n            self.max_length = max_length\n            self.nlp.max_length = max_length\n        if disable:\n            # self.nlp.disabled.extend(disable)\n            self.nlp.select_pipes(disable=disable)\n        return next(self.nlp.pipe([text], disable=disable, **kwargs))\n\n    @validate_call\n    def make_docs(\n        self,\n        texts: Iterable[str],\n        max_length: int = None,\n        disable: Iterable[str] = [],\n        **kwargs: Any,\n    ) -&gt; Iterable[Doc]:\n        \"\"\"Return a generator of docs from an iterable of texts.\n\n        Args:\n            texts (Iterable[str]): The texts to be parsed.\n            max_length (int): The maximum length of the docs.\n            kwargs (Any): Additional keyword arguments. Accepts any keyword arguments that\n                can be passed to spaCy's `Language.pipe` method, such as `batch_size`.\n\n        Yields:\n            Iterable[Doc]: A generator of spaCy doc objects.\n        \"\"\"\n        # Override instance settings with keyword arguments\n        if max_length:\n            self.max_length = max_length\n            self.nlp.max_length = max_length\n        if disable:\n            # self.nlp.disabled.extend(disable)\n            self.nlp.select_pipes(disable=disable)\n        return self.nlp.pipe(texts, disable=disable, **kwargs)\n\n    @validate_call\n    def remove_extension(self, name: str) -&gt; None:\n        \"\"\"Remove an extension from the spaCy Token class.\n\n        Args:\n            name (str): The name of the extension.\n        \"\"\"\n        if Token.has_extension(name):\n            Token.remove_extension(name)\n\n    @validate_call\n    def remove_stopwords(self, stopwords: str | list[str]) -&gt; None:\n        \"\"\"Remove stopwords from the tokenizer.\n\n        Args:\n            stopwords (str | list[str]): A list of stopwords to remove from the model.\n        \"\"\"\n        stopwords = ensure_list(stopwords)\n        for term in stopwords:\n            self.nlp.vocab[term].is_stop = False\n        self.stopwords = [word for word in self.stopwords if word not in stopwords]\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.components","title":"<code>components: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.disable","title":"<code>disable: Optional[list[str]] = []</code>  <code>pydantic-field</code>","text":"<p>A list of spaCy pipeline components to disable.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.disabled","title":"<code>disabled: list[str]</code>  <code>property</code>","text":"<p>Return the disabled spaCy pipeline components.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.max_length","title":"<code>max_length: Optional[int] = 2000000</code>  <code>pydantic-field</code>","text":"<p>The maximum length of the doc.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The name of the spaCy model to be used for tokenization.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.pipeline","title":"<code>pipeline: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.stopwords","title":"<code>stopwords: Optional[list[str] | str] = []</code>  <code>pydantic-field</code>","text":"<p>A list of stop words to apply to docs.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.__call__","title":"<code>__call__(texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]</code>","text":"<p>Tokenize a string or an iterable of strings.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>str | Iterable[str]</code> <p>The text(s) to be tokenized.</p> required <p>Returns:</p> Type Description <code>Doc | Iterable[Doc]</code> <p>Doc | Iterable[Doc]: The tokenized doc(s).</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]:\n    \"\"\"Tokenize a string or an iterable of strings.\n\n    Args:\n        texts (str | Iterable[str]): The text(s) to be tokenized.\n\n    Returns:\n        Doc | Iterable[Doc]: The tokenized doc(s).\n    \"\"\"\n    if isinstance(texts, str):\n        return self.make_doc(texts)\n    elif isinstance(texts, Iterable):\n        return self.make_docs(texts)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialise the Tokenizer class.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialise the Tokenizer class.\"\"\"\n    super().__init__(**data)\n    try:\n        self.nlp = spacy.load(self.model)\n        self.nlp.max_length = self.max_length\n    except OSError:\n        raise LexosException(\n            f\"Error loading model {self.model}. Please check the name and try again. You may need to install the model on your system.\"\n        )\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.add_extension","title":"<code>add_extension(name: str, default: str) -&gt; None</code>","text":"<p>Add an extension to the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required <code>default</code> <code>str</code> <p>The default value of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_extension(self, name: str, default: str) -&gt; None:\n    \"\"\"Add an extension to the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n        default (str): The default value of the extension.\n    \"\"\"\n    if not Token.has_extension(name):\n        Token.set_extension(name, default=default, force=True)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.add_stopwords","title":"<code>add_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Add stopwords to the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | Iterable[str]</code> <p>A list of stopwords to add to the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Add stopwords to the tokenizer.\n\n    Args:\n        stopwords (str | Iterable[str]): A list of stopwords to add to the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = True\n    self.stopwords.extend(stopwords)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.make_doc","title":"<code>make_doc(text: str, max_length: int = None, disable: list[str] = [], **kwargs: Any) -&gt; Doc</code>","text":"<p>Return a doc from a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be parsed.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the doc.</p> <code>None</code> <code>disable</code> <code>list[str]</code> <p>A list of spaCy pipeline components to disable.</p> <code>[]</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments. Accepts any keyword arguments that can be passed to spaCy's <code>Language.pipe</code> method, such as <code>batch_size</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A spaCy doc object.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef make_doc(\n    self, text: str, max_length: int = None, disable: list[str] = [], **kwargs: Any\n) -&gt; Doc:\n    \"\"\"Return a doc from a text.\n\n    Args:\n        text (str): The text to be parsed.\n        max_length (int): The maximum length of the doc.\n        disable (list[str]): A list of spaCy pipeline components to disable.\n        kwargs (Any): Additional keyword arguments. Accepts any keyword arguments that\n            can be passed to spaCy's `Language.pipe` method, such as `batch_size`.\n\n    Returns:\n        Doc: A spaCy doc object.\n    \"\"\"\n    # Override instance settings with keyword arguments\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n    if disable:\n        # self.nlp.disabled.extend(disable)\n        self.nlp.select_pipes(disable=disable)\n    return next(self.nlp.pipe([text], disable=disable, **kwargs))\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.make_docs","title":"<code>make_docs(texts: Iterable[str], max_length: int = None, disable: Iterable[str] = [], **kwargs: Any) -&gt; Iterable[Doc]</code>","text":"<p>Return a generator of docs from an iterable of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>The texts to be parsed.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the docs.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments. Accepts any keyword arguments that can be passed to spaCy's <code>Language.pipe</code> method, such as <code>batch_size</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Iterable[Doc]</code> <p>Iterable[Doc]: A generator of spaCy doc objects.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef make_docs(\n    self,\n    texts: Iterable[str],\n    max_length: int = None,\n    disable: Iterable[str] = [],\n    **kwargs: Any,\n) -&gt; Iterable[Doc]:\n    \"\"\"Return a generator of docs from an iterable of texts.\n\n    Args:\n        texts (Iterable[str]): The texts to be parsed.\n        max_length (int): The maximum length of the docs.\n        kwargs (Any): Additional keyword arguments. Accepts any keyword arguments that\n            can be passed to spaCy's `Language.pipe` method, such as `batch_size`.\n\n    Yields:\n        Iterable[Doc]: A generator of spaCy doc objects.\n    \"\"\"\n    # Override instance settings with keyword arguments\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n    if disable:\n        # self.nlp.disabled.extend(disable)\n        self.nlp.select_pipes(disable=disable)\n    return self.nlp.pipe(texts, disable=disable, **kwargs)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.remove_extension","title":"<code>remove_extension(name: str) -&gt; None</code>","text":"<p>Remove an extension from the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_extension(self, name: str) -&gt; None:\n    \"\"\"Remove an extension from the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n    \"\"\"\n    if Token.has_extension(name):\n        Token.remove_extension(name)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.remove_stopwords","title":"<code>remove_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Remove stopwords from the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | list[str]</code> <p>A list of stopwords to remove from the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Remove stopwords from the tokenizer.\n\n    Args:\n        stopwords (str | list[str]): A list of stopwords to remove from the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = False\n    self.stopwords = [word for word in self.stopwords if word not in stopwords]\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The name of the spaCy model to be used for tokenization.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.max_length","title":"<code>max_length: Optional[int] = 2000000</code>  <code>pydantic-field</code>","text":"<p>The maximum length of the doc.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.disable","title":"<code>disable: Optional[list[str]] = []</code>  <code>pydantic-field</code>","text":"<p>A list of spaCy pipeline components to disable.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.stopwords","title":"<code>stopwords: Optional[list[str] | str] = []</code>  <code>pydantic-field</code>","text":"<p>A list of stop words to apply to docs.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.nlp","title":"<code>nlp: Optional[Language]</code>  <code>pydantic-field</code>","text":""},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()), validate_assignment=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialise the Tokenizer class.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialise the Tokenizer class.\"\"\"\n    super().__init__(**data)\n    try:\n        self.nlp = spacy.load(self.model)\n        self.nlp.max_length = self.max_length\n    except OSError:\n        raise LexosException(\n            f\"Error loading model {self.model}. Please check the name and try again. You may need to install the model on your system.\"\n        )\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.__call__","title":"<code>__call__(texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]</code>","text":"<p>Tokenize a string or an iterable of strings.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>str | Iterable[str]</code> <p>The text(s) to be tokenized.</p> required <p>Returns:</p> Type Description <code>Doc | Iterable[Doc]</code> <p>Doc | Iterable[Doc]: The tokenized doc(s).</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]:\n    \"\"\"Tokenize a string or an iterable of strings.\n\n    Args:\n        texts (str | Iterable[str]): The text(s) to be tokenized.\n\n    Returns:\n        Doc | Iterable[Doc]: The tokenized doc(s).\n    \"\"\"\n    if isinstance(texts, str):\n        return self.make_doc(texts)\n    elif isinstance(texts, Iterable):\n        return self.make_docs(texts)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.pipeline","title":"<code>pipeline: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.components","title":"<code>components: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.disabled","title":"<code>disabled: list[str]</code>  <code>property</code>","text":"<p>Return the disabled spaCy pipeline components.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.add_extension","title":"<code>add_extension(name: str, default: str) -&gt; None</code>","text":"<p>Add an extension to the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required <code>default</code> <code>str</code> <p>The default value of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_extension(self, name: str, default: str) -&gt; None:\n    \"\"\"Add an extension to the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n        default (str): The default value of the extension.\n    \"\"\"\n    if not Token.has_extension(name):\n        Token.set_extension(name, default=default, force=True)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.add_stopwords","title":"<code>add_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Add stopwords to the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | Iterable[str]</code> <p>A list of stopwords to add to the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Add stopwords to the tokenizer.\n\n    Args:\n        stopwords (str | Iterable[str]): A list of stopwords to add to the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = True\n    self.stopwords.extend(stopwords)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.make_doc","title":"<code>make_doc(text: str, max_length: int = None, disable: list[str] = [], **kwargs: Any) -&gt; Doc</code>","text":"<p>Return a doc from a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be parsed.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the doc.</p> <code>None</code> <code>disable</code> <code>list[str]</code> <p>A list of spaCy pipeline components to disable.</p> <code>[]</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments. Accepts any keyword arguments that can be passed to spaCy's <code>Language.pipe</code> method, such as <code>batch_size</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A spaCy doc object.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef make_doc(\n    self, text: str, max_length: int = None, disable: list[str] = [], **kwargs: Any\n) -&gt; Doc:\n    \"\"\"Return a doc from a text.\n\n    Args:\n        text (str): The text to be parsed.\n        max_length (int): The maximum length of the doc.\n        disable (list[str]): A list of spaCy pipeline components to disable.\n        kwargs (Any): Additional keyword arguments. Accepts any keyword arguments that\n            can be passed to spaCy's `Language.pipe` method, such as `batch_size`.\n\n    Returns:\n        Doc: A spaCy doc object.\n    \"\"\"\n    # Override instance settings with keyword arguments\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n    if disable:\n        # self.nlp.disabled.extend(disable)\n        self.nlp.select_pipes(disable=disable)\n    return next(self.nlp.pipe([text], disable=disable, **kwargs))\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.make_docs","title":"<code>make_docs(texts: Iterable[str], max_length: int = None, disable: Iterable[str] = [], **kwargs: Any) -&gt; Iterable[Doc]</code>","text":"<p>Return a generator of docs from an iterable of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>The texts to be parsed.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the docs.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments. Accepts any keyword arguments that can be passed to spaCy's <code>Language.pipe</code> method, such as <code>batch_size</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Iterable[Doc]</code> <p>Iterable[Doc]: A generator of spaCy doc objects.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef make_docs(\n    self,\n    texts: Iterable[str],\n    max_length: int = None,\n    disable: Iterable[str] = [],\n    **kwargs: Any,\n) -&gt; Iterable[Doc]:\n    \"\"\"Return a generator of docs from an iterable of texts.\n\n    Args:\n        texts (Iterable[str]): The texts to be parsed.\n        max_length (int): The maximum length of the docs.\n        kwargs (Any): Additional keyword arguments. Accepts any keyword arguments that\n            can be passed to spaCy's `Language.pipe` method, such as `batch_size`.\n\n    Yields:\n        Iterable[Doc]: A generator of spaCy doc objects.\n    \"\"\"\n    # Override instance settings with keyword arguments\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n    if disable:\n        # self.nlp.disabled.extend(disable)\n        self.nlp.select_pipes(disable=disable)\n    return self.nlp.pipe(texts, disable=disable, **kwargs)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.remove_extension","title":"<code>remove_extension(name: str) -&gt; None</code>","text":"<p>Remove an extension from the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_extension(self, name: str) -&gt; None:\n    \"\"\"Remove an extension from the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n    \"\"\"\n    if Token.has_extension(name):\n        Token.remove_extension(name)\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.Tokenizer.remove_stopwords","title":"<code>remove_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Remove stopwords from the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | list[str]</code> <p>A list of stopwords to remove from the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Remove stopwords from the tokenizer.\n\n    Args:\n        stopwords (str | list[str]): A list of stopwords to remove from the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = False\n    self.stopwords = [word for word in self.stopwords if word not in stopwords]\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.SliceTokenizer","title":"<code>SliceTokenizer</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Simple slice tokenizer.</p> <p>Fields:</p> <ul> <li> <code>n</code>                 (<code>int</code>)             </li> <li> <code>drop_ws</code>                 (<code>Optional[bool]</code>)             </li> </ul> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>class SliceTokenizer(BaseModel, validate_assignment=True):\n    \"\"\"Simple slice tokenizer.\"\"\"\n\n    n: int = Field(description=\"The size of the tokens in characters.\")\n    drop_ws: Optional[bool] = Field(\n        default=True, description=\"Whether to drop whitespace from the tokens.\"\n    )\n\n    @validate_call\n    def __call__(self, text: str) -&gt; list[str]:\n        \"\"\"Slice the text into tokens of n characters.\n\n        Args:\n            text (str): The text to tokenize.\n\n        Returns:\n            list[str]: A list of tokens.\n        \"\"\"\n        if self.drop_ws:\n            text = text.replace(\" \", \"\")\n        return [\"\".join(t) for t in batched(text, self.n)]\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.SliceTokenizer.drop_ws","title":"<code>drop_ws: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to drop whitespace from the tokens.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.SliceTokenizer.n","title":"<code>n: int</code>  <code>pydantic-field</code>","text":"<p>The size of the tokens in characters.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.SliceTokenizer.__call__","title":"<code>__call__(text: str) -&gt; list[str]</code>","text":"<p>Slice the text into tokens of n characters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to tokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of tokens.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, text: str) -&gt; list[str]:\n    \"\"\"Slice the text into tokens of n characters.\n\n    Args:\n        text (str): The text to tokenize.\n\n    Returns:\n        list[str]: A list of tokens.\n    \"\"\"\n    if self.drop_ws:\n        text = text.replace(\" \", \"\")\n    return [\"\".join(t) for t in batched(text, self.n)]\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.SliceTokenizer.n","title":"<code>n: int</code>  <code>pydantic-field</code>","text":"<p>The size of the tokens in characters.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.SliceTokenizer.drop_ws","title":"<code>drop_ws: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to drop whitespace from the tokens.</p>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.SliceTokenizer.__call__","title":"<code>__call__(text: str) -&gt; list[str]</code>","text":"<p>Slice the text into tokens of n characters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to tokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of tokens.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, text: str) -&gt; list[str]:\n    \"\"\"Slice the text into tokens of n characters.\n\n    Args:\n        text (str): The text to tokenize.\n\n    Returns:\n        list[str]: A list of tokens.\n    \"\"\"\n    if self.drop_ws:\n        text = text.replace(\" \", \"\")\n    return [\"\".join(t) for t in batched(text, self.n)]\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.WhitespaceTokenizer","title":"<code>WhitespaceTokenizer</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Simple whitespace tokenizer.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>class WhitespaceTokenizer(BaseModel):\n    \"\"\"Simple whitespace tokenizer.\"\"\"\n\n    @validate_call\n    def __call__(self, text: str) -&gt; list[str]:\n        \"\"\"Split the text into tokens on whitespace.\n\n        Args:\n            text (str): The text to tokenize.\n\n        Returns:\n            list[str]: A list of tokens.\n        \"\"\"\n        return text.split()\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.WhitespaceTokenizer.__call__","title":"<code>__call__(text: str) -&gt; list[str]</code>","text":"<p>Split the text into tokens on whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to tokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of tokens.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, text: str) -&gt; list[str]:\n    \"\"\"Split the text into tokens on whitespace.\n\n    Args:\n        text (str): The text to tokenize.\n\n    Returns:\n        list[str]: A list of tokens.\n    \"\"\"\n    return text.split()\n</code></pre>"},{"location":"api/tokenizer/tokenizer/#lexos.tokenizer.WhitespaceTokenizer.__call__","title":"<code>__call__(text: str) -&gt; list[str]</code>","text":"<p>Split the text into tokens on whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to tokenize.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of tokens.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, text: str) -&gt; list[str]:\n    \"\"\"Split the text into tokens on whitespace.\n\n    Args:\n        text (str): The text to tokenize.\n\n    Returns:\n        list[str]: A list of tokens.\n    \"\"\"\n    return text.split()\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/","title":"Whitespace Counter Tokenizer","text":"<p>This class inherits from the main <code>Tokenizer</code> class and extends it by counting runs of spaces and line breaks.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter","title":"<code>WhitespaceCounter</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Whitespace tokenizer that captures line breaks and counts runs of spaces.</p> <p>Fields:</p> <ul> <li> <code>model</code>                 (<code>Optional[str]</code>)             </li> <li> <code>max_length</code>                 (<code>Optional[int]</code>)             </li> <li> <code>disable</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>stopwords</code>                 (<code>Optional[list[str] | str]</code>)             </li> <li> <code>nlp</code>                 (<code>Optional[Language]</code>)             </li> </ul> Source code in <code>lexos/tokenizer/whitespace_counter.py</code> <pre><code>class WhitespaceCounter(Tokenizer):\n    \"\"\"Whitespace tokenizer that captures line breaks and counts runs of spaces.\"\"\"\n\n    def _get_token_widths(self, text: str) -&gt; tuple[list[str], list[int]]:\n        \"\"\"Get the widths of tokens in a doc.\n\n        Args:\n            text (str): The input text.\n\n        Returns:\n            tuple[list[str], list[int]]: A tuple containing the tokens and widths.\n        \"\"\"\n        # Pattern: words, line breaks, or runs of spaces\n        pattern = re.compile(r\"([^\\s\\n]+)|(\\n)|([ ]{2,})|([ ])\")\n        tokens = []\n        widths = []\n        for match in pattern.finditer(text):\n            word, newline, multi_space, single_space = match.groups()\n            if word:\n                tokens.append(word)\n                widths.append(len(word))  # Use number of characters in word\n            elif newline:\n                tokens.append(\"\\n\")\n                widths.append(1)  # Use 1 to indicate a line break\n            elif multi_space:\n                tokens.append(\" \")\n                widths.append(len(multi_space))\n            elif single_space:\n                tokens.append(\" \")\n                widths.append(1)\n        return tokens, widths\n\n    @validate_call\n    def make_doc(\n        self, text: str, max_length: int = None, disable: list[str] = []\n    ) -&gt; Doc:\n        \"\"\"Return a doc from a text.\n\n        Args:\n            text (str): The text to be parsed.\n            max_length (int): The maximum length of the doc.\n            disable (list[str]): A list of spaCy pipeline components to disable.\n\n        Returns:\n            Doc: A spaCy doc object.\n        \"\"\"\n        # Override instance settings with keyword arguments\n        if max_length:\n            self.max_length = max_length\n            self.nlp.max_length = max_length\n        if disable:\n            self.nlp.select_pipes(disable=disable)\n        tokens, widths = self._get_token_widths(text)\n        if not Token.has_extension(\"width\"):\n            Token.set_extension(\"width\", default=0)\n        doc = Doc(self.nlp.vocab, words=tokens)\n        for token, count in zip(doc, widths):\n            token._.width = count\n        # Apply pipeline components manually, skipping those in 'disable'\n        for name, proc in self.nlp.pipeline:\n            if name not in disable:\n                doc = proc(doc)\n        return doc\n\n    @validate_call\n    def make_docs(\n        self,\n        texts: Iterable[str],\n        max_length: int = None,\n        disable: Iterable[str] = [],\n        chunk_size: int = 1000,\n    ) -&gt; Iterable[Doc]:\n        \"\"\"Return a generator of docs from an iterable of texts, processing in chunks.\n\n        Args:\n            texts (Iterable[str]): The texts to process.\n            max_length (int, optional): Maximum doc length.\n            disable (Iterable[str], optional): Pipeline components to disable.\n            chunk_size (int, optional): Number of docs to process per chunk.\n\n        Yields:\n            Doc: spaCy Doc objects.\n        \"\"\"\n        if max_length:\n            self.max_length = max_length\n            self.nlp.max_length = max_length\n\n        if not Token.has_extension(\"width\"):\n            Token.set_extension(\"width\", default=0)\n        enabled_pipes = [\n            (name, proc) for name, proc in self.nlp.pipeline if name not in disable\n        ]\n\n        def chunker(iterable, size):\n            chunk = []\n            for item in iterable:\n                chunk.append(item)\n                if len(chunk) == size:\n                    yield chunk\n                    chunk = []\n            if chunk:\n                yield chunk\n\n        for text_chunk in chunker(texts, chunk_size):\n            docs = []\n            for text in text_chunk:\n                tokens, widths = self._get_token_widths(text)\n                doc = Doc(self.nlp.vocab, words=tokens)\n                for token, count in zip(doc, widths):\n                    token._.width = count\n                docs.append(doc)\n            for _, proc in enabled_pipes:\n                docs = [proc(doc) for doc in docs]\n            yield from docs\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.components","title":"<code>components: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.disable","title":"<code>disable: Optional[list[str]] = []</code>  <code>pydantic-field</code>","text":"<p>A list of spaCy pipeline components to disable.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.disabled","title":"<code>disabled: list[str]</code>  <code>property</code>","text":"<p>Return the disabled spaCy pipeline components.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.max_length","title":"<code>max_length: Optional[int] = 2000000</code>  <code>pydantic-field</code>","text":"<p>The maximum length of the doc.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The name of the spaCy model to be used for tokenization.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.pipeline","title":"<code>pipeline: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.stopwords","title":"<code>stopwords: Optional[list[str] | str] = []</code>  <code>pydantic-field</code>","text":"<p>A list of stop words to apply to docs.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.__call__","title":"<code>__call__(texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]</code>","text":"<p>Tokenize a string or an iterable of strings.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>str | Iterable[str]</code> <p>The text(s) to be tokenized.</p> required <p>Returns:</p> Type Description <code>Doc | Iterable[Doc]</code> <p>Doc | Iterable[Doc]: The tokenized doc(s).</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]:\n    \"\"\"Tokenize a string or an iterable of strings.\n\n    Args:\n        texts (str | Iterable[str]): The text(s) to be tokenized.\n\n    Returns:\n        Doc | Iterable[Doc]: The tokenized doc(s).\n    \"\"\"\n    if isinstance(texts, str):\n        return self.make_doc(texts)\n    elif isinstance(texts, Iterable):\n        return self.make_docs(texts)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialise the Tokenizer class.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialise the Tokenizer class.\"\"\"\n    super().__init__(**data)\n    try:\n        self.nlp = spacy.load(self.model)\n        self.nlp.max_length = self.max_length\n    except OSError:\n        raise LexosException(\n            f\"Error loading model {self.model}. Please check the name and try again. You may need to install the model on your system.\"\n        )\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.add_extension","title":"<code>add_extension(name: str, default: str) -&gt; None</code>","text":"<p>Add an extension to the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required <code>default</code> <code>str</code> <p>The default value of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_extension(self, name: str, default: str) -&gt; None:\n    \"\"\"Add an extension to the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n        default (str): The default value of the extension.\n    \"\"\"\n    if not Token.has_extension(name):\n        Token.set_extension(name, default=default, force=True)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.add_stopwords","title":"<code>add_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Add stopwords to the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | Iterable[str]</code> <p>A list of stopwords to add to the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Add stopwords to the tokenizer.\n\n    Args:\n        stopwords (str | Iterable[str]): A list of stopwords to add to the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = True\n    self.stopwords.extend(stopwords)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.make_doc","title":"<code>make_doc(text: str, max_length: int = None, disable: list[str] = []) -&gt; Doc</code>","text":"<p>Return a doc from a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be parsed.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the doc.</p> <code>None</code> <code>disable</code> <code>list[str]</code> <p>A list of spaCy pipeline components to disable.</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A spaCy doc object.</p> Source code in <code>lexos/tokenizer/whitespace_counter.py</code> <pre><code>@validate_call\ndef make_doc(\n    self, text: str, max_length: int = None, disable: list[str] = []\n) -&gt; Doc:\n    \"\"\"Return a doc from a text.\n\n    Args:\n        text (str): The text to be parsed.\n        max_length (int): The maximum length of the doc.\n        disable (list[str]): A list of spaCy pipeline components to disable.\n\n    Returns:\n        Doc: A spaCy doc object.\n    \"\"\"\n    # Override instance settings with keyword arguments\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n    if disable:\n        self.nlp.select_pipes(disable=disable)\n    tokens, widths = self._get_token_widths(text)\n    if not Token.has_extension(\"width\"):\n        Token.set_extension(\"width\", default=0)\n    doc = Doc(self.nlp.vocab, words=tokens)\n    for token, count in zip(doc, widths):\n        token._.width = count\n    # Apply pipeline components manually, skipping those in 'disable'\n    for name, proc in self.nlp.pipeline:\n        if name not in disable:\n            doc = proc(doc)\n    return doc\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.make_docs","title":"<code>make_docs(texts: Iterable[str], max_length: int = None, disable: Iterable[str] = [], chunk_size: int = 1000) -&gt; Iterable[Doc]</code>","text":"<p>Return a generator of docs from an iterable of texts, processing in chunks.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>The texts to process.</p> required <code>max_length</code> <code>int</code> <p>Maximum doc length.</p> <code>None</code> <code>disable</code> <code>Iterable[str]</code> <p>Pipeline components to disable.</p> <code>[]</code> <code>chunk_size</code> <code>int</code> <p>Number of docs to process per chunk.</p> <code>1000</code> <p>Yields:</p> Name Type Description <code>Doc</code> <code>Iterable[Doc]</code> <p>spaCy Doc objects.</p> Source code in <code>lexos/tokenizer/whitespace_counter.py</code> <pre><code>@validate_call\ndef make_docs(\n    self,\n    texts: Iterable[str],\n    max_length: int = None,\n    disable: Iterable[str] = [],\n    chunk_size: int = 1000,\n) -&gt; Iterable[Doc]:\n    \"\"\"Return a generator of docs from an iterable of texts, processing in chunks.\n\n    Args:\n        texts (Iterable[str]): The texts to process.\n        max_length (int, optional): Maximum doc length.\n        disable (Iterable[str], optional): Pipeline components to disable.\n        chunk_size (int, optional): Number of docs to process per chunk.\n\n    Yields:\n        Doc: spaCy Doc objects.\n    \"\"\"\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n\n    if not Token.has_extension(\"width\"):\n        Token.set_extension(\"width\", default=0)\n    enabled_pipes = [\n        (name, proc) for name, proc in self.nlp.pipeline if name not in disable\n    ]\n\n    def chunker(iterable, size):\n        chunk = []\n        for item in iterable:\n            chunk.append(item)\n            if len(chunk) == size:\n                yield chunk\n                chunk = []\n        if chunk:\n            yield chunk\n\n    for text_chunk in chunker(texts, chunk_size):\n        docs = []\n        for text in text_chunk:\n            tokens, widths = self._get_token_widths(text)\n            doc = Doc(self.nlp.vocab, words=tokens)\n            for token, count in zip(doc, widths):\n                token._.width = count\n            docs.append(doc)\n        for _, proc in enabled_pipes:\n            docs = [proc(doc) for doc in docs]\n        yield from docs\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.remove_extension","title":"<code>remove_extension(name: str) -&gt; None</code>","text":"<p>Remove an extension from the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_extension(self, name: str) -&gt; None:\n    \"\"\"Remove an extension from the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n    \"\"\"\n    if Token.has_extension(name):\n        Token.remove_extension(name)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.remove_stopwords","title":"<code>remove_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Remove stopwords from the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | list[str]</code> <p>A list of stopwords to remove from the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Remove stopwords from the tokenizer.\n\n    Args:\n        stopwords (str | list[str]): A list of stopwords to remove from the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = False\n    self.stopwords = [word for word in self.stopwords if word not in stopwords]\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.model","title":"<code>model: Optional[str] = 'xx_sent_ud_sm'</code>  <code>pydantic-field</code>","text":"<p>The name of the spaCy model to be used for tokenization.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.max_length","title":"<code>max_length: Optional[int] = 2000000</code>  <code>pydantic-field</code>","text":"<p>The maximum length of the doc.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.disable","title":"<code>disable: Optional[list[str]] = []</code>  <code>pydantic-field</code>","text":"<p>A list of spaCy pipeline components to disable.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.stopwords","title":"<code>stopwords: Optional[list[str] | str] = []</code>  <code>pydantic-field</code>","text":"<p>A list of stop words to apply to docs.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.nlp","title":"<code>nlp: Optional[Language]</code>  <code>pydantic-field</code>","text":""},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()), validate_assignment=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialise the Tokenizer class.</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialise the Tokenizer class.\"\"\"\n    super().__init__(**data)\n    try:\n        self.nlp = spacy.load(self.model)\n        self.nlp.max_length = self.max_length\n    except OSError:\n        raise LexosException(\n            f\"Error loading model {self.model}. Please check the name and try again. You may need to install the model on your system.\"\n        )\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.__call__","title":"<code>__call__(texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]</code>","text":"<p>Tokenize a string or an iterable of strings.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>str | Iterable[str]</code> <p>The text(s) to be tokenized.</p> required <p>Returns:</p> Type Description <code>Doc | Iterable[Doc]</code> <p>Doc | Iterable[Doc]: The tokenized doc(s).</p> Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef __call__(self, texts: str | Iterable[str]) -&gt; Doc | Iterable[Doc]:\n    \"\"\"Tokenize a string or an iterable of strings.\n\n    Args:\n        texts (str | Iterable[str]): The text(s) to be tokenized.\n\n    Returns:\n        Doc | Iterable[Doc]: The tokenized doc(s).\n    \"\"\"\n    if isinstance(texts, str):\n        return self.make_doc(texts)\n    elif isinstance(texts, Iterable):\n        return self.make_docs(texts)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.pipeline","title":"<code>pipeline: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.components","title":"<code>components: list[str]</code>  <code>property</code>","text":"<p>Return the spaCy pipeline components.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.disabled","title":"<code>disabled: list[str]</code>  <code>property</code>","text":"<p>Return the disabled spaCy pipeline components.</p>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter._get_token_widths","title":"<code>_get_token_widths(text: str) -&gt; tuple[list[str], list[int]]</code>","text":"<p>Get the widths of tokens in a doc.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[int]]</code> <p>tuple[list[str], list[int]]: A tuple containing the tokens and widths.</p> Source code in <code>lexos/tokenizer/whitespace_counter.py</code> <pre><code>def _get_token_widths(self, text: str) -&gt; tuple[list[str], list[int]]:\n    \"\"\"Get the widths of tokens in a doc.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        tuple[list[str], list[int]]: A tuple containing the tokens and widths.\n    \"\"\"\n    # Pattern: words, line breaks, or runs of spaces\n    pattern = re.compile(r\"([^\\s\\n]+)|(\\n)|([ ]{2,})|([ ])\")\n    tokens = []\n    widths = []\n    for match in pattern.finditer(text):\n        word, newline, multi_space, single_space = match.groups()\n        if word:\n            tokens.append(word)\n            widths.append(len(word))  # Use number of characters in word\n        elif newline:\n            tokens.append(\"\\n\")\n            widths.append(1)  # Use 1 to indicate a line break\n        elif multi_space:\n            tokens.append(\" \")\n            widths.append(len(multi_space))\n        elif single_space:\n            tokens.append(\" \")\n            widths.append(1)\n    return tokens, widths\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.add_extension","title":"<code>add_extension(name: str, default: str) -&gt; None</code>","text":"<p>Add an extension to the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required <code>default</code> <code>str</code> <p>The default value of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_extension(self, name: str, default: str) -&gt; None:\n    \"\"\"Add an extension to the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n        default (str): The default value of the extension.\n    \"\"\"\n    if not Token.has_extension(name):\n        Token.set_extension(name, default=default, force=True)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.add_stopwords","title":"<code>add_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Add stopwords to the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | Iterable[str]</code> <p>A list of stopwords to add to the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef add_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Add stopwords to the tokenizer.\n\n    Args:\n        stopwords (str | Iterable[str]): A list of stopwords to add to the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = True\n    self.stopwords.extend(stopwords)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.make_doc","title":"<code>make_doc(text: str, max_length: int = None, disable: list[str] = []) -&gt; Doc</code>","text":"<p>Return a doc from a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be parsed.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of the doc.</p> <code>None</code> <code>disable</code> <code>list[str]</code> <p>A list of spaCy pipeline components to disable.</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>Doc</code> <code>Doc</code> <p>A spaCy doc object.</p> Source code in <code>lexos/tokenizer/whitespace_counter.py</code> <pre><code>@validate_call\ndef make_doc(\n    self, text: str, max_length: int = None, disable: list[str] = []\n) -&gt; Doc:\n    \"\"\"Return a doc from a text.\n\n    Args:\n        text (str): The text to be parsed.\n        max_length (int): The maximum length of the doc.\n        disable (list[str]): A list of spaCy pipeline components to disable.\n\n    Returns:\n        Doc: A spaCy doc object.\n    \"\"\"\n    # Override instance settings with keyword arguments\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n    if disable:\n        self.nlp.select_pipes(disable=disable)\n    tokens, widths = self._get_token_widths(text)\n    if not Token.has_extension(\"width\"):\n        Token.set_extension(\"width\", default=0)\n    doc = Doc(self.nlp.vocab, words=tokens)\n    for token, count in zip(doc, widths):\n        token._.width = count\n    # Apply pipeline components manually, skipping those in 'disable'\n    for name, proc in self.nlp.pipeline:\n        if name not in disable:\n            doc = proc(doc)\n    return doc\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.make_docs","title":"<code>make_docs(texts: Iterable[str], max_length: int = None, disable: Iterable[str] = [], chunk_size: int = 1000) -&gt; Iterable[Doc]</code>","text":"<p>Return a generator of docs from an iterable of texts, processing in chunks.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>The texts to process.</p> required <code>max_length</code> <code>int</code> <p>Maximum doc length.</p> <code>None</code> <code>disable</code> <code>Iterable[str]</code> <p>Pipeline components to disable.</p> <code>[]</code> <code>chunk_size</code> <code>int</code> <p>Number of docs to process per chunk.</p> <code>1000</code> <p>Yields:</p> Name Type Description <code>Doc</code> <code>Iterable[Doc]</code> <p>spaCy Doc objects.</p> Source code in <code>lexos/tokenizer/whitespace_counter.py</code> <pre><code>@validate_call\ndef make_docs(\n    self,\n    texts: Iterable[str],\n    max_length: int = None,\n    disable: Iterable[str] = [],\n    chunk_size: int = 1000,\n) -&gt; Iterable[Doc]:\n    \"\"\"Return a generator of docs from an iterable of texts, processing in chunks.\n\n    Args:\n        texts (Iterable[str]): The texts to process.\n        max_length (int, optional): Maximum doc length.\n        disable (Iterable[str], optional): Pipeline components to disable.\n        chunk_size (int, optional): Number of docs to process per chunk.\n\n    Yields:\n        Doc: spaCy Doc objects.\n    \"\"\"\n    if max_length:\n        self.max_length = max_length\n        self.nlp.max_length = max_length\n\n    if not Token.has_extension(\"width\"):\n        Token.set_extension(\"width\", default=0)\n    enabled_pipes = [\n        (name, proc) for name, proc in self.nlp.pipeline if name not in disable\n    ]\n\n    def chunker(iterable, size):\n        chunk = []\n        for item in iterable:\n            chunk.append(item)\n            if len(chunk) == size:\n                yield chunk\n                chunk = []\n        if chunk:\n            yield chunk\n\n    for text_chunk in chunker(texts, chunk_size):\n        docs = []\n        for text in text_chunk:\n            tokens, widths = self._get_token_widths(text)\n            doc = Doc(self.nlp.vocab, words=tokens)\n            for token, count in zip(doc, widths):\n                token._.width = count\n            docs.append(doc)\n        for _, proc in enabled_pipes:\n            docs = [proc(doc) for doc in docs]\n        yield from docs\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.remove_extension","title":"<code>remove_extension(name: str) -&gt; None</code>","text":"<p>Remove an extension from the spaCy Token class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the extension.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_extension(self, name: str) -&gt; None:\n    \"\"\"Remove an extension from the spaCy Token class.\n\n    Args:\n        name (str): The name of the extension.\n    \"\"\"\n    if Token.has_extension(name):\n        Token.remove_extension(name)\n</code></pre>"},{"location":"api/tokenizer/whitespace_counter/#lexos.tokenizer.whitespace_counter.WhitespaceCounter.remove_stopwords","title":"<code>remove_stopwords(stopwords: str | list[str]) -&gt; None</code>","text":"<p>Remove stopwords from the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>stopwords</code> <code>str | list[str]</code> <p>A list of stopwords to remove from the model.</p> required Source code in <code>lexos/tokenizer/__init__.py</code> <pre><code>@validate_call\ndef remove_stopwords(self, stopwords: str | list[str]) -&gt; None:\n    \"\"\"Remove stopwords from the tokenizer.\n\n    Args:\n        stopwords (str | list[str]): A list of stopwords to remove from the model.\n    \"\"\"\n    stopwords = ensure_list(stopwords)\n    for term in stopwords:\n        self.nlp.vocab[term].is_stop = False\n    self.stopwords = [word for word in self.stopwords if word not in stopwords]\n</code></pre>"},{"location":"api/topic_modeling/","title":"Topic Modeling","text":"<p>The <code>topic_modeling</code> module implements Lexos topic modeling functionality. Currently, it provides a wrapper for the MALLET topic modeling tool and the DFR Browser 2 tool for visualizing MALLET topic models.</p>"},{"location":"api/topic_modeling/#mallet","title":"Mallet","text":"<p>See Mallet for API details.</p>"},{"location":"api/topic_modeling/#dfr-browser-2","title":"DFR Browser 2","text":"<p>See DFR Browser 2 for API details.</p>"},{"location":"api/topic_modeling/dfr_browser2/","title":"DFR Browser 2","text":"<p>DFR Browser 2 is web-based topic modelling browser that provides interactive visualizations and analysis tools for exploring topic models generated by MALLET. DFR Browser 2 is based on Andrew Goldstone's original dfr-browser. It reproduces all the major functionality of the original, but with an entirely new architecture and additional features. For full documentation, see the DFR Browser 2.</p> <p>The <code>dfr_browser2</code> module provides a small helper class <code>Browser</code> that automates the steps required to prepare and open a a DFR Browser 2 distribution. This helper is designed to be used programmatically from Python and can be used to produce a small static browser bundle. It performs the following functions:</p> <ul> <li>Validates that all required MALLET output files exist</li> <li>Auto-generates <code>topic_coords.csv</code> from <code>topic-state.gz</code> if the coordinates file is missing</li> <li>Copies a template DFR Browser 2 folder into a working browser folder</li> <li>Copies all MALLET output and metadata files into the browser's <code>data/</code>, along with optional files (diagnostics.xml) if present</li> <li>Copies an optional file containing the documents used to generate the topic model</li> <li>Manages configuration settings for the browser</li> <li>Checks port availability before starting the server</li> <li>Starts a simple HTTP server to serve the browser and opens it in a web browser</li> </ul>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser","title":"<code>Browser</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Browser class to create and serve DFR Browser 2.</p> <p>filename_map usage: - Provide a mapping of <code>original_filename</code> -&gt; <code>destination_filename</code> where   <code>original_filename</code> is the filename present in <code>mallet_files_path</code> and   <code>destination_filename</code> is the name to use under the browser's <code>data/</code> folder.</p> Example <p>filename_map = {\"doc-topics.txt\": \"doc-topic.txt\"} Browser(..., filename_map=filename_map)</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>mallet_files_path</code>                 (<code>str</code>)             </li> <li> <code>browser_path</code>                 (<code>str</code>)             </li> <li> <code>template_path</code>                 (<code>str</code>)             </li> <li> <code>data_path</code>                 (<code>str | None</code>)             </li> <li> <code>config</code>                 (<code>dict | None</code>)             </li> <li> <code>port</code>                 (<code>int</code>)             </li> <li> <code>filename_map</code>                 (<code>dict[str, str]</code>)             </li> <li> <code>copied_files</code>                 (<code>dict</code>)             </li> </ul> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>class Browser(BaseModel):\n    \"\"\"Browser class to create and serve DFR Browser 2.\n\n    filename_map usage:\n    - Provide a mapping of `original_filename` -&gt; `destination_filename` where\n      `original_filename` is the filename present in `mallet_files_path` and\n      `destination_filename` is the name to use under the browser's `data/` folder.\n\n    Example:\n        filename_map = {\"doc-topics.txt\": \"doc-topic.txt\"}\n        Browser(..., filename_map=filename_map)\n    \"\"\"\n\n    mallet_files_path: str = Field(\n        ..., description=\"Path to the folder containing Mallet output files.\"\n    )\n    browser_path: str = Field(\n        None, description=\"The folder where the browser will be saved.\"\n    )\n    template_path: str = Field(\n        \"dist\", description=\"Path to the DFR Browser 2 template folder.\"\n    )\n    data_path: str | None = Field(\n        None,\n        description=(\n            \"Path to a tab-separated (TSV) file containing the original data used \"\n            \"to generate the topic model. Each row must contain 2 or 3 columns. \"\n            \"This file will be copied into the browser's data folder.\"\n        ),\n    )\n    config: dict | None = Field(\n        None, description=\"Configuration dictionary for the DFR Browser 2.\"\n    )\n    port: int = Field(8000, description=\"Port number for serving the DFR Browser 2.\")\n    filename_map: dict[str, str] = Field(\n        default_factory=dict,\n        description=\"Mapping of original filenames to new filenames.\",\n    )\n    copied_files: dict = Field(\n        default_factory=dict, description=\"Tracks copied files for config updates.\"\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    # Version of the browser distribution this class is creating/serving\n    BROWSER_VERSION: ClassVar[str] = \"0.2.3\"\n\n    # Class-level constants for required files\n    REQUIRED_FILES: ClassVar[set[str]] = {\n        \"metadata.csv\",\n        \"topic-keys.txt\",\n        \"doc-topic.txt\",\n        \"topic-state.gz\",\n        \"topic_coords.csv\",\n    }\n\n    ALT_NAME_GROUPS: ClassVar[dict[str, list[str]]] = {\n        \"doc-topic.txt\": [\"doc-topics.txt\", \"doc-topic.txt\"],\n        \"topic-state.gz\": [\"topic-state.gz\", \"state.gz\"],\n    }\n\n    @property\n    def version(self):\n        \"\"\"Return the version of the DFR Browser.\"\"\"\n        # If the user or template supplies a version, prefer that value\n        try:\n            if self.config and isinstance(self.config, dict):\n                app_cfg = self.config.get(\"application\") or {}\n                if isinstance(app_cfg, dict) and app_cfg.get(\"version\"):\n                    return app_cfg.get(\"version\")\n        except Exception:\n            # defensive fallback\n            pass\n        return self.BROWSER_VERSION\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"Intercept assignments to `config` and write merged config to disk.\n\n        This ensures that once `self.config` is set, the persisted `config.json`\n        file will be updated to match the merged configuration. We only write\n        after the instance is fully initialized to avoid partial writes during\n        construction.\n\n        Args:\n            name (str): Attribute name.\n            value (Any): Attribute value.\n        \"\"\"\n        # Use BaseModel's setattr to set attribute (ensures pydantic behavior)\n        super().__setattr__(name, value)\n        # Only write to disk after init and when config is updated\n        if name == \"config\" and getattr(self, \"_initialized\", False):\n            try:\n                self._write_config()\n            except Exception:\n                # Defensive: don't raise during attribute setting\n                pass\n\n    def __init__(self, **data: Any) -&gt; None:\n        \"\"\"Initialize the DFR Browser 2 class.\n\n        Args:\n            **data (Any): Keyword arguments for the BaseModel.\n        \"\"\"\n        # First call BaseModel initializer\n        super().__init__(**data)\n\n        # Initialize private attribute for server process (not a Pydantic field)\n        object.__setattr__(self, \"_server_process\", None)\n\n        # Execute initialization steps\n        self._validate_and_setup_paths()\n        self._check_and_prepare_required_files()\n        self._copy_template()\n        self._copy_data_file()\n        self._copy_mallet_files()\n        self._write_config()\n\n        # Mark initialization complete\n        object.__setattr__(self, \"_initialized\", True)\n\n    def _validate_and_setup_paths(self) -&gt; None:\n        \"\"\"Validate and setup file paths for the browser.\"\"\"\n        # Convert paths into Path objects\n        self.mallet_files_path = Path(self.mallet_files_path)\n\n        # If template_path is the default \"dist\", resolve it relative to this module\n        if self.template_path == \"dist\":\n            module_dir = Path(__file__).parent\n            self.template_path = module_dir / \"dist\"\n        else:\n            self.template_path = Path(self.template_path)\n\n        if self.browser_path:\n            self.browser_path = Path(self.browser_path)\n        else:\n            # Create temp directory if none provided\n            self.browser_path = Path(tempfile.mkdtemp(prefix=\"dfr_browser_\"))\n\n        # Check the template and mallet path exist\n        if not self.template_path.exists():\n            raise FileNotFoundError(\n                f\"Template path does not exist: {self.template_path}\"\n            )\n        if not self.mallet_files_path.exists():\n            raise FileNotFoundError(\n                f\"Mallet files path does not exist: {self.mallet_files_path}\"\n            )\n\n    def _check_file_exists_with_alternates(\n        self, canonical_name: str\n    ) -&gt; tuple[bool, Path | None]:\n        \"\"\"Check if a file exists, considering alternate names.\n\n        Args:\n            canonical_name: The canonical name of the file to check.\n\n        Returns:\n            tuple: (exists: bool, path: Path | None) where path is the actual file path if found.\n        \"\"\"\n        # Check if this file has alternate names\n        if canonical_name in self.ALT_NAME_GROUPS:\n            for alt in self.ALT_NAME_GROUPS[canonical_name]:\n                if (self.mallet_files_path / alt).exists():\n                    return True, self.mallet_files_path / alt\n        else:\n            # Check the canonical name directly\n            if (self.mallet_files_path / canonical_name).exists():\n                return True, self.mallet_files_path / canonical_name\n\n        # Check filename_map\n        if self.filename_map:\n            for alt in self.ALT_NAME_GROUPS.get(canonical_name, [canonical_name]):\n                if alt in self.filename_map:\n                    mapped_path = self.mallet_files_path / self.filename_map[alt]\n                    if mapped_path.exists():\n                        return True, mapped_path\n\n        return False, None\n\n    def _check_and_prepare_required_files(self) -&gt; None:\n        \"\"\"Check required files exist and generate topic_coords.csv if missing.\"\"\"\n        missing_files = []\n        missing_topic_coords = False\n\n        for required_file in self.REQUIRED_FILES:\n            exists, _ = self._check_file_exists_with_alternates(required_file)\n            if not exists:\n                if required_file == \"topic_coords.csv\":\n                    missing_topic_coords = True\n                else:\n                    missing_files.append(required_file)\n\n        # If topic_coords.csv is missing but topic-state.gz exists, generate it\n        if missing_topic_coords:\n            state_exists, state_path = self._check_file_exists_with_alternates(\n                \"topic-state.gz\"\n            )\n            if state_exists:\n                print(\n                    f\"topic_coords.csv not found. Generating from {state_path.name}...\"\n                )\n                try:\n                    process_mallet_state_file(\n                        state_file=str(state_path),\n                        output_dir=str(self.mallet_files_path),\n                        n_top_words=30,\n                        generate_all=False,\n                    )\n                    print(\"Successfully generated topic_coords.csv\")\n                except Exception as e:\n                    raise RuntimeError(\n                        f\"Failed to generate topic_coords.csv from topic-state file: {e}\"\n                    ) from e\n            else:\n                missing_files.append(\"topic_coords.csv\")\n\n        # Raise exception if any required files are still missing\n        if missing_files:\n            raise FileNotFoundError(\n                f\"Missing required files in {self.mallet_files_path}: {', '.join(missing_files)}\"\n            )\n\n    def _copy_template(self) -&gt; None:\n        \"\"\"Copy the browser template to the browser_path.\"\"\"\n        # Determine filenames to check. If user provided filename_map, treat\n        # keys as filenames that exist in the mallet files path (orig names),\n        # and values as the desired destination names.\n        filenames_to_check = set(self.REQUIRED_FILES)\n        if self.filename_map:\n            # Include keys from the filename_map (original filenames)\n            filenames_to_check.update(self.filename_map.keys())\n\n        # Store for use in _copy_mallet_files\n        self._filenames_to_check = filenames_to_check\n\n        try:\n            shutil.copytree(self.template_path, self.browser_path, dirs_exist_ok=True)\n        except Exception:\n            # On some systems, copying into existing directory fails for file metadata; fallback to per-file copy\n            for src in self.template_path.rglob(\"*\"):\n                rel = src.relative_to(self.template_path)\n                dest = self.browser_path / rel\n                if src.is_dir():\n                    dest.mkdir(parents=True, exist_ok=True)\n                else:\n                    shutil.copy2(src, dest)\n\n    def _copy_data_file(self) -&gt; None:\n        \"\"\"Copy the data file to the browser's data directory if provided.\"\"\"\n        if not self.data_path:\n            return\n\n        if self.data_path:\n            data_src = Path(self.data_path)\n            if not data_src.exists():\n                raise FileNotFoundError(f\"data_path does not exist: {data_src}\")\n            if data_src.is_dir():\n                raise ValueError(\n                    \"data_path must be a path to a TSV file, not a directory\"\n                )\n\n            # Validate TSV structure: each non-empty row must have 2 or 3 columns\n            with open(data_src, \"r\", encoding=\"utf-8\") as fh:\n                for i, line in enumerate(fh, 1):\n                    line = line.strip()\n                    if not line:\n                        continue\n                    cols = line.split(\"\\t\")\n                    if len(cols) not in (2, 3):\n                        raise ValueError(\n                            f\"Invalid TSV format in {data_src} at line {i}: expected 2 or 3 columns, got {len(cols)}\"\n                        )\n\n            data_target = self.browser_path / \"data\"\n            data_target.mkdir(parents=True, exist_ok=True)\n            # Copy file to the data directory using the expected filename 'docs.txt'\n            docs_filename = \"docs.txt\"\n            shutil.copy2(data_src, data_target / docs_filename)\n            # Track which files we copied (so we can update config.json paths)\n            self.copied_files = getattr(self, \"copied_files\", {})\n            # Save relative path as used by the template\n            self.copied_files[\"data_source\"] = f\"data/{docs_filename}\"\n\n    def _copy_mallet_files(self) -&gt; None:\n        \"\"\"Copy MALLET output files to the browser's data directory.\"\"\"\n        # Copy mallet files into the 'data' folder (not a mallet subfolder)\n        data_target = self.browser_path / \"data\"\n        data_target.mkdir(parents=True, exist_ok=True)\n        # Ensure we have a holder for copied files metadata\n        self.copied_files = getattr(self, \"copied_files\", {})\n        copied_destnames = set()\n        # Expand filenames to include alternate names so we can copy the actual file\n        filenames_to_check = self._filenames_to_check\n        copy_candidates = set(filenames_to_check)\n        for canonical, alts in self.ALT_NAME_GROUPS.items():\n            for alt in alts:\n                copy_candidates.add(alt)\n        for src_name in copy_candidates:\n            # If the src_name is a filename that only appears as a mapping value\n            # (i.e., the mapping is reversed where key is the destination), then\n            # skip copying this source directly \u2014 the mapping key will handle copying/rename.\n            if (\n                self.filename_map\n                and src_name in set(self.filename_map.values())\n                and (src_name not in self.filename_map)\n            ):\n                continue\n            # Interpret src_name as the original filename (key)\n            src_path = self.mallet_files_path / src_name\n            dest_filename = src_name\n            # If the filename is an alternate name for a canonical file, default to the canonical\n            # filename as the destination unless the user explicitly specified a mapping for it.\n            if not (self.filename_map and src_name in self.filename_map):\n                for canonical, alts in self.ALT_NAME_GROUPS.items():\n                    if src_name in alts:\n                        dest_filename = canonical\n                        break\n            # If a mapping exists for this src_name, the mapped value is the destination filename\n            if self.filename_map and src_name in self.filename_map:\n                dest_filename = self.filename_map[src_name]\n            # If the file doesn't exist under the original name, try the destination name\n            if not src_path.exists():\n                fallback_path = self.mallet_files_path / dest_filename\n                if fallback_path.exists():\n                    src_path = fallback_path\n                    # If the user supplied a mapping where the key was actually the\n                    # desired destination (i.e., mapping was reversed), then swap\n                    # the dest filename so we rename original-&gt;dest correctly.\n                    if (\n                        self.filename_map\n                        and src_name in self.filename_map\n                        and (self.filename_map[src_name] == fallback_path.name)\n                    ):\n                        # key was destination, value was source, so swap\n                        dest_filename = src_name\n                else:\n                    # If missing, skip\n                    continue\n            # Prevent duplicate destination filenames (e.g. doc-topic vs doc-topics)\n            if dest_filename in copied_destnames:\n                continue\n            dest_path = data_target / dest_filename\n            shutil.copy2(src_path, dest_path)\n            copied_destnames.add(dest_filename)\n\n            # Track copied files for config\n            self._track_copied_file(src_name, dest_filename)\n\n        # After copying required files, scan for and copy optional MALLET output files\n        optional_files = [\"diagnostics.xml\"]\n        for optional_file in optional_files:\n            src_path = self.mallet_files_path / optional_file\n            if src_path.exists() and optional_file not in copied_destnames:\n                dest_path = data_target / optional_file\n                shutil.copy2(src_path, dest_path)\n                copied_destnames.add(optional_file)\n                # Track for config\n                self._track_copied_file(optional_file, optional_file)\n\n    def _track_copied_file(self, src_name: str, dest_filename: str) -&gt; None:\n        \"\"\"Track a copied file for config.json updates.\n\n        Args:\n            src_name: Original source filename\n            dest_filename: Destination filename\n        \"\"\"\n        # Determine canonical group for src_name\n        canonical_for_src = None\n        for canonical, alts in self.ALT_NAME_GROUPS.items():\n            if src_name in alts:\n                canonical_for_src = canonical\n                break\n\n        lower = dest_filename.lower()\n        if canonical_for_src == \"doc-topic.txt\":\n            # If source belonged to doc-topic alt group, map regardless of dest filename\n            self.copied_files[\"doc_topic_file\"] = f\"data/{dest_filename}\"\n        if \"topic-keys\" in lower or \"topic_keys\" in lower:\n            self.copied_files[\"topic_keys_file\"] = f\"data/{dest_filename}\"\n        elif \"doc-topic\" in lower or \"doc-topics\" in lower or \"doc_topic\" in lower:\n            # Template uses 'doc_topic_file'\n            self.copied_files[\"doc_topic_file\"] = f\"data/{dest_filename}\"\n        elif canonical_for_src == \"topic-state.gz\":\n            # 'topic-state.gz' canonical group \u2014 ensure mapping written even if dest filename doesn't contain keyword\n            self.copied_files[\"topic_state_file\"] = f\"data/{dest_filename}\"\n        elif \"metadata\" in lower:\n            self.copied_files[\"metadata_file\"] = f\"data/{dest_filename}\"\n        elif \"topic-state\" in lower or \"topic_state\" in lower:\n            self.copied_files[\"topic_state_file\"] = f\"data/{dest_filename}\"\n        elif \"topic_coords\" in lower or \"topic-coords\" in lower:\n            self.copied_files[\"topic_coords_file\"] = f\"data/{dest_filename}\"\n        elif \"diagnostics\" in lower:\n            self.copied_files[\"diagnostics_file\"] = f\"data/{dest_filename}\"\n\n    def _is_port_available(self, port: int) -&gt; bool:\n        \"\"\"Check if a port is available for use.\n\n        Args:\n            port: Port number to check.\n\n        Returns:\n            bool: True if port is available, False if already in use.\n        \"\"\"\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.bind((\"localhost\", port))\n                return True\n        except OSError:\n            return False\n\n    def config_browser(self, config: dict) -&gt; None:\n        \"\"\"Set the browser configuration after initialization.\n\n        Args:\n            config (dict): Configuration dictionary for the DFR Browser 2.\n        \"\"\"\n        # Update the config attribute\n        self.config = config\n        # Write the new config to config.json in browser_path\n        self._write_config()\n\n    def serve(self) -&gt; None:\n        \"\"\"Serve the DFR Browser 2 using the server.py subprocess.\n\n        This method starts the server as a subprocess and provides instructions\n        for stopping it. Works in both command line and Jupyter notebook environments.\n        \"\"\"\n        if not self.browser_path.exists():\n            raise FileNotFoundError(f\"Browser path does not exist: {self.browser_path}\")\n\n        # Check if port is already in use\n        if not self._is_port_available(self.port):\n            error_msg = (\n                f\"Port {self.port} is already in use.\\n\\n\"\n                f\"To resolve this issue, you can:\\n\"\n                f\"  1. Stop the process using the port:\\n\"\n                f\"     - Check what's using it: lsof -i:{self.port}\\n\"\n                f\"     - Kill the process: kill $(lsof -t -i:{self.port})\\n\"\n                f\"     - Or in Jupyter: !kill $(lsof -t -i:{self.port})\\n\"\n                f\"  2. Use a different port by setting port=&lt;number&gt; when creating the Browser\\n\"\n            )\n            raise RuntimeError(error_msg)\n\n        # Find the server.py script in the same directory as this module\n        server_script = Path(__file__).parent / \"server.py\"\n        if not server_script.exists():\n            raise FileNotFoundError(f\"server.py not found at {server_script}\")\n\n        # Build the command to run the server\n        cmd = [sys.executable, str(server_script), str(self.port)]\n\n        url = f\"http://localhost:{self.port}/\"\n\n        # Detect if we're running in a Jupyter notebook\n        try:\n            # Check for IPython/Jupyter environment\n            get_ipython()  # type: ignore\n            in_notebook = True\n        except NameError:\n            in_notebook = False\n\n        if in_notebook:\n            # Jupyter notebook instructions\n            print(f\"Starting DFR Browser server at {url}\")\n            print(\"\\n\" + \"=\" * 60)\n            print(\"To stop the server in Jupyter, run:\")\n            print(\"  b.stop_server()\")\n            print(\"or\")\n            print(f\"  !kill $(lsof -t -i:{self.port})\")\n            print(\"or restart the kernel\")\n            print(\"=\" * 60 + \"\\n\")\n\n            # Start server as background process\n            process = subprocess.Popen(\n                cmd,\n                cwd=str(self.browser_path),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n            )\n\n            # Store the process so we can stop it later\n            self._server_process = process\n\n            # Give it a moment to start\n            import time\n\n            time.sleep(1)\n\n            # Check if process is still running\n            if process.poll() is not None:\n                stdout, stderr = process.communicate()\n                error_msg = f\"Server failed to start.\\n\\nError details:\\n{stderr}\"\n                if \"Address already in use\" in stderr or \"Errno 98\" in stderr:\n                    error_msg += (\n                        f\"\\n\\nPort {self.port} appears to be in use.\\n\"\n                        f\"Kill the process with: !kill $(lsof -t -i:{self.port})\"\n                    )\n                raise RuntimeError(error_msg)\n\n            # Open browser\n            try:\n                webbrowser.open(url)\n            except Exception:\n                print(f\"Please open {url} in your browser\")\n\n            print(f\"Server running (PID: {process.pid})\")\n\n        else:\n            # Command line - run with subprocess and wait\n            print(f\"Starting DFR Browser server at {url}\")\n            print(\"Press Ctrl+C to stop the server\\n\")\n\n            # Open browser before starting server\n            try:\n                webbrowser.open(url)\n            except Exception:\n                print(f\"Please open {url} in your browser\")\n\n            try:\n                # Run server and wait (blocks until interrupted)\n                subprocess.run(cmd, cwd=str(self.browser_path), check=True)\n            except KeyboardInterrupt:\n                print(\"\\nServer stopped\")\n            except subprocess.CalledProcessError as e:\n                raise RuntimeError(f\"Server error: {e}\")\n\n    def stop_server(self) -&gt; None:\n        \"\"\"Stop the running server process.\n\n        This method terminates the server subprocess if it's running in background mode\n        (typically in Jupyter notebooks). If the server is not running or was started in\n        blocking mode (command line), this method does nothing.\n\n        Example:\n            &gt;&gt;&gt; b = Browser(...)\n            &gt;&gt;&gt; b.serve()  # Starts server in background\n            &gt;&gt;&gt; # ... do some work ...\n            &gt;&gt;&gt; b.stop_server()  # Stops the server\n        \"\"\"\n        if self._server_process is None:\n            print(\n                \"No server process to stop (server may not have been started or was started in blocking mode)\"\n            )\n            return\n\n        if self._server_process.poll() is not None:\n            # Process already terminated\n            print(\"Server process has already stopped\")\n            self._server_process = None\n            return\n\n        try:\n            # Terminate the process\n            self._server_process.terminate()\n\n            # Wait for process to terminate (with timeout)\n            try:\n                self._server_process.wait(timeout=5)\n                print(f\"Server stopped (PID: {self._server_process.pid})\")\n            except subprocess.TimeoutExpired:\n                # If it doesn't terminate gracefully, force kill\n                self._server_process.kill()\n                self._server_process.wait()\n                print(f\"Server forcefully stopped (PID: {self._server_process.pid})\")\n        except Exception as e:\n            print(f\"Error stopping server: {e}\")\n        finally:\n            self._server_process = None\n\n    def _write_config(self) -&gt; None:\n        \"\"\"Write or update config.json in the browser_path.\"\"\"\n        if not self.browser_path:\n            raise ValueError(\"browser_path is not set\")\n        self.browser_path.mkdir(parents=True, exist_ok=True)\n        cfg_path = Path(self.browser_path) / \"config.json\"\n        # Load existing template config.json (if it exists) as the base\n        base_cfg = {}\n        if cfg_path.exists():\n            try:\n                with open(cfg_path, \"r\", encoding=\"utf-8\") as fh:\n                    base_cfg = json.load(fh)\n            except Exception:\n                base_cfg = {}\n\n        # Merge user-provided config into base (user overrides base)\n        merged_cfg = dict(base_cfg)\n        if self.config:\n            merged_cfg.update(self.config)\n\n        # Ensure file paths for known data files point to the data/ folder\n        copied = getattr(self, \"copied_files\", {}) or {}\n        for key, rel_path in copied.items():\n            # File path precedence:\n            # 1. User-specified config (self.config) \u2014 should win and be preserved\n            # 2. Copied file paths (this process) \u2014 override template values\n            # 3. Template defaults \u2014 only used if no user or copied path applies.\n            # If the user explicitly provided this key in the config, preserve it.\n            if self.config and key in self.config:\n                continue\n            # Otherwise, set the key to the path of the copied file (overriding template)\n            merged_cfg[key] = rel_path\n\n        # Ensure the application version is present in the config. Preserve user-provided values.\n        if \"application\" not in merged_cfg:\n            merged_cfg[\"application\"] = {}\n        if \"version\" not in merged_cfg[\"application\"]:\n            merged_cfg[\"application\"][\"version\"] = self.BROWSER_VERSION\n\n        # Save merged config back to the instance so callers/other methods can access the full merged config\n        self.config = merged_cfg\n\n        # Save merged config\n        with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(merged_cfg, f, indent=2)\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.browser_path","title":"<code>browser_path: str = None</code>  <code>pydantic-field</code>","text":"<p>The folder where the browser will be saved.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.config","title":"<code>config: dict | None = None</code>  <code>pydantic-field</code>","text":"<p>Configuration dictionary for the DFR Browser 2.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.copied_files","title":"<code>copied_files: dict</code>  <code>pydantic-field</code>","text":"<p>Tracks copied files for config updates.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.data_path","title":"<code>data_path: str | None = None</code>  <code>pydantic-field</code>","text":"<p>Path to a tab-separated (TSV) file containing the original data used to generate the topic model. Each row must contain 2 or 3 columns. This file will be copied into the browser's data folder.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.filename_map","title":"<code>filename_map: dict[str, str]</code>  <code>pydantic-field</code>","text":"<p>Mapping of original filenames to new filenames.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.mallet_files_path","title":"<code>mallet_files_path: str</code>  <code>pydantic-field</code>","text":"<p>Path to the folder containing Mallet output files.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.port","title":"<code>port: int = 8000</code>  <code>pydantic-field</code>","text":"<p>Port number for serving the DFR Browser 2.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.template_path","title":"<code>template_path: str = 'dist'</code>  <code>pydantic-field</code>","text":"<p>Path to the DFR Browser 2 template folder.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.version","title":"<code>version</code>  <code>property</code>","text":"<p>Return the version of the DFR Browser.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.__init__","title":"<code>__init__(**data: Any) -&gt; None</code>","text":"<p>Initialize the DFR Browser 2 class.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Keyword arguments for the BaseModel.</p> <code>{}</code> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"Initialize the DFR Browser 2 class.\n\n    Args:\n        **data (Any): Keyword arguments for the BaseModel.\n    \"\"\"\n    # First call BaseModel initializer\n    super().__init__(**data)\n\n    # Initialize private attribute for server process (not a Pydantic field)\n    object.__setattr__(self, \"_server_process\", None)\n\n    # Execute initialization steps\n    self._validate_and_setup_paths()\n    self._check_and_prepare_required_files()\n    self._copy_template()\n    self._copy_data_file()\n    self._copy_mallet_files()\n    self._write_config()\n\n    # Mark initialization complete\n    object.__setattr__(self, \"_initialized\", True)\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.__setattr__","title":"<code>__setattr__(name: str, value: Any) -&gt; None</code>","text":"<p>Intercept assignments to <code>config</code> and write merged config to disk.</p> <p>This ensures that once <code>self.config</code> is set, the persisted <code>config.json</code> file will be updated to match the merged configuration. We only write after the instance is fully initialized to avoid partial writes during construction.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Attribute name.</p> required <code>value</code> <code>Any</code> <p>Attribute value.</p> required Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"Intercept assignments to `config` and write merged config to disk.\n\n    This ensures that once `self.config` is set, the persisted `config.json`\n    file will be updated to match the merged configuration. We only write\n    after the instance is fully initialized to avoid partial writes during\n    construction.\n\n    Args:\n        name (str): Attribute name.\n        value (Any): Attribute value.\n    \"\"\"\n    # Use BaseModel's setattr to set attribute (ensures pydantic behavior)\n    super().__setattr__(name, value)\n    # Only write to disk after init and when config is updated\n    if name == \"config\" and getattr(self, \"_initialized\", False):\n        try:\n            self._write_config()\n        except Exception:\n            # Defensive: don't raise during attribute setting\n            pass\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.config_browser","title":"<code>config_browser(config: dict) -&gt; None</code>","text":"<p>Set the browser configuration after initialization.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary for the DFR Browser 2.</p> required Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def config_browser(self, config: dict) -&gt; None:\n    \"\"\"Set the browser configuration after initialization.\n\n    Args:\n        config (dict): Configuration dictionary for the DFR Browser 2.\n    \"\"\"\n    # Update the config attribute\n    self.config = config\n    # Write the new config to config.json in browser_path\n    self._write_config()\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.serve","title":"<code>serve() -&gt; None</code>","text":"<p>Serve the DFR Browser 2 using the server.py subprocess.</p> <p>This method starts the server as a subprocess and provides instructions for stopping it. Works in both command line and Jupyter notebook environments.</p> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def serve(self) -&gt; None:\n    \"\"\"Serve the DFR Browser 2 using the server.py subprocess.\n\n    This method starts the server as a subprocess and provides instructions\n    for stopping it. Works in both command line and Jupyter notebook environments.\n    \"\"\"\n    if not self.browser_path.exists():\n        raise FileNotFoundError(f\"Browser path does not exist: {self.browser_path}\")\n\n    # Check if port is already in use\n    if not self._is_port_available(self.port):\n        error_msg = (\n            f\"Port {self.port} is already in use.\\n\\n\"\n            f\"To resolve this issue, you can:\\n\"\n            f\"  1. Stop the process using the port:\\n\"\n            f\"     - Check what's using it: lsof -i:{self.port}\\n\"\n            f\"     - Kill the process: kill $(lsof -t -i:{self.port})\\n\"\n            f\"     - Or in Jupyter: !kill $(lsof -t -i:{self.port})\\n\"\n            f\"  2. Use a different port by setting port=&lt;number&gt; when creating the Browser\\n\"\n        )\n        raise RuntimeError(error_msg)\n\n    # Find the server.py script in the same directory as this module\n    server_script = Path(__file__).parent / \"server.py\"\n    if not server_script.exists():\n        raise FileNotFoundError(f\"server.py not found at {server_script}\")\n\n    # Build the command to run the server\n    cmd = [sys.executable, str(server_script), str(self.port)]\n\n    url = f\"http://localhost:{self.port}/\"\n\n    # Detect if we're running in a Jupyter notebook\n    try:\n        # Check for IPython/Jupyter environment\n        get_ipython()  # type: ignore\n        in_notebook = True\n    except NameError:\n        in_notebook = False\n\n    if in_notebook:\n        # Jupyter notebook instructions\n        print(f\"Starting DFR Browser server at {url}\")\n        print(\"\\n\" + \"=\" * 60)\n        print(\"To stop the server in Jupyter, run:\")\n        print(\"  b.stop_server()\")\n        print(\"or\")\n        print(f\"  !kill $(lsof -t -i:{self.port})\")\n        print(\"or restart the kernel\")\n        print(\"=\" * 60 + \"\\n\")\n\n        # Start server as background process\n        process = subprocess.Popen(\n            cmd,\n            cwd=str(self.browser_path),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n\n        # Store the process so we can stop it later\n        self._server_process = process\n\n        # Give it a moment to start\n        import time\n\n        time.sleep(1)\n\n        # Check if process is still running\n        if process.poll() is not None:\n            stdout, stderr = process.communicate()\n            error_msg = f\"Server failed to start.\\n\\nError details:\\n{stderr}\"\n            if \"Address already in use\" in stderr or \"Errno 98\" in stderr:\n                error_msg += (\n                    f\"\\n\\nPort {self.port} appears to be in use.\\n\"\n                    f\"Kill the process with: !kill $(lsof -t -i:{self.port})\"\n                )\n            raise RuntimeError(error_msg)\n\n        # Open browser\n        try:\n            webbrowser.open(url)\n        except Exception:\n            print(f\"Please open {url} in your browser\")\n\n        print(f\"Server running (PID: {process.pid})\")\n\n    else:\n        # Command line - run with subprocess and wait\n        print(f\"Starting DFR Browser server at {url}\")\n        print(\"Press Ctrl+C to stop the server\\n\")\n\n        # Open browser before starting server\n        try:\n            webbrowser.open(url)\n        except Exception:\n            print(f\"Please open {url} in your browser\")\n\n        try:\n            # Run server and wait (blocks until interrupted)\n            subprocess.run(cmd, cwd=str(self.browser_path), check=True)\n        except KeyboardInterrupt:\n            print(\"\\nServer stopped\")\n        except subprocess.CalledProcessError as e:\n            raise RuntimeError(f\"Server error: {e}\")\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.stop_server","title":"<code>stop_server() -&gt; None</code>","text":"<p>Stop the running server process.</p> <p>This method terminates the server subprocess if it's running in background mode (typically in Jupyter notebooks). If the server is not running or was started in blocking mode (command line), this method does nothing.</p> Example <p>b = Browser(...) b.serve()  # Starts server in background</p> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def stop_server(self) -&gt; None:\n    \"\"\"Stop the running server process.\n\n    This method terminates the server subprocess if it's running in background mode\n    (typically in Jupyter notebooks). If the server is not running or was started in\n    blocking mode (command line), this method does nothing.\n\n    Example:\n        &gt;&gt;&gt; b = Browser(...)\n        &gt;&gt;&gt; b.serve()  # Starts server in background\n        &gt;&gt;&gt; # ... do some work ...\n        &gt;&gt;&gt; b.stop_server()  # Stops the server\n    \"\"\"\n    if self._server_process is None:\n        print(\n            \"No server process to stop (server may not have been started or was started in blocking mode)\"\n        )\n        return\n\n    if self._server_process.poll() is not None:\n        # Process already terminated\n        print(\"Server process has already stopped\")\n        self._server_process = None\n        return\n\n    try:\n        # Terminate the process\n        self._server_process.terminate()\n\n        # Wait for process to terminate (with timeout)\n        try:\n            self._server_process.wait(timeout=5)\n            print(f\"Server stopped (PID: {self._server_process.pid})\")\n        except subprocess.TimeoutExpired:\n            # If it doesn't terminate gracefully, force kill\n            self._server_process.kill()\n            self._server_process.wait()\n            print(f\"Server forcefully stopped (PID: {self._server_process.pid})\")\n    except Exception as e:\n        print(f\"Error stopping server: {e}\")\n    finally:\n        self._server_process = None\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.stop_server--do-some-work","title":"... do some work ...","text":"<p>b.stop_server()  # Stops the server</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.version","title":"<code>version</code>  <code>property</code>","text":"<p>Return the version of the DFR Browser.</p>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.__setattr__","title":"<code>__setattr__(name: str, value: Any) -&gt; None</code>","text":"<p>Intercept assignments to <code>config</code> and write merged config to disk.</p> <p>This ensures that once <code>self.config</code> is set, the persisted <code>config.json</code> file will be updated to match the merged configuration. We only write after the instance is fully initialized to avoid partial writes during construction.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Attribute name.</p> required <code>value</code> <code>Any</code> <p>Attribute value.</p> required Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"Intercept assignments to `config` and write merged config to disk.\n\n    This ensures that once `self.config` is set, the persisted `config.json`\n    file will be updated to match the merged configuration. We only write\n    after the instance is fully initialized to avoid partial writes during\n    construction.\n\n    Args:\n        name (str): Attribute name.\n        value (Any): Attribute value.\n    \"\"\"\n    # Use BaseModel's setattr to set attribute (ensures pydantic behavior)\n    super().__setattr__(name, value)\n    # Only write to disk after init and when config is updated\n    if name == \"config\" and getattr(self, \"_initialized\", False):\n        try:\n            self._write_config()\n        except Exception:\n            # Defensive: don't raise during attribute setting\n            pass\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.__init__","title":"<code>__init__(**data: Any) -&gt; None</code>","text":"<p>Initialize the DFR Browser 2 class.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Keyword arguments for the BaseModel.</p> <code>{}</code> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"Initialize the DFR Browser 2 class.\n\n    Args:\n        **data (Any): Keyword arguments for the BaseModel.\n    \"\"\"\n    # First call BaseModel initializer\n    super().__init__(**data)\n\n    # Initialize private attribute for server process (not a Pydantic field)\n    object.__setattr__(self, \"_server_process\", None)\n\n    # Execute initialization steps\n    self._validate_and_setup_paths()\n    self._check_and_prepare_required_files()\n    self._copy_template()\n    self._copy_data_file()\n    self._copy_mallet_files()\n    self._write_config()\n\n    # Mark initialization complete\n    object.__setattr__(self, \"_initialized\", True)\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser._write_config","title":"<code>_write_config() -&gt; None</code>","text":"<p>Write or update config.json in the browser_path.</p> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def _write_config(self) -&gt; None:\n    \"\"\"Write or update config.json in the browser_path.\"\"\"\n    if not self.browser_path:\n        raise ValueError(\"browser_path is not set\")\n    self.browser_path.mkdir(parents=True, exist_ok=True)\n    cfg_path = Path(self.browser_path) / \"config.json\"\n    # Load existing template config.json (if it exists) as the base\n    base_cfg = {}\n    if cfg_path.exists():\n        try:\n            with open(cfg_path, \"r\", encoding=\"utf-8\") as fh:\n                base_cfg = json.load(fh)\n        except Exception:\n            base_cfg = {}\n\n    # Merge user-provided config into base (user overrides base)\n    merged_cfg = dict(base_cfg)\n    if self.config:\n        merged_cfg.update(self.config)\n\n    # Ensure file paths for known data files point to the data/ folder\n    copied = getattr(self, \"copied_files\", {}) or {}\n    for key, rel_path in copied.items():\n        # File path precedence:\n        # 1. User-specified config (self.config) \u2014 should win and be preserved\n        # 2. Copied file paths (this process) \u2014 override template values\n        # 3. Template defaults \u2014 only used if no user or copied path applies.\n        # If the user explicitly provided this key in the config, preserve it.\n        if self.config and key in self.config:\n            continue\n        # Otherwise, set the key to the path of the copied file (overriding template)\n        merged_cfg[key] = rel_path\n\n    # Ensure the application version is present in the config. Preserve user-provided values.\n    if \"application\" not in merged_cfg:\n        merged_cfg[\"application\"] = {}\n    if \"version\" not in merged_cfg[\"application\"]:\n        merged_cfg[\"application\"][\"version\"] = self.BROWSER_VERSION\n\n    # Save merged config back to the instance so callers/other methods can access the full merged config\n    self.config = merged_cfg\n\n    # Save merged config\n    with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(merged_cfg, f, indent=2)\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.config_browser","title":"<code>config_browser(config: dict) -&gt; None</code>","text":"<p>Set the browser configuration after initialization.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary for the DFR Browser 2.</p> required Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def config_browser(self, config: dict) -&gt; None:\n    \"\"\"Set the browser configuration after initialization.\n\n    Args:\n        config (dict): Configuration dictionary for the DFR Browser 2.\n    \"\"\"\n    # Update the config attribute\n    self.config = config\n    # Write the new config to config.json in browser_path\n    self._write_config()\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.serve","title":"<code>serve() -&gt; None</code>","text":"<p>Serve the DFR Browser 2 using the server.py subprocess.</p> <p>This method starts the server as a subprocess and provides instructions for stopping it. Works in both command line and Jupyter notebook environments.</p> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def serve(self) -&gt; None:\n    \"\"\"Serve the DFR Browser 2 using the server.py subprocess.\n\n    This method starts the server as a subprocess and provides instructions\n    for stopping it. Works in both command line and Jupyter notebook environments.\n    \"\"\"\n    if not self.browser_path.exists():\n        raise FileNotFoundError(f\"Browser path does not exist: {self.browser_path}\")\n\n    # Check if port is already in use\n    if not self._is_port_available(self.port):\n        error_msg = (\n            f\"Port {self.port} is already in use.\\n\\n\"\n            f\"To resolve this issue, you can:\\n\"\n            f\"  1. Stop the process using the port:\\n\"\n            f\"     - Check what's using it: lsof -i:{self.port}\\n\"\n            f\"     - Kill the process: kill $(lsof -t -i:{self.port})\\n\"\n            f\"     - Or in Jupyter: !kill $(lsof -t -i:{self.port})\\n\"\n            f\"  2. Use a different port by setting port=&lt;number&gt; when creating the Browser\\n\"\n        )\n        raise RuntimeError(error_msg)\n\n    # Find the server.py script in the same directory as this module\n    server_script = Path(__file__).parent / \"server.py\"\n    if not server_script.exists():\n        raise FileNotFoundError(f\"server.py not found at {server_script}\")\n\n    # Build the command to run the server\n    cmd = [sys.executable, str(server_script), str(self.port)]\n\n    url = f\"http://localhost:{self.port}/\"\n\n    # Detect if we're running in a Jupyter notebook\n    try:\n        # Check for IPython/Jupyter environment\n        get_ipython()  # type: ignore\n        in_notebook = True\n    except NameError:\n        in_notebook = False\n\n    if in_notebook:\n        # Jupyter notebook instructions\n        print(f\"Starting DFR Browser server at {url}\")\n        print(\"\\n\" + \"=\" * 60)\n        print(\"To stop the server in Jupyter, run:\")\n        print(\"  b.stop_server()\")\n        print(\"or\")\n        print(f\"  !kill $(lsof -t -i:{self.port})\")\n        print(\"or restart the kernel\")\n        print(\"=\" * 60 + \"\\n\")\n\n        # Start server as background process\n        process = subprocess.Popen(\n            cmd,\n            cwd=str(self.browser_path),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n\n        # Store the process so we can stop it later\n        self._server_process = process\n\n        # Give it a moment to start\n        import time\n\n        time.sleep(1)\n\n        # Check if process is still running\n        if process.poll() is not None:\n            stdout, stderr = process.communicate()\n            error_msg = f\"Server failed to start.\\n\\nError details:\\n{stderr}\"\n            if \"Address already in use\" in stderr or \"Errno 98\" in stderr:\n                error_msg += (\n                    f\"\\n\\nPort {self.port} appears to be in use.\\n\"\n                    f\"Kill the process with: !kill $(lsof -t -i:{self.port})\"\n                )\n            raise RuntimeError(error_msg)\n\n        # Open browser\n        try:\n            webbrowser.open(url)\n        except Exception:\n            print(f\"Please open {url} in your browser\")\n\n        print(f\"Server running (PID: {process.pid})\")\n\n    else:\n        # Command line - run with subprocess and wait\n        print(f\"Starting DFR Browser server at {url}\")\n        print(\"Press Ctrl+C to stop the server\\n\")\n\n        # Open browser before starting server\n        try:\n            webbrowser.open(url)\n        except Exception:\n            print(f\"Please open {url} in your browser\")\n\n        try:\n            # Run server and wait (blocks until interrupted)\n            subprocess.run(cmd, cwd=str(self.browser_path), check=True)\n        except KeyboardInterrupt:\n            print(\"\\nServer stopped\")\n        except subprocess.CalledProcessError as e:\n            raise RuntimeError(f\"Server error: {e}\")\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.stop_server","title":"<code>stop_server() -&gt; None</code>","text":"<p>Stop the running server process.</p> <p>This method terminates the server subprocess if it's running in background mode (typically in Jupyter notebooks). If the server is not running or was started in blocking mode (command line), this method does nothing.</p> Example <p>b = Browser(...) b.serve()  # Starts server in background</p> Source code in <code>lexos/topic_modeling/dfr_browser2/__init__.py</code> <pre><code>def stop_server(self) -&gt; None:\n    \"\"\"Stop the running server process.\n\n    This method terminates the server subprocess if it's running in background mode\n    (typically in Jupyter notebooks). If the server is not running or was started in\n    blocking mode (command line), this method does nothing.\n\n    Example:\n        &gt;&gt;&gt; b = Browser(...)\n        &gt;&gt;&gt; b.serve()  # Starts server in background\n        &gt;&gt;&gt; # ... do some work ...\n        &gt;&gt;&gt; b.stop_server()  # Stops the server\n    \"\"\"\n    if self._server_process is None:\n        print(\n            \"No server process to stop (server may not have been started or was started in blocking mode)\"\n        )\n        return\n\n    if self._server_process.poll() is not None:\n        # Process already terminated\n        print(\"Server process has already stopped\")\n        self._server_process = None\n        return\n\n    try:\n        # Terminate the process\n        self._server_process.terminate()\n\n        # Wait for process to terminate (with timeout)\n        try:\n            self._server_process.wait(timeout=5)\n            print(f\"Server stopped (PID: {self._server_process.pid})\")\n        except subprocess.TimeoutExpired:\n            # If it doesn't terminate gracefully, force kill\n            self._server_process.kill()\n            self._server_process.wait()\n            print(f\"Server forcefully stopped (PID: {self._server_process.pid})\")\n    except Exception as e:\n        print(f\"Error stopping server: {e}\")\n    finally:\n        self._server_process = None\n</code></pre>"},{"location":"api/topic_modeling/dfr_browser2/#lexos.topic_modeling.dfr_browser2.Browser.stop_server--do-some-work","title":"... do some work ...","text":"<p>b.stop_server()  # Stops the server</p>"},{"location":"api/topic_modeling/mallet/","title":"Mallet","text":"<p>Topic modeling is a statistical method for discovering abstract themes or \"topics\" within a collection of documents. MALLET is a mature tool for topic modeling used widely in the Humanities. It is a Java package that needs to be installed separately from Lexos. The Lexos <code>mallet</code> module provides a straightforward wrapper for running MALLET, managing outputs, and creating visualizations of your topic model.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.MALLET_BINARY_PATH","title":"<code>MALLET_BINARY_PATH = os.getenv('MALLET_BINARY_PATH') or 'mallet'</code>  <code>module-attribute</code>","text":""},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.read_file","title":"<code>read_file(file: Path | str) -&gt; list[str]</code>","text":"<p>Import data from a single text file with one document per line.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path | str</code> <p>A file containing the documents to import.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The training data.</p> Notes <p>This function uses an internal helper <code>_check_format</code> to validate and convert the input data to MALLET format. The helper accepts data with 1-3 tab-separated columns and normalizes it to the format: <code>id\\\\tlabel\\\\ttext</code>.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call\ndef read_file(file: Path | str) -&gt; list[str]:\n    r\"\"\"Import data from a single text file with one document per line.\n\n    Args:\n        file (Path | str): A file containing the documents to import.\n\n    Returns:\n        list[str]: The training data.\n\n    Notes:\n        This function uses an internal helper `_check_format` to validate and convert the input data to MALLET format. The helper accepts data with 1-3 tab-separated columns and normalizes it to the format: `id\\\\tlabel\\\\ttext`.\n    \"\"\"\n\n    # Check the format of the input data and convert to MALLET format if necessary\n    def _check_format(file: Path | str) -&gt; list[str]:\n        \"\"\"Check the format of the input data and convert to MALLET format if necessary.\n\n        Args:\n            file (Path | str): The input file to check.\n\n        Returns:\n            list[str]: The training data in MALLET format.\n        \"\"\"\n        df = pd.read_csv(file, sep=\"\\t\", header=None)\n        if len(df.columns) == 1:\n            df[\"label\"] = \"\"\n            df[\"id\"] = df.index\n            df = df[[\"id\", \"label\", 0]]\n        elif len(df.columns) == 2:\n            df[\"id\"] = df.index\n            df[\"label\"] = \"\"\n            df = df[[\"id\", \"label\", 1]]\n        elif len(df.columns) &gt;= 3:\n            # Merge column 2 with all subsequent columns\n            df[2] = df.iloc[:, 2:].apply(\n                lambda x: \" \".join(x.dropna().astype(str)), axis=1\n            )\n            df = df[[0, 1, 2]]\n        else:\n            raise ValueError(\"Input data must have between 1 and 3 columns.\")\n        df.columns = [\"id\", \"label\", \"text\"]\n        return [\n            f\"{str(row['id']).strip()}\\t{str(row['label']).strip()}\\t{str(row['text']).strip()}\"\n            for row in df.to_dict(orient=\"records\")\n        ]\n\n    # Validate the input\n    if isinstance(file, bool):\n        raise LexosException(\n            \"Invalid input for `file`. Expected a file path (str or Path), not a boolean.\"\n        )\n\n    # Retrieve the data from file\n    try:\n        return _check_format(file)\n    except FileNotFoundError:\n        raise LexosException(f\"File {file} does not exist.\")\n    except IOError:\n        raise LexosException(f\"File {file} could not be read.\")\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.read_dirs","title":"<code>read_dirs(dirs: Path | str | list[Path | str]) -&gt; list[str]</code>","text":"<p>Import a directory or list of directories.</p> <p>Parameters:</p> Name Type Description Default <code>dirs</code> <code>Path | str | list[Path | str]</code> <p>A directory or list of directories to import.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The training data.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call\ndef read_dirs(dirs: Path | str | list[Path | str]) -&gt; list[str]:\n    \"\"\"Import a directory or list of directories.\n\n    Args:\n        dirs (Path | str | list[Path | str]): A directory or list of directories to import.\n\n    Returns:\n        list[str]: The training data.\n    \"\"\"\n    # Ensure dirs is a list\n    dirs = ensure_list(dirs)\n\n    # Retrieve file paths or raise an error if the directory does not exist\n    training_data = []\n    for dir in dirs:\n        # Validate the argument type here to provide a clear error message\n        if isinstance(dir, bool) or not isinstance(dir, (str, Path)):\n            raise LexosException(\n                f\"Invalid directory argument '{dir}'. Expected a directory path (str or Path).\"\n            )\n        if not Path(dir).is_dir():\n            raise LexosException(f\"Directory {dir} does not exist.\")\n        else:\n            # NOTE: Cannot use Path.glob() here because it returns a generator, which disrupts testing.\n            filepaths = glob.glob(f\"{dir}/*.txt\")\n            for path in filepaths:\n                if Path(path).is_file():\n                    with open(path, \"r\", encoding=\"utf-8\") as f:\n                        training_data.append(f.read())\n\n    return training_data\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.import_files","title":"<code>import_files(files: Path | str | list[Path | str]) -&gt; list[str]</code>","text":"<p>Import the text content of a file or list of files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Path | str | list[Path | str]</code> <p>A file or list of files to read.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of file contents.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call\ndef import_files(files: Path | str | list[Path | str]) -&gt; list[str]:\n    \"\"\"Import the text content of a file or list of files.\n\n    Args:\n        files (Path | str | list[Path | str]): A file or list of files to read.\n\n    Returns:\n        list[str]: A list of file contents.\n    \"\"\"\n    if isinstance(files, (Path, str)):\n        files = [files]\n    contents = []\n    for file in files:\n        try:\n            with open(file, \"r\", encoding=\"utf-8\") as fh:\n                contents.append(fh.read())\n        except FileNotFoundError:\n            raise LexosException(f\"File {file} does not exist\")\n        except IOError:\n            raise LexosException(f\"File {file} could not be read\")\n    return contents\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.import_docs","title":"<code>import_docs(docs: list[str | Doc]) -&gt; list[str]</code>","text":"<p>Import a list of document strings or spaCy Docs.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list[str | Doc]</code> <p>List of documents.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of document texts.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\ndef import_docs(docs: list[str | Doc]) -&gt; list[str]:\n    \"\"\"Import a list of document strings or spaCy Docs.\n\n    Args:\n        docs (list[str | Doc]): List of documents.\n\n    Returns:\n        list[str]: List of document texts.\n    \"\"\"\n    training_data = []\n    for doc in docs:\n        if isinstance(doc, Doc):\n            training_data.append(doc.text)\n        else:\n            training_data.append(doc)\n    return training_data\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet","title":"<code>Mallet</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class for training and using MALLET topic models.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>path_to_mallet</code>                 (<code>str</code>)             </li> <li> <code>model_dir</code>                 (<code>Optional[Path | str]</code>)             </li> <li> <code>metadata</code>                 (<code>dict[str, Any]</code>)             </li> </ul> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>class Mallet(BaseModel):\n    \"\"\"A class for training and using MALLET topic models.\"\"\"\n\n    # IMPORTANT: The class initializes with only the `model_directory` key.\n    # Functions will add canonical metadata entries as needed (e.g.\n    # 'path_to_topic_distributions', 'path_to_term_weights', 'path_to_topic_keys').\n    # Legacy synonyms are not used; code reads canonical keys only.\n\n    path_to_mallet: str = MALLET_BINARY_PATH\n    # Accept either a string or a Path for `model_dir` to allow intuitive usage\n    model_dir: Optional[Path | str] = Field(\n        None,\n        description=\"The directory where the model is stored.\",\n    )\n    metadata: dict[str, Any] = Field(\n        {},\n        description=\"A dict containing metadata generated by the class instance.\",\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    # Canonical metadata keys used consistently across methods for common\n    # training outputs. To preserve backward compatibility when loading\n    # metadata produced by older flows, a set of synonyms is still supported\n    # but all internal methods should rely only on the canonical keys below.\n    # The synonyms list is used for migration to canonical form.\n    CANONICAL_DOC_TOPIC_KEY: ClassVar[str] = \"path_to_topic_distributions\"\n    # canonical key names\n    CANONICAL_DOC_TOPIC_KEY: ClassVar[str] = \"path_to_topic_distributions\"\n    CANONICAL_TERM_WEIGHTS_KEY: ClassVar[str] = \"path_to_term_weights\"\n    CANONICAL_TOPIC_KEYS_KEY: ClassVar[str] = \"path_to_topic_keys\"\n    CANONICAL_INFERENCER_KEY: ClassVar[str] = \"path_to_inferencer\"\n\n    def _metadata_get(self, keys: list[str]) -&gt; str | None:\n        \"\"\"Return the first metadata value present among the provided keys or None.\n\n        The method assumes callers pass canonical key names; no synonym\n        translation is performed.\n        \"\"\"\n        # Only accept the canonical key for each category. If a synonym key is\n        # present (legacy metadata), raise an error instructing users to use\n        # the canonical key. This ensures a single canonical name per category.\n        for k in keys:\n            if k in self.metadata and self.metadata[k]:\n                return self.metadata[k]\n        return None\n\n    def _metadata_has(self, keys: list[str]) -&gt; bool:\n        return self._metadata_get(keys) is not None\n\n    # No metadata canonicalization: initialization should only set model_directory\n    # and functions will add canonical keys as necessary.\n\n    def __init__(self, **data) -&gt; None:\n        \"\"\"Initialize the Mallet class.\"\"\"\n        super().__init__(**data)\n        # Assign the model directory if provided via constructor (model_dir)\n        # or via incoming metadata. Create the directory if it is provided\n        # to maintain a predictable filesystem state for later operations.\n        if self.model_dir is None and isinstance(self.metadata, dict):\n            # If the user provided a model_directory via metadata, accept it.\n            if \"model_directory\" in self.metadata:\n                self.model_dir = self.metadata[\"model_directory\"]\n        # If we now have a model_dir, validate and create it\n        if self.model_dir is not None:\n            # Validate that model_dir is not a boolean\n            if isinstance(self.model_dir, bool):\n                raise LexosException(\n                    \"Invalid `model_dir` argument: expected a path (str or Path), not a boolean.\"\n                )\n            # Convert Path objects to str\n            if isinstance(self.model_dir, Path):\n                model_dir_str = str(self.model_dir)\n            else:\n                model_dir_str = self.model_dir\n            # Ensure the model_dir is not a file\n            p = Path(model_dir_str)\n            if p.exists() and p.is_file():\n                raise LexosException(\n                    f\"The specified `model_dir` ({model_dir_str}) exists and is a file, expected a directory.\"\n                )\n            # Create the directory if it does not exist\n            p.mkdir(parents=True, exist_ok=True)\n            # Set metadata `model_directory` if not already set\n            if not isinstance(data.get(\"model_dir\"), type(None)):\n                self.metadata[\"model_directory\"] = model_dir_str\n        # Do not alter user-supplied metadata keys outside this logic.\n\n    @cached_property\n    def distributions(self) -&gt; list[str]:\n        \"\"\"Get the topic distributions of the model.\"\"\"\n        # Resolve the distribution path using canonical metadata resolution\n        distro_path = self._metadata_get([self.CANONICAL_DOC_TOPIC_KEY])\n        if distro_path is None:\n            raise LexosException(\n                \"No topic distributions have been set. Please designate a path for `path_to_topic_distributions` when you train your topic model.\"\n            )\n\n        topic_distributions = []\n        with open(distro_path, \"r\") as f:\n            for line in f.readlines():\n                # Skip blank lines\n                if not line.strip():\n                    continue\n                if line.split()[0] != \"#doc\":\n                    # Try tab-delimited format first: id \\t docid \\t val1 \\t val2 ...\n                    parts = line.strip().split(\"\\t\")\n                    if len(parts) &gt;= 3:\n                        raw_dist = parts[2:]\n                        # If the distribution token contains topic:prob pairs as a single\n                        # token (e.g. '0:0.1 1:0.9'), parse it out into a dense vector\n                        if len(raw_dist) == 1 and \":\" in raw_dist[0]:\n                            token = raw_dist[0]\n                            pairs = re.split(r\"\\s+\", token)\n                            if all(\":\" in p for p in pairs):\n                                tp_map = {}\n                                max_topic = -1\n                                for p in pairs:\n                                    try:\n                                        t, prob = p.split(\":\")\n                                        t_i = int(t)\n                                        prob_f = float(prob)\n                                        tp_map[t_i] = prob_f\n                                        if t_i &gt; max_topic:\n                                            max_topic = t_i\n                                    except Exception:\n                                        raise LexosException(\n                                            f\"Topic:prob pair malformed in: {p}\"\n                                        )\n                                raw_dist = [\n                                    str(tp_map.get(i, 0.0))\n                                    for i in range(max_topic + 1)\n                                ]\n                    else:\n                        # If no tabs, try whitespace-separated: id docid val1 val2 ...\n                        parts_ws = re.split(r\"\\s+\", line.strip())\n                        if len(parts_ws) &gt;= 3:\n                            raw_dist = parts_ws[2:]\n                        else:\n                            # If there is a single token containing topic:prob pairs like\n                            # '0:0.1 1:0.2 2:0.7', split and parse those\n                            token = line.strip().split()[-1]\n                            pairs = re.split(r\"\\s+\", token)\n                            if all(\":\" in p for p in pairs):\n                                # rebuild distribution as a full dense vector if possible\n                                # create a dict mapping topic idx to prob\n                                tp_map = {}\n                                max_topic = -1\n                                for p in pairs:\n                                    try:\n                                        t, prob = p.split(\":\")\n                                        t_i = int(t)\n                                        prob_f = float(prob)\n                                        tp_map[t_i] = prob_f\n                                        if t_i &gt; max_topic:\n                                            max_topic = t_i\n                                    except Exception:\n                                        raise LexosException(\n                                            f\"Topic:prob pair malformed in: {p}\"\n                                        )\n                                # convert mapping to list of floats (dense vector)\n                                raw_dist = [\n                                    str(tp_map.get(i, 0.0))\n                                    for i in range(max_topic + 1)\n                                ]\n                            else:\n                                raise LexosException(\n                                    f\"The line '{line.strip()}' in the topic distributions file is not formatted correctly.\"\n                                )\n                    try:\n                        distribution = [float(p) for p in raw_dist]\n                    except Exception:\n                        raise LexosException(\n                            f\"Unable to parse distribution values from line: '{line.strip()}'\"\n                        )\n                    topic_distributions.append(distribution)\n\n        return topic_distributions\n\n    @property\n    def num_docs(self) -&gt; int:\n        \"\"\"Get the number of docs in the model.\"\"\"\n        if \"num_docs\" in self.metadata:\n            return self.metadata[\"num_docs\"]\n        else:\n            return 0\n\n    @property\n    def mean_num_tokens(self) -&gt; int:\n        \"\"\"Get the mean number of tokens per document in the model.\"\"\"\n        if \"mean_num_tokens\" in self.metadata:\n            v = self.metadata[\"mean_num_tokens\"]\n            try:\n                return v.item()\n            except Exception:\n                return int(v)\n        else:\n            return 0\n\n    @property\n    def model_directory(self) -&gt; str:\n        \"\"\"Return the model_directory from metadata or raise LexosException if missing.\"\"\"\n        if isinstance(self.metadata, dict) and \"model_directory\" in self.metadata:\n            return self.metadata[\"model_directory\"]\n        raise LexosException(\n            \"No model directory has been set; provide one or set 'model_directory' in metadata.\"\n        )\n\n    @cached_property\n    def topic_keys(self) -&gt; list[list[str]]:\n        \"\"\"Get the keys of the model.\n\n        Returns:\n            list[list[str]]: A list of topics where each topic is a sublist containing the topic index, topic weight, and a space-separated list of keywords.\n        \"\"\"\n        topic_keys_path = self._metadata_get([self.CANONICAL_TOPIC_KEYS_KEY])\n        if not topic_keys_path:\n            raise LexosException(\n                f\"No topic keys have been set. Please designate a path for `{self.CANONICAL_TOPIC_KEYS_KEY}` when you train your topic model.\"\n            )\n        with open(self.metadata[self.CANONICAL_TOPIC_KEYS_KEY], \"r\") as f:\n            return [line.strip().split(\"\\t\") for line in f.readlines()]\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Get the vocabulary size of documents in the model.\"\"\"\n        if \"vocab_size\" in self.metadata:\n            return self.metadata[\"vocab_size\"]\n        else:\n            return 0\n\n    def _import_training_data(\n        self,\n        training_data: list[str],\n        path_to_training_data: Optional[str] = None,\n        keep_sequence: bool = True,\n        remove_stopwords: bool = True,\n        preserve_case: bool = True,\n        use_pipe_from: Optional[str] = None,\n        training_ids: Optional[list[int]] = None,\n    ) -&gt; None:\n        \"\"\"Import training data from a list of documents.\n\n        Args:\n            training_data (list[str]): A list of documents to import.\n            keep_sequence (bool): Whether to keep the word sequence in the documents.\n            remove_stopwords (bool): Whether to remove stopwords from the documents.\n            preserve_case (bool): Whether to preserve the case of the documents.\n            use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n            training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n        \"\"\"\n        # Save the training data file\n        path_to_training_data = (\n            path_to_training_data\n            if path_to_training_data is not None\n            else os.path.join(self.model_dir, \"training_data.txt\")\n        )\n        path_to_formatted_training_data = os.path.join(\n            self.model_dir, \"training_data.mallet\"\n        )\n        training_data_file = open(path_to_training_data, \"w\", encoding=\"utf-8\")\n        for i, doc in enumerate(training_data):\n            # Remove newlines and carriage returns from the document\n            doc = re.sub(\"[\\r\\n]+\", \" \", doc).strip()\n            if training_ids:\n                training_data_file.write(f\"{training_ids[i]}\\tno_label\\t{doc}\\n\")\n            else:\n                training_data_file.write(f\"{i}\\tno_label\\t {doc}\\n\")\n        training_data_file.close()\n        self.metadata[\"path_to_training_data\"] = path_to_training_data\n        self.metadata[\"path_to_formatted_training_data\"] = (\n            path_to_formatted_training_data\n        )\n        self.metadata[\"num_docs\"] = len(training_data)\n        # WARNING: Tokenisation relies on whitespace, so it may not be accurate for all languages\n        self.metadata[\"mean_num_tokens\"] = np.mean(\n            [len(doc.split()) for doc in training_data]\n        ).item()\n        self.metadata[\"vocab_size\"] = len(\n            list(set([token for doc in training_data for token in doc.split()]))\n        )\n\n        # Build and execute the command to format the training data for MALLET\n        cmd = f\"{self.path_to_mallet or 'mallet'} import-file --input {path_to_training_data} --output {path_to_formatted_training_data}\"\n        if keep_sequence:\n            cmd += \" --keep-sequence\"\n        if remove_stopwords:\n            cmd += \" --remove-stopwords\"\n        if preserve_case:\n            cmd += \" --preserve-case\"\n        if use_pipe_from:\n            cmd += f\" --use-pipe-from {use_pipe_from}\"\n        msg.info(cmd)\n        os.system(cmd)\n\n    @validate_call(config=model_config)\n    def import_data(\n        self,\n        training_data: list[str],\n        path_to_training_data: str = None,\n        keep_sequence: bool = True,\n        preserve_case: bool = True,\n        remove_stopwords: bool = True,\n        use_pipe_from: Optional[str] = None,\n        training_ids: Optional[list[int]] = None,\n    ) -&gt; None:\n        \"\"\"Convenience wrapper to import a list of documents and format them for MALLET.\n\n        Args:\n            training_data (list[str]): List of document texts.\n            path_to_training_data (str): Path to write raw training text file. If None, will default to model directory.\n            keep_sequence (bool): Keep token sequence.\n            preserve_case (bool): Preserve case.\n            remove_stopwords (bool): Remove stopwords.\n            use_pipe_from (Optional[str]): Pipe filename for MALLET import.\n            training_ids (Optional[list[int]]): Optional training IDs mapping.\n        \"\"\"\n        # Validate training_data is a list of strings\n        if isinstance(training_data, bool) or not isinstance(training_data, list):\n            raise LexosException(\n                \"Invalid `training_data` argument: expected a list of document strings.\"\n            )\n        for doc in training_data:\n            if isinstance(doc, bool) or not isinstance(doc, str):\n                raise LexosException(\n                    \"Invalid `training_data` element: expected document text (str) for each item.\"\n                )\n\n        # Determine output paths if not provided\n        if not path_to_training_data:\n            model_base = self.model_dir if self.model_dir else os.getcwd()\n            path_to_training_data = os.path.join(model_base, \"training_data.txt\")\n        self._import_training_data(\n            training_data,\n            path_to_training_data,\n            keep_sequence,\n            remove_stopwords,\n            preserve_case,\n            use_pipe_from,\n            training_ids,\n        )\n\n    def _setup_wordcloud(\n        self, round_mask, max_terms, **kwargs: dict[str, Any]\n    ) -&gt; WordCloud:\n        \"\"\"Set up the word cloud object.\n\n        Args:\n            round_mask (bool): Whether to use a round mask for the word cloud.\n            max_terms (int): The maximum number of keywords to display.\n            **kwargs (dict[str, Any])): Additional keyword arguments for the WordCloud object.\n\n        Returns:\n            WordCloud: A configured WordCloud object.\n        \"\"\"\n        # Define a mask to make the word cloud round (just some eye candy)\n        if round_mask:\n            x, y = np.ogrid[:300, :300]\n            mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; 130**2\n            mask = 255 * mask.astype(int)\n        else:\n            mask = None\n\n        # Configure the word cloud object\n        options = {\n            \"background_color\": \"white\",\n            \"mask\": mask,\n            \"contour_width\": 0.1,\n            \"contour_color\": \"white\",\n            \"max_words\": max_terms,\n            \"min_font_size\": 10,\n            \"max_font_size\": 150,\n            \"random_state\": 42,\n            \"colormap\": \"Dark2\",\n        }\n        for k, v in kwargs.items():\n            options[k] = v\n\n        return WordCloud(**options)\n\n    def _track_progress(\n        self, mallet_cmd: str, num_iterations: int, verbose: bool = True\n    ) -&gt; None:\n        \"\"\"Track the progress of the modeling.\n\n        Args:\n            mallet_cmd (str): The MALLET command to run.\n            num_iterations (int): The number of iterations for the model.\n            verbose (bool): Whether to print the MALLET output.\n\n        Notes:\n            - Prints MALLET output and updates the progress bar in 10% increments.\n        \"\"\"\n        console = Console()\n        # NOTE: This is a hack to make Jupyter notebooks in VS Code display all lines\n        # in the same cell. It may cause undesirable results in other environments and\n        # needs further testing. See https://github.com/Textualize/rich/issues/3483.\n        if verbose:\n            console.is_jupyter = False\n\n        # Create a progress display with rich\n        with Progress(\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            TaskProgressColumn(),\n            TimeElapsedColumn(),\n        ) as progress:\n            # Create a task with a total of 100 (percentage)\n            task = progress.add_task(\"[blue]Training model...\", total=100)\n\n            # Run the MALLET command\n            p = subprocess.Popen(\n                mallet_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n            )\n\n            # Regex to match progress information\n            prog = re.compile(r\"\\&lt;([^\\)]+)\\&gt;\")\n\n            # Track the last reported progress percentage to avoid duplicate updates\n            last_progress = -1\n\n            # Process the output line by line\n            while p.poll() is None:\n                line = p.stdout.readline().decode()\n                if verbose:\n                    # Print MALLET output without disrupting progress\n                    console.print(line, end=\"\")\n\n                # Keep track of modeling progress\n                try:\n                    # A float indicating the percentage, which is output by MALLET\n                    this_iter = float(prog.match(line).groups()[0])\n                    current_progress = int(100.0 * this_iter / num_iterations)\n\n                    # Only update on 10% multiples and avoid duplicate updates\n                    if current_progress % 10 == 0 and current_progress &gt; last_progress:\n                        # Update to the current progress percentage\n                        progress.update(task, completed=this_iter)\n                        last_progress = current_progress\n                    if current_progress == 100:\n                        progress.update(\n                            task, description=\"[green]Complete\", completed=100\n                        )\n                except AttributeError:  # Not every line will match.\n                    pass\n\n    @validate_call(config=model_config)\n    def get_keys(\n        self,\n        num_topics: int = None,\n        topics: list[int] = None,\n        num_keys: int = 10,\n        as_df: bool = False,\n    ) -&gt; str | Styler:\n        \"\"\"Get a string representation of the topic keys of the model.\n\n        Args:\n            num_topics (int): The number of topics to get keys for. If None, get keys for all topics.\n            topics (list[int]): A list of topic indices to get keys for. If None, get keys for all topics.\n            num_keys (int): The number of keys to output for each topic.\n            as_df (bool): Whether to return the result as a pandas DataFrame instead of a string.\n\n        Returns:\n            str | Styler: A string or DataFrame representation of the topic keys. The DataFrame is styled for presentation in a Jupyter notebook to prevent clipping of the keywords in a Jupyter notebook. If you need an actual `DataFrame` object, reference `df.data`.\n        \"\"\"\n        num_available_topics = len(self.topic_keys)\n        if num_topics and not topics:\n            if num_topics &gt; num_available_topics:\n                raise IndexError(\n                    f\"Requested num_topics={num_topics}, but only {num_available_topics} topics are available.\"\n                )\n            topic_keys = self.topic_keys[:num_topics]\n        elif topics:\n            # Validate all indices\n            for i in topics:\n                if i &lt; 0 or i &gt;= num_available_topics:\n                    raise IndexError(\n                        f\"Topic index {i} is out of range. Valid indices are 0 to {num_available_topics - 1}.\"\n                    )\n            topic_keys = [self.topic_keys[i] for i in topics]\n        else:\n            topic_keys = self.topic_keys\n        output = \"\"\n        for topic in topic_keys:\n            keywords = \" \".join(topic[2].split()[:num_keys])\n            output += f\"Topic {topic[0]}\\t{topic[1]}\\t{keywords}\\n\"\n        if as_df:\n            data = []\n            for topic in topic_keys:\n                keywords = \" \".join(topic[2].split()[:num_keys])\n                data.append(\n                    {\"Topic\": topic[0], \"Label\": topic[1], \"Keywords\": keywords}\n                )\n            df = pd.DataFrame(data)\n            show_index = True  # or False\n            offset = 2 if show_index else 1\n            nth = df.columns.get_loc(\"Keywords\") + offset\n\n            css = [\n                # header cell of Keywords column\n                {\n                    \"selector\": f\"thead th:nth-child({nth})\",\n                    \"props\": [(\"text-align\", \"left\")],\n                },\n                # the column cells\n                {\n                    \"selector\": f\"td.col{df.columns.get_loc('Keywords')}\",\n                    \"props\": [(\"text-align\", \"left\")],\n                },\n            ]\n\n            styled_df = df.style.set_table_styles(css).set_properties(\n                subset=[\"Keywords\"], **{\"text-align\": \"left\"}\n            )\n            return styled_df\n        return output\n\n    @validate_call(config=model_config)\n    def get_top_docs(\n        self, topic=0, n=10, metadata: pd.DataFrame = None, as_str: bool = False\n    ) -&gt; pd.DataFrame | str:\n        \"\"\"Get the top n documents for a given topic.\n\n        Args:\n            topic (int): Topic number.\n            n (int): Number of top documents to return.\n            metadata (pd.DataFrame): Dataframe with the metadata in the same order as the training data (optional).\n            as_str (bool): Whether to return the result as a string instead of a dataframe.\n\n        Returns:\n            A pd.DataFrame or str: A dataframe with the top n documents for the given topic, or a string representation of the dataframe.\n\n        Notes:\n            - The metadata must be in the same order as the training data.\n            - The document text will get ellided by the maximum width of a pandas column. An easy way to see the full text is to set `as_str=True` and output the result with a print statement. You can also use the pandas API to extract the information with something like `top_docs.Document.tolist()`.\n        \"\"\"\n        # Ensure that the path to doc-topic distributions exists (resolved via canonical keys)\n        if not self._metadata_has([self.CANONICAL_DOC_TOPIC_KEY]):\n            raise LexosException(\n                \"No topic distributions have been set. Please designate a path to the doc-topic distributions (e.g. `path_to_topic_distributions`) when you train your topic model.\"\n            )\n\n        if \"path_to_training_data\" not in self.metadata:\n            raise LexosException(\n                \"No training data has been set. Please designate a path for `path_to_training_data` when you train your topic model.\"\n            )\n\n        # Read the training data file\n        with open(self.metadata[\"path_to_training_data\"], \"r\", encoding=\"utf-8\") as f:\n            training_data = f.readlines()\n        training_data = [\n            line.split(\"\\t\")[2].strip() for line in training_data\n        ]  # Skip the id and label\n\n        # Validate topic index against model's known number of topics (0-based)\n        try:\n            topic = int(topic)\n        except Exception:\n            raise ValueError(\"Topic index must be an integer\")\n\n        num_topics = None\n        # Try the reliable metadata if present\n        if \"num_topics\" in self.metadata:\n            try:\n                num_topics = int(self.metadata[\"num_topics\"])\n            except Exception:\n                num_topics = None\n        # Fall back to topic_keys if available\n        if num_topics is None:\n            try:\n                num_topics = len(self.topic_keys)\n            except Exception:\n                num_topics = None\n        # As a last resort, infer from distributions\n        distribution_len = None\n        if len(self.distributions) &gt; 0:\n            # ensure all distributions have the same length; otherwise raise\n            lengths = set(len(d) for d in self.distributions)\n            if len(lengths) &gt; 1:\n                raise LexosException(\n                    \"Topic distribution lengths are inconsistent across documents; check `path_to_topic_distributions` format.\"\n                )\n            distribution_len = next(iter(lengths))\n            if num_topics is None:\n                num_topics = distribution_len\n        if num_topics is None:\n            raise LexosException(\n                \"Model does not have topic information yet. Train or load a model first.\"\n            )\n        # If we have both a metadata num_topics and inferred distribution length, they should match.\n        if (\n            distribution_len is not None\n            and num_topics is not None\n            and distribution_len != num_topics\n        ):\n            raise LexosException(\n                f\"Mismatch between declared number of topics ({num_topics}) and distribution vector length ({distribution_len}). Check your training outputs.\"\n            )\n\n        if not (0 &lt;= topic &lt; num_topics):\n            raise ValueError(\n                f\"Invalid topic index {topic}. Valid topic indices are 0..{num_topics - 1} (0-based).\"\n            )\n\n        # Combine the distribution and training data, then convert to a dataframe\n        distribution_data = [\n            (_distribution[topic], _document)\n            for _distribution, _document in zip(self.distributions, training_data)\n        ]\n        df = pd.DataFrame(distribution_data, columns=[\"Distribution\", \"Document\"])\n        df.index.name = \"Doc ID\"\n\n        # If metadata is provided, concatenate it to the dataframe\n        if metadata is not None:\n            df = pd.concat([df, metadata], axis=1)\n\n        # Sort the dataframe by distribution and return the top n documents\n        if as_str:\n            return (\n                df.sort_values(by=\"Distribution\", ascending=False).head(n).to_string()\n            )\n        return df.sort_values(by=\"Distribution\", ascending=False).head(n)\n\n    @validate_call(config=model_config)\n    def get_topic_term_probabilities(\n        self, topics: Optional[int | list[int]] = None, n: int = 5, as_df: bool = False\n    ) -&gt; str | pd.DataFrame:\n        \"\"\"Get a string representation of the term distribution for a given topic.\n\n        Args:\n            topics (int | list[int]): Topic number. If None, get the probabilities for all topics.\n            n (int): The number of keywords to display.\n            as_df (bool): Whether to display the result as a string or a pandas DataFrame.\n\n        Returns:\n            str: A string representation of the term distribution for the given topic.\n        \"\"\"\n        if isinstance(topics, int):\n            topics = [topics]\n        topic_term_probability_dict = self.load_topic_term_distributions()\n        # Build either a string (legacy behavior) or a DataFrame with columns\n        # Topic | Term | Probability based on the `as_df` parameter.\n        if as_df:\n            rows = []\n            for _topic, _term_probability_dict in topic_term_probability_dict.items():\n                if topics is None or _topic in topics:\n                    for _term, _probability in sorted(\n                        _term_probability_dict.items(), key=lambda x: x[1], reverse=True\n                    )[:n]:\n                        rows.append(\n                            {\n                                \"Topic\": _topic,\n                                \"Term\": _term,\n                                \"Probability\": _probability,\n                            }\n                        )\n            df = pd.DataFrame(rows)\n            return df\n        result = \"\"\n        for _topic, _term_probability_dict in topic_term_probability_dict.items():\n            if topics is None or _topic in topics:\n                result += f\"Topic {_topic}\\n\"\n                for _term, _probability in sorted(\n                    _term_probability_dict.items(), key=lambda x: x[1], reverse=True\n                )[:n]:\n                    result += f\"\\t{_term}: {_probability}\\n\"\n                result += \"\\n\"\n        return result\n\n    @validate_call(config=model_config)\n    def import_dir(\n        self,\n        data_source: str | list[str],\n        keep_sequence: bool = True,\n        preserve_case: bool = True,\n        remove_stopwords: bool = True,\n        use_pipe_from: Optional[str] = None,\n        training_ids: Optional[list[int]] = None,\n    ) -&gt; None:\n        \"\"\"Read training data from directories and save formatted training data file.\n\n        Args:\n            data_source (str | list[str]): A directory or list of directories to import.\n            keep_sequence (bool): Whether to keep the word sequence in the documents.\n            preserve_case (bool): Whether to preserve the case of the documents.\n            remove_stopwords (bool): Whether to remove stopwords from the documents.\n            use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n            training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n        \"\"\"\n        # Explicitly validate data_source to reject booleans\n        if isinstance(data_source, bool):\n            raise LexosException(\n                \"Invalid `data_source` argument: expected a directory path or list of paths, not a boolean.\"\n            )\n        training_data = read_dirs(ensure_list(data_source))\n        self._import_training_data(\n            training_data,\n            path_to_training_data=None,\n            keep_sequence=keep_sequence,\n            remove_stopwords=remove_stopwords,\n            preserve_case=preserve_case,\n            use_pipe_from=use_pipe_from,\n            training_ids=training_ids,\n        )\n\n    @validate_call(config=model_config)\n    def import_docs(\n        self,\n        data_source: str | list[str],\n        keep_sequence: bool = True,\n        preserve_case: bool = True,\n        remove_stopwords: bool = True,\n        use_pipe_from: Optional[str] = None,\n        training_ids: Optional[list[int]] = None,\n    ) -&gt; None:\n        \"\"\"Read training data from docs and save formatted training data file.\n\n        Args:\n            data_source (str | list[str]): A doc or list of docs to import.\n            keep_sequence (bool): Whether to keep the word sequence in the documents.\n            preserve_case (bool): Whether to preserve the case of the documents.\n            remove_stopwords (bool): Whether to remove stopwords from the documents.\n            use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n            training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n        \"\"\"\n        if isinstance(data_source, bool):\n            raise LexosException(\n                \"Invalid `data_source` argument: expected a doc or list of docs, not a boolean.\"\n            )\n        docs = ensure_list(data_source)\n        training_data = [\n            f\"{i}\\t\\t{doc.text}\" if isinstance(doc, Doc) else f\"{i}\\t\\t{doc}\"\n            for i, doc in enumerate(docs)\n        ]\n        self._import_training_data(\n            training_data,\n            path_to_training_data=None,\n            keep_sequence=keep_sequence,\n            remove_stopwords=remove_stopwords,\n            preserve_case=preserve_case,\n            use_pipe_from=use_pipe_from,\n            training_ids=training_ids,\n        )\n\n    @validate_call(config=model_config)\n    def import_file(\n        self,\n        data_source: str | list[str],\n        keep_sequence: bool = True,\n        preserve_case: bool = True,\n        remove_stopwords: bool = True,\n        use_pipe_from: Optional[str] = None,\n        training_ids: Optional[list[int]] = None,\n    ) -&gt; None:\n        \"\"\"Read training data from file and save formatted training data file.\n\n        Args:\n            data_source (str | list[str]): A file or list of files to import.\n            keep_sequence (bool): Whether to keep the word sequence in the documents.\n            preserve_case (bool): Whether to preserve the case of the documents.\n            remove_stopwords (bool): Whether to remove stopwords from the documents.\n            use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n            training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n        \"\"\"\n        if isinstance(data_source, bool):\n            raise LexosException(\n                \"Invalid `data_source` argument: expected a file path or list of paths, not a boolean.\"\n            )\n        training_data = read_file(ensure_list(data_source))\n        self._import_training_data(\n            training_data,\n            path_to_training_data=None,\n            keep_sequence=keep_sequence,\n            remove_stopwords=remove_stopwords,\n            preserve_case=preserve_case,\n            use_pipe_from=use_pipe_from,\n            training_ids=training_ids,\n        )\n\n    def load_topic_term_distributions(self) -&gt; dict[str, float]:\n        \"\"\"Load the topic-term distributions from a file.\n\n        Returns:\n            dict[str, float]: A dictionary of all topic-term distributions.\n        \"\"\"\n        # Ensure that the path to a term weights file has been set.\n        term_weight_path = self._metadata_get([self.CANONICAL_TERM_WEIGHTS_KEY])\n        if term_weight_path is None:\n            raise LexosException(\n                f\"No term weights have been set. Please designate a path to the term weights file (e.g. `{self.CANONICAL_TERM_WEIGHTS_KEY}`) when you train your topic model.\"\n            )\n        topic_term_weight_dict = defaultdict(lambda: defaultdict(float))\n        topic_sum_dict = defaultdict(float)\n        try:\n            with open(term_weight_path, \"r\") as f:\n                for _line in f:\n                    if not _line.strip():\n                        continue\n                    parts = _line.strip().split(\"\\t\")\n                    if len(parts) != 3:\n                        # Malformed line\n                        raise ValueError(\n                            f\"Malformed line in term weights file: '{_line.strip()}'\"\n                        )\n                    _topic, _term, _weight = parts\n                    try:\n                        weight_f = float(_weight)\n                    except Exception:\n                        raise ValueError(\n                            f\"Invalid weight value '{_weight}' in line: '{_line.strip()}'\"\n                        )\n                    topic_term_weight_dict[_topic][_term] = weight_f\n                    topic_sum_dict[_topic] += weight_f\n        except FileNotFoundError:\n            # Surface file not found as filesystem error\n            raise\n\n        topic_term_probability_dict = defaultdict(lambda: defaultdict(float))\n        for _topic, _term_weight_dict in topic_term_weight_dict.items():\n            for _term, _weight in _term_weight_dict.items():\n                topic_term_probability_dict[int(_topic)][_term] = (\n                    _weight / topic_sum_dict[_topic]\n                )\n\n        return topic_term_probability_dict\n\n    @validate_call(config=model_config)\n    def plot_categories_by_topic_boxplots(\n        self,\n        categories: list[str],\n        topics: Optional[int | list[int]] = None,\n        output_path: Optional[str] = None,\n        target_labels: Optional[list[str]] = None,\n        num_keys: int = 5,\n        figsize: Optional[tuple[int, int]] = (6, 6),\n        font_scale: Optional[float] = 1.2,\n        color: Optional[ColorType] = \"lightblue\",\n        show: Optional[bool] = True,\n        title: Optional[str] = None,\n        overlay: Optional[str] = \"strip\",\n        overlay_kws: Optional[dict[str, Any]] = None,\n        topic_distributions: Optional[list[list[float]]] = None,\n    ) -&gt; Figure | list[Figure]:\n        \"\"\"Plot boxplots showing the distribution of topic probabilities for each category.\n\n        Args:\n            categories (list[str]): The labels to use for the categories.\n            topics (int | list[int]): The index of the topic to plot.\n            output_path (str): The path to save the figure.\n            target_labels (list[str]): Unique labels for categories to classify.\n            num_keys (int): The number of keywords to display.\n            figsize: (Optional[tuple[int, int]]): The dimensions of the figure.\n            font_scale (Optional[float]): The font scale for the figure.\n            color (Optional[ColorType]): The color to use for the heatmap boxes. A matplotlib ColorType name or object.\n            show (Optional[bool]): Whether to show the figure.\n            title (Optional[str]): Optional figure title. If not supplied, each plot will use a default title of\n                `Topic {topic}: {keywords}`.\n            overlay (Optional[str]): How to display the individual points overlaid on each boxplot. Supported\n                values are 'strip' (default), 'swarm', or 'none'.\n            overlay_kws (Optional[dict]): Keyword arguments passed to the chosen overlay plotting method\n                (`seaborn.stripplot` or `seaborn.swarmplot`).\n\n        Returns:\n            Figure | list[Figure]: The boxplot showing the topic associations by category.\n        \"\"\"\n        # Load topic_keys\n        topic_keys = self.topic_keys\n\n        # Ensure that topics is a list\n        if topics is None:\n            topics = list(range(len(topic_keys)))\n        elif isinstance(topics, int):\n            topics = [topics]\n\n        # Ensure there are topic_labels\n        if not target_labels:\n            target_labels = list(set(categories))\n\n        # Combine the labels and distributions into a dataframe.\n        figs = []\n        import os\n\n        # Use user-provided topic_distributions if given, else default to self.distributions\n        distributions = (\n            topic_distributions\n            if topic_distributions is not None\n            else self.distributions\n        )\n\n        for topic in topics:\n            keywords = \" \".join(topic_keys[topic][2].split()[:num_keys])\n\n            dicts_to_plot = []\n            for _label, _distribution in zip(categories, distributions):\n                if not target_labels or _label in target_labels:\n                    dicts_to_plot.append(\n                        {\n                            \"Probability\": float(_distribution[topic]),\n                            \"Category\": _label,\n                            \"Topic\": keywords,\n                        }\n                    )\n            df_to_plot = pd.DataFrame(dicts_to_plot)\n\n            # Validate overlay option\n            if overlay not in (\"strip\", \"swarm\", \"none\", None):\n                raise LexosException(\n                    \"Invalid `overlay` argument: expected 'strip', 'swarm', or 'none'.\"\n                )\n\n            # Show the final plot\n            sns.set_theme(style=\"ticks\", font_scale=font_scale)\n            # Create a figure/axes so we can overlay points for small datasets\n            if figsize:\n                fig, ax = plt.subplots(figsize=figsize)\n            else:\n                fig, ax = plt.subplots()\n            sns.boxplot(\n                data=df_to_plot,\n                x=\"Category\",\n                y=\"Probability\",\n                color=color,\n                ax=ax,\n                showmeans=True,\n            )\n            # Overlay data points so users can see the raw values when there are\n            # too few observations to form a full box\n            overlay_kws = dict(overlay_kws or {})\n            try:\n                if overlay == \"strip\" or overlay is None:\n                    sns.stripplot(\n                        data=df_to_plot,\n                        x=\"Category\",\n                        y=\"Probability\",\n                        color=overlay_kws.pop(\"color\", \"black\"),\n                        size=overlay_kws.pop(\"size\", 4),\n                        jitter=overlay_kws.pop(\"jitter\", True),\n                        ax=ax,\n                        **overlay_kws,\n                    )\n                elif overlay == \"swarm\":\n                    sns.swarmplot(\n                        data=df_to_plot,\n                        x=\"Category\",\n                        y=\"Probability\",\n                        color=overlay_kws.pop(\"color\", \"black\"),\n                        size=overlay_kws.pop(\"size\", 4),\n                        ax=ax,\n                        **overlay_kws,\n                    )\n                # if overlay == 'none', do nothing\n            except Exception:\n                # Overlay plotting is optional; ignore any backend failures\n                pass\n            sns.despine()\n            plt.xticks(rotation=45, ha=\"right\")\n            # Set either the provided title or a sensible default including topic index and top keys\n            if title is None:\n                ax.set_title(f\"Topic {topic}: {keywords}\")\n            else:\n                # Use a figure-level title to avoid per-subplot clobbering\n                fig.suptitle(title)\n            plt.tight_layout()\n            # Save each plot to a unique file if output_path is set\n            if output_path:\n                base, ext = os.path.splitext(output_path)\n                save_path = f\"{base}_topic{topic}{ext}\"\n                fig.savefig(save_path)\n            figs.append(fig)\n            if show:\n                plt.show()\n            plt.close(fig)\n        if show:\n            return None\n        # If this function only generated a single figure, return it.\n        if len(figs) == 1:\n            return figs[0]\n        return figs\n\n    @validate_call(config=model_config)\n    def plot_categories_by_topics_heatmap(\n        self,\n        categories: list[str],\n        output_path: Path | str = None,\n        target_labels: list[str] = None,\n        num_keys: int = 5,\n        figsize: Optional[tuple[int, int]] = None,\n        font_scale: Optional[float] = 1.2,\n        cmap: Optional[ColorType] = sns.cm.rocket_r,\n        show: Optional[bool] = True,\n        title: Optional[str] = None,\n        topic_distributions: Optional[list[list[float]]] = None,\n    ) -&gt; Figure:\n        \"\"\"Plot heatmap showing topics by category.\n\n        Args:\n            categories (list[str]): The categories to use to classify topics.\n            output_path (Path | str): The path to save the figure.\n            target_labels (list[str]): Unique labels for categories to classify.\n            num_keys (int): The number of keywords to display.\n            figsize: (Optional[tuple[int, int]]): The dimensions of the figure.\n            font_scale (Optional[float]): The font scale for the figure.\n            cmap (Optional[ColorType]): The colormap to use for the heatmap. A matplotlib colormap name or object, or list of colors.\n            show (Optional[bool]): Whether to show the figure.\n            title (Optional[str]): Optional title for the figure. If not supplied, defaults to \"Topics by Category (N=x)\".\n\n        Returns:\n            Figure: The heatmap showing the topic associations by category.\n        \"\"\"\n        # Load topic_keys\n        topic_keys = self.topic_keys\n\n        # Use user-provided topic_distributions if given, else default to self.distributions\n        distributions = (\n            topic_distributions\n            if topic_distributions is not None\n            else self.distributions\n        )\n\n        dicts_to_plot = []\n        for _category_label, _distribution in zip(categories, distributions):\n            if not target_labels or _category_label in target_labels:\n                for _topic, _probability in enumerate(_distribution):\n                    keywords = \" \".join(topic_keys[_topic][2].split()[:num_keys])\n                    if num_keys:\n                        if keywords:\n                            _topic_label = f\"Topic {_topic}: {keywords}\"\n                        else:\n                            _topic_label = f\"Topic {_topic}\"\n                    else:\n                        _topic_label = f\"Topic {_topic}\"\n                    dicts_to_plot.append(\n                        {\n                            \"Probability\": float(_probability),\n                            \"Category\": _category_label,\n                            \"Topic\": _topic_label,\n                        }\n                    )\n\n        # Create a dataframe, format it for the heatmap function, and normalize the columns.\n        df_to_plot = pd.DataFrame(dicts_to_plot)\n        df_wide = df_to_plot.pivot_table(\n            index=\"Category\", columns=\"Topic\", values=\"Probability\"\n        )\n        df_norm_col = (df_wide - df_wide.mean()) / df_wide.std()\n\n        # Ensure the columns are ordered by numeric topic index where available (natural sort)\n        def _topic_key(col):\n            # Match 'Topic &lt;num&gt;' possibly followed by ': ...'\n            try:\n                m = re.match(r\"Topic\\s+(\\d+)\", str(col))\n                if m:\n                    return (0, int(m.group(1)))\n            except Exception:\n                pass\n            return (1, str(col))\n\n        try:\n            ordered_cols = sorted(list(df_norm_col.columns), key=_topic_key)\n            df_norm_col = df_norm_col[ordered_cols]\n        except Exception:\n            # If columns are not iterable or sorting fails (e.g., custom objects),\n            # we leave the DataFrame as-is rather than raising an exception.\n            pass\n\n        # Show the final plot\n        sns.set_theme(style=\"ticks\", font_scale=font_scale)\n        if figsize:\n            fig, ax = plt.subplots(figsize=figsize)\n        else:\n            fig, ax = plt.subplots()\n        ax = sns.heatmap(df_norm_col, cmap=cmap, ax=ax)\n        # Set either provided title or a sensible default that indicates the content and the number of topics\n        if title is None:\n            try:\n                num_topics = len(df_norm_col.columns)\n            except Exception:\n                num_topics = None\n            if num_topics is not None:\n                title = f\"Topics by Category ({num_topics} Topics)\"\n            else:\n                title = \"Topics by Category\"\n        if title:\n            fig.suptitle(title)\n        ax.xaxis.tick_top()\n        ax.xaxis.set_label_position(\"top\")\n        plt.xticks(rotation=30, ha=\"left\")\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        if output_path:\n            plt.savefig(output_path)\n        if show:\n            plt.show()\n            return None\n        else:\n            plt.close()\n            return fig\n\n    @validate_call(config=model_config)\n    def topic_clouds(\n        self,\n        topics: Optional[int | list[int]] = None,\n        max_terms: Optional[int] = 30,\n        figsize: Optional[tuple[int, int]] = (10, 10),\n        output_path: Optional[str] = None,\n        show: Optional[bool] = True,\n        round_mask: Any = True,\n        title: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Figure:\n        \"\"\"Get a `MultiCloud` object for the topic-term distributions.\n\n        This method converts the internal topic-term probability dictionary\n        to a DataFrame (topics as rows) and constructs a `lexos.visualization.cloud.MultiCloud`\n        instance for visualization.\n\n        Parameters:\n            topics (Optional[int | list[int]]): Topics to include (rows). If None, show all.\n            max_terms (Optional[int]): Maximum number of top keywords to display per topic. Maps\n                to the `limit` parameter of `MultiCloud` and `max_words` in `opts` when not set.\n            figsize (Optional[tuple[int, int]]): Size of the overall figure.\n            output_path (Optional[str]): If provided, the MultiCloud figure will be saved to this path.\n            show (Optional[bool]): If True, the figure will be displayed in the current environment.\n            round_mask (bool|int|str): Either a boolean indicating whether to use a default circular mask\n                (True maps to radius 120; False disables mask), or an integer radius to use for a custom\n                mask. Strings containing integer values will be converted. Passing invalid values will\n                raise a `LexosException`.\n            title (Optional[str]): Optional title for the overall MultiCloud figure. If None, a default\n                of \"Topic Clouds (N topics)\" will be used.\n            **kwargs (Any): Additional keyword arguments. Use `opts` to pass wordcloud options for each cloud.\n\n        Returns:\n            Figure: If `show` is False, returns a Matplotlib Figure object created by `MultiCloud`.\n            Otherwise returns None after displaying the figure.\n\n        Notes:\n            The labels displayed above each word cloud will be of the form `Topic 0`,\n            `Topic 1`, etc.; keywords are not included in the labels to keep the\n            display uncluttered.\n        \"\"\"\n        sns.set_theme()\n\n        # Load topic-term probabilities and convert to DataFrame with topics as rows\n        topic_term_probability_dict = self.load_topic_term_distributions()\n        df = pd.DataFrame.from_dict(topic_term_probability_dict, orient=\"index\").fillna(\n            0\n        )\n\n        # Filter the DataFrame to include only the specified topics (rows)\n        if topics is not None:\n            df = df.iloc[ensure_list(topics)]\n\n        # Build options dict for MultiCloud\n        opts = kwargs.get(\"opts\", {})\n        # Default to a white background unless overridden\n        opts.setdefault(\"background_color\", \"white\")\n        # Ensure `max_words` is present if not provided, mapping from max_terms\n        if \"max_words\" not in opts and max_terms is not None:\n            opts[\"max_words\"] = max_terms\n\n        # Convert round_mask boolean or int into the radius integer expected by MultiCloud\n        if isinstance(round_mask, bool):\n            round_radius = 120 if round_mask else 0\n        else:\n            try:\n                round_radius = int(round_mask) if round_mask is not None else 0\n            except Exception:\n                raise LexosException(\n                    \"Invalid `round_mask` argument: expected a boolean or integer radius.\"\n                )\n\n        # Build simple numeric labels for each topic to avoid clutter\n        labels = [f\"Topic {i}\" for i in range(len(df))]\n\n        # Build figure_opts forwarding and set a white facecolor by default\n        figure_opts = kwargs.get(\"figure_opts\", {})\n        figure_opts.setdefault(\"facecolor\", \"white\")\n\n        # Create the MultiCloud object with updated args compatible with the class\n        # If no explicit title supplied, create a helpful default\n        if title is None:\n            try:\n                num_topics = len(df)\n            except Exception:\n                num_topics = None\n            if num_topics is not None:\n                title = f\"Topic Clouds ({num_topics} topics)\"\n            else:\n                title = \"Topic Clouds\"\n\n        mc = MultiCloud(\n            data=df,\n            limit=max_terms,\n            figsize=figsize,\n            opts=opts,\n            round=round_radius,\n            labels=labels,\n            figure_opts=figure_opts,\n            title=title,\n        )\n\n        # Save the file if requested\n        if output_path:\n            mc.save(output_path)\n\n        # Show the file if requested\n        if show:\n            mc.show()\n            return None\n        else:\n            return mc.fig\n\n    @validate_call(config=model_config)\n    def plot_topics_over_time(\n        self,\n        times: list,\n        topic_index: int,\n        topic_distributions: Optional[list[list[float]]] = None,\n        topic_keys: Optional[list[list[str]]] = None,\n        output_path: Optional[str] = None,\n        figsize: Optional[tuple[int, int]] = (7, 2.5),\n        font_scale: Optional[float] = 1.2,\n        color: Optional[ColorType] = \"cornflowerblue\",\n        show: Optional[bool] = True,\n        title: Optional[str] = None,\n    ) -&gt; Figure | None:\n        \"\"\"Plot the probability of a topic over time.\n\n        Args:\n            times (list): List of time points corresponding to each document (must be same length as topic_distributions).\n            topic_index (int): The index of the topic to plot.\n            topic_distributions (Optional[list[list[float]]]): If provided, a list of topic distributions per document. If None, uses `self.distributions`.\n            topic_keys (Optional[list[list[str]]]): If provided, a list of topic keys; otherwise uses `self.topic_keys`.\n            output_path (Optional[str]): Path to save the output plot. If None the plot is shown but not saved.\n            figsize (Optional[tuple[int,int]]): Figure size.\n            font_scale (Optional[float]): Seaborn font_scale.\n            color (Optional[ColorType]): Line color.\n            show (Optional[bool]): Whether to display the figure.\n            title (Optional[str]): Optional figure title. Will default to the topic's keywords if not supplied.\n\n        Returns:\n            Figure | None: The matplotlib figure if `show=False`, otherwise None.\n        \"\"\"\n        # Use provided distributions / keys or fall back to instance data\n        distributions = (\n            topic_distributions\n            if topic_distributions is not None\n            else self.distributions\n        )\n        topic_keys = topic_keys if topic_keys is not None else self.topic_keys\n\n        if distributions is None or len(distributions) == 0:\n            raise LexosException(\"No topic distributions available to plot.\")\n\n        if topic_index &lt; 0:\n            raise ValueError(\"topic_index must be a non-negative integer\")\n\n        if len(times) != len(distributions):\n            raise LexosException(\n                \"Length mismatch: 'times' must be the same length as topic_distributions\"\n            )\n\n        data_dicts = []\n        for j, _distribution in enumerate(distributions):\n            if len(_distribution) &lt;= topic_index:\n                # skip documents that don't cover the requested topic index\n                continue\n            data_dicts.append(\n                {\"Probability\": _distribution[topic_index], \"Time\": times[j]}\n            )\n\n        if len(data_dicts) == 0:\n            raise LexosException(f\"No data found for topic index {topic_index}\")\n\n        data_df = pd.DataFrame(data_dicts)\n\n        sns.set_theme(style=\"ticks\", font_scale=font_scale)\n        fig, ax = plt.subplots(figsize=figsize)\n        sns.lineplot(data=data_df, x=\"Time\", y=\"Probability\", color=color, ax=ax)\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Topic Probability\")\n\n        # Default title\n        if title is None:\n            try:\n                keywords = \" \".join(topic_keys[topic_index][2].split()[:5])\n                title = f\"Topic {topic_index}: {keywords}\"\n            except Exception:\n                title = f\"Topic {topic_index}\"\n        if title:\n            fig.suptitle(title)\n\n        plt.tight_layout()\n        sns.despine()\n        if output_path:\n            fig.savefig(output_path)\n        if show:\n            plt.show()\n            return None\n        else:\n            return fig\n\n    @validate_call(config=model_config)\n    def train(\n        self,\n        num_topics: int = 20,\n        num_iterations: Optional[int] = 100,\n        optimize_interval: Optional[int] = 10,\n        verbose: Optional[bool] = True,\n        # Common output paths: caller may pass canonical keys or path_to_* names\n        path_to_model: Optional[str] = None,\n        path_to_state: Optional[str] = None,\n        path_to_topic_keys: Optional[str] = None,\n        path_to_topic_distributions: Optional[str] = None,\n        path_to_term_weights: Optional[str] = None,\n        path_to_diagnostics: Optional[str] = None,\n        path_to_inferencer: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Train the topic model using MALLET.\n\n        Args:\n            num_topics (int): The number of topics to train.\n            num_iterations (int): The number of iterations to train for.\n            optimize_interval (int): The interval at which to optimize the model.\n            verbose (bool): Whether to print the MALLET output.\n            path_to_inferencer (Optional[str]): Optional output filename for saving a trained inferencer object\n                that can be used with `mallet infer-topics`. If not provided, defaults to\n                `model_dir/inferencer.mallet`.\n        \"\"\"\n        path_to_formatted_training_data = os.path.join(\n            self.model_dir, \"training_data.mallet\"\n        )\n\n        # Build the MALLET command\n        cmd = f\"{self.path_to_mallet or 'mallet'} train-topics\"\n        flags = {\n            \"input\": path_to_formatted_training_data,\n            \"num-topics\": num_topics,\n            \"num-iterations\": num_iterations,\n            \"output-state\": path_to_state\n            or os.path.join(self.model_dir, \"topic-state.gz\"),\n            \"output-topic-keys\": path_to_topic_keys\n            or os.path.join(self.model_dir, \"topic-keys.txt\"),\n            \"output-doc-topics\": path_to_topic_distributions\n            or os.path.join(self.model_dir, \"doc-topic.txt\"),\n            \"topic-word-weights-file\": path_to_term_weights\n            or os.path.join(self.model_dir, \"topic-weights.txt\"),\n            \"diagnostics-file\": path_to_diagnostics\n            or os.path.join(self.model_dir, \"diagnostics.xml\"),\n            # Optional inferencer filename path to save a trained inferencer for later inference\n            \"inferencer-filename\": path_to_inferencer\n            or os.path.join(self.model_dir, \"inferencer.mallet\"),\n            \"optimize-interval\": optimize_interval,\n        }\n\n        for k, v in flags.items():\n            if v:\n                # Save file names in the model directory if they are not absolute paths\n                if isinstance(v, str) and len(Path(v).parts) == 1:\n                    v = f\"{self.metadata['model_directory']}/{v}\"\n                cmd += f\" --{k} {v}\"\n                # Set canonical metadata keys for common outputs so consumers can\n                # rely on a single key. Map train flags directly to the\n                # canonical metadata keys.\n                if k == \"output-doc-topics\":\n                    self.metadata[self.CANONICAL_DOC_TOPIC_KEY] = v\n                if k == \"topic-word-weights-file\":\n                    self.metadata[self.CANONICAL_TERM_WEIGHTS_KEY] = v\n                if k == \"output-topic-keys\":\n                    self.metadata[self.CANONICAL_TOPIC_KEYS_KEY] = v\n                if k == \"inferencer-filename\":\n                    self.metadata[self.CANONICAL_INFERENCER_KEY] = v\n\n        # Train the model\n        msg.good(\"Training topics...\")\n        self._track_progress(cmd, num_iterations, verbose)\n        self.metadata[\"num_topics\"] = num_topics\n        self.metadata[\"optimize_interval\"] = optimize_interval\n        # For flags we don't have a canonical mapping for, provide a path_to_ entry\n        # to preserve other easily accessible metadata entries. Do not set legacy\n        # keys when we are mapping to a canonical key.\n        mapping = {\n            \"output-doc-topics\": self.CANONICAL_DOC_TOPIC_KEY,\n            \"topic-word-weights-file\": self.CANONICAL_TERM_WEIGHTS_KEY,\n            \"output-topic-keys\": self.CANONICAL_TOPIC_KEYS_KEY,\n            \"inferencer-filename\": self.CANONICAL_INFERENCER_KEY,\n        }\n        for k, v in flags.items():\n            if k not in [\"num-topics\", \"optimize-interval\"]:\n                if k in mapping:\n                    # canonical keys already set earlier in the loop\n                    continue\n                self.metadata[f\"path_to_{k.replace('-', '_')}\"] = v\n        self.metadata[\"training_command\"] = cmd\n        msg.good(\"Complete\")\n\n    @validate_call(config=model_config)\n    def infer(\n        self,\n        docs: list[str] | Path | str,\n        path_to_inferencer: Optional[str] = None,\n        output_path: Optional[str] = None,\n        keep_sequence: bool = True,\n        preserve_case: bool = True,\n        remove_stopwords: bool = True,\n        use_pipe_from: Optional[str] = None,\n        show: bool = False,\n    ) -&gt; list[list[float]] | None:\n        \"\"\"Infer topic distributions for new documents using a saved MALLET inferencer.\n\n        Args:\n            docs (list[str] | Path | str): The documents to infer topics for or a path to a file with documents.\n            path_to_inferencer (Optional[str]): Path to the MALLET inferencer file. If None, use metadata.\n            output_path (Optional[str]): Path to write the output doc-topics file. If None, it defaults to model_dir/infer-doc-topics.txt\n            keep_sequence (bool): Whether to keep the sequence in the import-file step.\n            preserve_case (bool): Whether to preserve case in the import-file step.\n            remove_stopwords (bool): Whether to remove stopwords in the import-file step.\n            use_pipe_from (Optional[str]): Optional pipe file to reuse for formatting.\n            show (bool): If True, display the returned distributions (no-op in headless).\n\n        Returns:\n            list[list[float]] | None: The inferred topic distributions (list of lists), or None if `show` is True.\n        \"\"\"\n        # Accept a single file path or list of documents\n        if isinstance(docs, (Path, str)) and Path(docs).is_file():\n            # it's an input file\n            input_file = str(docs)\n            # ensure we have a formatted mallet file if not provided\n            path_to_formatted = os.path.join(self.model_dir, \"infer_input.mallet\")\n            # import-file to format the input for mallet\n            cmd_import = f\"{self.path_to_mallet or 'mallet'} import-file --input {input_file} --output {path_to_formatted}\"\n            if keep_sequence:\n                cmd_import += \" --keep-sequence\"\n            if remove_stopwords:\n                cmd_import += \" --remove-stopwords\"\n            if preserve_case:\n                cmd_import += \" --preserve-case\"\n            if use_pipe_from:\n                cmd_import += f\" --use-pipe-from {use_pipe_from}\"\n            # msg.info(cmd_import)\n            os.system(cmd_import)\n        else:\n            # assume a list of document strings\n            if isinstance(docs, bool) or not isinstance(docs, list):\n                raise LexosException(\n                    \"Invalid `docs` argument: expected a list of strings or a path to a file.\"\n                )\n            # Write a temporal input file\n            path_to_plain = os.path.join(self.model_dir, \"infer_input.txt\")\n            with open(path_to_plain, \"w\", encoding=\"utf-8\") as fh:\n                for i, doc in enumerate(docs):\n                    if isinstance(doc, bool) or not isinstance(doc, str):\n                        raise LexosException(\n                            \"Invalid `docs` element: expected document text (str) for each item.\"\n                        )\n                    fh.write(f\"{i}\\tno_label\\t{doc.replace('\\n', ' ')}\\n\")\n            # format it with import-file\n            path_to_formatted = os.path.join(self.model_dir, \"infer_input.mallet\")\n            cmd_import = f\"{self.path_to_mallet or 'mallet'} import-file --input {path_to_plain} --output {path_to_formatted}\"\n            if keep_sequence:\n                cmd_import += \" --keep-sequence\"\n            if remove_stopwords:\n                cmd_import += \" --remove-stopwords\"\n            if preserve_case:\n                cmd_import += \" --preserve-case\"\n            if use_pipe_from:\n                cmd_import += f\" --use-pipe-from {use_pipe_from}\"\n            # msg.info(cmd_import)\n            os.system(cmd_import)\n\n        # Determine the inferencer file to use\n        if not path_to_inferencer:\n            path_to_inferencer = self._metadata_get([self.CANONICAL_INFERENCER_KEY])\n        if not path_to_inferencer:\n            raise LexosException(\n                \"No inferencer has been set. Provide `path_to_inferencer` or set it in metadata when training.\"\n            )\n\n        path_to_formatted = path_to_formatted\n        if output_path is None:\n            output_path = os.path.join(self.model_dir, \"infer-doc-topics.txt\")\n\n        cmd = f\"{self.path_to_mallet or 'mallet'} infer-topics --inferencer {path_to_inferencer} --input {path_to_formatted} --output-doc-topics {output_path}\"\n        # msg.info(cmd)\n        os.system(cmd)\n\n        # Read the output file and return distributions\n        distributions = []\n        try:\n            with open(output_path, \"r\") as f:\n                for line in f.readlines():\n                    # Skip blank lines\n                    if not line.strip():\n                        continue\n                    if line.split()[0] != \"#doc\":\n                        parts = line.strip().split(\"\\t\")\n                        if len(parts) &gt;= 3:\n                            raw_dist = parts[2:]\n                            # If the distribution token contains topic:prob pairs as a single\n                            # token (e.g. '0:0.1 1:0.9'), parse it out into a dense vector\n                            if len(raw_dist) == 1 and \":\" in raw_dist[0]:\n                                token = raw_dist[0]\n                                pairs = re.split(r\"\\s+\", token)\n                                if all(\":\" in p for p in pairs):\n                                    tp_map = {}\n                                    max_topic = -1\n                                    for p in pairs:\n                                        try:\n                                            t, prob = p.split(\":\")\n                                            t_i = int(t)\n                                            prob_f = float(prob)\n                                            tp_map[t_i] = prob_f\n                                            if t_i &gt; max_topic:\n                                                max_topic = t_i\n                                        except Exception:\n                                            raise LexosException(\n                                                f\"Topic:prob pair malformed in: {p}\"\n                                            )\n                                    raw_dist = [\n                                        str(tp_map.get(i, 0.0))\n                                        for i in range(max_topic + 1)\n                                    ]\n                        else:\n                            parts_ws = re.split(r\"\\s+\", line.strip())\n                            if len(parts_ws) &gt;= 3:\n                                raw_dist = parts_ws[2:]\n                            else:\n                                # parse compressed topic:prob pairs\n                                token = line.strip().split()[-1]\n                                pairs = re.split(r\"\\s+\", token)\n                                if all(\":\" in p for p in pairs):\n                                    tp_map = {}\n                                    max_topic = -1\n                                    for p in pairs:\n                                        t, prob = p.split(\":\")\n                                        t_i = int(t)\n                                        prob_f = float(prob)\n                                        tp_map[t_i] = prob_f\n                                        if t_i &gt; max_topic:\n                                            max_topic = t_i\n                                    raw_dist = [\n                                        str(tp_map.get(i, 0.0))\n                                        for i in range(max_topic + 1)\n                                    ]\n                                else:\n                                    raise LexosException(\n                                        f\"The line '{line.strip()}' in the inferred doc-topics file is not formatted correctly.\"\n                                    )\n                        try:\n                            distribution = [float(p) for p in raw_dist]\n                        except Exception:\n                            raise LexosException(\n                                f\"Unable to parse distribution values from line: '{line.strip()}'\"\n                            )\n                        distributions.append(distribution)\n        except FileNotFoundError:\n            raise LexosException(\n                f\"Inferred doc-topic output file not found: {output_path}\"\n            )\n\n        if show:\n            # user wants to display; we return None in this case for parity with other methods\n            return None\n        return distributions\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.distributions","title":"<code>distributions: list[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the topic distributions of the model.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.mean_num_tokens","title":"<code>mean_num_tokens: int</code>  <code>property</code>","text":"<p>Get the mean number of tokens per document in the model.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.metadata","title":"<code>metadata: dict[str, Any] = {}</code>  <code>pydantic-field</code>","text":"<p>A dict containing metadata generated by the class instance.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.model_dir","title":"<code>model_dir: Optional[Path | str] = None</code>  <code>pydantic-field</code>","text":"<p>The directory where the model is stored.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.model_directory","title":"<code>model_directory: str</code>  <code>property</code>","text":"<p>Return the model_directory from metadata or raise LexosException if missing.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.num_docs","title":"<code>num_docs: int</code>  <code>property</code>","text":"<p>Get the number of docs in the model.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.topic_keys","title":"<code>topic_keys: list[list[str]]</code>  <code>cached</code> <code>property</code>","text":"<p>Get the keys of the model.</p> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: A list of topics where each topic is a sublist containing the topic index, topic weight, and a space-separated list of keywords.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.vocab_size","title":"<code>vocab_size: int</code>  <code>property</code>","text":"<p>Get the vocabulary size of documents in the model.</p>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.__init__","title":"<code>__init__(**data) -&gt; None</code>","text":"<p>Initialize the Mallet class.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>def __init__(self, **data) -&gt; None:\n    \"\"\"Initialize the Mallet class.\"\"\"\n    super().__init__(**data)\n    # Assign the model directory if provided via constructor (model_dir)\n    # or via incoming metadata. Create the directory if it is provided\n    # to maintain a predictable filesystem state for later operations.\n    if self.model_dir is None and isinstance(self.metadata, dict):\n        # If the user provided a model_directory via metadata, accept it.\n        if \"model_directory\" in self.metadata:\n            self.model_dir = self.metadata[\"model_directory\"]\n    # If we now have a model_dir, validate and create it\n    if self.model_dir is not None:\n        # Validate that model_dir is not a boolean\n        if isinstance(self.model_dir, bool):\n            raise LexosException(\n                \"Invalid `model_dir` argument: expected a path (str or Path), not a boolean.\"\n            )\n        # Convert Path objects to str\n        if isinstance(self.model_dir, Path):\n            model_dir_str = str(self.model_dir)\n        else:\n            model_dir_str = self.model_dir\n        # Ensure the model_dir is not a file\n        p = Path(model_dir_str)\n        if p.exists() and p.is_file():\n            raise LexosException(\n                f\"The specified `model_dir` ({model_dir_str}) exists and is a file, expected a directory.\"\n            )\n        # Create the directory if it does not exist\n        p.mkdir(parents=True, exist_ok=True)\n        # Set metadata `model_directory` if not already set\n        if not isinstance(data.get(\"model_dir\"), type(None)):\n            self.metadata[\"model_directory\"] = model_dir_str\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.get_keys","title":"<code>get_keys(num_topics: int = None, topics: list[int] = None, num_keys: int = 10, as_df: bool = False) -&gt; str | Styler</code>","text":"<p>Get a string representation of the topic keys of the model.</p> <p>Parameters:</p> Name Type Description Default <code>num_topics</code> <code>int</code> <p>The number of topics to get keys for. If None, get keys for all topics.</p> <code>None</code> <code>topics</code> <code>list[int]</code> <p>A list of topic indices to get keys for. If None, get keys for all topics.</p> <code>None</code> <code>num_keys</code> <code>int</code> <p>The number of keys to output for each topic.</p> <code>10</code> <code>as_df</code> <code>bool</code> <p>Whether to return the result as a pandas DataFrame instead of a string.</p> <code>False</code> <p>Returns:</p> Type Description <code>str | Styler</code> <p>str | Styler: A string or DataFrame representation of the topic keys. The DataFrame is styled for presentation in a Jupyter notebook to prevent clipping of the keywords in a Jupyter notebook. If you need an actual <code>DataFrame</code> object, reference <code>df.data</code>.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef get_keys(\n    self,\n    num_topics: int = None,\n    topics: list[int] = None,\n    num_keys: int = 10,\n    as_df: bool = False,\n) -&gt; str | Styler:\n    \"\"\"Get a string representation of the topic keys of the model.\n\n    Args:\n        num_topics (int): The number of topics to get keys for. If None, get keys for all topics.\n        topics (list[int]): A list of topic indices to get keys for. If None, get keys for all topics.\n        num_keys (int): The number of keys to output for each topic.\n        as_df (bool): Whether to return the result as a pandas DataFrame instead of a string.\n\n    Returns:\n        str | Styler: A string or DataFrame representation of the topic keys. The DataFrame is styled for presentation in a Jupyter notebook to prevent clipping of the keywords in a Jupyter notebook. If you need an actual `DataFrame` object, reference `df.data`.\n    \"\"\"\n    num_available_topics = len(self.topic_keys)\n    if num_topics and not topics:\n        if num_topics &gt; num_available_topics:\n            raise IndexError(\n                f\"Requested num_topics={num_topics}, but only {num_available_topics} topics are available.\"\n            )\n        topic_keys = self.topic_keys[:num_topics]\n    elif topics:\n        # Validate all indices\n        for i in topics:\n            if i &lt; 0 or i &gt;= num_available_topics:\n                raise IndexError(\n                    f\"Topic index {i} is out of range. Valid indices are 0 to {num_available_topics - 1}.\"\n                )\n        topic_keys = [self.topic_keys[i] for i in topics]\n    else:\n        topic_keys = self.topic_keys\n    output = \"\"\n    for topic in topic_keys:\n        keywords = \" \".join(topic[2].split()[:num_keys])\n        output += f\"Topic {topic[0]}\\t{topic[1]}\\t{keywords}\\n\"\n    if as_df:\n        data = []\n        for topic in topic_keys:\n            keywords = \" \".join(topic[2].split()[:num_keys])\n            data.append(\n                {\"Topic\": topic[0], \"Label\": topic[1], \"Keywords\": keywords}\n            )\n        df = pd.DataFrame(data)\n        show_index = True  # or False\n        offset = 2 if show_index else 1\n        nth = df.columns.get_loc(\"Keywords\") + offset\n\n        css = [\n            # header cell of Keywords column\n            {\n                \"selector\": f\"thead th:nth-child({nth})\",\n                \"props\": [(\"text-align\", \"left\")],\n            },\n            # the column cells\n            {\n                \"selector\": f\"td.col{df.columns.get_loc('Keywords')}\",\n                \"props\": [(\"text-align\", \"left\")],\n            },\n        ]\n\n        styled_df = df.style.set_table_styles(css).set_properties(\n            subset=[\"Keywords\"], **{\"text-align\": \"left\"}\n        )\n        return styled_df\n    return output\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.get_top_docs","title":"<code>get_top_docs(topic=0, n=10, metadata: pd.DataFrame = None, as_str: bool = False) -&gt; pd.DataFrame | str</code>","text":"<p>Get the top n documents for a given topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>int</code> <p>Topic number.</p> <code>0</code> <code>n</code> <code>int</code> <p>Number of top documents to return.</p> <code>10</code> <code>metadata</code> <code>DataFrame</code> <p>Dataframe with the metadata in the same order as the training data (optional).</p> <code>None</code> <code>as_str</code> <code>bool</code> <p>Whether to return the result as a string instead of a dataframe.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame | str</code> <p>A pd.DataFrame or str: A dataframe with the top n documents for the given topic, or a string representation of the dataframe.</p> Notes <ul> <li>The metadata must be in the same order as the training data.</li> <li>The document text will get ellided by the maximum width of a pandas column. An easy way to see the full text is to set <code>as_str=True</code> and output the result with a print statement. You can also use the pandas API to extract the information with something like <code>top_docs.Document.tolist()</code>.</li> </ul> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef get_top_docs(\n    self, topic=0, n=10, metadata: pd.DataFrame = None, as_str: bool = False\n) -&gt; pd.DataFrame | str:\n    \"\"\"Get the top n documents for a given topic.\n\n    Args:\n        topic (int): Topic number.\n        n (int): Number of top documents to return.\n        metadata (pd.DataFrame): Dataframe with the metadata in the same order as the training data (optional).\n        as_str (bool): Whether to return the result as a string instead of a dataframe.\n\n    Returns:\n        A pd.DataFrame or str: A dataframe with the top n documents for the given topic, or a string representation of the dataframe.\n\n    Notes:\n        - The metadata must be in the same order as the training data.\n        - The document text will get ellided by the maximum width of a pandas column. An easy way to see the full text is to set `as_str=True` and output the result with a print statement. You can also use the pandas API to extract the information with something like `top_docs.Document.tolist()`.\n    \"\"\"\n    # Ensure that the path to doc-topic distributions exists (resolved via canonical keys)\n    if not self._metadata_has([self.CANONICAL_DOC_TOPIC_KEY]):\n        raise LexosException(\n            \"No topic distributions have been set. Please designate a path to the doc-topic distributions (e.g. `path_to_topic_distributions`) when you train your topic model.\"\n        )\n\n    if \"path_to_training_data\" not in self.metadata:\n        raise LexosException(\n            \"No training data has been set. Please designate a path for `path_to_training_data` when you train your topic model.\"\n        )\n\n    # Read the training data file\n    with open(self.metadata[\"path_to_training_data\"], \"r\", encoding=\"utf-8\") as f:\n        training_data = f.readlines()\n    training_data = [\n        line.split(\"\\t\")[2].strip() for line in training_data\n    ]  # Skip the id and label\n\n    # Validate topic index against model's known number of topics (0-based)\n    try:\n        topic = int(topic)\n    except Exception:\n        raise ValueError(\"Topic index must be an integer\")\n\n    num_topics = None\n    # Try the reliable metadata if present\n    if \"num_topics\" in self.metadata:\n        try:\n            num_topics = int(self.metadata[\"num_topics\"])\n        except Exception:\n            num_topics = None\n    # Fall back to topic_keys if available\n    if num_topics is None:\n        try:\n            num_topics = len(self.topic_keys)\n        except Exception:\n            num_topics = None\n    # As a last resort, infer from distributions\n    distribution_len = None\n    if len(self.distributions) &gt; 0:\n        # ensure all distributions have the same length; otherwise raise\n        lengths = set(len(d) for d in self.distributions)\n        if len(lengths) &gt; 1:\n            raise LexosException(\n                \"Topic distribution lengths are inconsistent across documents; check `path_to_topic_distributions` format.\"\n            )\n        distribution_len = next(iter(lengths))\n        if num_topics is None:\n            num_topics = distribution_len\n    if num_topics is None:\n        raise LexosException(\n            \"Model does not have topic information yet. Train or load a model first.\"\n        )\n    # If we have both a metadata num_topics and inferred distribution length, they should match.\n    if (\n        distribution_len is not None\n        and num_topics is not None\n        and distribution_len != num_topics\n    ):\n        raise LexosException(\n            f\"Mismatch between declared number of topics ({num_topics}) and distribution vector length ({distribution_len}). Check your training outputs.\"\n        )\n\n    if not (0 &lt;= topic &lt; num_topics):\n        raise ValueError(\n            f\"Invalid topic index {topic}. Valid topic indices are 0..{num_topics - 1} (0-based).\"\n        )\n\n    # Combine the distribution and training data, then convert to a dataframe\n    distribution_data = [\n        (_distribution[topic], _document)\n        for _distribution, _document in zip(self.distributions, training_data)\n    ]\n    df = pd.DataFrame(distribution_data, columns=[\"Distribution\", \"Document\"])\n    df.index.name = \"Doc ID\"\n\n    # If metadata is provided, concatenate it to the dataframe\n    if metadata is not None:\n        df = pd.concat([df, metadata], axis=1)\n\n    # Sort the dataframe by distribution and return the top n documents\n    if as_str:\n        return (\n            df.sort_values(by=\"Distribution\", ascending=False).head(n).to_string()\n        )\n    return df.sort_values(by=\"Distribution\", ascending=False).head(n)\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.get_topic_term_probabilities","title":"<code>get_topic_term_probabilities(topics: Optional[int | list[int]] = None, n: int = 5, as_df: bool = False) -&gt; str | pd.DataFrame</code>","text":"<p>Get a string representation of the term distribution for a given topic.</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>int | list[int]</code> <p>Topic number. If None, get the probabilities for all topics.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of keywords to display.</p> <code>5</code> <code>as_df</code> <code>bool</code> <p>Whether to display the result as a string or a pandas DataFrame.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str | DataFrame</code> <p>A string representation of the term distribution for the given topic.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef get_topic_term_probabilities(\n    self, topics: Optional[int | list[int]] = None, n: int = 5, as_df: bool = False\n) -&gt; str | pd.DataFrame:\n    \"\"\"Get a string representation of the term distribution for a given topic.\n\n    Args:\n        topics (int | list[int]): Topic number. If None, get the probabilities for all topics.\n        n (int): The number of keywords to display.\n        as_df (bool): Whether to display the result as a string or a pandas DataFrame.\n\n    Returns:\n        str: A string representation of the term distribution for the given topic.\n    \"\"\"\n    if isinstance(topics, int):\n        topics = [topics]\n    topic_term_probability_dict = self.load_topic_term_distributions()\n    # Build either a string (legacy behavior) or a DataFrame with columns\n    # Topic | Term | Probability based on the `as_df` parameter.\n    if as_df:\n        rows = []\n        for _topic, _term_probability_dict in topic_term_probability_dict.items():\n            if topics is None or _topic in topics:\n                for _term, _probability in sorted(\n                    _term_probability_dict.items(), key=lambda x: x[1], reverse=True\n                )[:n]:\n                    rows.append(\n                        {\n                            \"Topic\": _topic,\n                            \"Term\": _term,\n                            \"Probability\": _probability,\n                        }\n                    )\n        df = pd.DataFrame(rows)\n        return df\n    result = \"\"\n    for _topic, _term_probability_dict in topic_term_probability_dict.items():\n        if topics is None or _topic in topics:\n            result += f\"Topic {_topic}\\n\"\n            for _term, _probability in sorted(\n                _term_probability_dict.items(), key=lambda x: x[1], reverse=True\n            )[:n]:\n                result += f\"\\t{_term}: {_probability}\\n\"\n            result += \"\\n\"\n    return result\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.import_data","title":"<code>import_data(training_data: list[str], path_to_training_data: str = None, keep_sequence: bool = True, preserve_case: bool = True, remove_stopwords: bool = True, use_pipe_from: Optional[str] = None, training_ids: Optional[list[int]] = None) -&gt; None</code>","text":"<p>Convenience wrapper to import a list of documents and format them for MALLET.</p> <p>Parameters:</p> Name Type Description Default <code>training_data</code> <code>list[str]</code> <p>List of document texts.</p> required <code>path_to_training_data</code> <code>str</code> <p>Path to write raw training text file. If None, will default to model directory.</p> <code>None</code> <code>keep_sequence</code> <code>bool</code> <p>Keep token sequence.</p> <code>True</code> <code>preserve_case</code> <code>bool</code> <p>Preserve case.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Remove stopwords.</p> <code>True</code> <code>use_pipe_from</code> <code>Optional[str]</code> <p>Pipe filename for MALLET import.</p> <code>None</code> <code>training_ids</code> <code>Optional[list[int]]</code> <p>Optional training IDs mapping.</p> <code>None</code> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef import_data(\n    self,\n    training_data: list[str],\n    path_to_training_data: str = None,\n    keep_sequence: bool = True,\n    preserve_case: bool = True,\n    remove_stopwords: bool = True,\n    use_pipe_from: Optional[str] = None,\n    training_ids: Optional[list[int]] = None,\n) -&gt; None:\n    \"\"\"Convenience wrapper to import a list of documents and format them for MALLET.\n\n    Args:\n        training_data (list[str]): List of document texts.\n        path_to_training_data (str): Path to write raw training text file. If None, will default to model directory.\n        keep_sequence (bool): Keep token sequence.\n        preserve_case (bool): Preserve case.\n        remove_stopwords (bool): Remove stopwords.\n        use_pipe_from (Optional[str]): Pipe filename for MALLET import.\n        training_ids (Optional[list[int]]): Optional training IDs mapping.\n    \"\"\"\n    # Validate training_data is a list of strings\n    if isinstance(training_data, bool) or not isinstance(training_data, list):\n        raise LexosException(\n            \"Invalid `training_data` argument: expected a list of document strings.\"\n        )\n    for doc in training_data:\n        if isinstance(doc, bool) or not isinstance(doc, str):\n            raise LexosException(\n                \"Invalid `training_data` element: expected document text (str) for each item.\"\n            )\n\n    # Determine output paths if not provided\n    if not path_to_training_data:\n        model_base = self.model_dir if self.model_dir else os.getcwd()\n        path_to_training_data = os.path.join(model_base, \"training_data.txt\")\n    self._import_training_data(\n        training_data,\n        path_to_training_data,\n        keep_sequence,\n        remove_stopwords,\n        preserve_case,\n        use_pipe_from,\n        training_ids,\n    )\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.import_dir","title":"<code>import_dir(data_source: str | list[str], keep_sequence: bool = True, preserve_case: bool = True, remove_stopwords: bool = True, use_pipe_from: Optional[str] = None, training_ids: Optional[list[int]] = None) -&gt; None</code>","text":"<p>Read training data from directories and save formatted training data file.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>str | list[str]</code> <p>A directory or list of directories to import.</p> required <code>keep_sequence</code> <code>bool</code> <p>Whether to keep the word sequence in the documents.</p> <code>True</code> <code>preserve_case</code> <code>bool</code> <p>Whether to preserve the case of the documents.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Whether to remove stopwords from the documents.</p> <code>True</code> <code>use_pipe_from</code> <code>Optional[str]</code> <p>Path to a MALLET pipe file to use for importing.</p> <code>None</code> <code>training_ids</code> <code>Optional[list[int]]</code> <p>Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.</p> <code>None</code> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef import_dir(\n    self,\n    data_source: str | list[str],\n    keep_sequence: bool = True,\n    preserve_case: bool = True,\n    remove_stopwords: bool = True,\n    use_pipe_from: Optional[str] = None,\n    training_ids: Optional[list[int]] = None,\n) -&gt; None:\n    \"\"\"Read training data from directories and save formatted training data file.\n\n    Args:\n        data_source (str | list[str]): A directory or list of directories to import.\n        keep_sequence (bool): Whether to keep the word sequence in the documents.\n        preserve_case (bool): Whether to preserve the case of the documents.\n        remove_stopwords (bool): Whether to remove stopwords from the documents.\n        use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n        training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n    \"\"\"\n    # Explicitly validate data_source to reject booleans\n    if isinstance(data_source, bool):\n        raise LexosException(\n            \"Invalid `data_source` argument: expected a directory path or list of paths, not a boolean.\"\n        )\n    training_data = read_dirs(ensure_list(data_source))\n    self._import_training_data(\n        training_data,\n        path_to_training_data=None,\n        keep_sequence=keep_sequence,\n        remove_stopwords=remove_stopwords,\n        preserve_case=preserve_case,\n        use_pipe_from=use_pipe_from,\n        training_ids=training_ids,\n    )\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.import_docs","title":"<code>import_docs(data_source: str | list[str], keep_sequence: bool = True, preserve_case: bool = True, remove_stopwords: bool = True, use_pipe_from: Optional[str] = None, training_ids: Optional[list[int]] = None) -&gt; None</code>","text":"<p>Read training data from docs and save formatted training data file.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>str | list[str]</code> <p>A doc or list of docs to import.</p> required <code>keep_sequence</code> <code>bool</code> <p>Whether to keep the word sequence in the documents.</p> <code>True</code> <code>preserve_case</code> <code>bool</code> <p>Whether to preserve the case of the documents.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Whether to remove stopwords from the documents.</p> <code>True</code> <code>use_pipe_from</code> <code>Optional[str]</code> <p>Path to a MALLET pipe file to use for importing.</p> <code>None</code> <code>training_ids</code> <code>Optional[list[int]]</code> <p>Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.</p> <code>None</code> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef import_docs(\n    self,\n    data_source: str | list[str],\n    keep_sequence: bool = True,\n    preserve_case: bool = True,\n    remove_stopwords: bool = True,\n    use_pipe_from: Optional[str] = None,\n    training_ids: Optional[list[int]] = None,\n) -&gt; None:\n    \"\"\"Read training data from docs and save formatted training data file.\n\n    Args:\n        data_source (str | list[str]): A doc or list of docs to import.\n        keep_sequence (bool): Whether to keep the word sequence in the documents.\n        preserve_case (bool): Whether to preserve the case of the documents.\n        remove_stopwords (bool): Whether to remove stopwords from the documents.\n        use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n        training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n    \"\"\"\n    if isinstance(data_source, bool):\n        raise LexosException(\n            \"Invalid `data_source` argument: expected a doc or list of docs, not a boolean.\"\n        )\n    docs = ensure_list(data_source)\n    training_data = [\n        f\"{i}\\t\\t{doc.text}\" if isinstance(doc, Doc) else f\"{i}\\t\\t{doc}\"\n        for i, doc in enumerate(docs)\n    ]\n    self._import_training_data(\n        training_data,\n        path_to_training_data=None,\n        keep_sequence=keep_sequence,\n        remove_stopwords=remove_stopwords,\n        preserve_case=preserve_case,\n        use_pipe_from=use_pipe_from,\n        training_ids=training_ids,\n    )\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.import_file","title":"<code>import_file(data_source: str | list[str], keep_sequence: bool = True, preserve_case: bool = True, remove_stopwords: bool = True, use_pipe_from: Optional[str] = None, training_ids: Optional[list[int]] = None) -&gt; None</code>","text":"<p>Read training data from file and save formatted training data file.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>str | list[str]</code> <p>A file or list of files to import.</p> required <code>keep_sequence</code> <code>bool</code> <p>Whether to keep the word sequence in the documents.</p> <code>True</code> <code>preserve_case</code> <code>bool</code> <p>Whether to preserve the case of the documents.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Whether to remove stopwords from the documents.</p> <code>True</code> <code>use_pipe_from</code> <code>Optional[str]</code> <p>Path to a MALLET pipe file to use for importing.</p> <code>None</code> <code>training_ids</code> <code>Optional[list[int]]</code> <p>Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.</p> <code>None</code> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef import_file(\n    self,\n    data_source: str | list[str],\n    keep_sequence: bool = True,\n    preserve_case: bool = True,\n    remove_stopwords: bool = True,\n    use_pipe_from: Optional[str] = None,\n    training_ids: Optional[list[int]] = None,\n) -&gt; None:\n    \"\"\"Read training data from file and save formatted training data file.\n\n    Args:\n        data_source (str | list[str]): A file or list of files to import.\n        keep_sequence (bool): Whether to keep the word sequence in the documents.\n        preserve_case (bool): Whether to preserve the case of the documents.\n        remove_stopwords (bool): Whether to remove stopwords from the documents.\n        use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n        training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n    \"\"\"\n    if isinstance(data_source, bool):\n        raise LexosException(\n            \"Invalid `data_source` argument: expected a file path or list of paths, not a boolean.\"\n        )\n    training_data = read_file(ensure_list(data_source))\n    self._import_training_data(\n        training_data,\n        path_to_training_data=None,\n        keep_sequence=keep_sequence,\n        remove_stopwords=remove_stopwords,\n        preserve_case=preserve_case,\n        use_pipe_from=use_pipe_from,\n        training_ids=training_ids,\n    )\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.infer","title":"<code>infer(docs: list[str] | Path | str, path_to_inferencer: Optional[str] = None, output_path: Optional[str] = None, keep_sequence: bool = True, preserve_case: bool = True, remove_stopwords: bool = True, use_pipe_from: Optional[str] = None, show: bool = False) -&gt; list[list[float]] | None</code>","text":"<p>Infer topic distributions for new documents using a saved MALLET inferencer.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list[str] | Path | str</code> <p>The documents to infer topics for or a path to a file with documents.</p> required <code>path_to_inferencer</code> <code>Optional[str]</code> <p>Path to the MALLET inferencer file. If None, use metadata.</p> <code>None</code> <code>output_path</code> <code>Optional[str]</code> <p>Path to write the output doc-topics file. If None, it defaults to model_dir/infer-doc-topics.txt</p> <code>None</code> <code>keep_sequence</code> <code>bool</code> <p>Whether to keep the sequence in the import-file step.</p> <code>True</code> <code>preserve_case</code> <code>bool</code> <p>Whether to preserve case in the import-file step.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Whether to remove stopwords in the import-file step.</p> <code>True</code> <code>use_pipe_from</code> <code>Optional[str]</code> <p>Optional pipe file to reuse for formatting.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, display the returned distributions (no-op in headless).</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[float]] | None</code> <p>list[list[float]] | None: The inferred topic distributions (list of lists), or None if <code>show</code> is True.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef infer(\n    self,\n    docs: list[str] | Path | str,\n    path_to_inferencer: Optional[str] = None,\n    output_path: Optional[str] = None,\n    keep_sequence: bool = True,\n    preserve_case: bool = True,\n    remove_stopwords: bool = True,\n    use_pipe_from: Optional[str] = None,\n    show: bool = False,\n) -&gt; list[list[float]] | None:\n    \"\"\"Infer topic distributions for new documents using a saved MALLET inferencer.\n\n    Args:\n        docs (list[str] | Path | str): The documents to infer topics for or a path to a file with documents.\n        path_to_inferencer (Optional[str]): Path to the MALLET inferencer file. If None, use metadata.\n        output_path (Optional[str]): Path to write the output doc-topics file. If None, it defaults to model_dir/infer-doc-topics.txt\n        keep_sequence (bool): Whether to keep the sequence in the import-file step.\n        preserve_case (bool): Whether to preserve case in the import-file step.\n        remove_stopwords (bool): Whether to remove stopwords in the import-file step.\n        use_pipe_from (Optional[str]): Optional pipe file to reuse for formatting.\n        show (bool): If True, display the returned distributions (no-op in headless).\n\n    Returns:\n        list[list[float]] | None: The inferred topic distributions (list of lists), or None if `show` is True.\n    \"\"\"\n    # Accept a single file path or list of documents\n    if isinstance(docs, (Path, str)) and Path(docs).is_file():\n        # it's an input file\n        input_file = str(docs)\n        # ensure we have a formatted mallet file if not provided\n        path_to_formatted = os.path.join(self.model_dir, \"infer_input.mallet\")\n        # import-file to format the input for mallet\n        cmd_import = f\"{self.path_to_mallet or 'mallet'} import-file --input {input_file} --output {path_to_formatted}\"\n        if keep_sequence:\n            cmd_import += \" --keep-sequence\"\n        if remove_stopwords:\n            cmd_import += \" --remove-stopwords\"\n        if preserve_case:\n            cmd_import += \" --preserve-case\"\n        if use_pipe_from:\n            cmd_import += f\" --use-pipe-from {use_pipe_from}\"\n        # msg.info(cmd_import)\n        os.system(cmd_import)\n    else:\n        # assume a list of document strings\n        if isinstance(docs, bool) or not isinstance(docs, list):\n            raise LexosException(\n                \"Invalid `docs` argument: expected a list of strings or a path to a file.\"\n            )\n        # Write a temporal input file\n        path_to_plain = os.path.join(self.model_dir, \"infer_input.txt\")\n        with open(path_to_plain, \"w\", encoding=\"utf-8\") as fh:\n            for i, doc in enumerate(docs):\n                if isinstance(doc, bool) or not isinstance(doc, str):\n                    raise LexosException(\n                        \"Invalid `docs` element: expected document text (str) for each item.\"\n                    )\n                fh.write(f\"{i}\\tno_label\\t{doc.replace('\\n', ' ')}\\n\")\n        # format it with import-file\n        path_to_formatted = os.path.join(self.model_dir, \"infer_input.mallet\")\n        cmd_import = f\"{self.path_to_mallet or 'mallet'} import-file --input {path_to_plain} --output {path_to_formatted}\"\n        if keep_sequence:\n            cmd_import += \" --keep-sequence\"\n        if remove_stopwords:\n            cmd_import += \" --remove-stopwords\"\n        if preserve_case:\n            cmd_import += \" --preserve-case\"\n        if use_pipe_from:\n            cmd_import += f\" --use-pipe-from {use_pipe_from}\"\n        # msg.info(cmd_import)\n        os.system(cmd_import)\n\n    # Determine the inferencer file to use\n    if not path_to_inferencer:\n        path_to_inferencer = self._metadata_get([self.CANONICAL_INFERENCER_KEY])\n    if not path_to_inferencer:\n        raise LexosException(\n            \"No inferencer has been set. Provide `path_to_inferencer` or set it in metadata when training.\"\n        )\n\n    path_to_formatted = path_to_formatted\n    if output_path is None:\n        output_path = os.path.join(self.model_dir, \"infer-doc-topics.txt\")\n\n    cmd = f\"{self.path_to_mallet or 'mallet'} infer-topics --inferencer {path_to_inferencer} --input {path_to_formatted} --output-doc-topics {output_path}\"\n    # msg.info(cmd)\n    os.system(cmd)\n\n    # Read the output file and return distributions\n    distributions = []\n    try:\n        with open(output_path, \"r\") as f:\n            for line in f.readlines():\n                # Skip blank lines\n                if not line.strip():\n                    continue\n                if line.split()[0] != \"#doc\":\n                    parts = line.strip().split(\"\\t\")\n                    if len(parts) &gt;= 3:\n                        raw_dist = parts[2:]\n                        # If the distribution token contains topic:prob pairs as a single\n                        # token (e.g. '0:0.1 1:0.9'), parse it out into a dense vector\n                        if len(raw_dist) == 1 and \":\" in raw_dist[0]:\n                            token = raw_dist[0]\n                            pairs = re.split(r\"\\s+\", token)\n                            if all(\":\" in p for p in pairs):\n                                tp_map = {}\n                                max_topic = -1\n                                for p in pairs:\n                                    try:\n                                        t, prob = p.split(\":\")\n                                        t_i = int(t)\n                                        prob_f = float(prob)\n                                        tp_map[t_i] = prob_f\n                                        if t_i &gt; max_topic:\n                                            max_topic = t_i\n                                    except Exception:\n                                        raise LexosException(\n                                            f\"Topic:prob pair malformed in: {p}\"\n                                        )\n                                raw_dist = [\n                                    str(tp_map.get(i, 0.0))\n                                    for i in range(max_topic + 1)\n                                ]\n                    else:\n                        parts_ws = re.split(r\"\\s+\", line.strip())\n                        if len(parts_ws) &gt;= 3:\n                            raw_dist = parts_ws[2:]\n                        else:\n                            # parse compressed topic:prob pairs\n                            token = line.strip().split()[-1]\n                            pairs = re.split(r\"\\s+\", token)\n                            if all(\":\" in p for p in pairs):\n                                tp_map = {}\n                                max_topic = -1\n                                for p in pairs:\n                                    t, prob = p.split(\":\")\n                                    t_i = int(t)\n                                    prob_f = float(prob)\n                                    tp_map[t_i] = prob_f\n                                    if t_i &gt; max_topic:\n                                        max_topic = t_i\n                                raw_dist = [\n                                    str(tp_map.get(i, 0.0))\n                                    for i in range(max_topic + 1)\n                                ]\n                            else:\n                                raise LexosException(\n                                    f\"The line '{line.strip()}' in the inferred doc-topics file is not formatted correctly.\"\n                                )\n                    try:\n                        distribution = [float(p) for p in raw_dist]\n                    except Exception:\n                        raise LexosException(\n                            f\"Unable to parse distribution values from line: '{line.strip()}'\"\n                        )\n                    distributions.append(distribution)\n    except FileNotFoundError:\n        raise LexosException(\n            f\"Inferred doc-topic output file not found: {output_path}\"\n        )\n\n    if show:\n        # user wants to display; we return None in this case for parity with other methods\n        return None\n    return distributions\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.load_topic_term_distributions","title":"<code>load_topic_term_distributions() -&gt; dict[str, float]</code>","text":"<p>Load the topic-term distributions from a file.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: A dictionary of all topic-term distributions.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>def load_topic_term_distributions(self) -&gt; dict[str, float]:\n    \"\"\"Load the topic-term distributions from a file.\n\n    Returns:\n        dict[str, float]: A dictionary of all topic-term distributions.\n    \"\"\"\n    # Ensure that the path to a term weights file has been set.\n    term_weight_path = self._metadata_get([self.CANONICAL_TERM_WEIGHTS_KEY])\n    if term_weight_path is None:\n        raise LexosException(\n            f\"No term weights have been set. Please designate a path to the term weights file (e.g. `{self.CANONICAL_TERM_WEIGHTS_KEY}`) when you train your topic model.\"\n        )\n    topic_term_weight_dict = defaultdict(lambda: defaultdict(float))\n    topic_sum_dict = defaultdict(float)\n    try:\n        with open(term_weight_path, \"r\") as f:\n            for _line in f:\n                if not _line.strip():\n                    continue\n                parts = _line.strip().split(\"\\t\")\n                if len(parts) != 3:\n                    # Malformed line\n                    raise ValueError(\n                        f\"Malformed line in term weights file: '{_line.strip()}'\"\n                    )\n                _topic, _term, _weight = parts\n                try:\n                    weight_f = float(_weight)\n                except Exception:\n                    raise ValueError(\n                        f\"Invalid weight value '{_weight}' in line: '{_line.strip()}'\"\n                    )\n                topic_term_weight_dict[_topic][_term] = weight_f\n                topic_sum_dict[_topic] += weight_f\n    except FileNotFoundError:\n        # Surface file not found as filesystem error\n        raise\n\n    topic_term_probability_dict = defaultdict(lambda: defaultdict(float))\n    for _topic, _term_weight_dict in topic_term_weight_dict.items():\n        for _term, _weight in _term_weight_dict.items():\n            topic_term_probability_dict[int(_topic)][_term] = (\n                _weight / topic_sum_dict[_topic]\n            )\n\n    return topic_term_probability_dict\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.plot_categories_by_topic_boxplots","title":"<code>plot_categories_by_topic_boxplots(categories: list[str], topics: Optional[int | list[int]] = None, output_path: Optional[str] = None, target_labels: Optional[list[str]] = None, num_keys: int = 5, figsize: Optional[tuple[int, int]] = (6, 6), font_scale: Optional[float] = 1.2, color: Optional[ColorType] = 'lightblue', show: Optional[bool] = True, title: Optional[str] = None, overlay: Optional[str] = 'strip', overlay_kws: Optional[dict[str, Any]] = None, topic_distributions: Optional[list[list[float]]] = None) -&gt; Figure | list[Figure]</code>","text":"<p>Plot boxplots showing the distribution of topic probabilities for each category.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>list[str]</code> <p>The labels to use for the categories.</p> required <code>topics</code> <code>int | list[int]</code> <p>The index of the topic to plot.</p> <code>None</code> <code>output_path</code> <code>str</code> <p>The path to save the figure.</p> <code>None</code> <code>target_labels</code> <code>list[str]</code> <p>Unique labels for categories to classify.</p> <code>None</code> <code>num_keys</code> <code>int</code> <p>The number of keywords to display.</p> <code>5</code> <code>figsize</code> <code>Optional[tuple[int, int]]</code> <p>(Optional[tuple[int, int]]): The dimensions of the figure.</p> <code>(6, 6)</code> <code>font_scale</code> <code>Optional[float]</code> <p>The font scale for the figure.</p> <code>1.2</code> <code>color</code> <code>Optional[ColorType]</code> <p>The color to use for the heatmap boxes. A matplotlib ColorType name or object.</p> <code>'lightblue'</code> <code>show</code> <code>Optional[bool]</code> <p>Whether to show the figure.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>Optional figure title. If not supplied, each plot will use a default title of <code>Topic {topic}: {keywords}</code>.</p> <code>None</code> <code>overlay</code> <code>Optional[str]</code> <p>How to display the individual points overlaid on each boxplot. Supported values are 'strip' (default), 'swarm', or 'none'.</p> <code>'strip'</code> <code>overlay_kws</code> <code>Optional[dict]</code> <p>Keyword arguments passed to the chosen overlay plotting method (<code>seaborn.stripplot</code> or <code>seaborn.swarmplot</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure | list[Figure]</code> <p>Figure | list[Figure]: The boxplot showing the topic associations by category.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef plot_categories_by_topic_boxplots(\n    self,\n    categories: list[str],\n    topics: Optional[int | list[int]] = None,\n    output_path: Optional[str] = None,\n    target_labels: Optional[list[str]] = None,\n    num_keys: int = 5,\n    figsize: Optional[tuple[int, int]] = (6, 6),\n    font_scale: Optional[float] = 1.2,\n    color: Optional[ColorType] = \"lightblue\",\n    show: Optional[bool] = True,\n    title: Optional[str] = None,\n    overlay: Optional[str] = \"strip\",\n    overlay_kws: Optional[dict[str, Any]] = None,\n    topic_distributions: Optional[list[list[float]]] = None,\n) -&gt; Figure | list[Figure]:\n    \"\"\"Plot boxplots showing the distribution of topic probabilities for each category.\n\n    Args:\n        categories (list[str]): The labels to use for the categories.\n        topics (int | list[int]): The index of the topic to plot.\n        output_path (str): The path to save the figure.\n        target_labels (list[str]): Unique labels for categories to classify.\n        num_keys (int): The number of keywords to display.\n        figsize: (Optional[tuple[int, int]]): The dimensions of the figure.\n        font_scale (Optional[float]): The font scale for the figure.\n        color (Optional[ColorType]): The color to use for the heatmap boxes. A matplotlib ColorType name or object.\n        show (Optional[bool]): Whether to show the figure.\n        title (Optional[str]): Optional figure title. If not supplied, each plot will use a default title of\n            `Topic {topic}: {keywords}`.\n        overlay (Optional[str]): How to display the individual points overlaid on each boxplot. Supported\n            values are 'strip' (default), 'swarm', or 'none'.\n        overlay_kws (Optional[dict]): Keyword arguments passed to the chosen overlay plotting method\n            (`seaborn.stripplot` or `seaborn.swarmplot`).\n\n    Returns:\n        Figure | list[Figure]: The boxplot showing the topic associations by category.\n    \"\"\"\n    # Load topic_keys\n    topic_keys = self.topic_keys\n\n    # Ensure that topics is a list\n    if topics is None:\n        topics = list(range(len(topic_keys)))\n    elif isinstance(topics, int):\n        topics = [topics]\n\n    # Ensure there are topic_labels\n    if not target_labels:\n        target_labels = list(set(categories))\n\n    # Combine the labels and distributions into a dataframe.\n    figs = []\n    import os\n\n    # Use user-provided topic_distributions if given, else default to self.distributions\n    distributions = (\n        topic_distributions\n        if topic_distributions is not None\n        else self.distributions\n    )\n\n    for topic in topics:\n        keywords = \" \".join(topic_keys[topic][2].split()[:num_keys])\n\n        dicts_to_plot = []\n        for _label, _distribution in zip(categories, distributions):\n            if not target_labels or _label in target_labels:\n                dicts_to_plot.append(\n                    {\n                        \"Probability\": float(_distribution[topic]),\n                        \"Category\": _label,\n                        \"Topic\": keywords,\n                    }\n                )\n        df_to_plot = pd.DataFrame(dicts_to_plot)\n\n        # Validate overlay option\n        if overlay not in (\"strip\", \"swarm\", \"none\", None):\n            raise LexosException(\n                \"Invalid `overlay` argument: expected 'strip', 'swarm', or 'none'.\"\n            )\n\n        # Show the final plot\n        sns.set_theme(style=\"ticks\", font_scale=font_scale)\n        # Create a figure/axes so we can overlay points for small datasets\n        if figsize:\n            fig, ax = plt.subplots(figsize=figsize)\n        else:\n            fig, ax = plt.subplots()\n        sns.boxplot(\n            data=df_to_plot,\n            x=\"Category\",\n            y=\"Probability\",\n            color=color,\n            ax=ax,\n            showmeans=True,\n        )\n        # Overlay data points so users can see the raw values when there are\n        # too few observations to form a full box\n        overlay_kws = dict(overlay_kws or {})\n        try:\n            if overlay == \"strip\" or overlay is None:\n                sns.stripplot(\n                    data=df_to_plot,\n                    x=\"Category\",\n                    y=\"Probability\",\n                    color=overlay_kws.pop(\"color\", \"black\"),\n                    size=overlay_kws.pop(\"size\", 4),\n                    jitter=overlay_kws.pop(\"jitter\", True),\n                    ax=ax,\n                    **overlay_kws,\n                )\n            elif overlay == \"swarm\":\n                sns.swarmplot(\n                    data=df_to_plot,\n                    x=\"Category\",\n                    y=\"Probability\",\n                    color=overlay_kws.pop(\"color\", \"black\"),\n                    size=overlay_kws.pop(\"size\", 4),\n                    ax=ax,\n                    **overlay_kws,\n                )\n            # if overlay == 'none', do nothing\n        except Exception:\n            # Overlay plotting is optional; ignore any backend failures\n            pass\n        sns.despine()\n        plt.xticks(rotation=45, ha=\"right\")\n        # Set either the provided title or a sensible default including topic index and top keys\n        if title is None:\n            ax.set_title(f\"Topic {topic}: {keywords}\")\n        else:\n            # Use a figure-level title to avoid per-subplot clobbering\n            fig.suptitle(title)\n        plt.tight_layout()\n        # Save each plot to a unique file if output_path is set\n        if output_path:\n            base, ext = os.path.splitext(output_path)\n            save_path = f\"{base}_topic{topic}{ext}\"\n            fig.savefig(save_path)\n        figs.append(fig)\n        if show:\n            plt.show()\n        plt.close(fig)\n    if show:\n        return None\n    # If this function only generated a single figure, return it.\n    if len(figs) == 1:\n        return figs[0]\n    return figs\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.plot_categories_by_topics_heatmap","title":"<code>plot_categories_by_topics_heatmap(categories: list[str], output_path: Path | str = None, target_labels: list[str] = None, num_keys: int = 5, figsize: Optional[tuple[int, int]] = None, font_scale: Optional[float] = 1.2, cmap: Optional[ColorType] = sns.cm.rocket_r, show: Optional[bool] = True, title: Optional[str] = None, topic_distributions: Optional[list[list[float]]] = None) -&gt; Figure</code>","text":"<p>Plot heatmap showing topics by category.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>list[str]</code> <p>The categories to use to classify topics.</p> required <code>output_path</code> <code>Path | str</code> <p>The path to save the figure.</p> <code>None</code> <code>target_labels</code> <code>list[str]</code> <p>Unique labels for categories to classify.</p> <code>None</code> <code>num_keys</code> <code>int</code> <p>The number of keywords to display.</p> <code>5</code> <code>figsize</code> <code>Optional[tuple[int, int]]</code> <p>(Optional[tuple[int, int]]): The dimensions of the figure.</p> <code>None</code> <code>font_scale</code> <code>Optional[float]</code> <p>The font scale for the figure.</p> <code>1.2</code> <code>cmap</code> <code>Optional[ColorType]</code> <p>The colormap to use for the heatmap. A matplotlib colormap name or object, or list of colors.</p> <code>rocket_r</code> <code>show</code> <code>Optional[bool]</code> <p>Whether to show the figure.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>Optional title for the figure. If not supplied, defaults to \"Topics by Category (N=x)\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Figure</code> <code>Figure</code> <p>The heatmap showing the topic associations by category.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef plot_categories_by_topics_heatmap(\n    self,\n    categories: list[str],\n    output_path: Path | str = None,\n    target_labels: list[str] = None,\n    num_keys: int = 5,\n    figsize: Optional[tuple[int, int]] = None,\n    font_scale: Optional[float] = 1.2,\n    cmap: Optional[ColorType] = sns.cm.rocket_r,\n    show: Optional[bool] = True,\n    title: Optional[str] = None,\n    topic_distributions: Optional[list[list[float]]] = None,\n) -&gt; Figure:\n    \"\"\"Plot heatmap showing topics by category.\n\n    Args:\n        categories (list[str]): The categories to use to classify topics.\n        output_path (Path | str): The path to save the figure.\n        target_labels (list[str]): Unique labels for categories to classify.\n        num_keys (int): The number of keywords to display.\n        figsize: (Optional[tuple[int, int]]): The dimensions of the figure.\n        font_scale (Optional[float]): The font scale for the figure.\n        cmap (Optional[ColorType]): The colormap to use for the heatmap. A matplotlib colormap name or object, or list of colors.\n        show (Optional[bool]): Whether to show the figure.\n        title (Optional[str]): Optional title for the figure. If not supplied, defaults to \"Topics by Category (N=x)\".\n\n    Returns:\n        Figure: The heatmap showing the topic associations by category.\n    \"\"\"\n    # Load topic_keys\n    topic_keys = self.topic_keys\n\n    # Use user-provided topic_distributions if given, else default to self.distributions\n    distributions = (\n        topic_distributions\n        if topic_distributions is not None\n        else self.distributions\n    )\n\n    dicts_to_plot = []\n    for _category_label, _distribution in zip(categories, distributions):\n        if not target_labels or _category_label in target_labels:\n            for _topic, _probability in enumerate(_distribution):\n                keywords = \" \".join(topic_keys[_topic][2].split()[:num_keys])\n                if num_keys:\n                    if keywords:\n                        _topic_label = f\"Topic {_topic}: {keywords}\"\n                    else:\n                        _topic_label = f\"Topic {_topic}\"\n                else:\n                    _topic_label = f\"Topic {_topic}\"\n                dicts_to_plot.append(\n                    {\n                        \"Probability\": float(_probability),\n                        \"Category\": _category_label,\n                        \"Topic\": _topic_label,\n                    }\n                )\n\n    # Create a dataframe, format it for the heatmap function, and normalize the columns.\n    df_to_plot = pd.DataFrame(dicts_to_plot)\n    df_wide = df_to_plot.pivot_table(\n        index=\"Category\", columns=\"Topic\", values=\"Probability\"\n    )\n    df_norm_col = (df_wide - df_wide.mean()) / df_wide.std()\n\n    # Ensure the columns are ordered by numeric topic index where available (natural sort)\n    def _topic_key(col):\n        # Match 'Topic &lt;num&gt;' possibly followed by ': ...'\n        try:\n            m = re.match(r\"Topic\\s+(\\d+)\", str(col))\n            if m:\n                return (0, int(m.group(1)))\n        except Exception:\n            pass\n        return (1, str(col))\n\n    try:\n        ordered_cols = sorted(list(df_norm_col.columns), key=_topic_key)\n        df_norm_col = df_norm_col[ordered_cols]\n    except Exception:\n        # If columns are not iterable or sorting fails (e.g., custom objects),\n        # we leave the DataFrame as-is rather than raising an exception.\n        pass\n\n    # Show the final plot\n    sns.set_theme(style=\"ticks\", font_scale=font_scale)\n    if figsize:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig, ax = plt.subplots()\n    ax = sns.heatmap(df_norm_col, cmap=cmap, ax=ax)\n    # Set either provided title or a sensible default that indicates the content and the number of topics\n    if title is None:\n        try:\n            num_topics = len(df_norm_col.columns)\n        except Exception:\n            num_topics = None\n        if num_topics is not None:\n            title = f\"Topics by Category ({num_topics} Topics)\"\n        else:\n            title = \"Topics by Category\"\n    if title:\n        fig.suptitle(title)\n    ax.xaxis.tick_top()\n    ax.xaxis.set_label_position(\"top\")\n    plt.xticks(rotation=30, ha=\"left\")\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    if output_path:\n        plt.savefig(output_path)\n    if show:\n        plt.show()\n        return None\n    else:\n        plt.close()\n        return fig\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.plot_topics_over_time","title":"<code>plot_topics_over_time(times: list, topic_index: int, topic_distributions: Optional[list[list[float]]] = None, topic_keys: Optional[list[list[str]]] = None, output_path: Optional[str] = None, figsize: Optional[tuple[int, int]] = (7, 2.5), font_scale: Optional[float] = 1.2, color: Optional[ColorType] = 'cornflowerblue', show: Optional[bool] = True, title: Optional[str] = None) -&gt; Figure | None</code>","text":"<p>Plot the probability of a topic over time.</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>list</code> <p>List of time points corresponding to each document (must be same length as topic_distributions).</p> required <code>topic_index</code> <code>int</code> <p>The index of the topic to plot.</p> required <code>topic_distributions</code> <code>Optional[list[list[float]]]</code> <p>If provided, a list of topic distributions per document. If None, uses <code>self.distributions</code>.</p> <code>None</code> <code>topic_keys</code> <code>Optional[list[list[str]]]</code> <p>If provided, a list of topic keys; otherwise uses <code>self.topic_keys</code>.</p> <code>None</code> <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output plot. If None the plot is shown but not saved.</p> <code>None</code> <code>figsize</code> <code>Optional[tuple[int, int]]</code> <p>Figure size.</p> <code>(7, 2.5)</code> <code>font_scale</code> <code>Optional[float]</code> <p>Seaborn font_scale.</p> <code>1.2</code> <code>color</code> <code>Optional[ColorType]</code> <p>Line color.</p> <code>'cornflowerblue'</code> <code>show</code> <code>Optional[bool]</code> <p>Whether to display the figure.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>Optional figure title. Will default to the topic's keywords if not supplied.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure | None</code> <p>Figure | None: The matplotlib figure if <code>show=False</code>, otherwise None.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef plot_topics_over_time(\n    self,\n    times: list,\n    topic_index: int,\n    topic_distributions: Optional[list[list[float]]] = None,\n    topic_keys: Optional[list[list[str]]] = None,\n    output_path: Optional[str] = None,\n    figsize: Optional[tuple[int, int]] = (7, 2.5),\n    font_scale: Optional[float] = 1.2,\n    color: Optional[ColorType] = \"cornflowerblue\",\n    show: Optional[bool] = True,\n    title: Optional[str] = None,\n) -&gt; Figure | None:\n    \"\"\"Plot the probability of a topic over time.\n\n    Args:\n        times (list): List of time points corresponding to each document (must be same length as topic_distributions).\n        topic_index (int): The index of the topic to plot.\n        topic_distributions (Optional[list[list[float]]]): If provided, a list of topic distributions per document. If None, uses `self.distributions`.\n        topic_keys (Optional[list[list[str]]]): If provided, a list of topic keys; otherwise uses `self.topic_keys`.\n        output_path (Optional[str]): Path to save the output plot. If None the plot is shown but not saved.\n        figsize (Optional[tuple[int,int]]): Figure size.\n        font_scale (Optional[float]): Seaborn font_scale.\n        color (Optional[ColorType]): Line color.\n        show (Optional[bool]): Whether to display the figure.\n        title (Optional[str]): Optional figure title. Will default to the topic's keywords if not supplied.\n\n    Returns:\n        Figure | None: The matplotlib figure if `show=False`, otherwise None.\n    \"\"\"\n    # Use provided distributions / keys or fall back to instance data\n    distributions = (\n        topic_distributions\n        if topic_distributions is not None\n        else self.distributions\n    )\n    topic_keys = topic_keys if topic_keys is not None else self.topic_keys\n\n    if distributions is None or len(distributions) == 0:\n        raise LexosException(\"No topic distributions available to plot.\")\n\n    if topic_index &lt; 0:\n        raise ValueError(\"topic_index must be a non-negative integer\")\n\n    if len(times) != len(distributions):\n        raise LexosException(\n            \"Length mismatch: 'times' must be the same length as topic_distributions\"\n        )\n\n    data_dicts = []\n    for j, _distribution in enumerate(distributions):\n        if len(_distribution) &lt;= topic_index:\n            # skip documents that don't cover the requested topic index\n            continue\n        data_dicts.append(\n            {\"Probability\": _distribution[topic_index], \"Time\": times[j]}\n        )\n\n    if len(data_dicts) == 0:\n        raise LexosException(f\"No data found for topic index {topic_index}\")\n\n    data_df = pd.DataFrame(data_dicts)\n\n    sns.set_theme(style=\"ticks\", font_scale=font_scale)\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.lineplot(data=data_df, x=\"Time\", y=\"Probability\", color=color, ax=ax)\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Topic Probability\")\n\n    # Default title\n    if title is None:\n        try:\n            keywords = \" \".join(topic_keys[topic_index][2].split()[:5])\n            title = f\"Topic {topic_index}: {keywords}\"\n        except Exception:\n            title = f\"Topic {topic_index}\"\n    if title:\n        fig.suptitle(title)\n\n    plt.tight_layout()\n    sns.despine()\n    if output_path:\n        fig.savefig(output_path)\n    if show:\n        plt.show()\n        return None\n    else:\n        return fig\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.topic_clouds","title":"<code>topic_clouds(topics: Optional[int | list[int]] = None, max_terms: Optional[int] = 30, figsize: Optional[tuple[int, int]] = (10, 10), output_path: Optional[str] = None, show: Optional[bool] = True, round_mask: Any = True, title: Optional[str] = None, **kwargs: Any) -&gt; Figure</code>","text":"<p>Get a <code>MultiCloud</code> object for the topic-term distributions.</p> <p>This method converts the internal topic-term probability dictionary to a DataFrame (topics as rows) and constructs a <code>lexos.visualization.cloud.MultiCloud</code> instance for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>Optional[int | list[int]]</code> <p>Topics to include (rows). If None, show all.</p> <code>None</code> <code>max_terms</code> <code>Optional[int]</code> <p>Maximum number of top keywords to display per topic. Maps to the <code>limit</code> parameter of <code>MultiCloud</code> and <code>max_words</code> in <code>opts</code> when not set.</p> <code>30</code> <code>figsize</code> <code>Optional[tuple[int, int]]</code> <p>Size of the overall figure.</p> <code>(10, 10)</code> <code>output_path</code> <code>Optional[str]</code> <p>If provided, the MultiCloud figure will be saved to this path.</p> <code>None</code> <code>show</code> <code>Optional[bool]</code> <p>If True, the figure will be displayed in the current environment.</p> <code>True</code> <code>round_mask</code> <code>bool | int | str</code> <p>Either a boolean indicating whether to use a default circular mask (True maps to radius 120; False disables mask), or an integer radius to use for a custom mask. Strings containing integer values will be converted. Passing invalid values will raise a <code>LexosException</code>.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>Optional title for the overall MultiCloud figure. If None, a default of \"Topic Clouds (N topics)\" will be used.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Use <code>opts</code> to pass wordcloud options for each cloud.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Figure</code> <code>Figure</code> <p>If <code>show</code> is False, returns a Matplotlib Figure object created by <code>MultiCloud</code>.</p> <code>Figure</code> <p>Otherwise returns None after displaying the figure.</p> Notes <p>The labels displayed above each word cloud will be of the form <code>Topic 0</code>, <code>Topic 1</code>, etc.; keywords are not included in the labels to keep the display uncluttered.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef topic_clouds(\n    self,\n    topics: Optional[int | list[int]] = None,\n    max_terms: Optional[int] = 30,\n    figsize: Optional[tuple[int, int]] = (10, 10),\n    output_path: Optional[str] = None,\n    show: Optional[bool] = True,\n    round_mask: Any = True,\n    title: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Figure:\n    \"\"\"Get a `MultiCloud` object for the topic-term distributions.\n\n    This method converts the internal topic-term probability dictionary\n    to a DataFrame (topics as rows) and constructs a `lexos.visualization.cloud.MultiCloud`\n    instance for visualization.\n\n    Parameters:\n        topics (Optional[int | list[int]]): Topics to include (rows). If None, show all.\n        max_terms (Optional[int]): Maximum number of top keywords to display per topic. Maps\n            to the `limit` parameter of `MultiCloud` and `max_words` in `opts` when not set.\n        figsize (Optional[tuple[int, int]]): Size of the overall figure.\n        output_path (Optional[str]): If provided, the MultiCloud figure will be saved to this path.\n        show (Optional[bool]): If True, the figure will be displayed in the current environment.\n        round_mask (bool|int|str): Either a boolean indicating whether to use a default circular mask\n            (True maps to radius 120; False disables mask), or an integer radius to use for a custom\n            mask. Strings containing integer values will be converted. Passing invalid values will\n            raise a `LexosException`.\n        title (Optional[str]): Optional title for the overall MultiCloud figure. If None, a default\n            of \"Topic Clouds (N topics)\" will be used.\n        **kwargs (Any): Additional keyword arguments. Use `opts` to pass wordcloud options for each cloud.\n\n    Returns:\n        Figure: If `show` is False, returns a Matplotlib Figure object created by `MultiCloud`.\n        Otherwise returns None after displaying the figure.\n\n    Notes:\n        The labels displayed above each word cloud will be of the form `Topic 0`,\n        `Topic 1`, etc.; keywords are not included in the labels to keep the\n        display uncluttered.\n    \"\"\"\n    sns.set_theme()\n\n    # Load topic-term probabilities and convert to DataFrame with topics as rows\n    topic_term_probability_dict = self.load_topic_term_distributions()\n    df = pd.DataFrame.from_dict(topic_term_probability_dict, orient=\"index\").fillna(\n        0\n    )\n\n    # Filter the DataFrame to include only the specified topics (rows)\n    if topics is not None:\n        df = df.iloc[ensure_list(topics)]\n\n    # Build options dict for MultiCloud\n    opts = kwargs.get(\"opts\", {})\n    # Default to a white background unless overridden\n    opts.setdefault(\"background_color\", \"white\")\n    # Ensure `max_words` is present if not provided, mapping from max_terms\n    if \"max_words\" not in opts and max_terms is not None:\n        opts[\"max_words\"] = max_terms\n\n    # Convert round_mask boolean or int into the radius integer expected by MultiCloud\n    if isinstance(round_mask, bool):\n        round_radius = 120 if round_mask else 0\n    else:\n        try:\n            round_radius = int(round_mask) if round_mask is not None else 0\n        except Exception:\n            raise LexosException(\n                \"Invalid `round_mask` argument: expected a boolean or integer radius.\"\n            )\n\n    # Build simple numeric labels for each topic to avoid clutter\n    labels = [f\"Topic {i}\" for i in range(len(df))]\n\n    # Build figure_opts forwarding and set a white facecolor by default\n    figure_opts = kwargs.get(\"figure_opts\", {})\n    figure_opts.setdefault(\"facecolor\", \"white\")\n\n    # Create the MultiCloud object with updated args compatible with the class\n    # If no explicit title supplied, create a helpful default\n    if title is None:\n        try:\n            num_topics = len(df)\n        except Exception:\n            num_topics = None\n        if num_topics is not None:\n            title = f\"Topic Clouds ({num_topics} topics)\"\n        else:\n            title = \"Topic Clouds\"\n\n    mc = MultiCloud(\n        data=df,\n        limit=max_terms,\n        figsize=figsize,\n        opts=opts,\n        round=round_radius,\n        labels=labels,\n        figure_opts=figure_opts,\n        title=title,\n    )\n\n    # Save the file if requested\n    if output_path:\n        mc.save(output_path)\n\n    # Show the file if requested\n    if show:\n        mc.show()\n        return None\n    else:\n        return mc.fig\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet.train","title":"<code>train(num_topics: int = 20, num_iterations: Optional[int] = 100, optimize_interval: Optional[int] = 10, verbose: Optional[bool] = True, path_to_model: Optional[str] = None, path_to_state: Optional[str] = None, path_to_topic_keys: Optional[str] = None, path_to_topic_distributions: Optional[str] = None, path_to_term_weights: Optional[str] = None, path_to_diagnostics: Optional[str] = None, path_to_inferencer: Optional[str] = None) -&gt; None</code>","text":"<p>Train the topic model using MALLET.</p> <p>Parameters:</p> Name Type Description Default <code>num_topics</code> <code>int</code> <p>The number of topics to train.</p> <code>20</code> <code>num_iterations</code> <code>int</code> <p>The number of iterations to train for.</p> <code>100</code> <code>optimize_interval</code> <code>int</code> <p>The interval at which to optimize the model.</p> <code>10</code> <code>verbose</code> <code>bool</code> <p>Whether to print the MALLET output.</p> <code>True</code> <code>path_to_inferencer</code> <code>Optional[str]</code> <p>Optional output filename for saving a trained inferencer object that can be used with <code>mallet infer-topics</code>. If not provided, defaults to <code>model_dir/inferencer.mallet</code>.</p> <code>None</code> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>@validate_call(config=model_config)\ndef train(\n    self,\n    num_topics: int = 20,\n    num_iterations: Optional[int] = 100,\n    optimize_interval: Optional[int] = 10,\n    verbose: Optional[bool] = True,\n    # Common output paths: caller may pass canonical keys or path_to_* names\n    path_to_model: Optional[str] = None,\n    path_to_state: Optional[str] = None,\n    path_to_topic_keys: Optional[str] = None,\n    path_to_topic_distributions: Optional[str] = None,\n    path_to_term_weights: Optional[str] = None,\n    path_to_diagnostics: Optional[str] = None,\n    path_to_inferencer: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Train the topic model using MALLET.\n\n    Args:\n        num_topics (int): The number of topics to train.\n        num_iterations (int): The number of iterations to train for.\n        optimize_interval (int): The interval at which to optimize the model.\n        verbose (bool): Whether to print the MALLET output.\n        path_to_inferencer (Optional[str]): Optional output filename for saving a trained inferencer object\n            that can be used with `mallet infer-topics`. If not provided, defaults to\n            `model_dir/inferencer.mallet`.\n    \"\"\"\n    path_to_formatted_training_data = os.path.join(\n        self.model_dir, \"training_data.mallet\"\n    )\n\n    # Build the MALLET command\n    cmd = f\"{self.path_to_mallet or 'mallet'} train-topics\"\n    flags = {\n        \"input\": path_to_formatted_training_data,\n        \"num-topics\": num_topics,\n        \"num-iterations\": num_iterations,\n        \"output-state\": path_to_state\n        or os.path.join(self.model_dir, \"topic-state.gz\"),\n        \"output-topic-keys\": path_to_topic_keys\n        or os.path.join(self.model_dir, \"topic-keys.txt\"),\n        \"output-doc-topics\": path_to_topic_distributions\n        or os.path.join(self.model_dir, \"doc-topic.txt\"),\n        \"topic-word-weights-file\": path_to_term_weights\n        or os.path.join(self.model_dir, \"topic-weights.txt\"),\n        \"diagnostics-file\": path_to_diagnostics\n        or os.path.join(self.model_dir, \"diagnostics.xml\"),\n        # Optional inferencer filename path to save a trained inferencer for later inference\n        \"inferencer-filename\": path_to_inferencer\n        or os.path.join(self.model_dir, \"inferencer.mallet\"),\n        \"optimize-interval\": optimize_interval,\n    }\n\n    for k, v in flags.items():\n        if v:\n            # Save file names in the model directory if they are not absolute paths\n            if isinstance(v, str) and len(Path(v).parts) == 1:\n                v = f\"{self.metadata['model_directory']}/{v}\"\n            cmd += f\" --{k} {v}\"\n            # Set canonical metadata keys for common outputs so consumers can\n            # rely on a single key. Map train flags directly to the\n            # canonical metadata keys.\n            if k == \"output-doc-topics\":\n                self.metadata[self.CANONICAL_DOC_TOPIC_KEY] = v\n            if k == \"topic-word-weights-file\":\n                self.metadata[self.CANONICAL_TERM_WEIGHTS_KEY] = v\n            if k == \"output-topic-keys\":\n                self.metadata[self.CANONICAL_TOPIC_KEYS_KEY] = v\n            if k == \"inferencer-filename\":\n                self.metadata[self.CANONICAL_INFERENCER_KEY] = v\n\n    # Train the model\n    msg.good(\"Training topics...\")\n    self._track_progress(cmd, num_iterations, verbose)\n    self.metadata[\"num_topics\"] = num_topics\n    self.metadata[\"optimize_interval\"] = optimize_interval\n    # For flags we don't have a canonical mapping for, provide a path_to_ entry\n    # to preserve other easily accessible metadata entries. Do not set legacy\n    # keys when we are mapping to a canonical key.\n    mapping = {\n        \"output-doc-topics\": self.CANONICAL_DOC_TOPIC_KEY,\n        \"topic-word-weights-file\": self.CANONICAL_TERM_WEIGHTS_KEY,\n        \"output-topic-keys\": self.CANONICAL_TOPIC_KEYS_KEY,\n        \"inferencer-filename\": self.CANONICAL_INFERENCER_KEY,\n    }\n    for k, v in flags.items():\n        if k not in [\"num-topics\", \"optimize-interval\"]:\n            if k in mapping:\n                # canonical keys already set earlier in the loop\n                continue\n            self.metadata[f\"path_to_{k.replace('-', '_')}\"] = v\n    self.metadata[\"training_command\"] = cmd\n    msg.good(\"Complete\")\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet._metadata_get","title":"<code>_metadata_get(keys: list[str]) -&gt; str | None</code>","text":"<p>Return the first metadata value present among the provided keys or None.</p> <p>The method assumes callers pass canonical key names; no synonym translation is performed.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>def _metadata_get(self, keys: list[str]) -&gt; str | None:\n    \"\"\"Return the first metadata value present among the provided keys or None.\n\n    The method assumes callers pass canonical key names; no synonym\n    translation is performed.\n    \"\"\"\n    # Only accept the canonical key for each category. If a synonym key is\n    # present (legacy metadata), raise an error instructing users to use\n    # the canonical key. This ensures a single canonical name per category.\n    for k in keys:\n        if k in self.metadata and self.metadata[k]:\n            return self.metadata[k]\n    return None\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet._metadata_has","title":"<code>_metadata_has(keys: list[str]) -&gt; bool</code>","text":"Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>def _metadata_has(self, keys: list[str]) -&gt; bool:\n    return self._metadata_get(keys) is not None\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet._import_training_data","title":"<code>_import_training_data(training_data: list[str], path_to_training_data: Optional[str] = None, keep_sequence: bool = True, remove_stopwords: bool = True, preserve_case: bool = True, use_pipe_from: Optional[str] = None, training_ids: Optional[list[int]] = None) -&gt; None</code>","text":"<p>Import training data from a list of documents.</p> <p>Parameters:</p> Name Type Description Default <code>training_data</code> <code>list[str]</code> <p>A list of documents to import.</p> required <code>keep_sequence</code> <code>bool</code> <p>Whether to keep the word sequence in the documents.</p> <code>True</code> <code>remove_stopwords</code> <code>bool</code> <p>Whether to remove stopwords from the documents.</p> <code>True</code> <code>preserve_case</code> <code>bool</code> <p>Whether to preserve the case of the documents.</p> <code>True</code> <code>use_pipe_from</code> <code>Optional[str]</code> <p>Path to a MALLET pipe file to use for importing.</p> <code>None</code> <code>training_ids</code> <code>Optional[list[int]]</code> <p>Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.</p> <code>None</code> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>def _import_training_data(\n    self,\n    training_data: list[str],\n    path_to_training_data: Optional[str] = None,\n    keep_sequence: bool = True,\n    remove_stopwords: bool = True,\n    preserve_case: bool = True,\n    use_pipe_from: Optional[str] = None,\n    training_ids: Optional[list[int]] = None,\n) -&gt; None:\n    \"\"\"Import training data from a list of documents.\n\n    Args:\n        training_data (list[str]): A list of documents to import.\n        keep_sequence (bool): Whether to keep the word sequence in the documents.\n        remove_stopwords (bool): Whether to remove stopwords from the documents.\n        preserve_case (bool): Whether to preserve the case of the documents.\n        use_pipe_from (Optional[str]): Path to a MALLET pipe file to use for importing.\n        training_ids: Optional[list[int]]: A list of document ids designating a subset of the entire data set. If None, the entire dataset will be imported.\n    \"\"\"\n    # Save the training data file\n    path_to_training_data = (\n        path_to_training_data\n        if path_to_training_data is not None\n        else os.path.join(self.model_dir, \"training_data.txt\")\n    )\n    path_to_formatted_training_data = os.path.join(\n        self.model_dir, \"training_data.mallet\"\n    )\n    training_data_file = open(path_to_training_data, \"w\", encoding=\"utf-8\")\n    for i, doc in enumerate(training_data):\n        # Remove newlines and carriage returns from the document\n        doc = re.sub(\"[\\r\\n]+\", \" \", doc).strip()\n        if training_ids:\n            training_data_file.write(f\"{training_ids[i]}\\tno_label\\t{doc}\\n\")\n        else:\n            training_data_file.write(f\"{i}\\tno_label\\t {doc}\\n\")\n    training_data_file.close()\n    self.metadata[\"path_to_training_data\"] = path_to_training_data\n    self.metadata[\"path_to_formatted_training_data\"] = (\n        path_to_formatted_training_data\n    )\n    self.metadata[\"num_docs\"] = len(training_data)\n    # WARNING: Tokenisation relies on whitespace, so it may not be accurate for all languages\n    self.metadata[\"mean_num_tokens\"] = np.mean(\n        [len(doc.split()) for doc in training_data]\n    ).item()\n    self.metadata[\"vocab_size\"] = len(\n        list(set([token for doc in training_data for token in doc.split()]))\n    )\n\n    # Build and execute the command to format the training data for MALLET\n    cmd = f\"{self.path_to_mallet or 'mallet'} import-file --input {path_to_training_data} --output {path_to_formatted_training_data}\"\n    if keep_sequence:\n        cmd += \" --keep-sequence\"\n    if remove_stopwords:\n        cmd += \" --remove-stopwords\"\n    if preserve_case:\n        cmd += \" --preserve-case\"\n    if use_pipe_from:\n        cmd += f\" --use-pipe-from {use_pipe_from}\"\n    msg.info(cmd)\n    os.system(cmd)\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet._setup_wordcloud","title":"<code>_setup_wordcloud(round_mask, max_terms, **kwargs: dict[str, Any]) -&gt; WordCloud</code>","text":"<p>Set up the word cloud object.</p> <p>Parameters:</p> Name Type Description Default <code>round_mask</code> <code>bool</code> <p>Whether to use a round mask for the word cloud.</p> required <code>max_terms</code> <code>int</code> <p>The maximum number of keywords to display.</p> required <code>**kwargs</code> <code>dict[str, Any])</code> <p>Additional keyword arguments for the WordCloud object.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>WordCloud</code> <code>WordCloud</code> <p>A configured WordCloud object.</p> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>def _setup_wordcloud(\n    self, round_mask, max_terms, **kwargs: dict[str, Any]\n) -&gt; WordCloud:\n    \"\"\"Set up the word cloud object.\n\n    Args:\n        round_mask (bool): Whether to use a round mask for the word cloud.\n        max_terms (int): The maximum number of keywords to display.\n        **kwargs (dict[str, Any])): Additional keyword arguments for the WordCloud object.\n\n    Returns:\n        WordCloud: A configured WordCloud object.\n    \"\"\"\n    # Define a mask to make the word cloud round (just some eye candy)\n    if round_mask:\n        x, y = np.ogrid[:300, :300]\n        mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; 130**2\n        mask = 255 * mask.astype(int)\n    else:\n        mask = None\n\n    # Configure the word cloud object\n    options = {\n        \"background_color\": \"white\",\n        \"mask\": mask,\n        \"contour_width\": 0.1,\n        \"contour_color\": \"white\",\n        \"max_words\": max_terms,\n        \"min_font_size\": 10,\n        \"max_font_size\": 150,\n        \"random_state\": 42,\n        \"colormap\": \"Dark2\",\n    }\n    for k, v in kwargs.items():\n        options[k] = v\n\n    return WordCloud(**options)\n</code></pre>"},{"location":"api/topic_modeling/mallet/#lexos.topic_modeling.mallet.Mallet._track_progress","title":"<code>_track_progress(mallet_cmd: str, num_iterations: int, verbose: bool = True) -&gt; None</code>","text":"<p>Track the progress of the modeling.</p> <p>Parameters:</p> Name Type Description Default <code>mallet_cmd</code> <code>str</code> <p>The MALLET command to run.</p> required <code>num_iterations</code> <code>int</code> <p>The number of iterations for the model.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print the MALLET output.</p> <code>True</code> Notes <ul> <li>Prints MALLET output and updates the progress bar in 10% increments.</li> </ul> Source code in <code>lexos/topic_modeling/mallet/__init__.py</code> <pre><code>def _track_progress(\n    self, mallet_cmd: str, num_iterations: int, verbose: bool = True\n) -&gt; None:\n    \"\"\"Track the progress of the modeling.\n\n    Args:\n        mallet_cmd (str): The MALLET command to run.\n        num_iterations (int): The number of iterations for the model.\n        verbose (bool): Whether to print the MALLET output.\n\n    Notes:\n        - Prints MALLET output and updates the progress bar in 10% increments.\n    \"\"\"\n    console = Console()\n    # NOTE: This is a hack to make Jupyter notebooks in VS Code display all lines\n    # in the same cell. It may cause undesirable results in other environments and\n    # needs further testing. See https://github.com/Textualize/rich/issues/3483.\n    if verbose:\n        console.is_jupyter = False\n\n    # Create a progress display with rich\n    with Progress(\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        TaskProgressColumn(),\n        TimeElapsedColumn(),\n    ) as progress:\n        # Create a task with a total of 100 (percentage)\n        task = progress.add_task(\"[blue]Training model...\", total=100)\n\n        # Run the MALLET command\n        p = subprocess.Popen(\n            mallet_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n        )\n\n        # Regex to match progress information\n        prog = re.compile(r\"\\&lt;([^\\)]+)\\&gt;\")\n\n        # Track the last reported progress percentage to avoid duplicate updates\n        last_progress = -1\n\n        # Process the output line by line\n        while p.poll() is None:\n            line = p.stdout.readline().decode()\n            if verbose:\n                # Print MALLET output without disrupting progress\n                console.print(line, end=\"\")\n\n            # Keep track of modeling progress\n            try:\n                # A float indicating the percentage, which is output by MALLET\n                this_iter = float(prog.match(line).groups()[0])\n                current_progress = int(100.0 * this_iter / num_iterations)\n\n                # Only update on 10% multiples and avoid duplicate updates\n                if current_progress % 10 == 0 and current_progress &gt; last_progress:\n                    # Update to the current progress percentage\n                    progress.update(task, completed=this_iter)\n                    last_progress = current_progress\n                if current_progress == 100:\n                    progress.update(\n                        task, description=\"[green]Complete\", completed=100\n                    )\n            except AttributeError:  # Not every line will match.\n                pass\n</code></pre>"},{"location":"api/visualization/","title":"Visualization","text":"<p>The Lexos <code>visualization</code> module is a container for assorted submodules which typically generate visualizations of document-term matrices. The following submodules are included:</p>"},{"location":"api/visualization/#bubbleviz","title":"<code>bubbleviz</code>","text":"<p>The <code>bubbleviz</code> module produces bubble visualizations, also known as bubble charts using <code>matplotlib</code>.</p>"},{"location":"api/visualization/#cloud","title":"<code>cloud</code>","text":"<p>The <code>cloud</code> module produces traditional word clouds using the Python <code>WordCloud</code> package and <code>matplotlib</code>. It also produces \"multclouds\" -- word clouds of multiple documents laid out in a grid.</p>"},{"location":"api/visualization/#processors","title":"<code>processors</code>","text":"<p>The <code>processors</code> module contains helper functions for processing input data in various formats. Its functions are shared by the other two modules.</p>"},{"location":"api/visualization/bubbleviz/","title":"Bubble Visualizations","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart","title":"<code>BubbleChart</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bubble chart.</p> <p>Notes: - If the counts are sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>data</code>                 (<code>Optional[single_doc_types | multi_doc_types | DataFrame]</code>)             </li> <li> <code>docs</code>                 (<code>Optional[int | str | list[int] | list[str]]</code>)             </li> <li> <code>limit</code>                 (<code>Optional[int]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>bubble_spacing</code>                 (<code>Optional[float | int]</code>)             </li> <li> <code>colors</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>figsize</code>                 (<code>Optional[int | float]</code>)             </li> <li> <code>font_family</code>                 (<code>Optional[str]</code>)             </li> <li> <code>showfig</code>                 (<code>Optional[bool]</code>)             </li> <li> <code>bubbles</code>                 (<code>Optional[ndarray]</code>)             </li> <li> <code>maxstep</code>                 (<code>Optional[int]</code>)             </li> <li> <code>step_dist</code>                 (<code>Optional[int]</code>)             </li> <li> <code>com</code>                 (<code>Optional[int]</code>)             </li> <li> <code>counts</code>                 (<code>dict[str, int]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>is_not_empty</code>                 \u2192                   <code>data</code> </li> </ul> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>class BubbleChart(BaseModel):\n    \"\"\"Bubble chart.\n\n    Notes:\n    - If the counts are sorted, the results might look weird.\n    - If \"limit\" is raised too high, it will take a long time to generate the plot\n    - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html.\n    \"\"\"\n\n    data: Optional[single_doc_types | multi_doc_types | pd.DataFrame] = Field(\n        description=\"The data to plot.\"\n    )\n    docs: Optional[int | str | list[int] | list[str]] = Field(\n        None, description=\"The document indices or labels to plot.\"\n    )\n    limit: Optional[int] = Field(\n        100, description=\"The maximum number of bubbles to plot.\"\n    )\n    title: Optional[str] = Field(None, description=\"The title of the plot.\")\n    bubble_spacing: Optional[float | int] = Field(\n        0.1, description=\"The spacing between bubbles.\"\n    )\n    colors: Optional[list[str]] = Field(\n        DEFAULT_COLORS, description=\"The colors of the bubbles.\"\n    )\n    figsize: Optional[int | float] = Field(\n        10, description=\"The size of the figure in inches.\"\n    )\n    font_family: Optional[str] = Field(\n        \"DejaVu Sans\", description=\"The font family of the plot.\"\n    )\n    showfig: Optional[bool] = Field(True, description=\"Whether to show the plot.\")\n    bubbles: Optional[np.ndarray] = Field(None, description=\"The bubbles.\")\n    maxstep: Optional[int] = Field(None, description=\"The maximum step.\")\n    step_dist: Optional[int] = Field(None, description=\"The step distance.\")\n    com: Optional[int] = Field(None, description=\"The center of mass.\")\n    counts: dict[str, int] = Field({}, description=\"A dictionary of word counts.\")\n    fig: Optional[plt.Figure] = Field(\n        None, description=\"The figure for the bubble chart.\"\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    @field_validator(\"data\", mode=\"after\")\n    @classmethod\n    def is_not_empty(cls, value: Any) -&gt; Any:\n        \"\"\"Check if the value is not empty.\"\"\"\n        if isinstance(value, pd.DataFrame):\n            if value.empty:\n                raise LexosException(\"Dataframe is empty.\")\n            return value\n\n        if value == \"\" or value == [] or value == {}:\n            raise LexosException(\"Data is an empty list or string.\")\n        return value\n\n    def __init__(self, **data):\n        \"\"\"Initialize the BubbleChart with the provided data.\"\"\"\n        super().__init__(**data)\n\n        # Process different data types to get individual document data\n        self.counts = processors.process_data(self.data, self.docs, self.limit)\n\n        # Set the figure dimensions\n        self.figsize = (self.figsize, self.figsize)\n\n        # Reduce the area to the limited number of terms\n        area = np.asarray(list(self.counts.values()))\n        r = np.sqrt(area / np.pi)\n\n        self.bubbles = np.ones((len(area), 4))\n        self.bubbles[:, 2] = r\n        self.bubbles[:, 3] = area\n        self.maxstep = 2 * self.bubbles[:, 2].max() + self.bubble_spacing\n        self.step_dist = self.maxstep / 2\n\n        # Calculate initial grid layout for bubbles\n        length = np.ceil(np.sqrt(len(self.bubbles)))\n        grid = np.arange(length) * self.maxstep\n        gx, gy = np.meshgrid(grid, grid)\n        self.bubbles[:, 0] = gx.flatten()[: len(self.bubbles)]\n        self.bubbles[:, 1] = gy.flatten()[: len(self.bubbles)]\n\n        self.com = self._center_of_mass()\n\n        # Create the figure\n        self._collapse()\n        fig, ax = plt.subplots(subplot_kw=dict(aspect=\"equal\"), figsize=self.figsize)\n        self._plot(ax, list(self.counts.keys()))\n        ax.axis(\"off\")\n        ax.relim()\n        ax.autoscale_view()\n\n        # Add title\n        if self.title:\n            ax.set_title(self.title)\n\n        # Save the fig variable\n        self.fig = fig\n\n        plt.close()\n\n    def _center_distance(self, bubble: np.ndarray, bubbles: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Centre distance.\n\n        Args:\n            bubble (np.ndarray): Bubble array.\n            bubbles (np.ndarray): Bubble array.\n\n        Returns:\n            np.ndarray: The centre distance.\n        \"\"\"\n        return np.hypot(bubble[0] - bubbles[:, 0], bubble[1] - bubbles[:, 1])\n\n    def _center_of_mass(self) -&gt; int:\n        \"\"\"Centre of mass.\n\n        Returns:\n            int: The centre of mass.\n        \"\"\"\n        return np.average(self.bubbles[:, :2], axis=0, weights=self.bubbles[:, 3])\n\n    def _check_collisions(self, bubble: np.ndarray, bubbles: np.ndarray) -&gt; int:\n        \"\"\"Check collisions.\n\n        Args:\n            bubble (np.ndarray): Bubble array.\n            bubbles (np.ndarray): Bubble array.\n\n        Returns:\n            int: The length of the distance between bubbles.\n        \"\"\"\n        distance = self._outline_distance(bubble, bubbles)\n        return len(distance[distance &lt; 0])\n\n    def _collapse(self, n_iterations: int = 50):\n        \"\"\"Move bubbles to the center of mass.\n\n        Args:\n            n_iterations (int): Number of moves to perform.\n        \"\"\"\n        for _i in range(n_iterations):\n            moves = 0\n            for i in range(len(self.bubbles)):\n                rest_bub = np.delete(self.bubbles, i, 0)\n                # Try to move directly towards the center of mass\n                # Direction vector from bubble to the center of mass\n                dir_vec = self.com - self.bubbles[i, :2]\n\n                # Shorten direction vector to have length of 1\n                # NOTE: Produces invalid value encountered in divide Runtime warnings if dir_vec is zero\n                # dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n\n                # Shorten direction vector to have length of 1\n                # Check if direction vector is non-zero to avoid division by zero\n                dir_vec_magnitude = np.sqrt(dir_vec.dot(dir_vec))\n                if dir_vec_magnitude &gt; 0:\n                    dir_vec = dir_vec / dir_vec_magnitude\n                else:\n                    # If bubble is already at center of mass, use a small random direction\n                    dir_vec = np.array([1.0, 0.0]) * self.step_dist * 0.01\n\n                # Calculate new bubble position\n                new_point = self.bubbles[i, :2] + dir_vec * self.step_dist\n                new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n\n                # Check whether new bubble collides with other bubbles\n                if not self._check_collisions(new_bubble, rest_bub):\n                    # NOTE: Produces invalid value encountered in cast Runtime warnings\n                    self.bubbles[i, :] = new_bubble\n                    self.com = self._center_of_mass()\n                    moves += 1\n                else:\n                    # Try to move around a bubble that you collide with\n                    # Find colliding bubble\n                    for colliding in self._collides_with(new_bubble, rest_bub):\n                        # Calculate direction vector\n                        dir_vec = rest_bub[colliding, :2] - self.bubbles[i, :2]\n                        dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n                        # Calculate orthogonal vector\n                        orth = np.array([dir_vec[1], -dir_vec[0]])\n                        # test which direction to go\n                        new_point1 = self.bubbles[i, :2] + orth * self.step_dist\n                        new_point2 = self.bubbles[i, :2] - orth * self.step_dist\n                        dist1 = self._center_distance(self.com, np.array([new_point1]))\n                        dist2 = self._center_distance(self.com, np.array([new_point2]))\n                        new_point = new_point1 if dist1 &lt; dist2 else new_point2\n                        new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n                        if not self._check_collisions(new_bubble, rest_bub):\n                            self.bubbles[i, :] = new_bubble\n                            self.com = self._center_of_mass()\n\n            if moves / len(self.bubbles) &lt; 0.1:\n                self.step_dist = self.step_dist / 2\n\n    def _collides_with(self, bubble: np.ndarray, bubbles: np.ndarray) -&gt; int:\n        \"\"\"Collide.\n\n        Args:\n            bubble (np.ndarray): Bubble array.\n            bubbles (np.ndarray): Bubble array.\n\n        Returns:\n            int: The minimum index.\n        \"\"\"\n        distance = self._outline_distance(bubble, bubbles)\n        idx_min = np.argmin(distance)\n        return idx_min if type(idx_min) is np.ndarray else [idx_min]\n\n    def _outline_distance(self, bubble: np.ndarray, bubbles: np.ndarray) -&gt; int:\n        \"\"\"Outline distance.\n\n        Args:\n            bubble (np.ndarray): Bubble array.\n            bubbles (np.ndarray): Bubble array.\n\n        Returns:\n            int: The outline distance.\n        \"\"\"\n        center_distance = self._center_distance(bubble, bubbles)\n        return center_distance - bubble[2] - bubbles[:, 2] - self.bubble_spacing\n\n    def _plot(\n        self,\n        ax: Axes,\n        labels: list[str],\n    ):\n        \"\"\"Draw the bubble plot.\n\n        Args:\n            ax (Axes): The matplotlib Axes.\n            labels (list[str]): The labels of the bubbles.\n        \"\"\"\n        plt.rcParams[\"font.family\"] = self.font_family\n        color_num = 0\n        for i in range(len(self.bubbles)):\n            if color_num == len(self.colors) - 1:\n                color_num = 0\n            else:\n                color_num += 1\n            circ = plt.Circle(\n                self.bubbles[i, :2], self.bubbles[i, 2], color=self.colors[color_num]\n            )\n            ax.add_patch(circ)\n            ax.text(\n                *self.bubbles[i, :2],\n                labels[i],\n                horizontalalignment=\"center\",\n                verticalalignment=\"center\",\n            )\n\n    @validate_call(config=model_config)\n    def save(self, path: Path | str, **kwargs: Any):\n        \"\"\"Save the figure as a file.\n\n        Args:\n            path (Path | str): The path to the file to save.\n            **kwargs (Any): Additional keyword arguments for `plt.savefig`.\n        \"\"\"\n        if path == \"\":\n            raise LexosException(\"You must provide a valid path.\")\n        if self.fig is None:\n            raise LexosException(\"The figure has not yet been generated.\")\n        self.fig.savefig(path, **kwargs)\n\n    def show(self):\n        \"\"\"Show the figure if it is hidden.\n\n        This is a helper method. You can also reference the figure using\n        `BubbleChart.fig`. This will generally display in a Jupyter notebook.\n        \"\"\"\n        return self.fig\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.bubble_spacing","title":"<code>bubble_spacing: Optional[float | int] = 0.1</code>  <code>pydantic-field</code>","text":"<p>The spacing between bubbles.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.colors","title":"<code>colors: Optional[list[str]] = DEFAULT_COLORS</code>  <code>pydantic-field</code>","text":"<p>The colors of the bubbles.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.data","title":"<code>data: Optional[single_doc_types | multi_doc_types | pd.DataFrame]</code>  <code>pydantic-field</code>","text":"<p>The data to plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.docs","title":"<code>docs: Optional[int | str | list[int] | list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>The document indices or labels to plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.font_family","title":"<code>font_family: Optional[str] = 'DejaVu Sans'</code>  <code>pydantic-field</code>","text":"<p>The font family of the plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.limit","title":"<code>limit: Optional[int] = 100</code>  <code>pydantic-field</code>","text":"<p>The maximum number of bubbles to plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.showfig","title":"<code>showfig: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to show the plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title of the plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the BubbleChart with the provided data.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the BubbleChart with the provided data.\"\"\"\n    super().__init__(**data)\n\n    # Process different data types to get individual document data\n    self.counts = processors.process_data(self.data, self.docs, self.limit)\n\n    # Set the figure dimensions\n    self.figsize = (self.figsize, self.figsize)\n\n    # Reduce the area to the limited number of terms\n    area = np.asarray(list(self.counts.values()))\n    r = np.sqrt(area / np.pi)\n\n    self.bubbles = np.ones((len(area), 4))\n    self.bubbles[:, 2] = r\n    self.bubbles[:, 3] = area\n    self.maxstep = 2 * self.bubbles[:, 2].max() + self.bubble_spacing\n    self.step_dist = self.maxstep / 2\n\n    # Calculate initial grid layout for bubbles\n    length = np.ceil(np.sqrt(len(self.bubbles)))\n    grid = np.arange(length) * self.maxstep\n    gx, gy = np.meshgrid(grid, grid)\n    self.bubbles[:, 0] = gx.flatten()[: len(self.bubbles)]\n    self.bubbles[:, 1] = gy.flatten()[: len(self.bubbles)]\n\n    self.com = self._center_of_mass()\n\n    # Create the figure\n    self._collapse()\n    fig, ax = plt.subplots(subplot_kw=dict(aspect=\"equal\"), figsize=self.figsize)\n    self._plot(ax, list(self.counts.keys()))\n    ax.axis(\"off\")\n    ax.relim()\n    ax.autoscale_view()\n\n    # Add title\n    if self.title:\n        ax.set_title(self.title)\n\n    # Save the fig variable\n    self.fig = fig\n\n    plt.close()\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.is_not_empty","title":"<code>is_not_empty(value: Any) -&gt; Any</code>  <code>pydantic-validator</code>","text":"<p>Check if the value is not empty.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>@field_validator(\"data\", mode=\"after\")\n@classmethod\ndef is_not_empty(cls, value: Any) -&gt; Any:\n    \"\"\"Check if the value is not empty.\"\"\"\n    if isinstance(value, pd.DataFrame):\n        if value.empty:\n            raise LexosException(\"Dataframe is empty.\")\n        return value\n\n    if value == \"\" or value == [] or value == {}:\n        raise LexosException(\"Data is an empty list or string.\")\n    return value\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.save","title":"<code>save(path: Path | str, **kwargs: Any)</code>","text":"<p>Save the figure as a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The path to the file to save.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for <code>plt.savefig</code>.</p> <code>{}</code> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>@validate_call(config=model_config)\ndef save(self, path: Path | str, **kwargs: Any):\n    \"\"\"Save the figure as a file.\n\n    Args:\n        path (Path | str): The path to the file to save.\n        **kwargs (Any): Additional keyword arguments for `plt.savefig`.\n    \"\"\"\n    if path == \"\":\n        raise LexosException(\"You must provide a valid path.\")\n    if self.fig is None:\n        raise LexosException(\"The figure has not yet been generated.\")\n    self.fig.savefig(path, **kwargs)\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.show","title":"<code>show()</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. You can also reference the figure using <code>BubbleChart.fig</code>. This will generally display in a Jupyter notebook.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def show(self):\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. You can also reference the figure using\n    `BubbleChart.fig`. This will generally display in a Jupyter notebook.\n    \"\"\"\n    return self.fig\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.data","title":"<code>data: Optional[single_doc_types | multi_doc_types | pd.DataFrame]</code>  <code>pydantic-field</code>","text":"<p>The data to plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.docs","title":"<code>docs: Optional[int | str | list[int] | list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>The document indices or labels to plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.limit","title":"<code>limit: Optional[int] = 100</code>  <code>pydantic-field</code>","text":"<p>The maximum number of bubbles to plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title of the plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.bubble_spacing","title":"<code>bubble_spacing: Optional[float | int] = 0.1</code>  <code>pydantic-field</code>","text":"<p>The spacing between bubbles.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.colors","title":"<code>colors: Optional[list[str]] = DEFAULT_COLORS</code>  <code>pydantic-field</code>","text":"<p>The colors of the bubbles.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.figsize","title":"<code>figsize: Optional[int | float] = (self.figsize, self.figsize)</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.font_family","title":"<code>font_family: Optional[str] = 'DejaVu Sans'</code>  <code>pydantic-field</code>","text":"<p>The font family of the plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.showfig","title":"<code>showfig: Optional[bool] = True</code>  <code>pydantic-field</code>","text":"<p>Whether to show the plot.</p>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.bubbles","title":"<code>bubbles: Optional[np.ndarray]</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.maxstep","title":"<code>maxstep: Optional[int] = 2 * self.bubbles[:, 2].max() + self.bubble_spacing</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.step_dist","title":"<code>step_dist: Optional[int] = self.maxstep / 2</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.com","title":"<code>com: Optional[int]</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.counts","title":"<code>counts: dict[str, int]</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.fig","title":"<code>fig: Optional[plt.Figure] = fig</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.is_not_empty","title":"<code>is_not_empty(value: Any) -&gt; Any</code>  <code>pydantic-validator</code>","text":"<p>Check if the value is not empty.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>@field_validator(\"data\", mode=\"after\")\n@classmethod\ndef is_not_empty(cls, value: Any) -&gt; Any:\n    \"\"\"Check if the value is not empty.\"\"\"\n    if isinstance(value, pd.DataFrame):\n        if value.empty:\n            raise LexosException(\"Dataframe is empty.\")\n        return value\n\n    if value == \"\" or value == [] or value == {}:\n        raise LexosException(\"Data is an empty list or string.\")\n    return value\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize the BubbleChart with the provided data.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"Initialize the BubbleChart with the provided data.\"\"\"\n    super().__init__(**data)\n\n    # Process different data types to get individual document data\n    self.counts = processors.process_data(self.data, self.docs, self.limit)\n\n    # Set the figure dimensions\n    self.figsize = (self.figsize, self.figsize)\n\n    # Reduce the area to the limited number of terms\n    area = np.asarray(list(self.counts.values()))\n    r = np.sqrt(area / np.pi)\n\n    self.bubbles = np.ones((len(area), 4))\n    self.bubbles[:, 2] = r\n    self.bubbles[:, 3] = area\n    self.maxstep = 2 * self.bubbles[:, 2].max() + self.bubble_spacing\n    self.step_dist = self.maxstep / 2\n\n    # Calculate initial grid layout for bubbles\n    length = np.ceil(np.sqrt(len(self.bubbles)))\n    grid = np.arange(length) * self.maxstep\n    gx, gy = np.meshgrid(grid, grid)\n    self.bubbles[:, 0] = gx.flatten()[: len(self.bubbles)]\n    self.bubbles[:, 1] = gy.flatten()[: len(self.bubbles)]\n\n    self.com = self._center_of_mass()\n\n    # Create the figure\n    self._collapse()\n    fig, ax = plt.subplots(subplot_kw=dict(aspect=\"equal\"), figsize=self.figsize)\n    self._plot(ax, list(self.counts.keys()))\n    ax.axis(\"off\")\n    ax.relim()\n    ax.autoscale_view()\n\n    # Add title\n    if self.title:\n        ax.set_title(self.title)\n\n    # Save the fig variable\n    self.fig = fig\n\n    plt.close()\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart._center_distance","title":"<code>_center_distance(bubble: np.ndarray, bubbles: np.ndarray) -&gt; np.ndarray</code>","text":"<p>Centre distance.</p> <p>Parameters:</p> Name Type Description Default <code>bubble</code> <code>ndarray</code> <p>Bubble array.</p> required <code>bubbles</code> <code>ndarray</code> <p>Bubble array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The centre distance.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def _center_distance(self, bubble: np.ndarray, bubbles: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Centre distance.\n\n    Args:\n        bubble (np.ndarray): Bubble array.\n        bubbles (np.ndarray): Bubble array.\n\n    Returns:\n        np.ndarray: The centre distance.\n    \"\"\"\n    return np.hypot(bubble[0] - bubbles[:, 0], bubble[1] - bubbles[:, 1])\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart._center_of_mass","title":"<code>_center_of_mass() -&gt; int</code>","text":"<p>Centre of mass.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The centre of mass.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def _center_of_mass(self) -&gt; int:\n    \"\"\"Centre of mass.\n\n    Returns:\n        int: The centre of mass.\n    \"\"\"\n    return np.average(self.bubbles[:, :2], axis=0, weights=self.bubbles[:, 3])\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart._check_collisions","title":"<code>_check_collisions(bubble: np.ndarray, bubbles: np.ndarray) -&gt; int</code>","text":"<p>Check collisions.</p> <p>Parameters:</p> Name Type Description Default <code>bubble</code> <code>ndarray</code> <p>Bubble array.</p> required <code>bubbles</code> <code>ndarray</code> <p>Bubble array.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the distance between bubbles.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def _check_collisions(self, bubble: np.ndarray, bubbles: np.ndarray) -&gt; int:\n    \"\"\"Check collisions.\n\n    Args:\n        bubble (np.ndarray): Bubble array.\n        bubbles (np.ndarray): Bubble array.\n\n    Returns:\n        int: The length of the distance between bubbles.\n    \"\"\"\n    distance = self._outline_distance(bubble, bubbles)\n    return len(distance[distance &lt; 0])\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart._collapse","title":"<code>_collapse(n_iterations: int = 50)</code>","text":"<p>Move bubbles to the center of mass.</p> <p>Parameters:</p> Name Type Description Default <code>n_iterations</code> <code>int</code> <p>Number of moves to perform.</p> <code>50</code> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def _collapse(self, n_iterations: int = 50):\n    \"\"\"Move bubbles to the center of mass.\n\n    Args:\n        n_iterations (int): Number of moves to perform.\n    \"\"\"\n    for _i in range(n_iterations):\n        moves = 0\n        for i in range(len(self.bubbles)):\n            rest_bub = np.delete(self.bubbles, i, 0)\n            # Try to move directly towards the center of mass\n            # Direction vector from bubble to the center of mass\n            dir_vec = self.com - self.bubbles[i, :2]\n\n            # Shorten direction vector to have length of 1\n            # NOTE: Produces invalid value encountered in divide Runtime warnings if dir_vec is zero\n            # dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n\n            # Shorten direction vector to have length of 1\n            # Check if direction vector is non-zero to avoid division by zero\n            dir_vec_magnitude = np.sqrt(dir_vec.dot(dir_vec))\n            if dir_vec_magnitude &gt; 0:\n                dir_vec = dir_vec / dir_vec_magnitude\n            else:\n                # If bubble is already at center of mass, use a small random direction\n                dir_vec = np.array([1.0, 0.0]) * self.step_dist * 0.01\n\n            # Calculate new bubble position\n            new_point = self.bubbles[i, :2] + dir_vec * self.step_dist\n            new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n\n            # Check whether new bubble collides with other bubbles\n            if not self._check_collisions(new_bubble, rest_bub):\n                # NOTE: Produces invalid value encountered in cast Runtime warnings\n                self.bubbles[i, :] = new_bubble\n                self.com = self._center_of_mass()\n                moves += 1\n            else:\n                # Try to move around a bubble that you collide with\n                # Find colliding bubble\n                for colliding in self._collides_with(new_bubble, rest_bub):\n                    # Calculate direction vector\n                    dir_vec = rest_bub[colliding, :2] - self.bubbles[i, :2]\n                    dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n                    # Calculate orthogonal vector\n                    orth = np.array([dir_vec[1], -dir_vec[0]])\n                    # test which direction to go\n                    new_point1 = self.bubbles[i, :2] + orth * self.step_dist\n                    new_point2 = self.bubbles[i, :2] - orth * self.step_dist\n                    dist1 = self._center_distance(self.com, np.array([new_point1]))\n                    dist2 = self._center_distance(self.com, np.array([new_point2]))\n                    new_point = new_point1 if dist1 &lt; dist2 else new_point2\n                    new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n                    if not self._check_collisions(new_bubble, rest_bub):\n                        self.bubbles[i, :] = new_bubble\n                        self.com = self._center_of_mass()\n\n        if moves / len(self.bubbles) &lt; 0.1:\n            self.step_dist = self.step_dist / 2\n</code></pre>"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart._collides_with","title":"<code>_collides_with(bubble: np.ndarray, bubbles: np.ndarray) -&gt; int</code>","text":"<p>Collide.</p> <p>Parameters:</p> Name Type Description Default <code>bubble</code> <code>ndarray</code> <p>Bubble array.</p> required <code>bubbles</code> <code>ndarray</code> <p>Bubble array.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The minimum index.</p> Source code in <code>lexos/visualization/bubbleviz.py</code> <pre><code>def _collides_with(self, bubble: np.ndarray, bubbles: np.ndarray) -&gt; int:\n    \"\"\"Collide.\n\n    Args:\n        bubble (np.ndarray): Bubble array.\n        bubbles (np.ndarray): Bubble array.\n\n    Returns:\n        int: The minimum index.\n    \"\"\"\n    distance = self._outline_distance(bubble, bubbles)\n    idx_min = np.argmin(distance)\n    return idx_min if type(idx_min) is np.ndarray else [idx_min]\n</code></pre>"},{"location":"api/visualization/cloud/","title":"Word Clouds","text":""},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud","title":"<code>WordCloud</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic model for WordCloud options.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>data</code>                 (<code>single_doc_types | multi_doc_types | DataFrame</code>)             </li> <li> <code>docs</code>                 (<code>Optional[int | str | list[int] | list[str]]</code>)             </li> <li> <code>limit</code>                 (<code>Optional[int]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>height</code>                 (<code>int</code>)             </li> <li> <code>width</code>                 (<code>int</code>)             </li> <li> <code>opts</code>                 (<code>Optional[dict[str, Any]]</code>)             </li> <li> <code>figure_opts</code>                 (<code>Optional[dict[str, Any]]</code>)             </li> <li> <code>round</code>                 (<code>Optional[int]</code>)             </li> <li> <code>counts</code>                 (<code>dict[str, int]</code>)             </li> <li> <code>cloud</code>                 (<code>WordCloud | None</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> </ul> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>class WordCloud(BaseModel):\n    \"\"\"A Pydantic model for WordCloud options.\"\"\"\n\n    data: single_doc_types | multi_doc_types | pd.DataFrame = Field(\n        ...,\n        description=\"The data to generate the word cloud from. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe.\",\n    )\n    docs: Optional[int | str | list[int] | list[str]] = Field(\n        None, description=\"A list of documents to be selected from the DTM.\"\n    )\n    limit: Optional[int] = Field(\n        None, description=\"The maximum number of terms to plot.\"\n    )\n    title: Optional[str] = Field(None, description=\"The title of the plot.\")\n    height: int = Field(\n        200, gt=50, description=\"The height of the word cloud in pixels.\"\n    )\n    width: int = Field(200, gt=50, description=\"The width of the word cloud in pixels.\")\n    opts: Optional[dict[str, Any]] = Field(\n        {\n            \"background_color\": \"white\",\n            \"max_words\": 2000,\n            \"contour_width\": 0,\n            \"contour_color\": \"steelblue\",\n        },\n        description=\"The WordCloud() options.\",\n    )\n    figure_opts: Optional[dict[str, Any]] = Field(\n        {}, description=\"A dict of matplotlib figure options.\"\n    )\n    round: Optional[int] = Field(\n        0,\n        description=\"An integer to apply a mask that rounds the word cloud. It is best to use 100 or higher for a circular mask, but it will depend on the height and width of the word cloud.\",\n    )\n    counts: dict[str, int] = Field(None, description=\"A dictionary of term counts.\")\n    cloud: PythonWordCloud | None = Field(\n        None, description=\"The generated WordCloud object.\"\n    )\n    fig: Optional[plt.Figure] = Field(\n        None, description=\"The matplotlib figure object for the word cloud.\"\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    def __init__(self, **data: Any) -&gt; None:\n        \"\"\"Initialize the WordCloud model.\"\"\"\n        super().__init__(**data)\n\n        # Set the figure dimensions\n        self.opts[\"height\"] = self.height\n        self.opts[\"width\"] = self.width\n\n        # Set the mask, if using\n        if self.round &gt; 0:\n            x, y = np.ogrid[:300, :300]\n            mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; self.round**2\n            mask = 255 * mask.astype(int)\n            self.opts[\"mask\"] = mask\n\n        # Process the data into a consistent format\n        self.counts = processors.process_data(self.data, self.docs, self.limit)\n\n        # Generate the word cloud\n        self.cloud = PythonWordCloud(**self.opts).generate_from_frequencies(self.counts)\n\n    @validate_call\n    def save(self, path: Path | str, **kwargs: Any) -&gt; None:\n        \"\"\"Save the WordCloud to a file.\n\n        Args:\n            path (Path | str): The file path to save the WordCloud image.\n            **kwargs (Any): Additional keyword arguments for `plt.savefig`.\n        \"\"\"\n        if self.cloud is None:\n            raise LexosException(\"No WordCloud object to save.\")\n        self.fig = plt.figure(**self.figure_opts)\n        if self.title:\n            self.fig.suptitle(self.title)\n        plt.axis(\"off\")\n        plt.imshow(self.cloud, interpolation=\"bilinear\")\n        plt.savefig(path, **kwargs)\n        plt.close()\n\n    def show(self) -&gt; None:\n        \"\"\"Show the figure if it is hidden.\n\n        This is a helper method. It will generally display in a\n        Jupyter notebook.\n        \"\"\"\n        self.fig = plt.figure(**self.figure_opts)\n        if self.title:\n            self.fig.suptitle(self.title)\n        plt.axis(\"off\")\n        plt.imshow(self.cloud, interpolation=\"bilinear\")\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.data","title":"<code>data: single_doc_types | multi_doc_types | pd.DataFrame</code>  <code>pydantic-field</code>","text":"<p>The data to generate the word cloud from. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.docs","title":"<code>docs: Optional[int | str | list[int] | list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>A list of documents to be selected from the DTM.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.fig","title":"<code>fig: Optional[plt.Figure] = None</code>  <code>pydantic-field</code>","text":"<p>The matplotlib figure object for the word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.figure_opts","title":"<code>figure_opts: Optional[dict[str, Any]] = {}</code>  <code>pydantic-field</code>","text":"<p>A dict of matplotlib figure options.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.height","title":"<code>height: int = 200</code>  <code>pydantic-field</code>","text":"<p>The height of the word cloud in pixels.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.limit","title":"<code>limit: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The maximum number of terms to plot.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.opts","title":"<code>opts: Optional[dict[str, Any]] = {'background_color': 'white', 'max_words': 2000, 'contour_width': 0, 'contour_color': 'steelblue'}</code>  <code>pydantic-field</code>","text":"<p>The WordCloud() options.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.round","title":"<code>round: Optional[int] = 0</code>  <code>pydantic-field</code>","text":"<p>An integer to apply a mask that rounds the word cloud. It is best to use 100 or higher for a circular mask, but it will depend on the height and width of the word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title of the plot.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.width","title":"<code>width: int = 200</code>  <code>pydantic-field</code>","text":"<p>The width of the word cloud in pixels.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.__init__","title":"<code>__init__(**data: Any) -&gt; None</code>","text":"<p>Initialize the WordCloud model.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"Initialize the WordCloud model.\"\"\"\n    super().__init__(**data)\n\n    # Set the figure dimensions\n    self.opts[\"height\"] = self.height\n    self.opts[\"width\"] = self.width\n\n    # Set the mask, if using\n    if self.round &gt; 0:\n        x, y = np.ogrid[:300, :300]\n        mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; self.round**2\n        mask = 255 * mask.astype(int)\n        self.opts[\"mask\"] = mask\n\n    # Process the data into a consistent format\n    self.counts = processors.process_data(self.data, self.docs, self.limit)\n\n    # Generate the word cloud\n    self.cloud = PythonWordCloud(**self.opts).generate_from_frequencies(self.counts)\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.save","title":"<code>save(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save the WordCloud to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the WordCloud image.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for <code>plt.savefig</code>.</p> <code>{}</code> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>@validate_call\ndef save(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save the WordCloud to a file.\n\n    Args:\n        path (Path | str): The file path to save the WordCloud image.\n        **kwargs (Any): Additional keyword arguments for `plt.savefig`.\n    \"\"\"\n    if self.cloud is None:\n        raise LexosException(\"No WordCloud object to save.\")\n    self.fig = plt.figure(**self.figure_opts)\n    if self.title:\n        self.fig.suptitle(self.title)\n    plt.axis(\"off\")\n    plt.imshow(self.cloud, interpolation=\"bilinear\")\n    plt.savefig(path, **kwargs)\n    plt.close()\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.show","title":"<code>show() -&gt; None</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. It will generally display in a Jupyter notebook.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. It will generally display in a\n    Jupyter notebook.\n    \"\"\"\n    self.fig = plt.figure(**self.figure_opts)\n    if self.title:\n        self.fig.suptitle(self.title)\n    plt.axis(\"off\")\n    plt.imshow(self.cloud, interpolation=\"bilinear\")\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.data","title":"<code>data: single_doc_types | multi_doc_types | pd.DataFrame</code>  <code>pydantic-field</code>","text":"<p>The data to generate the word cloud from. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.docs","title":"<code>docs: Optional[int | str | list[int] | list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>A list of documents to be selected from the DTM.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.limit","title":"<code>limit: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The maximum number of terms to plot.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>The title of the plot.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.height","title":"<code>height: int = 200</code>  <code>pydantic-field</code>","text":"<p>The height of the word cloud in pixels.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.width","title":"<code>width: int = 200</code>  <code>pydantic-field</code>","text":"<p>The width of the word cloud in pixels.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.opts","title":"<code>opts: Optional[dict[str, Any]] = {'background_color': 'white', 'max_words': 2000, 'contour_width': 0, 'contour_color': 'steelblue'}</code>  <code>pydantic-field</code>","text":"<p>The WordCloud() options.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.figure_opts","title":"<code>figure_opts: Optional[dict[str, Any]] = {}</code>  <code>pydantic-field</code>","text":"<p>A dict of matplotlib figure options.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.round","title":"<code>round: Optional[int] = 0</code>  <code>pydantic-field</code>","text":"<p>An integer to apply a mask that rounds the word cloud. It is best to use 100 or higher for a circular mask, but it will depend on the height and width of the word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.counts","title":"<code>counts: dict[str, int]</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.cloud","title":"<code>cloud: PythonWordCloud | None</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.fig","title":"<code>fig: Optional[plt.Figure] = None</code>  <code>pydantic-field</code>","text":"<p>The matplotlib figure object for the word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.__init__","title":"<code>__init__(**data: Any) -&gt; None</code>","text":"<p>Initialize the WordCloud model.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"Initialize the WordCloud model.\"\"\"\n    super().__init__(**data)\n\n    # Set the figure dimensions\n    self.opts[\"height\"] = self.height\n    self.opts[\"width\"] = self.width\n\n    # Set the mask, if using\n    if self.round &gt; 0:\n        x, y = np.ogrid[:300, :300]\n        mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; self.round**2\n        mask = 255 * mask.astype(int)\n        self.opts[\"mask\"] = mask\n\n    # Process the data into a consistent format\n    self.counts = processors.process_data(self.data, self.docs, self.limit)\n\n    # Generate the word cloud\n    self.cloud = PythonWordCloud(**self.opts).generate_from_frequencies(self.counts)\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.save","title":"<code>save(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save the WordCloud to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the WordCloud image.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for <code>plt.savefig</code>.</p> <code>{}</code> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>@validate_call\ndef save(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save the WordCloud to a file.\n\n    Args:\n        path (Path | str): The file path to save the WordCloud image.\n        **kwargs (Any): Additional keyword arguments for `plt.savefig`.\n    \"\"\"\n    if self.cloud is None:\n        raise LexosException(\"No WordCloud object to save.\")\n    self.fig = plt.figure(**self.figure_opts)\n    if self.title:\n        self.fig.suptitle(self.title)\n    plt.axis(\"off\")\n    plt.imshow(self.cloud, interpolation=\"bilinear\")\n    plt.savefig(path, **kwargs)\n    plt.close()\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.WordCloud.show","title":"<code>show() -&gt; None</code>","text":"<p>Show the figure if it is hidden.</p> <p>This is a helper method. It will generally display in a Jupyter notebook.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Show the figure if it is hidden.\n\n    This is a helper method. It will generally display in a\n    Jupyter notebook.\n    \"\"\"\n    self.fig = plt.figure(**self.figure_opts)\n    if self.title:\n        self.fig.suptitle(self.title)\n    plt.axis(\"off\")\n    plt.imshow(self.cloud, interpolation=\"bilinear\")\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud","title":"<code>MultiCloud</code>  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Pydantic model for creating multiple WordClouds arranged in a grid using the topic_clouds approach.</p> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>json_schema_extra</code>: <code>DocJSONSchema.schema()</code></li> </ul> <p>Fields:</p> <ul> <li> <code>data</code>                 (<code>list[str] | list[list[str]] | list[Doc] | list[Span] | DTM | DataFrame</code>)             </li> <li> <code>docs</code>                 (<code>Optional[int | str | list[int] | list[str]]</code>)             </li> <li> <code>limit</code>                 (<code>Optional[int]</code>)             </li> <li> <code>figsize</code>                 (<code>tuple[int, int]</code>)             </li> <li> <code>layout</code>                 (<code>Optional[str | tuple[int, int]]</code>)             </li> <li> <code>opts</code>                 (<code>Optional[dict[str, Any]]</code>)             </li> <li> <code>round</code>                 (<code>Optional[int]</code>)             </li> <li> <code>title</code>                 (<code>Optional[str]</code>)             </li> <li> <code>labels</code>                 (<code>Optional[list[str]]</code>)             </li> <li> <code>doc_data</code>                 (<code>Optional[list[dict[str, int | float]]]</code>)             </li> <li> <code>fig</code>                 (<code>Optional[Figure]</code>)             </li> <li> <code>wordcloud</code>                 (<code>Optional[WordCloud]</code>)             </li> </ul> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>class MultiCloud(BaseModel):\n    \"\"\"A Pydantic model for creating multiple WordClouds arranged in a grid using the topic_clouds approach.\"\"\"\n\n    data: list[str] | list[list[str]] | list[Doc] | list[Span] | DTM | pd.DataFrame = (\n        Field(\n            ...,\n            description=\"The data to generate word clouds from. Accepts list of documents, DTM, or DataFrame.\",\n        )\n    )\n    docs: Optional[int | str | list[int] | list[str]] = Field(\n        None, description=\"A list of documents to be selected from the DTM/DataFrame.\"\n    )\n    limit: Optional[int] = Field(\n        None, description=\"The maximum number of terms to plot per cloud.\"\n    )\n    figsize: tuple[int, int] = Field(\n        (10, 10), description=\"The size of the overall figure.\"\n    )\n    layout: Optional[str | tuple[int, int]] = Field(\n        \"auto\",\n        description=\"The number of rows and columns in the figure. Default is 'auto'.\",\n    )\n    opts: Optional[dict[str, Any]] = Field(\n        {\n            \"background_color\": \"white\",\n            \"max_words\": 2000,\n            \"contour_width\": 0,\n            \"contour_color\": \"steelblue\",\n        },\n        description=\"The WordCloud() options applied to each word cloud.\",\n    )\n    round: Optional[int] = Field(\n        0,\n        description=\"An integer to apply a mask that rounds each word cloud. It is best to use 100 or higher for a circular mask.\",\n    )\n    title: Optional[str] = Field(None, description=\"Overall title for the figure.\")\n    labels: Optional[list[str]] = Field(\n        None, description=\"Labels for each subplot/word cloud.\"\n    )\n    doc_data: Optional[list[dict[str, int | float]]] = Field(\n        None, description=\"Processed document data for each word cloud.\"\n    )\n    fig: Optional[plt.Figure] = Field(\n        None, description=\"The matplotlib figure object for the multi-cloud plot.\"\n    )\n    wordcloud: Optional[PythonWordCloud] = Field(\n        None, description=\"The WordCloud object used for generating clouds.\"\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n\n    def __init__(self, **data: Any) -&gt; None:\n        \"\"\"Initialize the MultiCloud model.\"\"\"\n        super().__init__(**data)\n\n        # Process different data types to get individual document data\n        self.doc_data = self._process_data()\n\n        # Setup the WordCloud object\n        self.wordcloud = self._setup_wordcloud()\n\n        # Render the figure\n        self._render()\n\n    def _process_data(self) -&gt; list[dict[str, int | float]]:\n        \"\"\"Process the input data into individual document dictionaries.\"\"\"\n        if isinstance(self.data, DTM):\n            # Make sure there is data\n            if (\n                self.data.doc_term_matrix is None\n                or self.data.doc_term_matrix.shape[0] == 0\n            ):\n                raise LexosException(\"Empty DTM provided.\")\n            # Extract documents from DTM\n            doc_data = []\n            selected_docs = (\n                self.docs\n                if self.docs is not None\n                else range(self.data.doc_term_matrix.shape[0])\n            )\n            if isinstance(selected_docs, (int, str)):\n                selected_docs = [selected_docs]\n\n            for doc_idx in selected_docs:\n                # Get term frequencies for this document\n                if isinstance(doc_idx, str):\n                    doc_idx = self.data.labels.index(doc_idx)\n                doc_counts = {}\n\n                # Get the row as a 1D array and convert to list/scalar values\n                doc_row = self.data.doc_term_matrix[doc_idx]\n                if hasattr(doc_row, \"toarray\"):  # sparse matrix\n                    doc_row = doc_row.toarray().flatten()\n\n                for term_idx, count in enumerate(doc_row):\n                    # Convert to scalar value before comparison\n                    count_value = (\n                        float(count.item()) if hasattr(count, \"item\") else float(count)\n                    )\n                    if count_value &gt; 0:\n                        doc_counts[self.data.vectorizer.terms_list[term_idx]] = (\n                            count_value\n                        )\n                doc_data.append(doc_counts)\n\n        elif isinstance(self.data, pd.DataFrame):\n            # Make sure there is data\n            if self.data.empty:\n                raise LexosException(\"Empty DataFrame provided.\")\n            # Process DataFrame - assume it's a document-term matrix\n            doc_data = []\n            selected_docs = (\n                self.docs if self.docs is not None else range(len(self.data))\n            )\n            if isinstance(selected_docs, (int, str)):\n                selected_docs = [selected_docs]\n\n            for doc_idx in selected_docs:\n                if isinstance(doc_idx, str):\n                    doc_idx = self.data.index.get_loc(doc_idx)\n                doc_counts = self.data.iloc[doc_idx].to_dict()\n                # Filter out zero counts and convert to float\n                doc_counts = {\n                    k: float(v.item() if hasattr(v, \"item\") else v)\n                    for k, v in doc_counts.items()\n                    if (float(v.item()) if hasattr(v, \"item\") else float(v)) &gt; 0\n                }\n                doc_data.append(doc_counts)\n\n        elif isinstance(self.data, list):\n            # Make sure the data is not empty\n            if not self.data or len(self.data) == 0:\n                raise LexosException(\"No valid data provided for MultiCloud.\")\n            # Process list of documents using the processors module\n            doc_data = [\n                processors.process_data(doc, None, self.limit) for doc in self.data\n            ]\n\n        else:\n            raise LexosException(\"Unsupported data type for MultiCloud.\")\n\n        return doc_data\n\n    def _setup_wordcloud(self) -&gt; PythonWordCloud:\n        \"\"\"Configure a single WordCloud object to be reused.\"\"\"\n        # Set the mask if using round\n        if self.round &gt; 0:\n            x, y = np.ogrid[:300, :300]\n            mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; self.round**2\n            mask = 255 * mask.astype(int)\n            self.opts[\"mask\"] = mask\n\n        # Set max_words if limit is specified\n        if self.limit:\n            self.opts[\"max_words\"] = self.limit\n\n        return PythonWordCloud(**self.opts)\n\n    def _render(self) -&gt; None:\n        \"\"\"Generate and display the multi-cloud figure.\"\"\"\n        # Set parameters for plotting\n        sns.set_theme()\n        plt.rcParams[\"figure.figsize\"] = self.figsize\n\n        # Calculate layout\n        n = len(self.doc_data)\n        if self.layout == \"auto\":\n            columns = math.floor(math.sqrt(n))\n            rows = math.ceil(n / columns)\n        elif isinstance(self.layout, tuple):\n            rows, columns = self.layout\n        else:\n            raise LexosException(\"Invalid layout specification.\")\n\n        # Create the figure\n        self.fig = plt.figure(figsize=self.figsize)\n\n        # Add overall title\n        if self.title:\n            self.fig.suptitle(self.title, fontsize=16)\n\n        # Generate the word clouds\n        for i, doc_counts in enumerate(self.doc_data):\n            self.wordcloud.generate_from_frequencies(doc_counts)\n            plt.subplot(rows, columns, i + 1)\n            plt.imshow(self.wordcloud, interpolation=\"bilinear\")\n            plt.axis(\"off\")\n\n            # Add label if provided\n            if self.labels and i &lt; len(self.labels):\n                plt.title(self.labels[i])\n            else:\n                plt.title(f\"Doc {i}\")\n\n        # Get the figure and close to prevent automatic display\n        self.fig = plt.gcf()\n        plt.close()\n\n    @validate_call\n    def save(self, path: Path | str, **kwargs: Any) -&gt; None:\n        \"\"\"Save the MultiCloud figure to a file.\n\n        Args:\n            path (Path | str): The file path to save the MultiCloud image.\n            **kwargs (Any): Additional keyword arguments for `plt.savefig`.\n        \"\"\"\n        if self.fig is None:\n            raise LexosException(\"No figure to save.\")\n        self.fig.savefig(path, **kwargs)\n\n    def show(self) -&gt; None:\n        \"\"\"Display the multi-cloud figure.\"\"\"\n        if self.fig is None:\n            raise LexosException(\"No figure to show.\")\n        # Use IPython display for Jupyter notebooks\n        try:\n            from IPython.display import display\n\n            display(self.fig)\n        except ImportError:\n            # Fallback for non-Jupyter environments\n            plt.figure(self.fig.number)\n            plt.show()\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.data","title":"<code>data: list[str] | list[list[str]] | list[Doc] | list[Span] | DTM | pd.DataFrame</code>  <code>pydantic-field</code>","text":"<p>The data to generate word clouds from. Accepts list of documents, DTM, or DataFrame.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.docs","title":"<code>docs: Optional[int | str | list[int] | list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>A list of documents to be selected from the DTM/DataFrame.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.fig","title":"<code>fig: Optional[plt.Figure] = None</code>  <code>pydantic-field</code>","text":"<p>The matplotlib figure object for the multi-cloud plot.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.figsize","title":"<code>figsize: tuple[int, int] = (10, 10)</code>  <code>pydantic-field</code>","text":"<p>The size of the overall figure.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.labels","title":"<code>labels: Optional[list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>Labels for each subplot/word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.layout","title":"<code>layout: Optional[str | tuple[int, int]] = 'auto'</code>  <code>pydantic-field</code>","text":"<p>The number of rows and columns in the figure. Default is 'auto'.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.limit","title":"<code>limit: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The maximum number of terms to plot per cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.opts","title":"<code>opts: Optional[dict[str, Any]] = {'background_color': 'white', 'max_words': 2000, 'contour_width': 0, 'contour_color': 'steelblue'}</code>  <code>pydantic-field</code>","text":"<p>The WordCloud() options applied to each word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.round","title":"<code>round: Optional[int] = 0</code>  <code>pydantic-field</code>","text":"<p>An integer to apply a mask that rounds each word cloud. It is best to use 100 or higher for a circular mask.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>Overall title for the figure.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.__init__","title":"<code>__init__(**data: Any) -&gt; None</code>","text":"<p>Initialize the MultiCloud model.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"Initialize the MultiCloud model.\"\"\"\n    super().__init__(**data)\n\n    # Process different data types to get individual document data\n    self.doc_data = self._process_data()\n\n    # Setup the WordCloud object\n    self.wordcloud = self._setup_wordcloud()\n\n    # Render the figure\n    self._render()\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.save","title":"<code>save(path: Path | str, **kwargs: Any) -&gt; None</code>","text":"<p>Save the MultiCloud figure to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>The file path to save the MultiCloud image.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for <code>plt.savefig</code>.</p> <code>{}</code> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>@validate_call\ndef save(self, path: Path | str, **kwargs: Any) -&gt; None:\n    \"\"\"Save the MultiCloud figure to a file.\n\n    Args:\n        path (Path | str): The file path to save the MultiCloud image.\n        **kwargs (Any): Additional keyword arguments for `plt.savefig`.\n    \"\"\"\n    if self.fig is None:\n        raise LexosException(\"No figure to save.\")\n    self.fig.savefig(path, **kwargs)\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.show","title":"<code>show() -&gt; None</code>","text":"<p>Display the multi-cloud figure.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def show(self) -&gt; None:\n    \"\"\"Display the multi-cloud figure.\"\"\"\n    if self.fig is None:\n        raise LexosException(\"No figure to show.\")\n    # Use IPython display for Jupyter notebooks\n    try:\n        from IPython.display import display\n\n        display(self.fig)\n    except ImportError:\n        # Fallback for non-Jupyter environments\n        plt.figure(self.fig.number)\n        plt.show()\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.data","title":"<code>data: list[str] | list[list[str]] | list[Doc] | list[Span] | DTM | pd.DataFrame</code>  <code>pydantic-field</code>","text":"<p>The data to generate word clouds from. Accepts list of documents, DTM, or DataFrame.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.docs","title":"<code>docs: Optional[int | str | list[int] | list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>A list of documents to be selected from the DTM/DataFrame.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.limit","title":"<code>limit: Optional[int] = None</code>  <code>pydantic-field</code>","text":"<p>The maximum number of terms to plot per cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.figsize","title":"<code>figsize: tuple[int, int] = (10, 10)</code>  <code>pydantic-field</code>","text":"<p>The size of the overall figure.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.layout","title":"<code>layout: Optional[str | tuple[int, int]] = 'auto'</code>  <code>pydantic-field</code>","text":"<p>The number of rows and columns in the figure. Default is 'auto'.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.opts","title":"<code>opts: Optional[dict[str, Any]] = {'background_color': 'white', 'max_words': 2000, 'contour_width': 0, 'contour_color': 'steelblue'}</code>  <code>pydantic-field</code>","text":"<p>The WordCloud() options applied to each word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.round","title":"<code>round: Optional[int] = 0</code>  <code>pydantic-field</code>","text":"<p>An integer to apply a mask that rounds each word cloud. It is best to use 100 or higher for a circular mask.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.title","title":"<code>title: Optional[str] = None</code>  <code>pydantic-field</code>","text":"<p>Overall title for the figure.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.labels","title":"<code>labels: Optional[list[str]] = None</code>  <code>pydantic-field</code>","text":"<p>Labels for each subplot/word cloud.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.doc_data","title":"<code>doc_data: Optional[list[dict[str, int | float]]]</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.fig","title":"<code>fig: Optional[plt.Figure] = None</code>  <code>pydantic-field</code>","text":"<p>The matplotlib figure object for the multi-cloud plot.</p>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.wordcloud","title":"<code>wordcloud: Optional[PythonWordCloud]</code>  <code>pydantic-field</code>","text":""},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.model_config","title":"<code>model_config = ConfigDict(arbitrary_types_allowed=True, json_schema_extra=(DocJSONSchema.schema()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud.__init__","title":"<code>__init__(**data: Any) -&gt; None</code>","text":"<p>Initialize the MultiCloud model.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"Initialize the MultiCloud model.\"\"\"\n    super().__init__(**data)\n\n    # Process different data types to get individual document data\n    self.doc_data = self._process_data()\n\n    # Setup the WordCloud object\n    self.wordcloud = self._setup_wordcloud()\n\n    # Render the figure\n    self._render()\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud._process_data","title":"<code>_process_data() -&gt; list[dict[str, int | float]]</code>","text":"<p>Process the input data into individual document dictionaries.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def _process_data(self) -&gt; list[dict[str, int | float]]:\n    \"\"\"Process the input data into individual document dictionaries.\"\"\"\n    if isinstance(self.data, DTM):\n        # Make sure there is data\n        if (\n            self.data.doc_term_matrix is None\n            or self.data.doc_term_matrix.shape[0] == 0\n        ):\n            raise LexosException(\"Empty DTM provided.\")\n        # Extract documents from DTM\n        doc_data = []\n        selected_docs = (\n            self.docs\n            if self.docs is not None\n            else range(self.data.doc_term_matrix.shape[0])\n        )\n        if isinstance(selected_docs, (int, str)):\n            selected_docs = [selected_docs]\n\n        for doc_idx in selected_docs:\n            # Get term frequencies for this document\n            if isinstance(doc_idx, str):\n                doc_idx = self.data.labels.index(doc_idx)\n            doc_counts = {}\n\n            # Get the row as a 1D array and convert to list/scalar values\n            doc_row = self.data.doc_term_matrix[doc_idx]\n            if hasattr(doc_row, \"toarray\"):  # sparse matrix\n                doc_row = doc_row.toarray().flatten()\n\n            for term_idx, count in enumerate(doc_row):\n                # Convert to scalar value before comparison\n                count_value = (\n                    float(count.item()) if hasattr(count, \"item\") else float(count)\n                )\n                if count_value &gt; 0:\n                    doc_counts[self.data.vectorizer.terms_list[term_idx]] = (\n                        count_value\n                    )\n            doc_data.append(doc_counts)\n\n    elif isinstance(self.data, pd.DataFrame):\n        # Make sure there is data\n        if self.data.empty:\n            raise LexosException(\"Empty DataFrame provided.\")\n        # Process DataFrame - assume it's a document-term matrix\n        doc_data = []\n        selected_docs = (\n            self.docs if self.docs is not None else range(len(self.data))\n        )\n        if isinstance(selected_docs, (int, str)):\n            selected_docs = [selected_docs]\n\n        for doc_idx in selected_docs:\n            if isinstance(doc_idx, str):\n                doc_idx = self.data.index.get_loc(doc_idx)\n            doc_counts = self.data.iloc[doc_idx].to_dict()\n            # Filter out zero counts and convert to float\n            doc_counts = {\n                k: float(v.item() if hasattr(v, \"item\") else v)\n                for k, v in doc_counts.items()\n                if (float(v.item()) if hasattr(v, \"item\") else float(v)) &gt; 0\n            }\n            doc_data.append(doc_counts)\n\n    elif isinstance(self.data, list):\n        # Make sure the data is not empty\n        if not self.data or len(self.data) == 0:\n            raise LexosException(\"No valid data provided for MultiCloud.\")\n        # Process list of documents using the processors module\n        doc_data = [\n            processors.process_data(doc, None, self.limit) for doc in self.data\n        ]\n\n    else:\n        raise LexosException(\"Unsupported data type for MultiCloud.\")\n\n    return doc_data\n</code></pre>"},{"location":"api/visualization/cloud/#lexos.visualization.cloud.MultiCloud._setup_wordcloud","title":"<code>_setup_wordcloud() -&gt; PythonWordCloud</code>","text":"<p>Configure a single WordCloud object to be reused.</p> Source code in <code>lexos/visualization/cloud.py</code> <pre><code>def _setup_wordcloud(self) -&gt; PythonWordCloud:\n    \"\"\"Configure a single WordCloud object to be reused.\"\"\"\n    # Set the mask if using round\n    if self.round &gt; 0:\n        x, y = np.ogrid[:300, :300]\n        mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; self.round**2\n        mask = 255 * mask.astype(int)\n        self.opts[\"mask\"] = mask\n\n    # Set max_words if limit is specified\n    if self.limit:\n        self.opts[\"max_words\"] = self.limit\n\n    return PythonWordCloud(**self.opts)\n</code></pre>"},{"location":"api/visualization/processors/","title":"Processors","text":"<pre><code>rendering:\n  show_root_heading: true\n  heading_level: 3\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.process_data","title":"<code>process_data(data: Any, docs: Optional[int | str | list[int] | list[str]] = None, limit: Optional[int] = Field(None, gt=0, description='Limit on number of terms to return')) -&gt; dict[str, int]</code>","text":"<p>Process any supported data type into a consistent format of term counts.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data to process</p> required <code>docs</code> <code>Optional[int | str | list[int] | list[str]]</code> <p>Optional document selection for multi-document data</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of terms to return</p> <code>Field(None, gt=0, description='Limit on number of terms to return')</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: Dictionary with terms as keys and counts as values</p> <p>Raises:</p> Type Description <code>LexosException</code> <p>If data type is unsupported</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>@validate_call(config=ConfigDict(allow_arbitrary_types=True))\ndef process_data(\n    data: Any,\n    docs: Optional[int | str | list[int] | list[str]] = None,\n    limit: Optional[int] = Field(\n        None, gt=0, description=\"Limit on number of terms to return\"\n    ),\n) -&gt; dict[str, int]:\n    \"\"\"Process any supported data type into a consistent format of term counts.\n\n    Args:\n        data: The input data to process\n        docs: Optional document selection for multi-document data\n        limit: Optional limit on number of terms to return\n\n    Returns:\n        dict[str, int]: Dictionary with terms as keys and counts as values\n\n    Raises:\n        LexosException: If data type is unsupported\n    \"\"\"\n    # Handle simple string input\n    if isinstance(data, str):\n        counts = Counter(data.split())  # TODO: Use better tokenizer\n\n    # Handle spaCy objects\n    elif isinstance(data, (Doc, Span)):\n        counts = Counter([token.text for token in data])\n\n    # Handle dictionary input (already in correct format)\n    elif isinstance(data, dict):\n        counts = Counter(data)\n\n    # Handle list inputs\n    elif isinstance(data, list):\n        counts = _process_list_data(data, docs)\n\n    # Handle DTM objects\n    elif isinstance(data, DTM):\n        counts = process_dtm(data, docs)\n\n    # Handle DataFrame objects\n    elif isinstance(data, pd.DataFrame):\n        counts = process_dataframe(data, docs)\n\n    # Unsupported data type\n    else:\n        raise LexosException(\n            f\"Unsupported data type: {type(data)}. \"\n            \"Supported types: str, dict, list, DTM, DataFrame, spaCy Doc/Span objects.\"\n        )\n\n    # WARNING: This renders the code unusable if the data contains float counts\n    # such as topic model distributions. It doesn't seem necessary for any of\n    # our current use cases, so I'm commenting it out for now.\n    # Ensure counts are integers\n    # counts = Counter({k: int(v) for k, v in counts.items()})\n\n    # Limit the number of terms if specified\n    if limit is not None:\n        counts = Counter(dict(counts.most_common(limit)))\n\n    return dict(counts)\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.filter_docs","title":"<code>filter_docs(df: pd.DataFrame, docs: Optional[list[int] | list[str]] = None) -&gt; pd.DataFrame</code>","text":"<p>Filter the documents in a DTM.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Document Term Matrix.</p> required <code>docs</code> <code>Optional[list[int] | list[str]]</code> <p>A list of document indices or labels to filter the DTM.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered DTM.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>def filter_docs(\n    df: pd.DataFrame, docs: Optional[list[int] | list[str]] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Filter the documents in a DTM.\n\n    Args:\n        df: A Document Term Matrix.\n        docs: A list of document indices or labels to filter the DTM.\n\n    Returns:\n        A filtered DTM.\n    \"\"\"\n    if docs:\n        if isinstance(docs[0], str):\n            return df[docs]\n        elif isinstance(docs[0], int):\n            return df.iloc[:, docs]\n    return df\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.process_dataframe","title":"<code>process_dataframe(df: pd.DataFrame, docs: Optional[int | str | list[int] | list[str]] = None) -&gt; Counter</code>","text":"<p>Generate a term frequency dictionary from a DTM.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Document Term Matrix object.</p> required <code>docs</code> <code>Optional[int | str | list[int] | list[str]]</code> <p>A list of document indices or labels to filter the DTM.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Counter</code> <code>Counter</code> <p>A Counter object with the terms as keys and the counts as values.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>@validate_call(\n    config=ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n)\ndef process_dataframe(\n    df: pd.DataFrame, docs: Optional[int | str | list[int] | list[str]] = None\n) -&gt; Counter:\n    \"\"\"Generate a term frequency dictionary from a DTM.\n\n    Args:\n        df (pd.DataFrame): A Document Term Matrix object.\n        docs (Optional[int | str | list[int] | list[str]]): A list of document indices or labels to filter the DTM.\n\n    Returns:\n        Counter: A Counter object with the terms as keys and the counts as values.\n    \"\"\"\n    # Filter the documents\n    df = filter_docs(df, ensure_list(docs))\n    # Add the counts\n    df = df.copy()\n    df[\"counts\"] = df.sum(axis=1)\n    # Remove terms with zero counts\n    df = df.query(\"counts &gt; 0\")\n    # Return the counts as a Counter\n    return Counter(df[\"counts\"].to_dict())\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.process_dtm","title":"<code>process_dtm(dtm: DTM, docs: Optional[int | str | list[int] | list[str]] = None) -&gt; dict[str, int]</code>","text":"<p>Generate a term frequency dictionary from a DTM.</p> <p>Parameters:</p> Name Type Description Default <code>dtm</code> <code>DTM</code> <p>A Document Term Matrix object.</p> required <code>docs</code> <code>Optional[int | str | list[int] | list[str]]</code> <p>A list of document indices or labels to filter the DTM.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: A dictionary with the terms as keys and the counts as values.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>@validate_call(\n    config=ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n)\ndef process_dtm(\n    dtm: DTM, docs: Optional[int | str | list[int] | list[str]] = None\n) -&gt; dict[str, int]:\n    \"\"\"Generate a term frequency dictionary from a DTM.\n\n    Args:\n        dtm (DTM): A Document Term Matrix object.\n        docs (Optional[int | str | list[int] | list[str]]): A list of document indices or labels to filter the DTM.\n\n    Returns:\n        dict[str, int]: A dictionary with the terms as keys and the counts as values.\n    \"\"\"\n    df = dtm.to_df()\n    # Filter the documents\n    df = filter_docs(df, ensure_list(docs))\n    return process_dataframe(df)\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.process_list","title":"<code>process_list(data: list[list[Doc | Span] | list[str] | list[Token]], docs: Optional[int | list[int]]) -&gt; Counter</code>","text":"<p>Process a list of docs, spans, strings, or tokens.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[list[Doc | Span] | list[str] | list[Token]]</code> <p>The data.</p> required <code>docs</code> <code>Optional[int | list[int]]</code> <p>A list of document indices to be selected from the DTM.</p> required <p>Returns:</p> Name Type Description <code>Counter</code> <code>Counter</code> <p>A Counter object with the terms as keys and the counts as values.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>@validate_call(\n    config=ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n)\ndef process_list(\n    data: list[list[Doc | Span] | list[str] | list[Token]],\n    docs: Optional[int | list[int]],\n) -&gt; Counter:\n    \"\"\"Process a list of docs, spans, strings, or tokens.\n\n    Args:\n        data (list[list[Doc | Span] | list[str] | list[Token]]): The data.\n        docs (Optional[int | list[int]]): A list of document indices to be selected from the DTM.\n\n    Returns:\n        Counter: A Counter object with the terms as keys and the counts as values.\n    \"\"\"\n    if docs:\n        # Filter the docs\n        docs = ensure_list(docs)\n        data = [item for i, item in enumerate(data) if i in docs]\n        # Flatten the list\n        data = list(chain(*data))\n    # Get the terms\n    if all(isinstance(item, str) for item in data):\n        terms = [item for item in data]\n    elif all(isinstance(item, Token) for item in data):\n        terms = [item.text for item in data]\n    elif all(isinstance(item, (Doc, Span)) for item in data):\n        terms = [t.text for doc in data for t in doc]\n    else:\n        terms = list(chain(*data))\n    return Counter(terms)\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.process_docs","title":"<code>process_docs(data: list[Doc] | list[Span], docs: Optional[int | list[int]]) -&gt; Counter</code>","text":"<p>Process multiple docs or spans.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[Doc] | list[Span]</code> <p>The data.</p> required <code>docs</code> <code>Optional[int | list[int]]</code> <p>A list of document indices to be selected from the DTM.</p> required <p>Returns:</p> Name Type Description <code>Counter</code> <code>Counter</code> <p>A Counter object with the terms as keys and the counts as values.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>@validate_call(\n    config=ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n)\ndef process_docs(\n    data: list[Doc] | list[Span], docs: Optional[int | list[int]]\n) -&gt; Counter:\n    \"\"\"Process multiple docs or spans.\n\n    Args:\n        data (list[Doc] | list[Span]): The data.\n        docs (Optional[int | list[int]]): A list of document indices to be selected from the DTM.\n\n    Returns:\n        Counter: A Counter object with the terms as keys and the counts as values.\n    \"\"\"\n    if docs:\n        # Filter the docs\n        docs = ensure_list(docs)\n        data = [item for i, item in enumerate(data) if i in docs]\n    # Get the terms\n    terms = [[token.text for token in doc] for doc in data]\n    # Flatten the list\n    terms = list(chain(*terms))\n    return Counter(terms)\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.process_item","title":"<code>process_item(data: Doc | Span | list[str] | list[Token]) -&gt; Counter</code>","text":"<p>Process single docs, spans, and strings, or flat lists of strings or tokens.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Doc | Span | list[str] | list[Token]</code> <p>The data.</p> required <p>Returns:</p> Type Description <code>Counter</code> <p>dict[str, int]: A dictionary with the terms as keys and the counts as values.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>@validate_call(\n    config=ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n)\ndef process_item(\n    data: Doc | Span | list[str] | list[Token],\n) -&gt; Counter:\n    \"\"\"Process single docs, spans, and strings, or flat lists of strings or tokens.\n\n    Args:\n        data (Doc | Span | list[str] | list[Token]): The data.\n\n    Returns:\n        dict[str, int]: A dictionary with the terms as keys and the counts as values.\n    \"\"\"\n    # Get the terms\n    if isinstance(data, list) and isinstance(data[0], str):\n        terms = [item for item in data]\n    elif isinstance(data, list) and isinstance(data[0], Token):\n        terms = [item.text for item in data]\n    elif isinstance(data, (Doc, Span)):\n        terms = [t.text for t in data]\n    return Counter(terms)\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.multicloud_processor","title":"<code>multicloud_processor(data: DTM | pd.DataFrame | list[Doc] | list[Span] | list[list[str]] | list[list[Token]] | list[dict[str, int]], docs: Optional[int | str | list[int] | list[str]] = None) -&gt; list[dict[str, int]]</code>","text":"<p>Process data into list of term-count dicts for multicloud visualization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DTM | pd.DataFrame | list[Doc] | list[Span] | list[list[str]] | list[list[Token]] | list[dict[str, int]]]</code> <p>The data.</p> required <code>docs</code> <code>Optional[int | str | list[int] | list[str]]</code> <p>A list of document indices or labels to be selected from the DTM.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, int]]</code> <p>list[dict[str, int]]: A list of dictionaries with the terms as keys and the counts as values.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>@validate_call(\n    config=ConfigDict(\n        arbitrary_types_allowed=True, json_schema_extra=DocJSONSchema.schema()\n    )\n)\ndef multicloud_processor(\n    data: DTM\n    | pd.DataFrame\n    | list[Doc]\n    | list[Span]\n    | list[list[str]]\n    | list[list[Token]]\n    | list[dict[str, int]],\n    docs: Optional[int | str | list[int] | list[str]] = None,\n) -&gt; list[dict[str, int]]:\n    \"\"\"Process data into list of term-count dicts for multicloud visualization.\n\n    Args:\n        data (DTM | pd.DataFrame | list[Doc] | list[Span] | list[list[str]] | list[list[Token]] | list[dict[str, int]]]): The data.\n        docs (Optional[int | str | list[int] | list[str]]): A list of document indices or labels to be selected from the DTM.\n\n    Returns:\n        list[dict[str, int]]: A list of dictionaries with the terms as keys and the counts as values.\n    \"\"\"\n    # Convert DTM to DataFrame\n    if isinstance(data, DTM):\n        data = data.to_df()\n\n    # Process DataFrame\n    if isinstance(data, pd.DataFrame):\n        df = filter_docs(data, ensure_list(docs))\n        records = df.T.to_dict(orient=\"records\")\n        # Eliminate tokens with zero counts in each doc\n        return [{k: v for k, v in record.items() if v != 0} for record in records]\n\n    # Process other data types\n    else:\n        if docs:\n            # Filter the docs\n            docs = ensure_list(docs)\n            if isinstance(docs[0], str):\n                raise LexosException(\n                    \"Filtering by document labels is not yet supported for your data type. You may use list index numbers to select documents for processing.\"\n                )\n            else:\n                data = [item for i, item in enumerate(data) if i in docs]\n        try:\n            # Docs and Spans\n            if isinstance(data[0], (Doc, Span)):\n                return [dict(Counter([token.text for token in doc])) for doc in data]\n\n            # Lists of dicts\n            elif isinstance(data, list) and isinstance(data[0], dict):\n                return data\n\n            # Lists of strings\n            elif isinstance(data[0][0], str):\n                return [dict(Counter(doc)) for doc in data]\n\n            # Lists of Tokens\n            elif isinstance(data[0][0], Token):\n                return [dict(Counter([token.text for token in doc])) for doc in data]\n        except IndexError:\n            raise LexosException(\n                \"Data is empty or not in the expected format. \"\n                \"Ensure you are passing a non-empty list of documents, spans, or strings.\"\n            )\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors.get_rows","title":"<code>get_rows(lst, n) -&gt; Iterator[int]</code>","text":"<p>Yield successive n-sized rows from a list of documents.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>A list of documents.</p> required <code>n</code> <code>int</code> <p>The number of columns in the row.</p> required <p>Yields:</p> Type Description <code>int</code> <p>A generator with the documents separated into rows.</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>def get_rows(lst, n) -&gt; Iterator[int]:\n    \"\"\"Yield successive n-sized rows from a list of documents.\n\n    Args:\n        lst (list): A list of documents.\n        n (int): The number of columns in the row.\n\n    Yields:\n        A generator with the documents separated into rows.\n    \"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i : i + n]\n</code></pre>"},{"location":"api/visualization/processors/#lexos.visualization.processors._process_list_data","title":"<code>_process_list_data(data: list, docs: Optional[int | str | list[int] | list[str]] = None) -&gt; Counter</code>","text":"<p>Process list-type data inputs.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>List data to process</p> required <code>docs</code> <code>Optional[int | str | list[int] | list[str]]</code> <p>Optional document selection</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Counter</code> <code>Counter</code> <p>Counter object with term counts</p> Source code in <code>lexos/visualization/processors.py</code> <pre><code>def _process_list_data(\n    data: list, docs: Optional[int | str | list[int] | list[str]] = None\n) -&gt; Counter:\n    \"\"\"Process list-type data inputs.\n\n    Args:\n        data: List data to process\n        docs: Optional document selection\n\n    Returns:\n        Counter: Counter object with term counts\n    \"\"\"\n    if not data:\n        return Counter()\n\n    # Ensure all items in the list are of the same type\n    first_item = data[0]\n    first_type = type(first_item)\n    if not all(isinstance(x, first_type) for x in data):\n        raise LexosException(\n            f\"Mixed types found in list: {first_type} and {[type(x) for x in data]}\"\n        )\n\n    # List of lists\n    if isinstance(first_item, list):\n        return process_list(data, docs)\n\n    # List of spaCy objects\n    if isinstance(first_item, (Doc, Span)):\n        return process_docs(data, docs)\n\n    # List of tokens\n    if isinstance(first_item, Token):\n        return Counter([token.text for token in data])\n\n    # Simple list of strings\n    return process_item(data)\n</code></pre>"},{"location":"development/","title":"Contribute to Lexos","text":"<p>Thanks for your interest in contributing to Lexos. This page will give you a quick overview of our contribution guidelines and how to proceed.</p>"},{"location":"development/#issues-and-bug-reports","title":"Issues and Bug Reports","text":"<p>First, search the GitHub issues to see if the issue has already been reported. If so, please leave a comment on the existing issue.</p> <p>If are just looking for help using Lexos, please post you question on the GitHub Discussions board.</p>"},{"location":"development/#submitting-issues","title":"Submitting Issues","text":"<p>When opening an issue, use a descriptive title and include your environment (operating system, Python version, Lexos version). If you are fixing a bug with a pull request, please refer to your pull request when reporting the bug in your issue on GitHub.</p> <p>When reporting issues, follow GitHub Markdown conventions, especially for code and code blocks.</p> <p>Please consider adding a GitHub label to your post to help us categorize the issue.</p>"},{"location":"development/#contributing-to-the-code-base-or-documentation","title":"Contributing to the Code Base or Documentation","text":"<p>The Lexos source code and documentation currently reside in the same repository under the <code>lexos/src</code> and <code>lexos/doc_src</code> folders respectively. To work with them, you will need to fork and clone the repository and work with it in a local development environment. When you are ready, you can submit the code as a pull request to the main Lexos repository.</p>"},{"location":"development/#fork-and-clone-the-repository","title":"Fork and Clone the Repository","text":"<p>Start by forking the project on GitHub to your own account. Then clone your fork locally. On the command line, run</p> <pre><code>git clone https://github.com/your-username/lexos.git\ncd lexos\n</code></pre> <p>You can also clone the repository with a preferred <code>git</code> client or code editor.</p>"},{"location":"development/#set-up-your-development-environment","title":"Set Up Your Development Environment","text":"<p>To make changes to the Lexos source code or documentation, you will need to have a development environment consisting of a Python 3.12+, (preferably) uv to manage dependencies and your virtual environment, and git installed.</p> <p>\ud83d\udcd6 For more detailed information, see Setting Up Your Development Environment.</p>"},{"location":"development/#fixing-bugs","title":"Fixing Bugs","text":"<p>When fixing a bug, first create an issue if one does not already exist.</p> <p>Next, add a test to the relevant file in the <code>lexos/tests</code> folder. Then add a pytestmark, <code>@pytest.mark.issue(NUMBER)</code>, to reference the issue number.</p> <pre><code># Assume you're fixing Issue #1234\n@pytest.mark.issue(1234)\ndef test_issue1234():\n    ...\n</code></pre> <p>Test for the bug you're fixing, and make sure the test fails. Next, add and commit your test file. Finally, fix the bug, make sure your test passes and reference the issue number in your pull request description.</p> <p>\ud83d\udcd6 For more information on how to add tests, see the Lexos Tests page.</p>"},{"location":"development/#contributing-features","title":"Contributing Features","text":"<p>When contributing new features, it is important to understand the architecture of the Lexos Library. New features should be added to existing modules or should be created in new modules, as appropriate for the type of feature. All new features must be accompanied by tests and documentation as outlined on the Lexos Tests page.</p> <p>\ud83d\udcd6 For more information on how to create new features, see the Creating Features page.</p>"},{"location":"development/#updating-the-documentation-website","title":"Updating the Documentation Website","text":"<p>Documentation for Lexos takes three forms:</p> <ol> <li>The User Guide: Readable web pages with code samples describing the use of Lexos modules</li> <li>The API docs: Technical documentation for Lexos classes and functions auto-generated from their docstrings</li> <li>Tutorials: Jupyter notebooks (and accompanying sample data) guiding users through a workflow</li> </ol> <p>Contributions for all three are welcome. If you design a new feature or module, you should submit new documentation to accompany it (or make pull requests for changes to the current documentation, if appropriate).</p> <p>\ud83d\udcd6 For instructions on how to edit, build, and run the documentation website locally see the Lexos Documentation page.</p>"},{"location":"development/#policy-on-ai-assisted-contributions","title":"Policy on AI-Assisted Contributions","text":"<p>See the separate Policy on AI-Assisted Contributions page for information on the use of AI tools in contributing to this repository.</p>"},{"location":"development/#submitting-your-contribution","title":"Submitting Your Contribution","text":"<p>Before committing your changes, make sure to read the Code Conventions and the Code Style Workflow section on that page.</p> <p>Once you are satisfied that you have observed all project conventions, commit your changes with <code>git</code>. Make sure you write clear, descriptive commit messages.</p> <p>An example using the command line would be</p> <pre><code>git add .\ngit commit -m \"Fix bug in tokenizer and update docs\"\n</code></pre> <p>However, you may also use the <code>git</code> client of your choice.</p> <ol> <li>Push to Your Fork</li> </ol> <pre><code>git push origin fix-bug-in-tokenizer\n</code></pre> <ol> <li> <p>Open a Pull Request</p> </li> <li> <p>Go to the Lexos repo and open a pull request from your branch.</p> </li> <li>Fill out the description field by describing your changes.</li> </ol>"},{"location":"development/#code-conventions","title":"Code Conventions","text":"<p>All Python code must be compatible with Python 3.12+ and follow the project's code conventions.</p> <p>\ud83d\udcd6 The Lexos code conventions are described on the separate Code Conventions page.</p>"},{"location":"development/#adding-tests","title":"Adding Tests","text":"<p>Lexos uses the pytest framework for testing. For more info on this, see the pytest documentation. Tests for Lexos modules and classes live in their own directories of the same name. For example, tests for the <code>Tokenizer</code> class can be found in the <code>/lexos/tests/tokenizer</code> folder. To be interpreted and run, all test files and test functions need to be prefixed with <code>test_</code>.</p> <p>\ud83d\udcd6 For more guidelines and information on how to add tests, check out the separate Lexos Tests page.</p>"},{"location":"development/#code-of-conduct","title":"Code of Conduct","text":"<p>Lexos adheres to the Contributor Covenant Code of Conduct. By participating, you are expected to uphold this code.</p>"},{"location":"development/ai-policy/","title":"Policy on AI-Assisted Contributions","text":"<p>Writing this page is something of a fool's errand, given the rapidly evolving nature of AI tools and their usage in software development. However, to provide some clarity for contributors, the following policy is established regarding the use of AI tools in this repository.</p> <p>This repository permits the use of AI tools to assist in code and documentation development, provided that all contributions are reviewed and verified by human maintainers before being merged. Contributors must ensure that AI-generated content adheres to the project's quality standards and does not introduce errors, vulnerabilities, or significant inconsistencies in the codebase.</p> <p>The following guidelines should be observed when using AI tools:</p> <ul> <li>Transparency: Contributors should disclose the use of AI tools in their pull requests or contributions, specifying which parts were generated or assisted by AI in their pull request descriptions.</li> <li>Review: All AI-assisted contributions will undergo thorough human review to ensure accuracy, relevance, and compliance with project standards.</li> <li>Attribution: If AI-generated content is used, appropriate attribution of the AI tool used should be provided in accordance with the tool's terms of service.</li> <li>Limitations: AI tools should not be used to generate large portions of code or documentation without significant human oversight and modification. See the further notes below on the ways AI tools have been used in this repository.</li> <li>Ethical Use: Contributors must ensure that the use of AI tools does not violate any ethical guidelines, including but not limited to issues of bias, privacy, and intellectual property rights.</li> <li>Responsibility: Contributors are fully responsible for the content they submit, regardless of whether it was generated by AI tools.</li> </ul> <p>The above list was in fact generated with the assistance of GitHub Copilot, and subsequently edited by a human to fit the specific context of this repository.</p> <p>In the initial phase of development, GitHub Copilot was used with various GPT or Claude models available at the time, predominantly for writing test functions to achieve 100% or near 100% code coverage. To the extent that this is possible, we would like to continue this pattern of mostly human coding to implement the library features, and AI-assisted generation of test functions.</p> <p>Models implemented through GitHub Copilot were also used to help write documentation files. However, AI models produced documentation of uneven quality, often containing inaccuracies or extraneous information, requiring extensive human editing. Even where the documentation was accurate, it was often generated in a style not appropriate for the intended audience or inconsistent with the rest of the documentation. As of the time of writing, some of these qualities are still being addressed in the documentation pages. The lesson is that AI-generated documentation should be used with extreme caution.</p>"},{"location":"development/code-conventions/","title":"Code Conventions","text":"<p>This page provides a general overview of code conventions used in the Lexos project.</p> <p>Note</p> <p>Some discussion and examples in this coding guide are reproduced from spaCy's very well-documented code conventions, which we consider an excellent guide to best practices. Adjustments have been made where Lexos differs from or adds to spaCy's guidelines.</p>"},{"location":"development/code-conventions/#policy-on-ai-assisted-contributions","title":"Policy on AI-Assisted Contributions","text":"<p>See the separate Policy on AI-Assisted Contributions page for information on the use of AI tools in contributing to this repository.</p>"},{"location":"development/code-conventions/#code-compatibility-and-consistency","title":"Code Compatibility and Consistency","text":"<p>Lexos supports Python 3.12 and above, so all code should be written compatible with 3.12.</p> <p>Please observe the following conventions:</p> <ul> <li>Every Python file should begin with a docstring that serves as the Python header.</li> <li>Use Python type hints to annotate your methods and functions with expected data types for variables, function parameters, and return values.</li> <li>Document your methods and functions with docstrings in the appropriate Docstring style.</li> <li>Add comments that explain the purpose of code where it might not be clear or that point out areas for further development.</li> </ul> <p>Before submitting code, follow the Code Style Workflow to check for errors.</p>"},{"location":"development/code-conventions/#python-headers","title":"Python Headers","text":"<p>For the purpose of this guide, a \"Python header\" is the beginning of a Python file, which must start with a docstring containing specific content:</p> <ol> <li>The docstring must be enclosed in three double quotation marks, the first line of which must begin with the name of the file and which must end with a period.</li> <li>The file must contain a line \"Last Updated: \" followed by a date.</li> <li>Code files in the <code>doc_src</code> folder must additionally contain a line reading \"Last Tested: \", followed by a date.</li> <li>The date format should be something like \"November 5, 2025\". If you use a different format, <code>pre-commit</code> will try to fix it but will generate an error if it cannot do so (see pre-commit).</li> <li>Code files in the <code>tests</code> folder must additionally contain a line \"Coverage: \" followed by a percentage and a period. If coverage is not 100%, the period must be followed by a space and \"Missing: \". This should be followed by a comma-separated list of lines in the module being tests that are not covered by the tests. The list may contain ranges like \"105-107\".</li> </ol> <p>Here are two examples:</p> <pre><code># Example of a module file\n\"\"\"mymodule.py.\n\nLast Updated: November 25, 2025\nLast Tested: November 25, 2025\n\"\"\"\n\n# Example of a test file\n\"\"\"test_mymodule.py.\n\nCoverage: 97%. Missing: 201, 305-306\nLast Updated: November 25, 2025\n\"\"\"\n</code></pre> <p>Generally, the coverage line goes before the \"Last Updated\" line.</p> <p>Warning</p> <p>Pay attention to the punctuation, as incorrectly punctuated headers will generate <code>pre-commit</code> errors.</p> <p>Python headers can contain other information such as notes and sample usage, but you should try to keep Python headers short.</p>"},{"location":"development/code-conventions/#type-hints","title":"Type Hints","text":"<p>We use Python type hints across the <code>.py</code> files wherever possible. This makes it easy to understand what a function expects and returns, and modern editors will be able to show this information to you when you call an annotated function. Type hints are also used to auto-generate the API documentation files.</p> <p>If possible, you should always use the more descriptive type hints like <code>List[str]</code> or even <code>List[Any]</code> instead of only <code>list</code>. We also annotate arguments and return types of <code>Callable</code> \u2013 although, you can simplify this if the type otherwise gets too verbose. Remember that <code>Callable</code> takes two values: a list of the argument type(s) in order, and the return values.</p> <pre><code>- def func(some_arg: dict) -&gt; None:\n+ def func(some_arg: Dict[str, Any]) -&gt; None:\n    ...\n</code></pre> <pre><code>def create_callback(some_arg: bool) -&gt; Callable[[str, int], List[str]]:\n    def callback(arg1: str, arg2: int) -&gt; List[str]:\n        ...\n\n    return callback\n</code></pre> <p>The only caveat on type hinting is if fully-descriptive type hints become too verbose for human readability. In the case of the example above, describing both the arguments and the return values of the <code>Callable</code> type might be more complex than is desirable for a human reader. It is a judgement call when the attempt for full and precise type hinting becomes too complex to be meaningful.</p>"},{"location":"development/code-conventions/#docstring-style","title":"Docstring Style","text":"<p>All functions and methods you write should be documented with a docstring inline. The docstring provides a simple summary, and an overview of the arguments and their types. Modern editors will show this information to users when they call the function or method in their code, and this information is also used to auto-generate the API documentation.</p> <p>The Lexos project follows the Google Style Python Docstrings for docstrings. This is a widely used style guide that provides a consistent format for writing docstrings in Python code. It is recommended to follow this style guide for all docstrings in the Lexos project.</p> <p>Note</p> <p>The Lexos project uses MkDocs to generate API documentation from directly docstrings. The API documentation is automatically generated from the docstrings in the codebase, so it is important to keep the docstrings up to date and consistent with the code.</p> <p>The basic structure of a docstring in the Google Style is as follows:</p> <pre><code>def function_name(param1: int, param2: str) -&gt; bool:\n    \"\"\"Summary of the function.\n\n    Args:\n        param1 (int): Description of parameter 1.\n        param2 (str): Description of parameter 2.\n\n    Returns:\n        bool: Description of return value.\n\n    Raises:\n        ValueError: Description of the error condition.\n    \"\"\"\n    # Function implementation goes here\n</code></pre> <p>Only the \"Args\" section is required. Note that type hints should be reproduced in the docstring. The \"Returns\" section should be provided if the function returns a value, and the \"Raises\" section should only be included if the function raises exceptions. The \"Summary\" line should be a short, one-line description of the function's purpose. A \"Notes\" section can provide additional details about the function's behavior, if necessary.</p> <p>For test functions, only a \"Summary\" line is required. The other sections can be provided optionally if they help to explain the function's behaviour.</p>"},{"location":"development/code-conventions/#comments","title":"Comments","text":""},{"location":"development/code-conventions/#inline-code-comments","title":"Inline Code Comments","text":"<p>Code comments do not need to be extensive. However, if your code includes complex logic or aspects that may be unintuitive at first glance (or even included a subtle bug that you ended up fixing), you should leave a comment that provides more context. Comments should preferably begin with a capital letter.</p> <pre><code>token_index = indices[value]\n+ # Index describes Token.i of last token but Span indices are inclusive\nspan = doc[prev_token_index:token_index + 1]\n</code></pre> <pre><code>+ # To create the components we need to use the final interpolated config\n+ # so all values are available (if component configs use variables).\n+ # Later we replace the component config with the raw config again.\ninterpolated = filled.interpolate() if not filled.is_interpolated else filled\n</code></pre> <p>If your change implements a fix to a specific issue, it can often be helpful to include the issue number in the comment, especially if it's a relatively straightforward adjustment:</p> <pre><code>+ # Ensure object is a Span, not a Doc (#1234)\nif isinstance(obj, Doc):\n    obj = obj[obj.start:obj.end]\n</code></pre>"},{"location":"development/code-conventions/#including-todos","title":"Including TODOs","text":"<p>You are encouraged to include comments about future improvements using the <code>TODO:</code> prefix.</p> <pre><code>+ # TODO: This is currently pretty slow\ndir_checksum = hashlib.md5()\nfor sub_file in sorted(fp for fp in path.rglob(\"*\") if fp.is_file()):\n    dir_checksum.update(sub_file.read_bytes())\n</code></pre> <p>If any of the TODOs you've added are important and should be fixed soon, you should add a GitHub issue that details the task.</p>"},{"location":"development/code-conventions/#formatting-strings","title":"Formatting Strings","text":"<p>Wherever possible, use f-strings for any formatting of strings.</p>"},{"location":"development/code-conventions/#structuring-logic","title":"Structuring Logic","text":""},{"location":"development/code-conventions/#positional-and-keyword-arguments","title":"Positional and Keyword Arguments","text":"<p>Try to avoid writing functions and methods with too many arguments, and use keyword-only arguments wherever possible. Python lets you define arguments as keyword-only by separating them with a <code>, *</code>. If you are writing functions with additional arguments that customize the behavior, you typically want to make those arguments keyword-only, so their names have to be provided explicitly.</p> <pre><code>- def do_something(name: str, validate: bool = False):\n+ def do_something(name: str, *, validate: bool = False):\n    ...\n\n- do_something(\"some_name\", True)\n+ do_something(\"some_name\", validate=True)\n</code></pre> <p>This makes the function calls easier to read, because it is immediately clear what the additional values mean. It also makes it easier to extend arguments or change their order later on, because you don't end up with any function calls that depend on a specific positional order.</p> <p>Important</p> <p>User-facing functions and methods that accept data should be validated with Pydantic. Note that Pydantic enforces the use of keyword arguments instead of positional arguments.</p>"},{"location":"development/code-conventions/#avoid-mutable-default-arguments","title":"Avoid Mutable Default Arguments","text":"<p>A common Python gotcha are mutable default arguments: if your argument defines a mutable default value like <code>[]</code> or <code>{}</code> and then goes and mutates it, the default value is created once when the function is created and the same object is then mutated every time the function is called. This can be pretty unintuitive when you first encounter it. We therefore avoid writing logic that does this.</p>"},{"location":"development/code-conventions/#dont-use-tryexcept-for-control-flow","title":"Don't Use <code>try</code>/<code>except</code> for Control Flow","text":"<p>We discourage using <code>try</code>/<code>except</code> blocks for anything that's not third-party error handling or error handling that we otherwise have little control over. There's typically always a way to anticipate the actual problem and check for it explicitly, which makes the code easier to follow and understand, and prevents bugs:</p> <pre><code>- try:\n-     token = doc[i]\n- except IndexError:\n-     token = doc[-1]\n\n+ if i &lt; len(doc):\n+     token = doc[i]\n+ else:\n+     token = doc[-1]\n</code></pre> <p>If you have to use <code>try</code>/<code>except</code>, make sure to only include what's absolutely necessary in the <code>try</code> block and define the exception(s) explicitly. Otherwise, you may end up masking very different exceptions caused by other bugs.</p> <pre><code>- try:\n-     value1 = get_some_value()\n-     value2 = get_some_other_value()\n-     score = external_library.compute_some_score(value1, value2)\n- except:\n-     score = 0.0\n\n+ value1 = get_some_value()\n+ value2 = get_some_other_value()\n+ try:\n+     score = external_library.compute_some_score(value1, value2)\n+ except ValueError:\n+     score = 0.0\n</code></pre>"},{"location":"development/code-conventions/#avoid-lambda-functions","title":"Avoid Lambda Functions","text":"<p><code>lambda</code> functions can be useful for defining simple anonymous functions in a single line, but they also introduce problems: for instance, they require additional logic in order to be pickled and are pretty ugly to type-annotate. So we typically avoid them in the code base and only use them in the serialization handlers and within tests for simplicity. Instead of <code>lambda</code>s, check if your code can be refactored to not need them, or use helper functions instead.</p> <pre><code>- split_string: Callable[[str], List[str]] = lambda value: [v.strip() for v in value.split(\",\")]\n\n+ def split_string(value: str) -&gt; List[str]:\n+     return [v.strip() for v in value.split(\",\")]\n</code></pre>"},{"location":"development/code-conventions/#iteration-and-comprehensions","title":"Iteration and Comprehensions","text":"<p>Wherever possible, use list, dict, or generator comprehension instead of built-in functions like <code>filter</code> or <code>map</code>.</p> <pre><code>- filtered = filter(lambda x: x in [\"foo\", \"bar\"], values)\n+ filtered = (x for x in values if x in [\"foo\", \"bar\"])\n- filtered = list(filter(lambda x: x in [\"foo\", \"bar\"], values))\n+ filtered = [x for x in values if x in [\"foo\", \"bar\"]]\n\n- result = map(lambda x: { x: x in [\"foo\", \"bar\"]}, values)\n+ result = ({x: x in [\"foo\", \"bar\"]} for x in values)\n- result = list(map(lambda x: { x: x in [\"foo\", \"bar\"]}, values))\n+ result = [{x: x in [\"foo\", \"bar\"]} for x in values]\n</code></pre> <p>If your logic is more complex, it's often better to write a loop instead, even if it adds more lines of code in total. The result will be much easier to follow and understand.</p> <pre><code>- result = [{\"key\": key, \"scores\": {f\"{i}\": score for i, score in enumerate(scores)}} for key, scores in values]\n\n+ result = []\n+ for key, scores in values:\n+     scores_dict = {f\"{i}\": score for i, score in enumerate(scores)}\n+     result.append({\"key\": key, \"scores\": scores_dict})\n</code></pre>"},{"location":"development/code-conventions/#dont-use-print","title":"Don't Use <code>print</code>","text":"<p>The core library never <code>print</code>s anything. While we encourage using <code>print</code> statements for simple debugging (it's the most straightforward way of looking at what's happening), make sure to clean them up once you're ready to submit your pull request. If you want to output warnings or debugging information for users, use the respective dedicated mechanisms for this instead (see sections on warnings and logging for details).</p> <p>Note</p> <p>We make occasional exceptions to this guideline. For instance, when the <code>topic_modeling/mallet</code> module calls the Java Mallet tool, it uses the <code>wasabi</code> and code&gt;rich libraries to provide aesthetically pleasing console output that tracks the progress of the Java feedback.</p>"},{"location":"development/code-conventions/#naming","title":"Naming","text":"<p>Naming is hard. The best we can hope for is if everyone follows some basic conventions. Consistent with general Python conventions, we use the following naming formats:</p> <ul> <li><code>CamelCase</code> for class names including dataclasses</li> <li><code>snake_case</code> for methods, functions and variables</li> <li><code>UPPER_SNAKE_CASE</code> for constants, typically defined at the top of a module.</li> <li>Avoid using variable names that shadow the names of built-in functions, e.g. <code>input</code>, <code>help</code> or <code>list</code></li> </ul>"},{"location":"development/code-conventions/#naming-variables","title":"Naming Variables","text":"<p>Variable names should always make it clear what exactly the variable is and what it's used for. Choosing short and descriptive names wherever possible and imperative verbs for methods that do something, e.g. <code>disable_pipes</code>, <code>add_patterns</code> or <code>get_vector</code>.</p> <p>Private methods and functions that are not intended to be part of the user-facing API should be prefixed with an underscore <code>_</code>.</p> <p>Note</p> <p>In some cases, Pydantic will not let you use an underscore for a class attribute that should be private. In this case, it is acceptable to name the attribute wihtout an underscore.</p>"},{"location":"development/code-conventions/#io-and-handling-paths","title":"I/O and Handling Paths","text":"<p>Code that interacts with the file-system should, if possible accept objects that follow the <code>pathlib.Path</code> API. Ideally, user-facing functions and methods should accept <code>pathlib.Path</code> objects as input, although in some cases string inputs may be converted to <code>pathlib.Path</code> objects early in the function's operation. It is acceptable to convert <code>pathlib.Path</code> objects to strings for internal operation.</p>"},{"location":"development/code-conventions/#error-handling","title":"Error Handling","text":"<p>We always encourage writing helpful and detailed custom error messages for everything we can anticipate going wrong, and including as much detail as possible. These should be passed to the <code>LexosException</code> exception handler.</p> <pre><code>if something_went_wrong:\n    raise LexosException(\"Something went wrong!\")\n</code></pre> <p>or</p> <pre><code>try:\n    # code that raises a ValueError\nexcept ValueError as e:\n    raise LexosException(f\"Something went wrong: {e}\")\n</code></pre> <p>The second example exemplifes what we might do if we anticipate possible errors in third-party code that we don't control, or our own code in a very different context, we typically try to provide custom and more specific error messages if possible. This is an example of re-raising from the original caught exception so the user sees both the original error, as well as the custom message.</p> <p>Note that if you are designing an app that uses Lexos in its backend, Python errors are not necessarily what you want to relay to your user interface. Using the <code>LexosException</code> class to pass custom errors helps solve this problem.</p>"},{"location":"development/code-conventions/#avoid-using-naked-assert","title":"Avoid Using Naked <code>assert</code>","text":"<p>During development, it can sometimes be helpful to add <code>assert</code> statements throughout your code to make sure that the values you are working with are what you expect. However, as you clean up your code, those should either be removed or replaced by more explicit error handling:</p> <pre><code>- assert score &gt;= 0.0\n+ if score &lt; 0.0:\n+     raise ValueError(Errors.789.format(score=score))\n</code></pre>"},{"location":"development/code-conventions/#warnings","title":"Warnings","text":"<p>Instead of raising an error, some parts of the code base can raise warnings to notify the user of a potential problem. This is done using Python's <code>warnings.warn</code>. Whether or not warnings are shown can be controlled by the user, including custom filters for disabling specific warnings using a regular expression matching our internal codes, e.g. <code>W123</code>.</p> <pre><code>- print(\"Warning: No examples provided for validation\")\n+ warnings.warn(Warnings.W123)\n</code></pre> <p>When adding warnings, make sure you're not calling <code>warnings.warn</code> repeatedly, e.g. in a loop, which will clog up the terminal output. Instead, you can collect the potential problems first and then raise a single warning. If the problem is critical, consider raising an error instead.</p> <pre><code>+ n_empty = 0\nfor spans in lots_of_annotations:\n    if len(spans) == 0:\n-       warnings.warn(Warnings.456)\n+       n_empty += 1\n+ warnings.warn(Warnings.456.format(count=n_empty))\n</code></pre>"},{"location":"development/code-conventions/#code-style-workflow","title":"Code Style Workflow","text":"<p>Code prepared for the Lexos project should undergo linting and formatting to detect errors and enforce a consistent style.</p> <p>Lexos uses two tools for checking for linting and formatting errors:</p> <ul> <li><code>ruff</code>: an opinionated linter and formatter</li> <li>code&gt;pre-commit: a tool for running tests and fixing errors before code is committed to the project repository</li> </ul> <p>Code you write should be compatible with our the default <code>ruff</code> rules and the Lexos <code>pre-commit</code> hooks. It should not cause any errors or warnings.</p> <p>Depending on your setup, you can perform linting and formatting checks repeatedly at various stages of development, but running <code>pre-commit</code> on your code should always be the final stage of the process.</p>"},{"location":"development/code-conventions/#using-ruff","title":"Using <code>ruff</code>","text":"<p><code>ruff</code> should be installed when your set up your development environment. The following examples show how to run <code>ruff</code> from the command-line.</p>"},{"location":"development/code-conventions/#lint-your-code","title":"Lint Your Code","text":"<pre><code>uv run ruff check .\n</code></pre>"},{"location":"development/code-conventions/#auto-fix-linting-issues","title":"Auto-Fix Linting Issues","text":"<pre><code>uv run ruff check . --fix\n</code></pre>"},{"location":"development/code-conventions/#format-your-code","title":"Format Your Code","text":"<pre><code>uv run ruff format .\n</code></pre> <p>You can also run <code>ruff</code> in your code editor. For example, if you're using Visual Studio Code, you can install the Ruff extension and set up <code>ruff</code> as the default formatter. Add the following to your <code>settings.json</code> (in the command pallette, type <code>Preferences: Open Settings (JSON)</code>):</p> <pre><code>{\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n    \"editor.formatOnSave\": true,\n    \"ruff.linting.enabled\": true,\n    \"ruff.linting.run\": \"onType\"\n}\n</code></pre> <p>You may wish to comment out <code>editor.formatOnSave</code> if you want to manually format your code. You can automatically format your code by running the command palette (<code>Ctrl+Shift+P</code>) and selecting \"Format Document\" or \"Format Selection\" or by right-clicking and selecting these options.</p> <p>In some specific cases, e.g. in the tests, it can make sense to disable auto-formatting for a specific block. You can do this by wrapping the code you wish to exclude from formatting in <code># fmt: off</code> and <code># fmt: on</code>. It is also possible to ignore several comma-separated codes at once, e.g. <code># noqa: E731,E123</code>. Pull requests containing disabled linting will be considered on a case by case basis.</p> <p>Before committing code, you should ensure that it contains no linting and formatting errors. Depending on your code editing setup, most errors may be detected and fixed by <code>ruff</code> when you save your code files. If not, you should manually check your files with <code>ruff</code> in your editor and the command line.</p>"},{"location":"development/code-conventions/#using-pre-commit","title":"Using <code>pre-commit</code>","text":"<p>As a further check, you should run <code>pre-commit</code>. This tool provides various hooks to check your code for stylistic conformity and consistency. It should be installed when you create your development environment.</p> <p>Start by installing the Lexos <code>pre-commit</code> hooks in your environment with</p> <pre><code>uv run pre-commit install`\n</code></pre> <p>Lexos pre-commit hooks include:</p> <ul> <li>check yaml format (used in some configuration files, especially in the documentation)</li> <li>ensure that every file ends in a blank line</li> <li>ensures that there is no trailing whitespace at the end of a line</li> <li>running <code>ruff</code> linting and formatting (in case you forgot)</li> <li>enforcing the Python header format</li> </ul> <p>For instance, the following command will run checks on all Python files in the <code>tokenizer</code> module.</p> <pre><code>uv run pre-commit run --files $(find src/lexos/tokenizer -name \"*.py\")\n</code></pre> <p>You can run <code>pre-commit</code> on all files in the Lexos project with</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> <p>However, typically, you will want to run <code>pre-commit</code> only on the files you have changed. For instance, if you stage a new module with the first line below, the second line will test only those files.</p> <pre><code>git add src/lexos/new_module/*.py\nuv run pre-commit run\n</code></pre> <p>Running <code>pre-commit</code> fixes any styling errors it can and generates a report of the remaining errors so that you can fix them manually. Running the following command can be very useful:</p> <pre><code>uv run pre-commit run &gt; precommit-log.txt 2&gt;&amp;1\n</code></pre> <p>This will redirect the console output to a text file, which can make it easier to read. Be careful not to commit the log file to the repo.</p> <p>Important</p> <p>If you contribute new code, don't forget to update and/or add appropriate documentation and tests. If your run <code>pre-commit</code> on files staged for committing, <code>pre-commit</code> will check them for errors as well.</p> <p>Lexos uses continuous integration to perform the same error checking when you push your code to the repository or make a pull request. However, this runs the full suite of tests on the entire Lexos package, which can take a lot of time. Therefore, it is important to run <code>pre-commit</code> prior to committing your code (as the name implies) in order to catch any errors earlier in the process.</p>"},{"location":"development/creating-features/","title":"Creating Features","text":"<p>New features can be added to existing modules or in the form of new modules if they offer functionality that is different in nature from the existing modules.</p>"},{"location":"development/creating-features/#general-principles","title":"General Principles","text":"<p>Lexos modules begin at the top of the file with a docstring like this:</p> <pre><code>\"\"\"modulename.py.\n\nLast Updated: [date here]\nLast Tested: [date here]\n</code></pre> <p>The docstring may contain additional explanatory material describing the module or providing examples of its usage, but this material should be kept short. Each time you make changes to the module or test the module, update the \"Last Updated\" and \"Last Tested\" sections.</p> <p>When adding new features, keep in mind that the goal of Lexos is to provide an easy interface for something that might otherwise be more complicated when coded from scratch or working with third-party libraries on a more ad hoc basis. In particular, it should help users to put together workflows moving from preprocessing data to statistical analysis and visualization. Lexos functions should be easy to implement in standalone scripts or Jupyter notebooks but also be usable as part of a back end for a more complex tool or application. Above all, the goal of Lexos is to help students and scholars in the Humanities (or those helping them) to make use of computational tools to address the questions that matter to them. To this end, a major goal of Lexos is to be as language-agnostic as possible and to enable as much as possible the processing of historical and under-resourced languages.</p>"},{"location":"development/creating-features/#creating-a-module","title":"Creating a Module","text":"<p>To create a new module in the Lexos project, you should create a new branch from <code>main</code>, check out the new branch, and follow these steps:</p> <ol> <li> <p>Create a New Directory: Create a new directory for your module inside the Lexos <code>src/lexos</code> package. The directory name should be descriptive and follow the naming conventions of the project. If you are creating a submodule of an existing module, simply create the new directory inside the parent module's folder.</p> </li> <li> <p>Create an <code>__init__.py</code> File: Inside your new module directory, create an <code>__init__.py</code> file. This file can be empty or contain initialization code for your module. It is required to make Python treat the directory as a package. The <code>__init__.py</code> file should begin with <code>__init__.py.</code> (note the period at the end), but it does not need to contain anything else. If you are creating a submodule, you should also create the <code>__init__.py</code> file in your submodule's folder.</p> </li> <li> <p>Create Module Files: For some simple modules, you can add your code to the <code>__init__.py</code> file. You can then import the module with <code>import lexos.your_module</code> or <code>from lexos.your_module import some_function</code>.</p> </li> </ol>"},{"location":"development/creating-features/#handling-exceptions-in-your-module","title":"Handling Exceptions in Your Module","text":"<p>When creating a module, you should handle exceptions properly. If your module raises an exception, it should be a subclass of <code>lexos.exceptions.LexosException</code>. This ensures that the exception is consistent with the rest of the Lexos API and can be handled appropriately by users of your module. In general, it is a good idea to add <code>from lexos.exceptions import LexosException</code> at the top of your module file to ensure you can raise exceptions correctly.</p>"},{"location":"development/creating-features/#documenting-your-code","title":"Documenting Your Code","text":"<p>Ruff has a large number of built-in rules which will be enforced when you perform linting. For instance, Python modules should be documented with a docstring at the top of the file that contains the name of the file and ends with a period. For further information about formatting docstrings, see the Code Conventions page.</p>"},{"location":"development/creating-features/#using-spacy-language-models","title":"Using spaCy Language Models","text":"<p>Lexos aims to be as language-agnostic as possible, so the default spaCy language pipeline should always be <code>xx_sent_ud_sm</code> (spaCy's Multi-language model without named entity recognition). Users should be able to designate other models to be used by any function offered as part of the Lexos toolset.</p>"},{"location":"development/creating-features/#adding-pydantic-data-validation","title":"Adding Pydantic Data Validation","text":"<p>Lexos also tries to be agnostic about the context in which it will be used. One possibility is to build a backend for a text analysis application like the original Lexos web app. For this purpose, it offers its own data validation using the Python Pydantic library. Wherever possible, Lexos classes and functions should offer data validation using Pydantic. This section contains a short primer on how to use it.</p> <p>Note</p> <p>Note that Lexos uses Pydantic v2.</p> <p>In Python, you define a class like this:</p> <pre><code>class MyPythonClass:\n    def __init__(self, value: int):\n        self.value = value\n</code></pre> <p>In Pydantic, the class would like this:</p> <pre><code>from pydantic import BaseModel\n\nclass MyPydanticClass(BaseModel):\n    value: int\n</code></pre> <p>So far, so good. Pydantic looks like a Python dataclass and has much cleaner code. However, compare the following instantiations of our two classes:</p> <pre><code>python_instance = MyPythonClass(\"1\")\npydantic_instance = MyPydanticClass(value=\"1\")\npydantic_instance = MyPydanticClass(value={\"myvalue\": 1})\n</code></pre> <p>The <code>python_instance</code> will not raise an error because there is no type checking. The first <code>pydantic_instance</code> will also raise an error because, by default, Pydantic attempts to coerce data into the expected data type (you can change this behaviour). By default, it knows to convert strings to integers. However, it will raise a <code>ValidaError</code> for the second <code>pydantic instance</code> since it doesn't know how to coerce dicts.</p> <p>Important</p> <p>Pydantic requires keyword arguments when instantiating a class, and you cannot use positional arguments. This is a design choice made by Pydantic to avoid ambiguity in the order of arguments. This comes at some cost to libraries like Lexos, where all Pydantic-validated functions (as well as code samples and tutorials) need to supply keywords for every parameter.</p>"},{"location":"development/creating-features/#validating-functions-with-validate_call","title":"Validating Functions with @validate_call","text":"<p>The @validate_call decorator can be used to apply Pydantic validation to a function.</p> <pre><code>from pydantic import validate_call\n\n@validate_call\ndef print_value(value: int) -&gt; None:\n    print(value)\n\nprint_value({\"value\": 1})\n</code></pre> <p>This will raise a <code>ValidationError</code> because the <code>@validate_call</code> decorator tells, Pydantic to validate the arguments passed to the function based on the type annotation. It works with any function; you don't need to instantiate a class.</p> <pre><code>from pydantic import validate_call\nimport spacy\nfrom spacy.tokens import Doc\nnlp = spacy.load(\"en_core_web_sm\")\n\n@validate_call\ndef print_spacy_doc(doc: Doc) -&gt; str:\n    print(doc.text)\n\ndoc = nlp(\"This is a test.\")\nprint_spacy_doc(doc)\n</code></pre> <p>This will return a <code>ValidationError</code> because the spaCy <code>Doc</code> class is not recognised in the Pydantic <code>BaseModel</code>. Luckily, spaCy also uses Pydantic and has a schema available. So you need to remember to import it and add it to the Pydantic class's configuration:</p> <pre><code>from pydantic import ConfigDict, validate_call\nimport spacy\nfrom spacy.schemas import DocJSONSchema\nfrom spacy.tokens import Doc\nnlp = spacy.load(\"en_core_web_sm\")\n\nconfig = ConfigDict(json_schema=DocJSONSchema.schema())\n\n@validate_call(config=config)\ndef print_spacy_doc(doc: Doc) -&gt; str:\n    print(doc.text)\n\ndoc = nlp(\"This is a test.\")\nprint_spacy_doc(doc)\n</code></pre> <p>Note</p> <p>If working with a class, you simply add <code>model_config=config</code> as a class attribute.</p> <p>However, not all third-party libraries have importable JSON schemas. For instance, I have not found a way to match the <code>pd.DataFrame</code> type, so validating that input data is a dataframe involves writing a custom validator (which is also possible in Pydantic but naturally adds to the codebase). Sometimes this requires a procedural re-think such as making input a dict and having the function convert it to a dataframe.</p> <p>These complications may occasionally slow development, but, since we don't know what kind of applications may be using Lexos, it seems worthwhile to implement Pydantic validation so that Lexos functions fail as early as possible when input data is not as expected.</p>"},{"location":"development/creating-features/#documenting-pydantic-models-using-the-field-function","title":"Documenting Pydantic Models Using the <code>Field</code> Function","text":"<p>Pydantic models are used to define data structures in Python (see below). When using Pydantic, you can use the <code>Field</code> function to provide additional metadata for model fields. This metadata can include descriptions, default values, and validation constraints. Here is an example of how to use the <code>Field</code> function to document a Pydantic model:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass Person(BaseModel):\n    name: str = Field(default=\"John Doe\", description=\"The name of the person.\")\n    age: Optional[int] = Field(default=20, description=\"The age of the person.\")\n</code></pre> <p>Important</p> <p>Since the Pydantic <code>Field</code> function provides the same information as the docstring, the docstring should only contain a summary of the class and any additional information that is not already provided by the <code>Field</code> function. For instance, if the <code>description</code> parameter is used, there is no need for an <code>Args</code> section.</p>"},{"location":"development/creating-features/#computed-fields-and-model_dump","title":"Computed Fields and <code>model_dump()</code>","text":"<p>Pydantic models can define computed or derived properties that are evaluated on demand. These properties of a Pydantic class can be accessed via its <code>model_dump()</code> method. In Lexos, we intentionally use several computed behaviors (for instance, on <code>Record</code> objects) to provide convenience accessors such as <code>terms</code>, <code>tokens</code>, <code>num_terms</code> and <code>num_tokens</code>.</p> <p>However, developers should be aware of two important caveats when using <code>model_dump()</code>:</p> <ul> <li><code>model_dump()</code> may evaluate computed fields. If a computed property depends on runtime state (for example, <code>Record.terms</code> depends on the <code>content</code> being a parsed <code>Doc</code>), evaluating it via <code>model_dump()</code> may trigger exceptions such as <code>LexosException(\"Record is not parsed.\")</code>.</li> <li>Computed fields may be expensive. If a computed field does heavy computation (e.g., building a DTM, calculating statistics, or serializing spaCy tokens), calling <code>model_dump()</code> on the object could result in unexpected slowdowns.</li> </ul> <p>It is recommended that you guard <code>model_dump()</code> calls when the model may not be in a state that supports computed properties. If a model has a boolean state property such as <code>is_parsed</code> or <code>is_ready</code>, prefer checking it before calling <code>model_dump()</code> and constructing a dictionary of required fields manually if the condition is not satisfied. Alternatively, you can call <code>model_dump()</code> with the <code>exclude</code> and <code>mode=\"json\"</code> parameters. For example:</p> <pre><code>meta = record.model_dump(exclude=[\"terms\", \"tokens\", \"text\"], mode=\"json\")\n</code></pre>"},{"location":"development/creating-features/#testing-your-module","title":"Testing Your Module","text":"<p>New modules should be accompanied by tests functions covering as many lines of your code as possible. To create a test suite for your module, add a new folder for your module in the <code>lexos/tests</code> directory, and add your test files there. Tests should be run on your module before you submit a pull request.</p> <p>For further information on writing tests, see the separate Tests page.</p>"},{"location":"development/creating-features/#submitting-your-module","title":"Submitting Your Module","text":"<p>Start by committing your changes. Make sure you write clear, descriptive commit messages.</p> <p>An example using the command line would be      <pre><code>git add .\ngit commit -m \"New module offering a fancy new feature\"\n</code></pre></p> <p>However, you may use the <code>git</code> client of your choice.</p> <ol> <li>Push to Your Fork</li> </ol> <pre><code>git push origin new-module\n</code></pre> <ol> <li> <p>Open a Pull Request</p> </li> <li> <p>Go to the original repo and open a pull request from your branch.</p> </li> <li> <p>Fill out the pull request form describing the new module.</p> </li> <li> <p>Review and Collaboration</p> </li> <li> <p>Respond to feedback from maintainers.</p> </li> <li>Make requested changes and push updates.</li> <li>Once approved, your changes will be merged!</li> </ol>"},{"location":"development/documentation/","title":"Lexos Documentation","text":"<p>Documentation for Lexos takes three forms:</p> <ol> <li>The User Guide: Readable web pages with code samples describing the use of Lexos modules</li> <li>The API docs: Technical documentation for Lexos classes and functions auto-generated from their docstrings</li> <li>Tutorials: Jupyter notebooks (and accompanying sample data) guiding users through a workflow</li> </ol> <p>Contributions for all three are welcome. If you design a new feature or module, you should submit new documentation to accompany it (or make pull requests for changes to the current documentation, if appropriate).</p> <p>If you contribute a new feature to Lexos, you do not have to produce a tutorial for that feature, but it will be greatly appreciated.</p>"},{"location":"development/documentation/#the-documentation-website","title":"The Documentation Website","text":"<p>The documentation website is static website generated with MkDocs and Material for MkDocs. Each page is a Markdown file, which is converted to HTML when the site is built.</p> <p>To preview changes to the documentation, serve it locally with</p> <pre><code>cd lexos/doc_src\nuv run mkdocs serve\n</code></pre> <p>This will start a local server and automatically build a <code>docs</code> folder in the project root to contain the built website. If you do not want to serve the site, you can call <code>uv run mkdocs build</code>. However, in most case you will want to serve it to observe your changes in a web browser.</p> <p>If you make a new page, you must add it to the <code>doc_src/docs/mkdocs.yml</code> configuration. If the page is under an <code>overview.md</code> page, check to see if the <code>overview.md</code> page has discussion or a table of contents where you might want to link to the new page. Note that the <code>mkdocs.yml</code> file is very easy to corrupt, so be careful.</p> <p>When building the documentation, errors and warnings will be printed to the console. Please check and resolve them before making a pull request. If there are many warnings, it can be helpful to redirect the console output to a file. You can do this with <code>uv run mkdocs build &gt; build_full.log 2&gt;&amp;1</code> and then inspect the generated <code>build_full.log</code> file, which will be saved in the <code>doc_src</code> folder. Make sure that you don't push this file to the repository.</p> <p>Whether you make a change to an existing page or add a new one, your text should follow GitHub Markdown conventions, especially for code and code blocks. To see examples, you may find it helpful to review the current documentation files in the <code>doc_src/docs</code> folder or via <code>mkdocs serve</code>.</p> <p>Before you make a pull request, check that the site builds properly in your local environment and make sure that your content does not contain any Markdown linting errors Lexos uses the default Markdown linting rules of of Markdownlint, except as specified in the <code>doc_src/.markdownlint.json</code> file.</p> <p>Note</p> <p>It is recommended that you install the Markdownlint extension in VS Code for linting Markdown files when producing documentation. The Markdownlint extension will show you any errors.</p>"},{"location":"development/documentation/#the-user-guide","title":"The User Guide","text":"<p>The User Guide is intended to provide an entry-level introduction to the major features of Lexos. Pages in the User Guide are primarily intended to provide user-friendly overviews of the Lexos modules without being too exhaustive or too technical. It is acceptable to provide more technical explanations or notes for developers in admonitions (see below), but these should be relatively infrequent. Whether you are considering contributing to existing User Guide pages or adding a new one, use the existing pages as guides for the appropriate content, tone, and technical depth. For instance, you do not necessarily need to give an account of every parameter available in a given function, just those most likely to be used by an entry-level user. You can assume that the user has some familiarity with the Python programming language, but it may be worthwhile to define some terms or explain certain concepts.</p> <p>Where possible, provide code samples in code blocks. Sample code should follow the conventions described on the Code Conventions page. If your code generates visualizations, provide links to static images in <code>.png</code> format. Typically, your page would be in a folder along with accompanying images.</p> <p>User Guide pages should follow the Markdown principles noted above under The Documentation Website. Since User Guide pages are mostly written description, they should be well-edited and follow established standards for published writing. The Lexos documentation does not follow a specific style guide, but we recommend The Chicago Manual of Style, 17th Edition if you are in need of guidance but what written convention to adopt. This obviously only applies to documents in English. At present, the Lexos documentation does not have any pages in other languages, but we can imagine adding sections in other languages if users contribute them.</p>"},{"location":"development/documentation/#the-api-documentation","title":"The API Documentation","text":"<p>Each API documentation is meant primarily for developers, as it is highly technical, but it is also the only portion of the documentation that describes the full functionality of all Lexos features. For instance, a User Guide page or a Tutorial may describe only the major parameters of a function or method \u2014 those most likely to be used or most relevant to the workflow being discussed. If the User Guide or a tutorial does not mention a possible configuration or customization of a function, it is worth checking the API Documentation to see if the function has a parameter to do what you want.</p> <p>Unlike the User Guide, the API documentation is mostly generated automatically from the type hints and docstrings in the Python source code. This information is converted to HTML with <code>mkdocstrings</code> when you build the documentation website.</p> <p>Each module should have its own folder, the name of which should correspond to the name of the module. Inside, there should be an <code>index.md</code> page, which is the starting point for the module's API documentation. The <code>index.md</code> file should contain a brief Markdown description of the module and a link to any other pages in the module's API documentation (these should be additional Markdown files). API pages should follow the Markdown principles noted above under The Documentation Website.</p> <p>Each Markdown file in the module's API documentation should contain a <code>mkdocstrings</code> templates like the following:</p> <pre><code>### ::: lexos.cutter.TextCutter\n\n    rendering:\n      show_root_heading: true\n      heading_level: 3\n</code></pre> <p>A separate template should be provided for each member such as</p> <pre><code>### ::: lexos.cutter.TextCutter.__init__\n\n    rendering:\n      show_root_heading: true\n      heading_level: 3\n</code></pre> <p>Note</p> <p>It is possible to do this concisely with mkdocstrings \"selection\" syntax.</p> <pre><code>  ::: lexos.module\n      handler: python\n      selection:\n        members:\n          - MyClass\n          - MyClass.my_method\n          - my_function\n</code></pre> <p>However, this method is discouraged because the current version of <code>mkdocstrings</code> does not provide a way to hide unformattable material that may be in your docstring at the top of the module.</p> <p>The only API documentation file that does not require any <code>mkdocstrings</code> templates is the <code>index.md</code> file. This file only needs templates if it is the only Markdown file in the API's documentation.</p> <p>A short introduction (in Markdown) may be placed above the template, an further explanations can be added below, if necessary. You can also provide further discussion between member templates.</p> <p>To preview changes to the documentation, serve it locally with</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>To create direct links to individual classes, properties, and methods anywhere in the documentation, use syntax like the following:</p> <ul> <li>To link to <code>BaseLoader.data</code>, use <code>base_loader/#lexos.io.base_loader.BaseLoader.data</code></li> <li>To link to <code>BaseLoader.load_dataset</code>, use <code>base_loader/#lexos.io.base_loader.BaseLoader.load_dataset</code></li> </ul> <p>If you create API documentation for a new module, be sure to add it to the HTML table in <code>doc_src/docs/api/index.md</code>. When you add another row, make sure that you edit the <code>row-even</code> and <code>row-odd</code> class names so that the table striping alternates in the generated output.</p>"},{"location":"development/documentation/#the-tutorials","title":"The Tutorials","text":"<p>The User Guide is the beginner's entry-point into using Lexos, but there is no substitute for hands-on experience. So, as part of the \"documentation\" offer a series of Jupyter notebooks with executable code where the user can try out Lexos features. Notebooks may or may not come with sample datasets. If they do, the dataset should be compressed to a zip file in a subfolder inside the tutorial's folder. This allows the user to download both the tutorial notebook and the data to run locally. If you create a new tutorial, make sure to add it to the table of contents in <code>docs_src/docs/tutorials/index.md</code>.</p> <p>Tutorials should be aimed at entry-level users and their Markdown narrative should follow the same principles as outlined for the User Guide (except that sample code should mostly be in executable Python cells). All code blocks and Python cells should follow the conventions described on the Code Conventions page.</p>"},{"location":"development/documentation/#submitting-changes","title":"Submitting Changes","text":"<p>Start by committing your changes. Make sure you write clear, descriptive commit messages.</p> <p>An example using the command line would be      <pre><code>git add .\ngit commit -m \"New documentation page about a fancy new feature\"\n</code></pre></p> <p>However, you may use the <code>git</code> client of your choice.</p> <ol> <li>Push to Your Fork</li> </ol> <pre><code>git push origin new-module-doc\n</code></pre> <ol> <li> <p>Open a Pull Request</p> </li> <li> <p>Go to the original repo and open a pull request from your branch.</p> </li> <li> <p>Fill out the pull request form describing the new module.</p> </li> <li> <p>Review and Collaboration</p> </li> <li> <p>Respond to feedback from maintainers.</p> </li> <li>Make requested changes and push updates.</li> <li>Once approved, your changes will be merged!</li> </ol>"},{"location":"development/setup/","title":"Setting Up Your Development Environment","text":"<p>To make changes to the Lexos source code or documentation, you will need to have a development environment consisting of a Python 3.12+, (preferably) uv to manage dependencies and your virtual environment, and git installed. The steps below detail how to set up your development environment step by step.</p>"},{"location":"development/setup/#install-uv-globally","title":"Install <code>uv</code> Globally","text":"<p>We recommend <code>uv</code> for dependency management. If you haven't already, install <code>uv</code> according to the official documentation, follow these steps:</p> <p>For Windows (PowerShell):</p> <pre><code>irm https://astral.sh/uv/install.ps1 | iex\n</code></pre> <p>If you have Anaconda installed, you can run the command in a new Anaconda Prompt.</p> <p>For macOS/Linux (Bash/Zsh):</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Some Mac users have reported issues with the above command. <code>uv</code> appears to install correctly, but you can't run <code>uv</code> commands. If you encounter this problem, you can try homebrew instead (you may need to install homebrew first):</p> <pre><code>brew install uv\n</code></pre> <p>Close and reopen your terminal, then run:</p> <pre><code>uv --version\n</code></pre> <p>or</p> <pre><code>uv --help\n</code></pre> <p>This will verify the installation.</p> <p>Most of the instructions in this manual assume that you are using <code>uv</code> to manage dependencies.</p>"},{"location":"development/setup/#install-python","title":"Install Python","text":"<p>Lexos requires Python 3.12 or greater. If you do not already have it installed, you can install it from the command-line by running</p> <pre><code>uv python install 3.12\n</code></pre> <p>However, if you already have it installed using a distribution like Anaconda, <code>uv</code> will detect that installation, so there is no need for a fresh install.</p> <p>Important</p> <p>We recommend installing Anaconda, which comes with some dependencies needed by Lexos pre-installed. If you use <code>uv</code> to download a fresh installation of Python, it may not have these dependencies, and you may need to installed them independently.</p>"},{"location":"development/setup/#install-git","title":"Install Git","text":"<p>Git is used for version control. If you don't have it installed, you can download it from git.</p>"},{"location":"development/setup/#install-visual-studio-code-vs-code-recommended","title":"Install Visual Studio Code (VS Code) (Recommended)","text":"<p>We recommend using Visual Studio Code as your code editor. It has excellent support for Python and Git, and you can install extensions for code linting and formatting. Although you are not required to use it, the discussion in this documentation assumes that you are working in VS Code.</p> <p>The Lexos repo has a file called <code>.vscode.json</code>, where you can configure the path to your Python interpreter in your local virtual environment. If you are on Windows, you need to change that path to the appropriate one on your computer, probably something like <code>C:\\\\Users\\\\Your_Name\\\\Documents\\\\uv_lexos\\\\.venv\\\\Scripts\\\\Python.exe</code>. If you are on a Mac or Linux, it will be something like <code>/Users/Your_Name/Documents/uv_lexos/.venv/bin/python</code>. This will point VS Code towards the Python interpreter installed for Lexos.</p> <p>Important</p> <p>Don't forget to configure this path. Without it, VS Code may not recognize your virtual environment correctly, and you may encounter issues running Python code or Jupyter notebooks.</p> <p>We also recommend installing the following VS Code extensions for Python development:</p> <ul> <li>Even Better TOML: For better syntax highlighting and formatting of <code>pyproject.toml</code>.</li> <li>Jupyter: If you plan to work with Jupyter notebooks.</li> <li>Markdownlint: For linting Markdown files.</li> <li>Pylance: Provides rich type information and IntelliSense for Python.</li> <li>Python: Official extension for Python development.</li> <li>Ruff: For linting and formatting Python code (see below for further instructions).</li> </ul> <p>You can install these extensions from the VS Code marketplace or by searching for them in the Extensions view (<code>Ctrl+Shift+X</code>).</p> <p>Lexos uses the <code>ruff</code> code formatter and linter to produce readable code. Althouh you can run <code>ruff</code> from the command-line, it is advisable to set up your code editor to use it. In VS Code, you can use the Ruff extension. Add the following to your <code>settings.json</code> to use <code>ruff</code> for formatting and auto-format your files on save:</p> <pre><code>{\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n    \"editor.formatOnSave\": true,\n    \"ruff.linting.enabled\": true,\n    \"ruff.linting.run\": \"onType\"\n}\n</code></pre>"},{"location":"development/setup/#get-the-code","title":"Get the Code","text":""},{"location":"development/setup/#fork-and-clone-the-repository","title":"Fork and Clone the Repository","text":"<p>Start by forking the project on GitHub to your own account. Then clone your fork locally. On the command line, run</p> <pre><code>git clone https://github.com/your-username/lexos.git\ncd lexos\n</code></pre> <p>Alternatively, if you are using VS Code or a client like GitHub Desktop, go to the GitHub repository page, click on the green \"Code\" button, and copy the HTTPS URL. Use this URL with your client's clone feature to clone the repository.</p>"},{"location":"development/setup/#navigate-into-the-project-directory","title":"Navigate into the Project Directory","text":"<p>Use whatever path leads to the <code>lexos</code> directory.</p> <pre><code>cd lexos\n</code></pre>"},{"location":"development/setup/#create-a-virtual-environment-and-install-the-project-dependencies","title":"Create a Virtual Environment and Install the Project Dependencies","text":"<p>From the <code>lexos</code> project root:</p> <pre><code>uv venv\nuv sync\n</code></pre> <p>This creates a <code>.venv</code> directory and installs all dependencies listed in <code>pyproject.toml</code>.</p>"},{"location":"development/setup/#installing-spacy","title":"Installing SpaCy","text":"<p>Lexos relies on the spaCy for Natural Language Processing library for much of its functionality. SpaCy itself is written in Cython, which compiles Python code into C or C++ for better memory management. However, Cython does not come pre-installed in vanilla downloads of Python, and, as of June 2025, the spaCy installer wheels cannot themselves install all of Cython's dependencies (or cannot do so for all common operating systems and processors). This, at least, is our theory of why installation of spaCy fails when you call <code>uv sync</code> in a vanilla installation of Python. To remedy the problem, we recommend that you install Anaconda, which is distributed with Cython. This should allow spaCy to install correctly.</p> <p>The alternative is to install Cython's dependencies, and then Cython, independently. Cython requires a GCC-compatible C compiler to be present on your system. We have not thoroughly tested the following procedure, but it has worked in the a linux environment running on Windows 11 with an ARM64 processor (a challenging setup).</p> <pre><code>sudo apt-get install build-essential python3-dev\nuv pip install cython\n</code></pre> <p>The first command will install <code>build-essential</code>, which provides the C compiler and other development tools, along with the Python development headers. We have read that there may be some discrepancy between the installation paths used by <code>uv</code> and <code>pip</code>. To be safe, we suggest trying to install Cython using <code>pip</code> as shown in the command above. You may need to install <code>pip</code> in your environment first.</p> <p>Once you have a working version of Cython, <code>uv sync</code> should properly install spaCy.</p>"},{"location":"development/setup/#installing-spacy-models","title":"Installing SpaCy Models","text":"<p>SpaCy itself is installed as a dependency package via <code>uv</code>; however, its language models are downloaded as a separate process from urls. The two default models, \"xx_sent_ud_sm\" and \"en_core_web_sm\", are downloaded and installed automatically from commands in <code>pyproject.toml</code> when you run <code>uv sync</code>. If for any reason this fails, you can manually download the models. From your activated virtual environment in the project root, run:</p> <pre><code>uv run python -m spacy download xx_sent_ud_sm\nuv run python -m spacy download en_core_web_sm\n</code></pre> <p>You can also use these commands to download additional models, if required.</p>"},{"location":"development/setup/#activate-the-virtual-environment","title":"Activate the Virtual Environment","text":"<p><code>uv</code> commands will intelligently activate the virtual environment when you run them. However, for other commands (like <code>python</code> or <code>pip</code>), you need to activate the virtual environment manually. So it's a good idea to do this every time you start a new terminal session.</p> <p>**Windows (PowerShell):</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>macOS/Linux:</p> <p><code>bash source .venv/bin/activate</code></p> <p>Your terminal prompt should now show <code>(lexos)</code> or <code>(.venv)</code> at the beginning.</p> <p>Your local development environment should now be up and running.</p>"},{"location":"development/tests/","title":"Lexos Tests","text":"<p>Lexos uses the pytest framework for testing. For more info on this, see the pytest documentation.</p> <p>Tests for Lexos modules and classes live in their own directories of the same name. For example, tests for the <code>Tokenizer</code> can be found in <code>/tests/tokenizer</code>. To be interpreted and run, all test files and test functions need to be prefixed with <code>test_</code>.</p> <p>When adding tests, make sure to use descriptive names, keep the code short and concise and only test for one behavior at a time. Try to <code>parametrize</code> test cases wherever possible, use our pre-defined fixtures for spaCy components and avoid unnecessary imports. Extensive tests that take a long time should be marked with <code>@pytest.mark.slow</code>.</p>"},{"location":"development/tests/#running-the-tests","title":"Running the Tests","text":"<p>Lexos uses continuous integration to run tests when you push to the public repository or create a pull request. However, this runs the full suite of tests on the entire Lexos package, which can take a lot of time. Therefore, it is important to run all tests locally and make sure they pass before committing. In the header for your test file, indicate the coverage and any uncovered lines as follows:</p> <pre><code>\"\"\"test_mymodule.py.\n\nCoverage: 99% Uncovered: 127, 315\nLast update: [add date]\n\"\"\"\n</code></pre> <p>You can run tests in a specific file or directory, or even only one specific test:</p> <pre><code>uv run pytest  # run all tests - this will take a long time\nuv run pytest tests/tokenizer  # run all tests in directory\nuv run pytest tests/tokenizer/test_exceptions.py # run all tests in file\nuv run pytest tests/tokenizer/test_exceptions.py::test_tokenizer_init # run specific test\n</code></pre>"},{"location":"development/tests/#writing-tests","title":"Writing Tests","text":"<p>Tests for Lexos modules and classes live in their own directories of the same name and all test files should be prefixed with <code>test_</code>.</p>"},{"location":"development/tests/#test-suite-structure","title":"Test Suite Structure","text":"<p>When adding tests, make sure to use descriptive names and only test for one behavior at a time. Tests should be grouped into modules dedicated to the same type of functionality and some test modules are organized as directories of test files related to the same larger area of the library, e.g. <code>matcher</code> or <code>tokenizer</code>.</p>"},{"location":"development/tests/#fixtures","title":"Fixtures","text":"<p>If multiple tests in a file require a specific configuration, or use the same complex example, it can be helpful to create a separate fixture. This fixture should be added at the top of each file. It is helpful to add comments designating the fixtures and tests sections of the file.</p>"},{"location":"development/tests/#parametrizing-tests","title":"Parametrizing Tests","text":"<p>If you need to run the same test function over different input examples, you usually want to parametrize the test cases instead of using a loop within your test. This lets you keep a better separation between test cases and test logic, and it'll result in more useful output because <code>pytest</code> will be able to tell you which exact test case failed.</p> <p>The <code>@pytest.mark.parametrize</code> decorator takes two arguments: a string defining one or more comma-separated arguments that should be passed to the test function and a list of corresponding test cases (or a list of tuples to provide multiple arguments).</p> <pre><code>@pytest.mark.parametrize(\"words\", [[\"hello\", \"world\"], [\"this\", \"is\", \"a\", \"test\"]])\ndef test_doc_length(words):\n    doc = Doc(Vocab(), words=words)\n    assert len(doc) == len(words)\n</code></pre> <pre><code>@pytest.mark.parametrize(\"text,expected_len\", [(\"hello world\", 2), (\"I can't!\", 4)])\ndef test_token_length(en_tokenizer, text, expected_len):  # en_tokenizer is a fixture\n    doc = en_tokenizer(text)\n    assert len(doc) == expected_len\n</code></pre> <p>You can also stack <code>@pytest.mark.parametrize</code> decorators, although this is not recommended unless it's absolutely needed or required for the test. When stacking decorators, keep in mind that this will run the test with all possible combinations of the respective parametrized values, which is often not what you want and can slow down the test suite.</p>"},{"location":"development/tests/#handling-failing-tests","title":"Handling Failing Tests","text":"<p><code>xfail</code> means that a test should pass but currently fails, i.e. is expected to fail. You can mark a test as currently xfailing by adding the <code>@pytest.mark.xfail</code> decorator. This should only be used for tests that don't yet work, not for logic that cause errors we raise on purpose (see the section on testing errors for this). It's often very helpful to implement tests for edge cases that we don't yet cover and mark them as <code>xfail</code>. You can also provide a <code>reason</code> keyword argument to the decorator with an explanation of why the test currently fails.</p> <pre><code>+ @pytest.mark.xfail(reason=\"Issue #225 - not yet implemented\")\ndef test_en_tokenizer_splits_em_dash_infix(en_tokenizer):\n    doc = en_tokenizer(\"Will this road take me to Puddleton?\\u2014No.\")\n    assert doc[8].text == \"\\u2014\"\n</code></pre> <p>When you run the test suite, you may come across tests that are reported as <code>xpass</code>. This means that they're marked as <code>xfail</code> but didn't actually fail. This is worth looking into: sometimes, it can mean that we have since fixed a bug that caused the test to previously fail, so we can remove the decorator. In other cases, especially when it comes to machine learning model implementations, it can also indicate that the test is flaky: it sometimes passes and sometimes fails. This can be caused by a bug, or by constraints being too narrowly defined. If a test shows different behavior depending on whether its run in isolation or not, this can indicate that it reacts to global state set in a previous test, which is unideal and should be avoided.</p>"},{"location":"development/tests/#writing-slow-tests","title":"Writing Slow Tests","text":"<p>If a test is useful but potentially quite slow, you can mark it with the <code>@pytest.mark.slow</code> decorator. This is a special marker we introduced and tests decorated with it only run if you run the test suite with <code>--slow</code>, but not as part of the main CI process. Before introducing a slow test, double-check that there isn't another and more efficient way to test for the behavior. You should also consider adding a simpler test with maybe only a subset of the test cases that can always run, so we at least have some coverage.</p>"},{"location":"development/tests/#skipping-tests","title":"Skipping Tests","text":"<p>The <code>@pytest.mark.skip</code> decorator lets you skip tests entirely. You only want to do this for failing tests that may be slow to run or cause memory errors or segfaults, which would otherwise terminate the entire process and wouldn't be caught by <code>xfail</code>. We also sometimes use the <code>skip</code> decorator for old and outdated regression tests that we want to keep around but that don't apply anymore. When using the <code>skip</code> decorator, make sure to provide the <code>reason</code> keyword argument with a quick explanation of why you chose to skip this test.</p>"},{"location":"development/tests/#testing-errors-and-warnings","title":"Testing Errors and Warnings","text":"<p><code>pytest</code> lets you check whether a given error is raised by using the <code>pytest.raises</code> contextmanager. This is very useful when implementing custom error handling, so make sure you're not only testing for the correct behavior but also for errors resulting from incorrect inputs. If you're testing errors, you should always check for <code>pytest.raises</code> explicitly and not use <code>xfail</code>.</p> <pre><code>words = [\"a\", \"b\", \"c\", \"d\", \"e\"]\nents = [\"Q-PERSON\", \"I-PERSON\", \"O\", \"I-PERSON\", \"I-GPE\"]\nwith pytest.raises(ValueError):\n    Doc(Vocab(), words=words, ents=ents)\n</code></pre> <p>You can also use the <code>pytest.warns</code> contextmanager to check that a given warning type is raised. The first argument is the warning type or <code>None</code> (which will capture a list of warnings that you can <code>assert</code> is empty).</p> <pre><code>def test_phrase_matcher_validation(en_vocab):\n    doc1 = Doc(en_vocab, words=[\"Test\"], deps=[\"ROOT\"])\n    doc2 = Doc(en_vocab, words=[\"Test\"])\n    matcher = PhraseMatcher(en_vocab, validate=True)\n    with pytest.warns(UserWarning):\n        # Warn about unnecessarily parsed document\n        matcher.add(\"TEST1\", [doc1])\n    with pytest.warns(None) as record:\n        matcher.add(\"TEST2\", [docs])\n        assert not record.list\n</code></pre> <p>Keep in mind that your tests will fail if you're using the <code>pytest.warns</code> contextmanager with a given warning and the warning is not shown. So you should only use it to check that spaCy handles and outputs warnings correctly. If your test outputs a warning that's expected but not relevant to what you're testing, you can use the <code>@pytest.mark.filterwarnings</code> decorator and ignore specific warnings starting with a given code:</p> <pre><code>@pytest.mark.filterwarnings(\"ignore:\\\\[W036\")\ndef test_matcher_empty(en_vocab):\n    matcher = Matcher(en_vocab)\n    matcher(Doc(en_vocab, words=[\"test\"]))\n</code></pre>"},{"location":"development/tests/#dos-and-donts","title":"Dos and Don'ts","text":"<p>To keep the behavior of the tests consistent and predictable, we try to follow a few basic conventions:</p> <ul> <li>Test names should follow a pattern of <code>test_[module]_[tested behaviour]</code>. For example: <code>test_tokenizer_keeps_email</code> or <code>test_spans_override_sentiment</code>.</li> <li>Only use <code>@pytest.mark.xfail</code> for tests that should pass, but currently fail.</li> <li>Try to keep the tests readable and concise. Use clear and descriptive variable names (<code>doc</code>, <code>tokens</code> and <code>text</code> are great), keep it short and only test for one behavior at a time.</li> </ul>"},{"location":"development/tests/#checking-coverage","title":"Checking Coverage","text":"<p>We aim to include tests with coverage for 100% or near 100% of the code lines of a Python module. To run tests with coverage, you can use the following command:</p> <pre><code>uv run pytest --cov --cov-report=term-missing test_dtm.py\n</code></pre> <p>This displays a coverage report in your terminal, indicating which lines in the model are not covered by your test functions.</p> <p>To generate an HTML coverage report, something like use:</p> <pre><code>uv run pytest --cov --cov-report=html test_dtm.py\n</code></pre> <p>After running, open <code>htmlcov/index.html</code> in your browser to inspect coverage. As with the terminal report, you can adjust the <code>--cov</code> option to target specific modules or directories.</p> <p>Make sure that you test coverage before submitting a pull request for your test files.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This folder will contain tutorial notebooks to guide you in learning how to code with Lexos. Each tutorial will provide its own sample dataset, along with running commentary following the literate programming paradigm.</p> <p>Each tutorial notebook should be downloaded in your chosen notebook environment (Jupyter Lab, VS Code, Google Colab, etc.). Make sure that you have Lexos installed in your environment.</p> <p>Here is a list of the currently available tutorials, organised by module:</p>"},{"location":"tutorials/#cluster","title":"Cluster","text":"<ul> <li>Dendrograms (Hierarchical Agglomerative Clustering): \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> <li>Clustermaps (Hierarchical Agglomerative Clustering): \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> <li>Bootstrap Consenus Trees (Hierarchical Agglomerative Clustering): \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> <li>K-Means Clustering: \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> </ul>"},{"location":"tutorials/#corpus","title":"Corpus","text":"<ul> <li>Corpus Tutorial: \ud83d\udcd3 Notebook</li> <li>SQLite Tutorial: \ud83d\udcd3 Notebook</li> </ul>"},{"location":"tutorials/#cutter","title":"Cutter","text":"<ul> <li>Text Cutter and Token Cutter: \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> </ul>"},{"location":"tutorials/#dtm","title":"DTM","text":"<ul> <li>Making and Using a Document-Term Matrix: \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> </ul>"},{"location":"tutorials/#filter","title":"Filter","text":"<ul> <li>Filter Tutorial: \ud83d\udcd3 Notebook</li> </ul>"},{"location":"tutorials/#io","title":"IO","text":"<ul> <li>Loading Data: \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> </ul>"},{"location":"tutorials/#keywords-in-context-kwic","title":"Keywords in Context (KWIC)","text":"<ul> <li>Keywords in Context (KWIC): \ud83d\udcd3 Notebook</li> </ul>"},{"location":"tutorials/#milestones","title":"Milestones","text":"<ul> <li>Using Milestones: \ud83d\udcd3 Notebook</li> </ul>"},{"location":"tutorials/#rolling-windows","title":"Rolling Windows","text":"<ul> <li>Using Rolling Windows: \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> </ul>"},{"location":"tutorials/#scrubber","title":"Scrubber","text":"<ul> <li>Using Scrubber: \ud83d\udcd3 Notebook</li> <li>Scrubbing Markup Tags: \ud83d\udcd3 Notebook</li> </ul>"},{"location":"tutorials/#tokenizer","title":"Tokenizer","text":"<ul> <li>Using Tokenizer: \ud83d\udcd3 Notebook</li> </ul>"},{"location":"tutorials/#topic-modeling","title":"Topic Modeling","text":"<ul> <li>Mallet Topic Modeling Tutorial: \ud83d\udcd3 Notebook | \ud83d\uddc4\ufe0f Data</li> </ul>"},{"location":"tutorials/#topwords","title":"TopWords","text":"<ul> <li>TopicWords Tutorial: \ud83d\udcd3 Notebook</li> <li>Evaluating Key Terms with the Mann-Whitney U Test: \ud83d\udcd3 Notebook</li> </ul>"},{"location":"tutorials/#visualization","title":"Visualization","text":"<ul> <li>Word Clouds: \ud83d\udcd3 Notebook</li> </ul>"},{"location":"user_guide/","title":"User Guide","text":"<p>The User Guide is designed to provide an explanation of the features of the Lexos library. It is aimed at users who are new to the library and want to understand what it can do and how to use it effectively. Each section of the guide will cover a specific feature, providing examples and explanations to help users get started, as well as tips for more advanced usage. The discussion is designed to provide a conceptual basis for Lexos, its underlying principles and technologies, so that users can apply Lexos tools in their own projects. The User Guide is also intended to be a comprehensive resource for users of all levels, from beginners to more advanced users. It contains many code samples; however, when introducing functions, it does not provide examples of every possible setting or configuration. For more detailed information on specific functions, users are encouraged to refer to the API documentation.</p> <p>The User Guide is divided into several sections, each focusing on a specific aspect or module of the Lexos library. You can navigate through the sections using the links in the sidebar. Within pages, a table of contents on the right allows you navigate between sections on the page to explore individual topics.</p>"},{"location":"user_guide/#requirements","title":"Requirements","text":"<p>The Lexos library requires Python 3.12 or later. If you are running Lexos on your own machine, it is recommended to use a virtual environment to manage dependencies and avoid conflicts with other Python packages. For development, we use the <code>uv</code> package manager, which simplifies dependency management and packaging. You can install it by following the instructions on the <code>uv</code> website. However, Lexos should work in any Python 3.12+ environment.</p>"},{"location":"user_guide/#installation","title":"Installation","text":"<p>All of Lexos' dependencies are downloaded when Lexos is installed. For installation instructions, please refer to the Installation Guide.</p>"},{"location":"user_guide/analyzing_documents/","title":"Analyzing Documents","text":""},{"location":"user_guide/analyzing_documents/#overview","title":"Overview","text":"<p>Important</p> <p>This page is currently under construction.</p> <p>Lexos provides a variety of different ways of analyzing your documents. This page is a starting point for exploring the different methods.</p>"},{"location":"user_guide/clustering/","title":"Cluster Analysis","text":""},{"location":"user_guide/clustering/#overview","title":"Overview","text":"<p>Cluster analysis is an unsupervised technique that groups similar data points into \"clusters,\" where items within a cluster have similarities that distinguish them from items in other clusters. Linguistic clustering takes place by calculating distances or similarities between term frequencies, without reference to any pre-existing classification labels. It can be used to identify latent patterns that can be hard to observe by the human eye.</p> <p>Lexos provides two different ways to cluster your documents. This page is a starting point for exploring the different methods.</p> <ul> <li>Hierarchical Agglomerative Clustering</li> <li>K-Means Clustering</li> </ul>"},{"location":"user_guide/cutting_documents/","title":"Cutting Documents","text":"<p>The Lexos <code>cutter</code> module is used to split documents into smaller, more manageable pieces, variously called \"segments\" or \"chunks.\" This is particularly useful for processing large texts, enabling more efficient analysis and manipulation. If your documents are raw text files or strings, you can use the <code>TextCutter</code> class. If your documents are tokenized spaCy <code>Doc</code> objects, you can use the <code>TokenCutter</code> class. Documents can be cut based on byte size, number of tokens, number of sentences, line breaks, or custom-defined spans called milestones. The different cutting methods are described below.</p>"},{"location":"user_guide/cutting_documents/#cutting-text-strings-with-textcutter","title":"Cutting Text Strings with <code>TextCutter</code>","text":"<p>Let's say that you have a  long text string that you wanted to break into smaller chunks every n characters. The cell below demonstrates a simple way you can do this with the <code>TextCutter</code> class. To begin, we'll use a very short sample text for demonstration purposes, but you can replace the <code>text</code> variable with any long string of your choice. You can also change the <code>chunksize</code> parameter to specify how many characters you want in each chunk.</p> <pre><code># Import the TextCutter class\nfrom lexos.cutter.text_cutter import TextCutter\n\n# Create a sample text\ntext = (\n    \"It is a truth universally acknowledged, that a single  man in possession of a good fortune, \"\n    \"must be in want of a wife.\"\n)\n\n# Initialize TextCutter\ncutter = TextCutter()\n\n# Split text into chunks of 50 characters each\ncutter.split(text, chunksize=5)\n</code></pre> <p>The first parameter (for which you can use the keyword <code>docs</code>) is the text string to be cut. You can also supply a list of text strings (e.g., multiple documents) to the <code>docs</code> parameter, and each document will be cut separately.</p> <p>Note</p> <p>Occasionally, you may want to cut text based on byte size rather than character count. By default, the <code>TextCutter</code> counts characters, but you can set the <code>by_bytes</code> attribute to <code>True</code> if you want to cut based on byte size instead. However, be aware that cutting by bytes may split multi-byte characters in the middle, which can lead to encoding issues.</p> <p>Once the text has been split, its chunks are stored in a list of lists (one list per document). This list can be accessed three different ways:</p> <ul> <li>By calling <code>cutter.chunks</code>.</li> <li>By returning a value with <code>chunks = cutter.split(docs=text, chunksize=50)</code>.</li> <li>By iterating through the <code>Cutter</code> object:</li> </ul> <pre><code>for chunk in cutter:\n    print(chunk)\n</code></pre> <p>You can check how many documents are in your <code>TextCutter</code> instance by using the <code>len()</code> function:</p> <pre><code>print(len(cutter))\n</code></pre> <p>If you have multiple documents, you can get a dictionary of the chunks for each document using the <code>to_dict()</code> method. The dictionary keys are the document names and the values are lists of string chunks.</p> <pre><code>chunks_dict = cutter.to_dict()\nprint(chunks_dict)\n# {\n#     \"doc001\": [\"First 50 chars...\", \"Next 50 chars...\", ...]\n#     \"doc002\": [\"First 50 chars...\", \"Next 50 chars...\", ...], etc.\n# }\n</code></pre> <p>By default, each doc is named with the prefix \"doc\" followed by the doc number (starting from 001). Doc names can be accessed calling the <code>names</code> attribute. You can provide a custom list of names using the <code>names</code> parameter. You can also adjust zero padding with the <code>pad</code> parameter. For instance,</p> <pre><code>cutter.split(text, chunksize=50, pad=2, names=[\"Doc\"])\n</code></pre> <p>This will produce doc names like \"Doc01\", \"Doc02\", \"Doc03\", etc.</p> <p>Cutting texts can often leave small dangling pieces at the end. To address this, you can use the <code>merge_threshold</code> and <code>merge_final</code> parameters to control whether the last two chunks should be merged based on their size.</p> <ul> <li><code>merge_threshold</code>: The threshold for merging the last two chunks. The default is 0.5 (50%).</li> <li><code>merge_final</code>: Whether to merge the last two chunks. The default is <code>False</code>.</li> </ul> <p>Important</p> <p>Always inspect your chunks to see if the merging behaviour meets your expectations.</p> <p>You can also generate chunks that overlap with each other by using the <code>overlap</code> parameter. This parameter specifies the number of characters that should be repeated at the start of each chunk (except the first one). For example, if you set <code>chunksize=100</code> and <code>overlap=20</code>, each chunk will contain 20 characters from the end of the previous chunk.</p>"},{"location":"user_guide/cutting_documents/#cutting-text-files-with-textcutter","title":"Cutting Text Files with <code>TextCutter</code>","text":"<p>You can also cut texts directly from files, and the resulting chunks can be saved to disk. The <code>split()</code> method accepts the following parameters for file-based cutting:</p> <ul> <li><code>docs</code>: The a file path or buffer, or a list of file paths or buffers.</li> <li><code>file</code>: If <code>True</code>, treat each doc as a file path.</li> </ul> <p>By default, the <code>docs</code> parameter is assumed to be a list of text strings. If you set <code>file=True</code>, each doc will be treated as a file path, and the contents of the files will be read and cut accordingly. Each chunk will be named using the original file name (unless you provide custom names through the <code>names</code> parameter), followed by the chunk number, separated by the specified <code>delimiter</code> and padded to the specified length.</p> <p>Note</p> <p>You can also assign the docs when creating the <code>TextCutter</code> instance by using the <code>docs</code> parameter in the constructor. (e.g., <code>cutter = TextCutter(docs=[\"file1.txt\", \"file2.txt\"], file=True)</code>).</p>"},{"location":"user_guide/cutting_documents/#cutting-documents-into-a-fixed-number-of-chunks","title":"Cutting Documents into a Fixed Number of Chunks","text":"<p>The <code>n</code> parameter allows you to specify the number of chunks to split the text into. When you provide a value for <code>n</code>, the text will be divided into that many approximately equal parts. This is useful when you want to ensure that the text is split into a specific number of segments, regardless of their size. The <code>n</code> parameter overrides the <code>chunksize</code> parameter if both are provided.</p> <pre><code>cutter.split(text, n=5)\nprint(len(cutter.chunks))  # 5\n</code></pre>"},{"location":"user_guide/cutting_documents/#cutting-documents-by-line","title":"Cutting Documents by Line","text":"<p>The <code>newline</code> parameter allows you to split the text based on line breaks instead of byte size. When <code>newline=True</code>, the <code>chunksize</code> or <code>n</code> parameters will refer to the number of lines per chunk or the number of chunks based on lines, respectively.</p> <pre><code>cutter.split(text, n=5, newline=True)\nprint(len(cutter.chunks))  # 5\n</code></pre> <p>This will return chunks each containing 5 lines, except the last chunk, the length of which will depend on your merge settings.</p>"},{"location":"user_guide/cutting_documents/#setting-textcutter-attributes","title":"Setting <code>TextCutter</code> Attributes","text":"<p>Most of the parameters described above can also be set as attributes of the <code>TextCutter</code> instance. For example:</p> <pre><code>cutter = TextCutter()\ncutter.chunksize = 100\ncutter.delimiter = \"-\"\ncutter.pad = 2\ncutter.split(docs=text)\n</code></pre> <p>This allows you to configure the cutter once and then use it multiple times with the same settings.</p>"},{"location":"user_guide/cutting_documents/#saving-chunks-to-disk","title":"Saving Chunks to Disk","text":"<p>The save your chunks to disk, call the <code>save()</code> method. Each chunk will be saved as a separate <code>.txt</code> file in the specified with the <code>output_dir</code> parameter.</p> <pre><code>cutter.save(output_dir=\"output_chunks\")\n</code></pre> <p>This will save files like <code>doc001_001.txt</code>, <code>doc001_002.txt</code>, <code>doc002_001.txt</code>, <code>doc002_002.txt</code> in the output directory.</p> <p>You can customize the file naming convention by providing your own list of document names, changing the delimiter, and adjusting the padding. For example:</p> <pre><code>cutter.save(output_dir=\"output_chunks\", names=[\"A\",\"B\"], delimiter=\"-\", pad=2)\n</code></pre> <p>This will save files like <code>A-01.txt</code>, <code>A-02.txt</code>, <code>B-01.txt</code>, <code>B-02.txt</code>.</p> <p>Note</p> <p>In some cases, your chunks may have leading or trailing whitespace. By default, Lexos will strip this whitespace, but you can control this by setting <code>strip_chunks= False</code>.</p>"},{"location":"user_guide/cutting_documents/#merging-text-chunks","title":"Merging Text Chunks","text":"<p>You can also merge a list of string chunks back into a single string using the <code>merge()</code> method. This is useful if you want to recombine the chunks after processing.</p> <pre><code>chunks = [\"This is chunk one.\", \"This is chunk two.\", \"This is chunk three.\"]\nmerged_text = cutter.merge(chunks, sep=\" \")\nprint(merged_text)\n# \"This is chunk one. This is chunk two. This is chunk three.\"\n</code></pre> <p>By default, chunks are separated by spaces, but you can specify a different separator using the <code>sep</code> parameter.</p>"},{"location":"user_guide/cutting_documents/#cutting-spacy-doc-objects-with-tokencutter","title":"Cutting spaCy <code>Doc</code> Objects with <code>TokenCutter</code>","text":"<p>If you have tokenized documents represented as spaCy <code>Doc</code> objects, you can use the <code>TokenCutter</code> class to split them into smaller segments. <code>TokenCutter</code> has the same attributes as <code>TextCutter</code> and works in a similar way.</p> <p>However, one important difference is that the resulting chunks will be spaCy <code>Doc</code> objects rather than plain text strings. This allows you to preserve token-level information and annotations. If you wish to access the chunk string, you can do so using the <code>.text</code> attribute of each <code>Doc</code> chunk.</p> <p>If your spaCy <code>Doc</code> objects are stored in files, you can load them and cut them using them by setting <code>file=True</code> and the name of the spaCy model with the <code>model</code> parameter:</p> <pre><code>cutter.split(docs=[\"doc1.spacy\", \"doc2.spacy\"], file=True, model=\"en_core_web_sm\", chunksize=100)\n</code></pre> <p>The files must be in spaCy's binary format, which can be created using the <code>Doc.to_bytes()</code> or <code>Doc.to_disk()</code> methods. You must also specify the spaCy model used to create the <code>Doc</code> objects so that they can be deserialised correctly.</p>"},{"location":"user_guide/cutting_documents/#cutting-on-sentences-breaks","title":"Cutting on Sentences Breaks","text":"<p>Some spaCy language models include sentence boundary detection. If your <code>Doc</code> objects have sentence boundaries defined, you can use the <code>split_on_sentences()</code> method to cut the documents into chunks based on a specified number of sentences. For instances, assume that your spaCy <code>Doc</code> object has ten sentences. You can split it into chunks of 5 sentences each as follows:</p> <pre><code>cutter.split_on_sentences(doc, n=5)\n</code></pre> <p>If a <code>Doc</code> does not have sentence boundaries defined, Lexos will raise an error.</p> <p>As with other <code>TokenCutter</code> methods, the resulting chunks will be spaCy <code>Doc</code> objects.</p>"},{"location":"user_guide/cutting_documents/#setting-tokencutter-attributes","title":"Setting <code>TokenCutter</code> Attributes","text":"<p>Just like in <code>TextCutter</code>, you can set attributes in the <code>TokenCutter</code> instance for re-use:</p> <pre><code>cutter = TokenCutter()\ncutter.chunksize = 100\ncutter.delimiter = \"-\"\ncutter.pad = 2\ncutter.split(docs=text)\n</code></pre>"},{"location":"user_guide/cutting_documents/#saving-spacy-doc-files","title":"Saving spaCy <code>Doc</code> Files","text":"<p>By default, the <code>save()</code> method saves the chunk text strings, rather than the spaCy <code>Doc</code> objects. If you would like to store the spaCy <code>Doc</code> objects themselves, set the <code>as_text</code> parameter to <code>False</code>. This is the equivalent of calling spaCy's <code>Doc.to_bytes()</code> method on each chunk and saving the resulting bytes to disk.</p>"},{"location":"user_guide/cutting_documents/#merging-token-chunks","title":"Merging Token Chunks","text":"<p>You can also merge a list of string chunks back into a single string using the <code>merge()</code> method. This is useful if you want to recombine the chunks after processing. We can demonstrate its usage by starting with the chunks produced in the example above.</p> <pre><code># Split the doc on the milestone \"quick\" (into two chunks)\ncutter = TokenCutter()\nchunks = cutter.split_on_milestones(docs=doc, milestones=spans)\nprint(chunks[0]) # The\nprint(chunks[1]) # brown fox jumps over the lazy dog.\n\nmerged_doc = cutter.merge(chunks)\nprint(merged_doc.text)\n# \"The brown fox jumps over the lazy dog.\"\n</code></pre> <p>Unlike <code>TextCutter</code>, the <code>TokenCutter</code> <code>merge()</code> does not require a <code>sep</code> parameter. The start token for each chunk is appended to the end token of the previous chunk, and spacing is handled according to the language model used to create the original <code>Doc</code> objects.</p>"},{"location":"user_guide/cutting_documents/#splitting-on-milestones","title":"Splitting on Milestones","text":"<p>Milestones are specified locations in the text that designate structural or sectional divisions. A milestone can be either a designated unit within the text or a placemarker inserted between sections of text. The Lexos <code>milestones</code> module provides methods for identifying milestone locations by searching for patterns you designate. You can use the <code>StringMilestones</code> class in the <code>milestones</code> module to generate a list of <code>StringSpan</code> objects that mark the locations of milestones in your text. The <code>TextCutter.split_on_milestones()</code> method can then use these spans to split the text into chunks at the specified locations. Here is a quick example of how to do it.</p> <pre><code># Import the StringMilestones class\nfrom lexos.milestones.string_milestones import StringMilestones\n\n# A sample text\ntext = \"The quick brown fox jumps over the lazy dog.\"\n\n# Create a String Milestones instance and search for the pattern \"quick\"\nmilestones = StringMilestones(doc=text, patterns=\"quick\")\n\n# Create a TextCutter instance and split on the found milestones\ncutter = TextCutter()\n\n# Split the text on the milestone \"quick\"\nchunks = cutter.split_on_milestones(\n    docs=text,\n    milestones=milestones.spans, # The list of StringSpan objects\n)\nprint(chunks[0]) # The\nprint(chunks[1]) # brown fox jumps over the lazy dog.\n</code></pre> <p>You will notice that the milestone itself (\"quick\") is not included in either chunk. By default, the milestone text is removed during the split. You can control this behaviour by setting the <code>keep_spans</code> parameter to either <code>'preceding'</code> or <code>'following'</code>, which will keep the milestone text in the preceding or following chunk, respectively.</p> <pre><code>chunks = cutter.split_on_milestones(\n    docs=text,\n    milestones=milestones.spans,\n    keep_spans='preceding'  # or 'following' to keep in the next chunk\n)\n</code></pre> <p>If you do not want to use the <code>milestones</code> module to find milestones, you can also create your own list of <code>StringSpan</code> objects manually and pass them to the <code>split_on_milestones()</code> method.</p> <p>Cutting on milestones works similarly for <code>TokenCutter</code> objects, except that the milestones are specified as lists of spaCy <code>Span</code> objects rather than <code>StringSpan</code> objects.</p> <pre><code># Import the TokenMilestones class\nfrom lexos.milestones.token_milestones import TokenMilestones\n\n# Assume this is a spaCy `Doc` object with the text shown\ndoc = \"The quick brown fox jumps over the lazy dog.\"\n\n# Create a Token Milestones instance and search for the pattern \"quick\"\nmilestones = TokenMilestones(doc=doc)\nspans = milestones.get_matches(patterns=\"quick\") # The list of Span objects\n\n# Create a TokenCutter instance and split on the found milestones\ncutter = TokenCutter()\nchunks = cutter.split_on_milestones(docs=doc, milestones=spans)\nprint(chunks[0]) # The\nprint(chunks[1]) # brown fox jumps over the lazy dog.\n</code></pre> <p>In the case above, the token \"quick\" is used as the milestone, rather than the string \"quick\".</p> <p>The <code>milestones</code> module provides additional methods for finding milestones based on patterns, regular expressions, or custom logic. See the Using Milestones guide for more information.</p>"},{"location":"user_guide/filtering_documents/","title":"Filtering Tokens","text":""},{"location":"user_guide/filtering_documents/#overview","title":"Overview","text":"<p>The <code>filter</code> module provides a set of tools for filtering and identifying tokens within spaCy <code>Doc</code> objects. Filters allow you to identify specific types of tokens (such as words, Roman numerals, or stop words) and work with the matched results. This is useful for preprocessing, analysis, and text transformation tasks.</p>"},{"location":"user_guide/filtering_documents/#basic-concepts","title":"Basic Concepts","text":"<p>Lexos filters are built around the concept of matching tokens based on specific criteria. Each filter accepts a spaCy <code>Doc</code> object as input and outputs either a modified <code>Doc</code> or lists of matched token IDs.</p>"},{"location":"user_guide/filtering_documents/#basic-usage","title":"Basic Usage","text":"<p>The basic procedure for using filters is as follows:</p> <pre><code># Import the filter class\nfrom lexos.filter import IsWordFilter\n\n# Create a spaCy doc from your text\nfrom lexos.tokenizer import Tokenizer\n\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ntext = \"Hello, world! This is a test.\"\ndoc = tokenizer.make_doc(text)\n\n# Create an instance of a filter\nword_filter = IsWordFilter()\n\n# Apply the filter to the doc\nfiltered_doc = word_filter(doc)\n</code></pre>"},{"location":"user_guide/filtering_documents/#filter-classes","title":"Filter Classes","text":""},{"location":"user_guide/filtering_documents/#basefilter","title":"BaseFilter","text":"<p>The <code>BaseFilter</code> class is the foundation for all filters. It provides common functionality for matching tokens using spaCy matchers.</p>"},{"location":"user_guide/filtering_documents/#key-properties","title":"Key Properties","text":"<ul> <li><code>matched_token_ids</code>: Returns a set of token IDs that matched the filter criteria</li> <li><code>matched_tokens</code>: Returns a list of tokens that matched the filter criteria</li> <li><code>filtered_token_ids</code>: Returns a set of token IDs that did NOT match the filter criteria</li> <li><code>filtered_tokens</code>: Returns a list of tokens that did NOT match the filter criteria</li> </ul>"},{"location":"user_guide/filtering_documents/#key-methods","title":"Key Methods","text":"<ul> <li><code>get_matched_doc()</code>: Creates a new spaCy <code>Doc</code> containing only the matched tokens</li> <li><code>get_filtered_doc()</code>: Creates a new spaCy <code>Doc</code> containing only the filtered (non-matched) tokens</li> </ul> <p>Filtering tokens with <code>BaseFilter</code> methods can cause the neighbouring tokens to run together in the new document. You can use the <code>add_spaces</code> parameter in the above methods to insert spaces between tokens in the new document to prevent this.</p>"},{"location":"user_guide/filtering_documents/#iswordfilter","title":"IsWordFilter","text":"<p>The <code>IsWordFilter</code> class identifies tokens that are words (as opposed to punctuation, whitespace, or other non-word tokens).</p>"},{"location":"user_guide/filtering_documents/#parameters","title":"Parameters","text":"<ul> <li><code>attr</code> (optional): The name of the custom token attribute to add (default: \"is_word\")</li> <li><code>default</code> (optional): The default value for the custom attribute (default: False)</li> <li><code>exclude</code>: A string pattern or list of string patterns to exclude from being considered words (default: [\" \", \"\\n\"])</li> <li><code>exclude_digits</code>: If True, numeric tokens will not be treated as words (default: False)</li> <li><code>exclude_roman_numerals</code>: If True, Roman numerals (capital letters only) will not be treated as words (default: False)</li> <li><code>exclude_pattern</code>: Additional regex pattern or list of regex patterns to exclude</li> </ul>"},{"location":"user_guide/filtering_documents/#example","title":"Example","text":"<pre><code># Python imports\nfrom lexos.filter import IsWordFilter\nfrom lexos.tokenizer import Tokenizer\n\n# Create a spaCy doc from your text\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ntext = \"Hello, world! 123 and IV are not words.\"\ndoc = tokenizer.make_doc(text)\n\n# Create a word filter that excludes digits\nword_filter = IsWordFilter(exclude_digits=True, exclude_roman_numerals=True)\nfiltered_doc = word_filter(doc)\n\n# Access matched words\nmatched_words = word_filter.matched_tokens\nprint([token.text for token in matched_words])\n# Output: ['Hello', 'world', 'and', 'are', 'not', 'words']\n\n# Get a new doc with only words\nwords_only_doc = word_filter.get_matched_doc()\n</code></pre>"},{"location":"user_guide/filtering_documents/#isromanfilter","title":"IsRomanFilter","text":"<p>The <code>IsRomanFilter</code> class identifies tokens that are Roman numerals (capital letters only).</p> <p>The class has the following attributes:</p> <ul> <li><code>attr</code> (str, optional): The name of the custom token attribute to add</li> <li><code>default</code> (any, optional): The default value for the custom attribute</li> </ul> <p>For example:</p> <pre><code># Python imports\nfrom lexos.filter import IsRomanFilter\nfrom lexos.tokenizer import Tokenizer\n\n# Create a spaCy doc from your text\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ntext = \"Chapter IV begins here. Not iv, but IV.\"\ndoc = tokenizer.make_doc(text)\n\n# Create a Roman numeral filter\nroman_filter = IsRomanFilter(attr=\"is_roman\")\nresult_doc = roman_filter(doc)\n\n# Access matched Roman numerals\nroman_numerals = roman_filter.matched_tokens\nprint([token.text for token in roman_numerals])\n# Output: ['IV', 'IV']\n</code></pre>"},{"location":"user_guide/filtering_documents/#isstopwordfilter","title":"IsStopwordFilter","text":"<p>The <code>IsStopwordFilter</code> class manages stop words in a spaCy model. Stop words are common words that are often filtered out during text processing (such as \"the\", \"a\", \"is\", etc.).</p> <p>Important</p> <p>This filter modifies the model's default stop words. Changes will apply to any document created with that model unless the model is reloaded.</p> <p>The class has the following attributes:</p> <ul> <li><code>stopwords</code> (list[str] | str, optional): A list or string containing the stop word(s) to add or remove</li> <li><code>remove</code> (bool, optional): If True, the specified stop words will be removed from the model. If False, they will be added (default: False)</li> <li><code>case_sensitive</code> (bool, optional): If False (default), stop word changes apply to all case variations (lowercase, original, and capitalized). If True, only the exact case provided is modified (default: False)</li> </ul> <p>Here are some examples:</p> <p>Adding Stop Words:</p> <pre><code># Python imports\nfrom lexos.filter import IsStopwordFilter\nfrom lexos.tokenizer import Tokenizer\n\n# Create tokenizer and doc\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(\"The quick brown fox jumps over the lazy dog.\")\n\n# Add custom stop words\nstopword_filter = IsStopwordFilter()\nresult_doc = stopword_filter(doc, stopwords=[\"quick\", \"brown\"], remove=False)\n\n# Now \"quick\" and \"brown\" are marked as stop words in any docs created with this tokenizer\ndoc2 = tokenizer.make_doc(\"The quick brown fox\")\nfor token in doc2:\n    if token.is_stop:\n        print(f\"'{token.text}' is a stop word\")\n</code></pre> <p>Removing Stop Words:</p> <pre><code># Python imports\nfrom lexos.filter import IsStopwordFilter\nfrom lexos.tokenizer import Tokenizer\n\n# Create tokenizer\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\n\n# Create a doc\ndoc = tokenizer.make_doc(\"The quick brown fox jumps over the lazy dog.\")\n\n# Remove \"the\" from stop words (case-insensitive by default)\nstopword_filter = IsStopwordFilter()\nresult_doc = stopword_filter(doc, stopwords=\"the\", remove=True)\n\n# Now \"the\" and \"The\" are no longer marked as stop words in any docs created with this tokenizer\ndoc2 = tokenizer.make_doc(\"The quick brown fox\")\nfor token in doc2:\n    if token.text.lower() == \"the\":\n        print(f\"'{token.text}' is_stop: {token.is_stop}\")  # Output: is_stop: False for both\n</code></pre> <p>Case-Sensitive Stop Word Removal:</p> <pre><code># Remove only lowercase \"the\" (case-sensitive)\nstopword_filter = IsStopwordFilter()\nresult_doc = stopword_filter(doc, stopwords=\"the\", remove=True, case_sensitive=True)\n\n# Now lowercase \"the\" is not a stop word, but capitalized \"The\" still is\ndoc2 = tokenizer.make_doc(\"The quick brown fox. See the dog.\")\nfor token in doc2:\n    if token.text.lower() == \"the\":\n        print(f\"'{token.text}' is_stop: {token.is_stop}\")\n# Output: 'The' is_stop: True, 'the' is_stop: False\n</code></pre>"},{"location":"user_guide/filtering_documents/#working-with-matched-tokens","title":"Working with Matched Tokens","text":"<p>After applying a filter, you can access and work with the results in several ways:</p>"},{"location":"user_guide/filtering_documents/#accessing-token-ids","title":"Accessing Token IDs","text":"<pre><code>word_filter = IsWordFilter()\nword_filter(doc)\n\n# Get the IDs of matched tokens\nmatched_ids = word_filter.matched_token_ids\nprint(f\"Matched token IDs: {matched_ids}\")\n\n# Get the IDs of filtered-out tokens\nfiltered_ids = word_filter.filtered_token_ids\nprint(f\"Filtered token IDs: {filtered_ids}\")\n</code></pre>"},{"location":"user_guide/filtering_documents/#creating-new-documents","title":"Creating New Documents","text":"<pre><code># Create a document with only matched tokens\nmatched_doc = word_filter.get_matched_doc()\nprint(matched_doc.text)\n\n# Create a document with only filtered-out tokens\nfiltered_doc = word_filter.get_filtered_doc()\nprint(filtered_doc.text)\n</code></pre>"},{"location":"user_guide/filtering_documents/#accessing-token-objects","title":"Accessing Token Objects","text":"<pre><code># Get the actual token objects\nmatched_tokens = word_filter.matched_tokens\nfor token in matched_tokens:\n    print(f\"{token.text}: POS={token.pos_}, LEMMA={token.lemma_}\")\n\n# Get tokens that were filtered out\nfiltered_tokens = word_filter.filtered_tokens\nfor token in filtered_tokens:\n    print(f\"Excluded: {token.text}\")\n</code></pre>"},{"location":"user_guide/filtering_documents/#custom-token-attributes","title":"Custom Token Attributes","text":"<p>When you apply a filter with a custom attribute name, the filter adds a custom extension to each token. This allows you to programmatically check whether a token matches the filter criteria:</p> <pre><code>from lexos.filter import IsWordFilter\nfrom lexos.tokenizer import Tokenizer\n\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(\"Hello, world!\")\n\nword_filter = IsWordFilter(attr=\"is_word\")\nfiltered_doc = word_filter(doc)\n\n# Check the custom attribute on each token\nfor token in filtered_doc:\n    print(f\"{token.text}: is_word={token._.is_word}\")\n# Output:\n# Hello: is_word=True\n# ,: is_word=False\n# world: is_word=True\n# !: is_word=False\n</code></pre>"},{"location":"user_guide/filtering_documents/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"user_guide/filtering_documents/#excluding-specific-token-types","title":"Excluding Specific Token Types","text":"<pre><code>word_filter = IsWordFilter(\n    exclude_digits=True,           # Exclude numeric tokens\n    exclude_roman_numerals=True,   # Exclude Roman numerals\n    exclude_pattern=r\"[^\\w\\s]\"     # Exclude special characters\n)\nfiltered_doc = word_filter(doc)\n</code></pre>"},{"location":"user_guide/filtering_documents/#combining-multiple-filters","title":"Combining Multiple Filters","text":"<p>You can apply multiple filters sequentially to progressively refine your results:</p> <pre><code>from lexos.filter import IsWordFilter, IsRomanFilter\nfrom lexos.tokenizer import Tokenizer\n\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(\"Chapter IV: The quick brown fox (123) jumps.\")\n\n# First, filter out non-words\nword_filter = IsWordFilter(exclude_digits=True, exclude_roman_numerals=True)\ndoc = word_filter(doc)\nwords_only = word_filter.get_matched_doc()\n\n# Then, filter for Roman numerals (on the original doc)\nroman_filter = IsRomanFilter()\ndoc = tokenizer.make_doc(\"Chapter IV: The quick brown fox (123) jumps.\")  # Reset to original\nroman_filter(doc)\nroman_numerals = roman_filter.matched_tokens\nprint([token.text for token in roman_numerals])\n</code></pre>"},{"location":"user_guide/filtering_documents/#common-use-cases","title":"Common Use Cases","text":""},{"location":"user_guide/filtering_documents/#extract-only-words-no-punctuation","title":"Extract Only Words (No Punctuation)","text":"<pre><code>from lexos.filter import IsWordFilter\nfrom tokenizer import Tokenizer\n\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(\"Hello, world! How are you?\")\n\nword_filter = IsWordFilter()\nwords_doc = word_filter.get_matched_doc()\nprint(words_doc.text)\n# Output: \"Hello world How are you\"\n</code></pre>"},{"location":"user_guide/filtering_documents/#identify-roman-numerals-in-text","title":"Identify Roman Numerals in Text","text":"<pre><code>from lexos.filter import IsRomanFilter\nfrom tokenizer import Tokenizer\n\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(\"Book I begins with Chapter III and ends with Chapter VII.\")\n\nroman_filter = IsRomanFilter(attr=\"is_roman\")\nroman_filter(doc)\n\n# Extract Roman numerals\nfor token in doc:\n    if hasattr(token._, \"is_roman\") and token._.is_roman:\n        print(f\"Roman numeral found: {token.text}\")\n</code></pre>"},{"location":"user_guide/filtering_documents/#manage-custom-stop-words","title":"Manage Custom Stop Words","text":"<pre><code>from lexos.filter import IsStopwordFilter\nfrom lexos.tokenizer import Tokenizer\n\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\n\n# Add domain-specific stop words (case-insensitive by default)\ndomain_stopwords = [\"moreover\", \"furthermore\", \"therefore\"]\nstopword_filter = IsStopwordFilter()\ndoc = tokenizer.make_doc(\"dummy text\")\nstopword_filter(doc, stopwords=domain_stopwords, remove=False)\n\n# Now these are treated as stop words (including \"Moreover\", \"FURTHERMORE\", etc.)\ntext = \"Moreover, the data shows something. Therefore, we conclude.\"\ndoc = tokenizer.make_doc(text)\nmeaningful_words = [token.text for token in doc if not token.is_stop]\nprint(meaningful_words)\n# Output: ['data', 'shows', 'conclude']\n</code></pre>"},{"location":"user_guide/getting_started/","title":"Getting Started","text":""},{"location":"user_guide/getting_started/#overview","title":"Overview","text":"<p>Lexos is a library for constructing text analysis workflows. This normally means a step-by-step pipeline of collecting, processing, analyzing, and visualizating data. (The distinction between the analysis and visualization, however, is often blurred because most visualizations require some form of analysis.) Lexos offers different modules for performing these steps. The <code>Loader</code> and <code>Corpus</code> modules collect and create containers for storing and accessing data. The <code>Scrubber</code> module enables you to perform preprocessing steps on the texts in your data, such as normalizing whitespace or removing certain character patterns. The <code>Tokenizer</code> module uses Natural Language Processing (NLP) tools to extract features from your data \u2014 most importantly, countable tokens. This can be transformed into a document-term matrix with the <code>DTM</code> module. A typical workflow is shown below (dotted lines indicate optional steps).</p> <pre><code>flowchart LR\n   id1{Data} --&gt; id2(((Loader))) &amp; id3[(Corpus)]-. Preprocessing .-&gt; id4{Scrubber}-. Feature Recognition .-&gt; id5{Tokenizer} --&gt; id6{DTM}</code></pre> <p>The <code>DTM</code> module allows you to extract basic statistics which you can use to interpret your data.</p> <p>Lexos modules do not always have to be used in a strict sequential order. For instance, you can feed scrubbed or tokenized texts back into a corpus. You can also split your data at any time in the workflow with the <code>Cutter</code> module.</p> <p>The workflow above might be supplemented by another leading to analysis and visualization.</p> <pre><code>flowchart LR\n   id1{DTM}-.-&gt;id2(Analysis) &amp; id3([Visualization]) &amp; id4{Export}</code></pre> <p>Admittedly, the line between analysis and visualization can be blurred because visualizations are often produced by computational transformations of your data. In some cases, these tools may bypass the DTM by accepting data in other formats. Conceptually, however, Lexos is designed to provide tools for you to implement an end-to-end workflow whilst allowing you to export your data for use with other tools, if needed.</p>"},{"location":"user_guide/getting_started/#before-you-get-started","title":"Before You Get Started","text":"<p>Before you get started, make sure that you have installed Lexos.</p>"},{"location":"user_guide/getting_started/#basic-usage","title":"Basic Usage","text":"<p>Lexos workflows can be run conveniently in Jupyter notebooks simply by importing the relevant module (or the required functions and classes from the module). For instance, you can import the <code>Loader</code> with</p> <pre><code># Import the Lexos Loader class\nfrom lexos.io.loader import Loader\n\n# Instantiate a Loader object\nloader = Loader()\n\n# Load a text file\nloader.load(\"myfile.txt\")\n</code></pre> <p>This will work in a standalone script as well. Any errors will be printed to your notebook or console.</p> <p>Note for Developers</p> <p>If you are designing an app that uses Lexos \"under the hood\", it is good practice to import the <code>LexosException</code> class and re-write the last line above in a <code>try...except</code> clause:</p> <pre><code>from lexos.exceptions import LexosException\n\ntry:\n    loader.load(\"myfile.txt\")\nexcept LexosException as e:\n    print(e)\n</code></pre> <p>This will enable your application to handle errors without stopping the program.</p> <p>To learn about each of the individual Lexos modules, browse through the pages in this guide. More examples of the use of Lexos functions can be found in the Tutorial notebooks, and a full descriptions can be found in the API documentation.</p>"},{"location":"user_guide/kwic/","title":"Keywords in Context (KWIC)","text":""},{"location":"user_guide/kwic/#overview","title":"Overview","text":"<p>Keywords in Context (KWIC) is a common method of finding all the examples of a term in a document in the context of the text immediately before and after the term. Lexos provides sophisticated methods of searching for keywords that make the process easy.</p>"},{"location":"user_guide/kwic/#basic-usage","title":"Basic Usage","text":"<p>The basic procedure is as follows:</p> <pre><code># Import the Kwic class\nfrom lexos.kwic import Kwic\n\n# Define a text\ntext = \"It is a truth universally acknowledged, that a single  man in possession of a good fortune must be in want of a wife.\"\n\n# Define a pattern\npattern = \"universally\"\n\n# Create an instance of a Kwic object\nkwic = Kwic()\n\n# Pass the object the desired parameters\nkwic(docs=text, patterns=pattern, window=10)\n</code></pre> <p>This will display</p> doc context_before keyword context_after 0 Doc 1 s a truth universally acknowled <p>You will notice that the keywords <code>docs</code> and <code>patterns</code> are plural. This is because the <code>Kwic</code> class will also accept multiple documents and lists of patterns. A document, in the case, can be a raw text string, but spaCy <code>Doc</code> objects are also accepted. Patterns may be raw strings or regex patterns.</p> <p>The <code>window</code> keyword will by default provide a window of n characters around each keyword found.</p> <p>The standard output is a pandas DataFrame. You can either display this DataFrame directly, or assign it to a variable for further processing:</p> <pre><code>df = kwic(docs=text, patterns=pattern, window=10)\n</code></pre> <p>By default, your documents will be labelled \"Doc 1\", \"Doc 2\", \"Doc3\", etc. However, you can supply a list of labels with the <code>labels</code> keyword.</p> <p>Here are some other useful parameters:</p> <ul> <li><code>case_sensitive</code>: If set to <code>False</code>, the <code>Kwic</code> class will perform case-insensitive searches.</li> <li><code>sort_by</code>: If you wish to sort the DataFrame, use this keyword to set the column for sorting.</li> <li><code>ascending</code>: Set to <code>True</code> or <code>False</code> to set the order for sorting the DataFrame.</li> <li><code>as_df</code>: If set to <code>False</code>, the output will be a list of tuples, where the first item is the \"before\" window, the second item is the matched pattern, and the third item is the \"after\" window.</li> </ul>"},{"location":"user_guide/kwic/#searching-token-patterns","title":"Searching Token Patterns","text":"<p>If you have spaCy <code>Docs</code>, you may wish to search for token patterns. A simple way to do this is to set the matcher to \"tokens\".</p> <p>Consider the example above. If we have a spaCy <code>Doc</code>, we can re-write it as</p> <pre><code>kwic(docs=doc, labels=None, patterns=patterns, window=5, matcher=\"tokens\")\n</code></pre> doc context_before keyword context_after 0 Doc 1 that a single man in possession of a good fortune must <p>Now the <code>Kwic</code> class will search for the token \"universally\", and the \"before\" and \"after\" windows will be counted in tokens, rather than characters.</p> <p>We can use the <code>use_regex</code> keyword to use a regular expression for our pattern:</p> <pre><code>patterns = r\".ingle\"\n\nkwic(docs=doc, labels=None, patterns=patterns, window=5, matcher=\"tokens\", use_regex=True, case_sensitive=False)\n</code></pre> <p>This will find any token containing a character followed by \"ingle\" (and the search will be case insensitive).</p> doc context_before keyword context_after 0 Doc 1 universally acknowledged, that a single man in possession of a <p>We can also perform more sophisticated token-based searches using spaCy's rule-matching syntax. To use it, we set the <code>matcher</code> parameter to \"rule\". See the spaCy documentation for details of how to construct rules for token-based matching.</p> <pre><code>pattern1 = [{\"LOWER\": \"truth\"}, {\"LOWER\": \"universally\"}, {\"LOWER\": \"acknowledged\"}]\npattern2 = [{\"TEXT\": \"possession\"}]\npatterns = [pattern1, pattern2]\n\nkwic(docs=doc, patterns=patterns, window=5, matcher=\"rule\")\n</code></pre> doc context_before keyword context_after 0 Doc 1 It is a truth universally acknowledged , that a single man 1 Doc 1 that a single man in possession of a good fortune must <p>Finally, we can also search multi-token patterns using spaCy's <code>PhraseMatcher</code> by setting <code>matcher</code> to \"phrase\".</p> <pre><code>patterns = [\"truth universally acknowledged\", \"possession\"]\n\nkwic(docs=doc, patterns=patterns, window=5, matcher=\"phrase\")\n</code></pre> doc context_before keyword context_after 0 Doc 1 It is a truth universally acknowledged , that a single man 1 Doc 1 that a single man in possession of a good fortune must <p>This differs from the previous example because the patterns are first pre-processed into <code>Doc</code> objects, which can be significantly faster to process if you have large numbers of patterns.</p>"},{"location":"user_guide/loading_texts/","title":"IO","text":"<p>The <code>IO</code> module contains the classes and methods useful for loading in texts and text data from various souces and formats into a consistant structure so they can be analyzed within the Lexos enviroment.</p> <p>This module contains three main components:</p> <ol> <li><code>Loader</code>: The main loader used for Lexos. Designed to handle individual files (.txt, .pdf, and docx), directories of files, and zip archives.</li> <li><code>ParallelLoader</code>: An optimized version of <code>Loader</code> for loading large numbers of files using concurrent processing.</li> <li><code>DataLoader</code>: A specialized loader for structured data files such as CSVs, JSON, or Excel files.</li> </ol> <p>All loaders inherit from a common <code>BaseLoader</code> class that provides a bluepring and common features for other classes. It includes methods for loading files, processing text, and handling errors. If neither of the provided classes can accommodate the content you are trying to load, you can build a custom loader that derives from this class.</p> <p>All loaders built on <code>BaseLoader</code> have the following attributes for storing loaded data:</p> <ul> <li><code>paths</code>: File paths or other sources of the loaded texts.</li> <li><code>mime_types</code>: MIME types of the loaded items.</li> <li><code>names</code>: Names assigned to each loaded text.</li> <li><code>texts</code>: The text content of the loaded items.</li> <li><code>errors</code>: Any errors encountered during loading.</li> </ul> <p>Additionally loaders will have access to the following properties:</p> <ul> <li><code>records</code>: Returns a list of dictionaries, with each representing a loaded item with keys such as <code>name</code>, <code>path</code>, and <code>mime_type</code>.</li> <li><code>data</code>: Returns a single dictionary containing all of the data stored in the loader.</li> <li><code>df</code>: Returns the loaded file records in the form of a Pandas DataFrame.</li> </ul> <p>Common methods available to all loaders include:</p> <ul> <li><code>load_dataset</code>: Abstract method to be implemented by loaders.</li> <li><code>dedupe</code>: Removes duplicate entries from the loaded data and returns a DataFrame with the duplicates removed. The fields to be checked for duplication can be specified.</li> <li><code>show_duplicates</code>: Returns a DataFrame containing any duplicates found in the data. Can specify which fields to check for duplicates.</li> <li><code>reset</code>: Clear all data from a loader instance. Reset to an empty loader.</li> <li><code>to_csv</code>: Save the loaded data to a CSV file.</li> <li><code>to_excel</code>: Save the loaded data as an Excel file.</li> <li><code>to_json</code>: Save the loaded data to a JSON file.</li> </ul> <p>The features inherited from <code>BaseLoader</code> will be demonstrated when we look at the <code>Loader</code> and <code>BaseLoader</code> classes below.</p>"},{"location":"user_guide/loading_texts/#loader","title":"<code>Loader</code>","text":"<p>The <code>Loader</code> class is the main loader used in Lexos. It is designed to handle a variety of input formats and sources, including individual text files, directories of files, and zip archives.</p> <p>The <code>Loader</code> class can load files with the following extensions:</p> <ul> <li><code>.txt</code>: Plain text files.</li> <li><code>.pdf</code>: PDF documents.</li> <li><code>.docx</code>: Microsoft Word documents.</li> <li><code>.zip</code>: Zip archives containing any of the above file types.</li> </ul> <p>The <code>Loader</code> class automatically detects the file type based on the file extension and uses the appropriate method to extract the text content. It also handles errors gracefully, logging any issues encountered during loading. The path to the source file can be a local file path or a URL. For multiple files, the path can be a list of file paths or a path to a directory.</p> <p>Here is an example of how to use <code>Loader</code>:</p> <pre><code>from lexos.io.loader import Loader\n\n# Create a Loader instance\nloader = Loader()\n\n# Sample texts from various sources\nloader.load(\"path/to/file1.txt\")\nloader.load([\"path/to/file2.txt\", \"path/to/file3.txt\"])\nloader.load(\"path/to/directory_of_files\")\nloader.load(\"url/to/file4.txt\")\n</code></pre> <p>Once texts are loaded, they can be accessed through the <code>texts</code> attribute or the <code>df</code> property, which returns a pandas DataFrame of the loaded records. If there is a problem loading a file, the error will be logged in the <code>errors</code> attribute.</p> <p>By default, the <code>Loader</code> class assigns names to loaded texts based on the file name, minus the extension. However, custom names can be provided using the <code>names</code> parameter when loading files.</p> <pre><code>from lexos.io.loader import Loader\n\n# Create a Loader instance\nloader = Loader(names=[\"Doc1\", \"Doc2\"])\n\n# Sample texts\nloader.load([\"path/to/file1.txt\", \"path/to/file2.txt\"])\n\nprint(loader.names)\n# [\"Doc1\", \"Doc2\"]\n</code></pre> <p>Note</p> <p>Names assigned to documents can be useful as labels, especially when generating tabular representations or visualisations of your data.</p>"},{"location":"user_guide/loading_texts/#parallelloader","title":"<code>ParallelLoader</code>","text":"<p>The <code>ParallelLoader</code> class is an optimized version of <code>Loader</code> designed for loading large numbers of files efficiently using concurrent processing. It uses Python's <code>ThreadPoolExecutor</code> to load multiple files in parallel, which can significantly speed up loading times when working with hundreds or thousands of files.</p> <p>For large-scale corpus loading (1000+ documents), <code>ParallelLoader</code> can provide 5-20x speedup compared to sequential loading, especially when loading from network storage or processing CPU-intensive formats like PDFs.</p>"},{"location":"user_guide/loading_texts/#when-to-use-parallelloader","title":"When to Use ParallelLoader","text":"<p><code>ParallelLoader</code> is most beneficial for:</p> <ul> <li>Large file counts: Loading 100+ files, especially 1000+</li> <li>Remote files: Loading files from URLs with network latency</li> <li>Network storage: Files on network drives or remote filesystems where I/O latency is high</li> <li>CPU-intensive formats: PDFs and DOCX files that require parsing</li> <li>Mixed file types: Processing different file types that can be handled independently</li> </ul> <p>For small file counts (&lt;50 files) on fast local storage, the standard <code>Loader</code> may be faster due to threading overhead.</p>"},{"location":"user_guide/loading_texts/#basic-usage","title":"Basic Usage","text":"<p>The <code>ParallelLoader</code> API is identical to the standard <code>Loader</code>, making it a drop-in replacement:</p> <pre><code>from lexos.io.parallel_loader import ParallelLoader\n\n# Create a ParallelLoader instance\nloader = ParallelLoader()\n\n# Load files just like with Loader\nloader.load([\"path/to/file1.txt\", \"path/to/file2.txt\"])\nloader.load(\"path/to/directory_of_files\")\n\n# Access results the same way\nprint(loader.texts)\nprint(loader.names)\nprint(loader.df)\n</code></pre>"},{"location":"user_guide/loading_texts/#configuration-options","title":"Configuration Options","text":"<p><code>ParallelLoader</code> provides several options to customize performance:</p> <pre><code>from lexos.io.parallel_loader import ParallelLoader\n\n# Customize worker threads and batch size\nloader = ParallelLoader(\n    max_workers=16,          # Number of concurrent threads (default: auto-calculated)\n    worker_strategy=\"auto\",  # Worker allocation strategy (default: \"auto\")\n    batch_size=50,           # Files per batch (default: 100)\n    show_progress=True,      # Show progress bar (default: True)\n)\n\n# Load with custom callback for progress tracking\ndef my_progress(path, processed, total):\n    print(f\"Loaded {processed}/{total}: {path}\")\n\nloader = ParallelLoader(\n    show_progress=False,\n    callback=my_progress\n)\nloader.load(file_list)\n</code></pre>"},{"location":"user_guide/loading_texts/#worker-strategy-auto-tuning","title":"Worker Strategy (Auto-tuning)","text":"<p>The <code>worker_strategy</code> parameter controls how many worker threads are allocated based on your workload:</p> <ul> <li><code>\"auto\"</code> (default): Analyzes file types and automatically chooses the optimal strategy</li> <li>Detects CPU-intensive formats (PDF, DOCX) vs I/O-intensive formats (text files)</li> <li> <p>Adjusts worker count accordingly for best performance</p> </li> <li> <p><code>\"io_bound\"</code>: Allocates more workers for I/O-intensive operations</p> </li> <li>Best for: Large numbers of text files, network storage, remote URLs</li> <li> <p>Workers: <code>min(32, cpu_count * 4)</code></p> </li> <li> <p><code>\"cpu_bound\"</code>: Allocates fewer workers for CPU-intensive operations</p> </li> <li>Best for: Predominantly PDF or DOCX files requiring parsing</li> <li> <p>Workers: <code>min(16, cpu_count * 2)</code></p> </li> <li> <p><code>\"balanced\"</code>: Middle ground between I/O and CPU strategies</p> </li> <li>Best for: Mixed file types</li> <li>Workers: <code>min(24, cpu_count * 3)</code></li> </ul> <pre><code># Let ParallelLoader analyze and choose the best strategy\nloader = ParallelLoader(worker_strategy=\"auto\")\n\n# Or explicitly choose a strategy\nloader = ParallelLoader(worker_strategy=\"io_bound\")  # For text-heavy workloads\nloader = ParallelLoader(worker_strategy=\"cpu_bound\")  # For PDF-heavy workloads\n\n# Override with explicit worker count (ignores strategy)\nloader = ParallelLoader(max_workers=8)  # Always use 8 workers\n</code></pre>"},{"location":"user_guide/loading_texts/#progress-tracking","title":"Progress Tracking","text":"<p>By default, <code>ParallelLoader</code> displays a Rich progress bar showing the loading progress:</p> <pre><code>loader = ParallelLoader(show_progress=True)\nloader.load(files)  # Shows: Detecting file types... \u2501\u2501\u2501\u2501 100%\n                    #        Loading files...        \u2501\u2501\u2501\u2501 100%\n</code></pre> <p>You can disable the progress bar or provide a custom callback function:</p> <pre><code># No progress bar\nloader = ParallelLoader(show_progress=False)\n\n# Custom callback\ndef track_progress(path, processed, total):\n    if processed % 10 == 0:\n        print(f\"Progress: {processed}/{total} files\")\n\nloader = ParallelLoader(show_progress=False, callback=track_progress)\n</code></pre>"},{"location":"user_guide/loading_texts/#performance-tuning","title":"Performance Tuning","text":"<p>For optimal performance, adjust settings based on your use case:</p> <pre><code># For maximum speed (disable progress bar)\nloader = ParallelLoader(show_progress=False, max_workers=32)\n\n# For memory-constrained environments\nloader = ParallelLoader(batch_size=20, max_workers=4)\n\n# For network/remote files (more aggressive parallelization)\nloader = ParallelLoader(max_workers=64, batch_size=200)\n</code></pre>"},{"location":"user_guide/loading_texts/#dataloader","title":"<code>DataLoader</code>","text":"<p>Collections of texts are frequently stored or distributed in a single file, often with one document per line, or in a structured format like JSON. The <code>DataLoader</code> class allows you to load these files directly into a Lexos loader.</p>"},{"location":"user_guide/loading_texts/#loading-lineated-text-files","title":"Loading Lineated Text Files","text":"<p>The basic method for loading a file with one document per line is as follows:</p> <pre><code># Import the DataLoader class\nfrom lexos.io.data_loader import DataLoader\n\nloader = DataLoader()\nloader.load_lineated_text(\"path/to/file.txt\")\n</code></pre> <p>Note that each document will be named \"text001\", \"text002\", \"text003\", etc. unless you provide a list of document names with the <code>names</code> parameter:</p> <pre><code># Import the DataLoader class\nfrom lexos.io.data_loader import DataLoader\n\nloader = DataLoader(names=[\"author1\", \"author2\", \"author3\"])\nloader.load_lineated_text(\"path/to/file.txt\")\n</code></pre>"},{"location":"user_guide/loading_texts/#loading-csv-and-excel-files","title":"Loading CSV and Excel Files","text":"<p>The procedure is similar for CSV and Excel files. However, you must designate which columns contain the document name and text by indicating their headers with the <code>name_col</code> and <code>text_col</code> parameters.</p> <pre><code># Import the DataLoader class\nfrom lexos.io.data_loader import DataLoader\n\nloader = DataLoader()\nloader.load_csv(\"path/to/file.csv\", name_col=\"name\", text_col=\"content\")\nloader.load_csv(\"path/to/file.tsv\", sep=\"\\t\", name_col=\"name\", text_col=\"content\")\nloader.load_excel(\"path/to/file.xlsx\", name_col=\"name\", text_col=\"content\")\n</code></pre> <p>If you are working with a tab-separated file, just use the <code>sep</code>, parameter as shown above.</p> <p>Note</p> <p>Currently, your file must have headers. Setting the <code>name_col</code> and <code>text_col</code> by column index is on the roadmap.</p>"},{"location":"user_guide/loading_texts/#loading-json-files","title":"Loading JSON Files","text":"<p>In a JSON-formatted file, each document is a separate object consisting of fields in which the value is referenced by the field's key (e.g. <code>{\"text\": \"Some text here\"}</code>). When loading JSON files, it is necessary to specify the key indicating which field contains the text name and which field contains the text content. This is done with the <code>name_field</code> and <code>text_field</code> parameters, as shown below:</p> <pre><code># Import the DataLoader class\nfrom lexos.io.data_loader import DataLoader\n\nloader = DataLoader()\nloader.load_json(\"path/to/file.json\", name_field=\"name\", text_field=\"content\")\n</code></pre> <p>In standard JSON format, each document is separated by a comma. However, data is frequently formatted with each document separated by a new line, known as JSONL format. If your data is formatted as JSONL, indicate this with the <code>lines</code> parameter:</p> <pre><code>loader.load_json(\"path/to/file.json\", lines=True, name_field=\"name\", text_field=\"content\")\n</code></pre>"},{"location":"user_guide/loading_texts/#merging-data-into-standard-loaders","title":"Merging Data into Standard Loaders","text":"<p>Texts loaded from a dataset can be merged into a standard loader with the <code>Loader.load_dataset</code> method:</p> <pre><code># Create a Dataset instance and load some data\ndataset = DataSet()\ndataset.load_json(\"path/to/file.json\", name_field=\"name\", text_field=\"content\")\n\n# Create a Loader instance and load a single file\nloader.load(\"path/to/file.txt\")\n\n# Merge the dataset into the loader\nloader.load_dataset(dataset)\n</code></pre>"},{"location":"user_guide/loading_texts/#working-with-other-forms-of-data","title":"Working with Other Forms of Data","text":"<p>If your data is not in a format that can be loaded with the <code>Loader</code> or <code>DataLoader</code> classes, it is generally possible to use the Python standard library or third-party tools to load the data into memory and then assign it to an instance of <code>Loader</code>.</p> <p>However, you may wish to create your own custom loader class (e.g. one that uses an authentication token to access an online service) to introduce the logic required for your particular type of data. Custom loaders that inherit from the <code>BaseLoader</code> class are welcome as pull requests. If they seem useful to other users, they will be accepted into the main Lexos library.</p>"},{"location":"user_guide/preprocessing_steps/","title":"Proprocessing Steps","text":""},{"location":"user_guide/preprocessing_steps/#overview","title":"Overview","text":"<p>Before you begin to analyze your documents, you may which to perform a number of preprocessing procedures on them to prepare them for analysis.</p> <p>Lexos offers four methods of preprocessing:</p> <ol> <li>Scrubbing (also called text cleaning): Transformation of the raw text string for of your data</li> <li>Tokenization: Split your text into countable tokens, optionally with language-specific rules that may also annotate tokens with linguistic metadata.</li> <li>Cutting: Split your text into smaller texts (or perform the reverse by merging smaller texts). Boundaries for cutting can be based on a variety of string or token patterns, as well as structural divisions (milestones) in your texts.</li> <li>Filtering: Remove tokens from pre-tokenized texts based on token annotations.</li> </ol> <p>Click on each of the links to learn more.</p>"},{"location":"user_guide/rolling_windows/","title":"The Rolling Window Tool","text":"<p>Rolling window analysis is a method for tracing the frequency of terms within a designated window of tokens over the course of a document. It can be used to identify small- and large-scale patterns of individual features or to compare these patterns for multiple features. Rolling window analysis tabulates term frequency as part of a continuously moving metric, rather than in discrete segments. Beginning with the selection of a window, say 100 tokens, rolling window analysis traces the frequency of a term's occurrence first within tokens 1-100, then 2 to 101, then 3, 102, and so on until the end of the document is reached. The result can be plotted as a line graph so that it is possible to observe gradual changes in a token\u2019s frequency as the text progresses. Plotting different tokens on the same graph allows us to compare their frequencies over the progression of a document.</p> <p>Note</p> <p>Rolling Windows only works on a single document. If you would like to perform an analysis on a sequence of documents, you must first merge them into a single document.</p>"},{"location":"user_guide/rolling_windows/#basic-terminology","title":"Basic Terminology","text":"<p>In a Rolling Window analysis, you search for specific units within windows of n units. A unit can be a character, a token (span of characters), or a span of tokens. If you are using a spaCy language model, spans can correspond to semantic units, such as words or sentences. Lexos can perform complex searches of units within different types of windows, so it is important to have in mind what kind of search you want to perform.</p> <p>The first step is to generate a set of windows with the <code>Windows</code> class based on your desired units. You will then want to calculate how often your desired search terms occur in these windows. \"How often\" can be understood mathematically in a number of ways. The most obvious is a rolling count of the number of times a search term occurs in each window. But you can also understand this as the average number of times it occurs relative to the overall vocabulary, or the ratio of averages between two search terms. To obtain this information, you apply a calculator class. Lexos has three built-in calculator classes <code>Counts</code>, <code>Averages</code>, and <code>Ratios</code>, but you can also add custom calculator classes if the built-in ones do not provide the information you need.</p> <p>The results of the calculations can be viewed in a Pandas DataFrame or CSV file, but it can be useful to visualize them on a line graph so that you can see peaks and valleys in the occurrence of your search terms. Lexos provides built-in plotter classes to do this. The <code>SimplePlotter</code> class produces static images, and the <code>PlotlyPlotter</code> class produces interactive ones. The latter may be better for presentations and the latter can sometimes be easier to read.</p>"},{"location":"user_guide/rolling_windows/#generating-windows","title":"Generating Windows","text":"<p>When creating windows, you can use data as <code>input</code> in a number of formats:</p> <ul> <li>A string or list of strings (the latter might correspond words)</li> <li>A spaCy Doc or Span object</li> <li>A list of spaCy Token or Span objects</li> </ul> <p>You will need to specify the size of your window (<code>n</code>) and the type of unit used (<code>window_type</code>): \"characters\", \"spans\", or \"tokens\". In other words, if you choose <code>n=1000</code>, that will produce windows of 1000 characters, spans, or tokens, dependingo on what you choose for <code>window</code> type. Finally, you must choose an <code>output</code> type, which can be \"strings\" or \"tokens\".</p> <p>Here is a sample setup.</p> <pre><code># Import the Windows class\nfrom lexos.rolling_windows import Windows\n\n# Assume that you have a raw text string\nwindows = Windows(input=text, n=1000, window_type=\"characters\", output=\"strings\")\n</code></pre> <p>This will produce windows where each window is 1000 characters long. Here's another example:</p> <pre><code># Assume that you have a spaCy Doc\nwindows = Windows(input=doc, n=1000, window_type=\"tokens\", output=\"tokens\")\n</code></pre> <p>This will produce windows where each window is 1000 tokens long. If you chance <code>output</code> to strings, each window will be the character length of a window of 1000 tokens. In other words, since not all tokens are the same length in chararacters, the length of your windows will not be consistent.</p> <p>Note</p> <p>Some combinations of <code>input</code>, <code>window type</code>, and <code>output</code> are not possible. If Lexos cannot combine your choices, it will raise an error.</p> <p>You will then need to generate the instance with <code>windows()</code>. Alternatively, you can configure your <code>Windows</code> object and generate the windows at the same time like this:</p> <pre><code># Assume that you have a spaCy Doc\nwindows = Windows()\nwindows(input=doc, n=1000, window_type=\"tokens\", output=\"tokens\")\n</code></pre> <p>You can view the windows with <code>list(windows)</code>.</p> <p>Important</p> <p>The <code>Windows</code> class creates a generator, not a list of windows. If you try to access it in any way, the generator will be exhausted, and you will not be able to access them again. In this case, you will have to call <code>windows()</code> again to re-create the windows.</p>"},{"location":"user_guide/rolling_windows/#choosing-window-type-and-size","title":"Choosing Window Type and Size","text":"<p>The are no hard and fast rules to how to select window types and sizes. To some degree, it will depend on your data type and research question. It is always best to try several options to see what is most revealing. The following general guidelines may be helpful.</p> Doc  Type Window Type Suggested Size Reasoning Short story characters 200-500 May capture local patterns Novel/Book characters 500-2000 May balance detail and trends Short text tokens 20-50 Enough words for patterns Novel/Book tokens 50-200 May captures thematic shifts Poetry tokens (lines) 5-20 May respects verse structure"},{"location":"user_guide/rolling_windows/#aligning-window-boundaries-to-token-boundaries","title":"Aligning Window Boundaries to Token Boundaries","text":"<p>Imagine you had a spaCy Doc object from which you wanted to generate windows of characters. Since windows simply slide one unit at a time, in most cases window boundaries will not correspond to token boundaries. To make this clearer, consider a simple document with five tokens: \"The\", \"cat\", \"in\", \"the\", \"hat\". If we wanted windows of three tokens, the first window would be \"The\", but the second would be \"he c\". Perhaps this does not matter for your analysis, but, if it does, Lexos provides the possibility of aligning your windows on token boundaries with the <code>alignment_mode</code> keyword. \"Strict\" alignment (the default) behaves exactly as we have just observed, but, if you set <code>alignment_mode</code> to <code>None</code>, Lexos will attempt to snap the windows to token boundaries. It is recommended that you play with different alignment modes and examine the results to make sure they are satisfactory before you continue your analysis.</p>"},{"location":"user_guide/rolling_windows/#using-calculators","title":"Using Calculators","text":"<p>We can now import a calculator and perform a search on our windows. Each search term is considered a \"pattern\", which we can pass as a list using the <code>patterns</code> keyword.</p> <pre><code>from lexos.rolling_windows.calculators.counts import Counts\n\n# Using the `windows` object previously created\ncalculator = Counts(\n    windows=windows,\n    patterns=[\"a\", \"e\"],\n    case_sensitive=False,\n    mode=\"exact\"\n)\n</code></pre> <p>Here we are searching for the patterns \"a\" and \"e\" in our windows. Set <code>case_sensitive</code> to <code>True</code> if you want the search to distinguish between \"A\" and \"a\".</p> <p>Lexos can search for patterns using five different modes. The default \"exact\" mode will search for the exact character pattern in each unit in the window. The \"regex\" mode will interpret your pattern as a regular expression. In other words, if your window contains \"The end\", a search for \"e.\" will not find a match in \"exact\" mode. In \"regex\" mode, it will find that the pattern occurs twice, once followed by a space and once followed by \"n\". The \"spacy_rule\" setting employs spaCy's <code>Matcher</code> class, which allows you to set up powerful rules to do all sorts of complex token matching. The \"multi_token\" mode allows you to search for sequences of tokens without matching the exact token boundaries (for this, use \"multi_token_exact\"). It is recommended that you play with these settings to determine what best suits your research question.</p> <p>!!! \"Technical Note\"</p> <pre><code>- `exact` mode uses Python's string `count()` method.\n- `regex` mode uses `re.findall()` with appropriate flags.\n- `spacy_rule` mode requires spaCy Token objects, not strings.\n</code></pre> <p>If you wish to see the numerical data produced by the calculator, use the <code>to_df()</code> method to display it as a Pandas DataFrame. You can also use Pandas to save it to a CSV file (or other format), as shown below:</p> <pre><code>calculator = Counts(\n    windows=windows,\n    patterns=[\"a\", \"e\"],\n    case_sensitive=False,\n    mode=\"exact\"\n)\ndf = calculator.to_df()\ndf.to_csv(\"filename.csv\")\n</code></pre> <p>The <code>Averages</code> and <code>Ratios</code> calculators work in a similar fashion. Rolling averages provides a useful statistic if you have windows of different sizes or you need a normalized frequency statistic.</p> <p>Rolling ratios compares the frequencies of exactly two patterns. A statistic of <code>0.0</code> means that only the second pattern appears in a window. <code>0.5</code> means that both patterns appear equally. <code>1.0</code> means that only the first pattern appears in the window. Values closer 0 favor the second pattern. Values closer to 1 favor the first pattern.</p>"},{"location":"user_guide/rolling_windows/#plotting-the-results","title":"Plotting the Results","text":"<p>Once you have a pandas DataFrame, you can also use the pandas interface to save it, for instance, as a CSV file: <code>calculator.to_df().to_csv(\"filename\")</code>. You can also the built-in pandas plotting function to generate charts based on the results. See pandas.DataFrame.plot for further information.</p>"},{"location":"user_guide/rolling_windows/#plotter-classes","title":"Plotter Classes","text":"<p>Lexos has its own built-in plotting classes, <code>SimplePlotter</code> and <code>PlotlyPlotter</code>, which are specifically designed for the type of data produced by Rolling Windows. Each of the plotter classes has numerous parameters you can use to change the appearance of the plot, including the size of the image, title, and access label. Only some of the options are shown below. See the API documentation for explanations of all of the available parameters.</p> <p>Developer's Note</p> <p>If one of these plotters does not suit your need, you can also write your own custom calculator class that inherits from <code>BasePlotter</code>.</p>"},{"location":"user_guide/rolling_windows/#simpleplotter","title":"<code>SimplePlotter</code>","text":"<p>The <code>SimplePlotter</code> class generates high-quality static plots suitable for publications using Python's <code>matplotlib</code> library. This well-suited for reports, publications, presentations.</p> <pre><code>from lexos.rolling_windows.plotters import SimplePlotter\n\nplotter = SimplePlotter(title=\"Word Frequencies Over Time\")\nplotter.plot(averages.to_df())\n</code></pre> <p>To save the plot use the <code>save()</code> method:</p> <pre><code>plotter.save(\"filename.png\")\n</code></pre> <p>Files can be saved in <code>.jpg</code>, <code>.svg</code>, and <code>.pdf</code> formats by changing the file extension.</p>"},{"location":"user_guide/rolling_windows/#plotlyplotter","title":"<code>PlotlyPlotter</code>","text":"<p>The <code>PlotlyPlotter</code> class generates interactive web-based visualizations with hover tooltips and zoom capabilities. <code>PlotlyPlotter</code> is best for exploration, web presentation, detailed analysis with hover information.</p> <pre><code>from lexos.rolling_windows.plotters import PlotlyPlotter\n\ninteractive_plotter = PlotlyPlotter()\ninteractive_plotter.plot(averages.to_df(), show_plot=True)\n</code></pre> <p>Hovering over the top of the diagram displays a menu bar with the following interactive features:</p> <ul> <li>Hover over points to see exact values</li> <li>Zoom in/out with mouse wheel or zoom controls</li> <li>Pan by clicking and dragging</li> <li>Toggle lines on/off by clicking legend items</li> <li>Download plot as PNG using the camera icon</li> </ul>"},{"location":"user_guide/rolling_windows/#using-milestones","title":"Using Milestones","text":"<p>When tracing rolling windows patterns over the course of a document, it can be useful to see how the patterns occur with respect to structural divisions in the text. For instance, if you are analyzing a novel, you may wish to know the changes in the rolling average over a feature in each chapter. The point of transition between one section of the text and the next is known as a \"milestone\". Both rolling windows plotters allow you to display milestones using <code>show_milestones=True</code>, which places a vertical line on the graph at the milestone location. If <code>show_milestone_labels=True</code>, the graph will additionally display labels such as \"chapter\", \"scene\", \"line\", etc. above each line.</p> <p>You can manually create milestones dictionary consisting of labels and token locations (which will be character locations, if your windows are composed of characters). An example is shown below:</p> <pre><code>milestones = {\"Chapter 1\": 0, \"Chapter 2\": 500, \"Chapter 3\": 1000}\nplotter = SimplePlotter(\n    milestone_labels=milestones\n    show_milestones=True,\n    show_milestone_labels=True,\n    milestone_label_rotation=45\n)\n</code></pre> <p>Note</p> <p>The Lexos <code>milestones</code> module allows you to detect milestone locations and create a milestone dictionary. This can be much easier than constructing a milestone dictionary yourself.</p>"},{"location":"user_guide/scrubbing_texts/","title":"Scrubbing Texts","text":"<p>\"Scrubbing\" is Lexos jargon for preprocessing raw text strings. This normally done prior to any analysis in order to clean up idiosyncracies in the text which we don't want to factor into our analysis. Lexos provides many functions for performing this text cleaning through the <code>scrubber</code> module, or Scrubber.</p> <p>The Scrubber module has the following features:</p> <ul> <li>A built-in registry of commonly used, and reusable, component functions</li> <li>A modular pipeline for text scrubbing</li> <li>Easy addition and removal of pipeline components</li> <li>Support for custom components and configuration</li> </ul> <p>Scrubbing works by applying a single function or a pipeline of functions to the text, with each function applied in the order given.</p> <p>Scrubber can be defined as a destructive preprocessor. In other words, it changes the text as loaded in ways that potentially make mapping the results onto the original text impossible. It is therefore best used before other procedures so that the scrubbed text is essentially treated as the \"original\" text. This differs from the Tokenizer, which divides the text into \"tokens\" (often words) without destroying the original text.</p> <p>Note</p> <p>In the Lexos web app, Scrubber is used to tokenize the text before any other scrubbing actions occur. In the Lexos Python library, these preprocessing and tokenization are kept strictly separate.</p>"},{"location":"user_guide/scrubbing_texts/#scrubber-components","title":"Scrubber Components","text":"<p>Scrubber components are divided into four categories:</p> <ol> <li>Normalize components are used to manipulate text into a standardized form.</li> <li>Remove components are used to remove strings and patterns from text.</li> <li>Replace components are used to replace strings and patterns in text.</li> <li>Tags components are used to remove and replace tags, elements, attributes, and their values in texts marked up in HTML or XML.</li> </ol> <p>Follow the links above to read about the functions in each of Scrubber's components. To learn more about how the <code>scrubber</code> module works, take a look at the documentation on its internal components.</p> <p>Developer's Note</p> <p>Many of the functions and resources in Scrubber are built on top of the preprocessing functions in the Python Textacy library, although sometimes with modifications. Textacy is installed with Lexos, so it can also be imported and called directly if necessary.</p>"},{"location":"user_guide/scrubbing_texts/#loading-components","title":"Loading Components","text":"<p>Components must be loaded before they can be used. Each loaded component is a function, which we can then use or feed to a scrubbing pipeline.</p> <p>There are several ways to load Scrubber components into a Python file. The simplest is to import the component directly:</p> <pre><code>from scrubber.normalize import lower_case\n</code></pre> <p>It is also possible to load the entire registry, called <code>scrubber_components</code>, and get individual components as needed:</p> <pre><code># Load the Scrubber components registry\nfrom lexos.scrubber import scrubber_components\n\n# Load a single component from the registry\nlower_case = scrubber_components.get(\"lower_case\")\n</code></pre> <p>In addition, Lexos provides a <code>get_components()</code> helper function to load components from the registry:</p> <pre><code># Load the helper functions\nfrom lexos.scrubber import get_components\n\n# Load multiple components using the helper function\npunctuation, remove_digits = get_components(\"punctuation\", \"digits\")\n</code></pre> <p>Note</p> <p>The <code>get_components()</code> function will also accept lists and tuples of component names, as well as single component names. However, if you wish to get a single component, you can also import <code>get_component()</code> and use that function instead.</p> <p>Which method you choose will depend on your preference and your workflow. Getting components from the registry by their string names can be especially helpful when developing applications. In the examples below, we will use the <code>get_components()</code> function for consistency.</p>"},{"location":"user_guide/scrubbing_texts/#custom-scrubbing-components","title":"Custom Scrubbing Components","text":"<p>Although Scrubber provides many component functions that perform common tasks like removing punctuation or HTML tags, users can also write custom components for use with Scrubber. These components are written like a normal functions and then added to the component registry. Below is an example with a custom <code>title_case</code> function.</p> <pre><code># Define the custom function\ndef title_case(text: str) -&gt; str:\n    \"\"\"Our custom function to convert text to title case.\"\"\"\n    return text.title()\n\n# Register the custom function\nscrubber_components.register(\"title_case\", func=title_case)\n</code></pre> <p>To use a custom scrubbing function, you must register it before you call <code>get_component()</code> or <code>get_components()</code>.</p> <p>Developer's Note</p> <p>The Scrubber component registry is managed using the Python catalogue library, which also allows you to register functions with a decorator.</p> <pre><code>@scrubber_functions.register(\"title_case\")\ndef title_case(text: str) -&gt; str:\n    \"\"\"Our custom function to convert text to title case.\"\"\"\n    return text.title()\n</code></pre>"},{"location":"user_guide/scrubbing_texts/#using-components","title":"Using Components","text":"<p>Loaded component functions can be called like any normal function. For example: <code>scrubbed_text = remove_digits(\"Lexos123\")</code> will return \"Lexos\".</p> <p>If you are intending to apply multiple components to a single text, the more efficient method is to use a pipeline (discussed below).</p>"},{"location":"user_guide/scrubbing_texts/#scrubbing-pipelines","title":"Scrubbing Pipelines","text":"<p>When scrubbing texts, we may need to apply Scrubber components in a particular order. For this, we need a scrubbing pipeline, which we can create using either the <code>make_pipeline()</code> function or the <code>Scrubber</code> class. Each of these methods is detailed below.</p>"},{"location":"user_guide/scrubbing_texts/#using-make_pipeline","title":"Using <code>make_pipeline()</code>","text":"<p>To make a pipeline with the <code>make_pipeline()</code> function, we import the function and pass our components to it in the order we want them to be implemented.</p> <pre><code>from lexos.scrubber import make_pipeline\nfrom lexos.scrubber import get_components\n\nlower_case, punctuation = get_components(\"lower_case\", \"punctuation\")\n\n# Make the pipeline\npipe = make_pipeline(lower_case, punctuation)\n\n# Scrub the text\nscrubbed_text = pipe(\"Lexos is the number 12 text analysis tool!!\")\n</code></pre> <p>This will return \"lexos is the number 12 text analysis tool\".</p> <p>Many Scrubber functions take additional keyword arguments. To pass them to a pipeline, it is necesary to use the <code>functools.partial</code> function:</p> <pre><code>from functools.import partial\nfrom lexos.scrubber import get_components\n\nlower_case, punctuation, remove_digits = get_components(\"lower_case\", \"punctuation\", \"digits\")\n\n# Make the pipeline\npipe = make_pipeline(\n    lower_case,\n    punctuation,\n    partial(remove_digits, only=[\"1\"])\n)\n\n# Scrub the text\nscrubbed_text = pipe(\"Lexos is the number 12 text analysis tool!!\")\n</code></pre> <p>This will return \"lexos is the number 2 text analysis tool\". Notice that our <code>remove_digits()</code> function accepts the <code>only</code> keyword. So we pass it and its keyword arguments to the pipeline by wrapping it in the <code>partial()</code> function.</p> <p>You can also pass a list or tuple of components to <code>make_pipeline()</code> directly for single-use pipelines:</p> <pre><code>pipes = (lower_case, punctuation)\npipe = make_pipeline(pipes)\n\nscrubbed_text = pipe(\"Lexos is the number 12 text analysis tool!!\")\n</code></pre> <p>This will produce the same result.</p> <p>Lexos also provides the <code>scrub()</code> function, which takes a text and pipeline as arguments.</p> <pre><code>from lexos.scrubber import scrub\n\npipes = (lower_case, punctuation)\npipeline = make_pipeline(pipes)\n\nscrubbed_text = scrub(\n    \"Lexos is the number 12 text analysis tool!!\",\n    pipeline\n)\n</code></pre> <p>This is an alternative way of applying the pipeline.</p>"},{"location":"user_guide/scrubbing_texts/#using-the-scrubber-class","title":"Using the <code>Scrubber</code> Class","text":"<p>The Lexos <code>Scrubber</code> class also provides a more object-oriented approach, which may be more useful for some workflows. Start by creating an instance of the class. You can then add components to the pipeline with the <code>add_pipe()</code> method:</p> <pre><code>from lexos.scrubber import Scrubber\n\nscrubber = Scrubber()\nscrubber.add_pipe(\"lower_case\")\n</code></pre> <p>Notice that if the input is the string name of a component, it will automatically be fetched from the registry.</p> <p>The <code>add_pipe()</code> method can also take a list or tuple of components such as <code>[\"lower_case\", \"remove_digits\"]</code>. If a function takes keyword arguments, it can be passed as a <code>partial</code>, just as in the <code>pipe()</code> function discussed above.</p> <pre><code>from functools import partial\nscrubber.add_pipe([\"lower_case\", partial(remove_digits, only=[\"1\"])])\n</code></pre> <p>As an alternative, you can pass a tuple with the keyword arguments in a dictionary:</p> <pre><code>scrubber.add_pipe([\"lower_case\", (\"digits\", {\"only\": [\"1\"]})])\n</code></pre> <p>Multiple components will be added to the pipeline in the order they are passed. You can insert components in particular positions at any time using the <code>first</code>, <code>last</code>, <code>before</code>, and <code>after</code> arguments:</p> <pre><code># Add `remove_digits` to the beginning of the pipeline\nscrubber.add_pipe(\"digits\", first=True)\n\n# Add `remove_digits` to the end of the pipeline\nscrubber.add_pipe(\"digits\", last=True)\n\n# Add `remove_digits` before `lower_case`\nscrubber.add_pipe(\"digits\", before=\"lower_case\")\n\n# Add `remove_digits` after `lower_case`\nscrubber.add_pipe(\"digits\", after=\"lower_case\")\n</code></pre> <p>The <code>before</code> and <code>after</code> arguments can also take an integer indicating the position (starting with 0) within the pipeline.</p> <p>Once the pipeline is set up, you can scrub text with the <code>Scrubber.scrub()</code> method:</p> <pre><code>scrubbed_text = scrubber.scrub(\"Lexos is the number 12 text analysis tool!!\")\n</code></pre> <p>This returns \"lexos is the number 2 text analysis tool!!\".</p> <p>It is also possible to apply the scrubbing pipeline to a list or tuple of texts using the <code>Scrubber.pipe()</code> method:</p> <pre><code>texts = [\n    \"Lexos is the number 12 text analysis tool!!\",\n    \"Lexos is the number 1 text analysis tool!!\"\n]\nscrubbed_texts = scrubber.pipe(texts)\nfor text in scrubbed_texts:\n    print(text)\n\n# or, better:\nscrubbed_texts = list(scrubber.pipe(texts))\nfor text in scrubbed_texts:\n    print(text)\n</code></pre> <p>Important</p> <p>The <code>Scrubber.pipe()</code> method returns a generator, so use <code>list(scrubber.pipe(texts))</code> if you need a list of texts. Otherwise, <code>scrubbed_texts</code> will be <code>None</code> if you try to access it again.</p> <p>Under the hood, <code>Scrubber</code> uses the <code>Pipe</code> class to manage the pipeline. Each component added to the pipeline is converted to a <code>Pipe</code> object, which has a string <code>name</code> attribute and an <code>opts</code> dictionary to store keyword arguments accepted by the component function. It also has a <code>__call__()</code> method that applies the component to the text. You can create a <code>Pipe</code> object directly and use it to scrub text:</p> <pre><code>from lexos.scrubber import Pipe\n\ntext = \"Number 12 is the best number!\"\nmy_pipe = Pipe(\"digits\", {\"only\": [\"1\"]})\nmy_pipe(text) # Returns \"Number 2 is the best number!\"\n</code></pre> <p>You can even create and apply your own pipeline:</p> <pre><code>text = \"Number 12 is the best number!\"\npipes = [Pipe(\"lower_case\"), Pipe(\"digits\", {\"only\": [\"1\"]})]\nfor pipe in pipes:\n    text = pipe(text) # Returns \"number 2 is the best number!\"\n</code></pre> <p>The Scrubber <code>add_pipe()</code> and <code>pipe()</code> methods also accept <code>Pipe</code> objects or iterables of <code>Pipe</code> objects, which in many use cases can be a more convenient way to manage a pipeline.</p> <p>Developer's Note</p> <p>When a <code>Pipe</code> object is instantiated, it automatically validates that the <code>name</code> and <code>opts</code> are of the correct data types, that the registry has been imported, and that the specified component is in the registry. If is not, the <code>Pipe</code> object raises an error.</p>"},{"location":"user_guide/scrubbing_texts/#managing-the-pipeline","title":"Managing the Pipeline","text":"<p>After the pipeline is set up, you can use the following methods to manage it:</p> <ol> <li><code>add_pipe()</code>: Add additional components to the pipeline.</li> <li><code>remove_pipe()</code>: Remove a pipe from the pipeline (takes the string name of the component or a list of component names).</li> <li><code>reset()</code>: Reset the pipeline to an empty list.</li> </ol> <p>If an existing component is added to the pipeline, any options will be merged with the existing options.</p> <p>The <code>pipe()</code> method allows the existing configuration to be overridden using the <code>disable</code> and <code>component_cfg</code> arguments. The <code>disable</code> argument takes a list of component names to disable, while the <code>component_cfg</code> argument takes a dictionary of component names and the options to override. Scrubbing will be applied to the text according to these settings, but the stored pipeline will not be modified. The code below provides some examples:</p> <pre><code>scrubber = Scrubber()\nscrubber.add_pipe([\n    \"lower_case\",\n    \"punctuation\",\n    partial(\"digits\", only=[\"1\"])\n])\n\ntext = \"This is a sample text with some digits, 12345, and some punctuation! Let's see how it works.\"\n\n# Scrub the text using `pipe()` with the `digits` component disabled\nscrubbed_text = list(scrubber.pipe([text], disable=[\"digits\"]))[0]\n# Returns \"this is a sample text with some digits 12345 and some punctuation lets see how it works\"\n\n# Remove the \"punctuation\" component from the pipeline\nscrubber.remove_pipe(\"punctuation\")\n\n# Scrub the text with `scrub()`\nscrubbed_text = scrubber.scrub(text)\n\n# Returns \"this is a sample text with some digits, 2345, and some punctuation! let's see how it works.\n</code></pre> <p>Notice that the <code>disable</code> keyword in <code>pipe()</code> only applies to that operation. The <code>remove_pipe()</code> method removes a component from the pipeline entirely.</p>"},{"location":"user_guide/the_document_term_matrix/","title":"The Document-Term Matrix","text":""},{"location":"user_guide/the_document_term_matrix/#overview","title":"Overview","text":""},{"location":"user_guide/the_document_term_matrix/#about-the-document-term-matrix","title":"About the Document-Term Matrix","text":"<p>A document-term matrix (DTM) is the standard interface for analysis and information of document data. It consists in its raw form of a list of token counts per document in the corpus. Each unique token form is called a term. Thus it is really a list of term counts per document, arranged as matrix.</p> <p>Producing a DTM is easy with Lexos. All you need a is list of document tokens and a list of labels for each document (the labels are human-readable names which would otherwise be referenced by numeric indices). In the example below, we will use spaCy docs as the input since we can iterate through their tokens just like a list.</p> <pre><code>from lexos.dtm import DTM\nfrom lexos.tokenizer import Tokenizer\n\n# Define some texts and their labels\ntexts = [\n    \"Our first text.\",\n    \"Our second text.\",\n    \"Out third text.\"\n]\nlabels = [\"Doc1\", \"Doc2\", \"Doc3\"]\n\n# Tokenize the texts\ntokenizer = Tokenizer()\ndocs = list(tokenizer.make_docs(texts=texts))\n\n# Create a Document-Term Matrix (DTM)\ndtm = DTM()\ndtm(docs=docs, labels=labels)\n</code></pre> <p>If we did not want to use spaCy docs, we would need to have a list containing lists of tokens like this:</p> <pre><code>docs = [\n    [\"Our\", \"first\", \"text\"],\n    [\"Our\", \"second\", \"text\"],\n    [\"Our\", \"third\", \"text\"]\n]\n</code></pre> <p>Developer's Note</p> <p>Lexos uses Textacy's Vectorizer as the default vectorizer. It is possible to use Textacy directly to produce a DTM. For instance, the following method will produce a a DTM containing the raw term counts for each document.</p> <pre><code>from textacy.representations.vectorizers import Vectorizer\nvectorizer = Vectorizer(tf_type=\"linear\", idf_type=None, norm=None)\ntokenized_docs = []\nfor doc in docs:\n    tokenized_docs.append(token.text for token in doc)\nvectorizer.fit_transform(tokenized_docs)\n</code></pre> <p>Using the Lexos <code>DTM</code> class allows you to swap in your own custom vectorizer and gives access to additional helper methods such as <code>to_df()</code> to output the DTM as a pandas DataFrame.</p>"},{"location":"user_guide/the_document_term_matrix/#understanding-the-vectorizer","title":"Understanding the Vectorizer","text":"<p>When you create an instance of the <code>DTM</code> class, you automatically assign it a vectorizer. By default, this is Textacy's <code>Vectorizer</code> class. Here's how it works:</p> <ul> <li>The <code>Vectorizer</code> scans all documents to build a vocabulary of unique terms (token forms).</li> <li>It then counts the occurrences of each term in each document, resulting in a sparse matrix where rows represent documents and columns represent terms.</li> </ul> <p>About Sparse Matrixes</p> <p>Since each document only contains a small subset of all terms in the corpus, a document-term matrix can be very large and mostly filled with zeros. A sparse matrix is useful for storage, especially with large corpora, because it only stores nonzero values. Lexos uses data structures from the <code>scipy.sparse</code> library to store the DTM as a sparse matrix to make computations faster, which allows you to work with large corpora without running into memory issues. You can learn more about the <code>scipy.sparse</code> library.</p> <p>If you need a dense (regular) matrix for certain operations or compatibility with other libraries, you can convert the sparse DataFrame to a dense one by calling:</p> <pre><code>dense_df = dtm.to_df().sparse.to_dense()\n</code></pre> <p>Be aware that this may use a lot of memory for large corpora.</p> <p>The default vectorizer can be configured to perform additional culling or normalization functions. Culling refers to reducing the size of the matrix to include only part of the data. Normalization refers to performing additional calculations on the raw term counts in order to better represent the sigificance of those counts within the broader corpus (e.g. to take into account document varying lengths). Each of these categories is discussed below.</p>"},{"location":"user_guide/the_document_term_matrix/#culling-the-dtm","title":"Culling the DTM","text":"<p>In many cases, you will want to cull terms from your DTM in order to reduce the size of the data or to remove terms which you think might not be meaningful for your research. A common form of culling is to restrict the data to the n most-frequently occurring terms. You can do this with the <code>max_n_terms</code> parameter. You can also restrict your data to terms occurring in a minimum number of documents with <code>min_df</code> or a maximum number of documents with <code>max_df</code>. Here is an example using all three:</p> <pre><code>dtm = DTM(max_n_terms=100, min_df=2, max_df=5)\n</code></pre> <p>Depending on your workflow, you can also configure the vectorizer directly or when you call the <code>DTM</code> instance. Here are some examples show the three alternative ways to do it:</p> <pre><code># Configure the DTM instance\ndtm = DTM(max_n_terms=100)\n\n# Configure the DTM tokenizer directly\ndtm.vectorizer.min_df=2\n\n# Set the parameters when calling the DTM instance\ndtm(docs=docs, labels=labels, max_df=5)\n</code></pre> <p>Feel free to use whichever approach you find most comfortable.</p> <p>Note</p> <p>A further method of limiting the vocabulary is to provide a list of specific terms to be included in the matrix using the <code>vocabulary_terms</code> parameter.</p>"},{"location":"user_guide/the_document_term_matrix/#normalizing-the-values","title":"Normalizing the Values","text":"<p>The vectorizer is configured by default to generate a matrix of raw counts. However, it can often be beneficial to normalize the values in some way such as by calculating the term's frequency in proportion to all terms in the corpus. Or, if your documents vary in length, it can be beneficial to apply a weighting function. The vectorizer allows you to do that with the <code>tf_type</code>, <code>idf_type</code>, <code>dl_type</code>, and <code>norm</code> parameters:</p> <ul> <li><code>tf_type</code> controls how term frequencies are calculated (e.g., raw counts, log-scaled, binary presence/absence). Options: \"linear\", \"sqrt\", \"log\", \"binary\". Default is \"linear\".</li> <li><code>idf_type</code> controls inverse document frequency type, how document frequency scaling is applied (for TF-IDF weighting). Options: None, \"linear\", \"sqrt\", \"log\". Default is None.</li> <li><code>dl_type</code>: Controls normalization based on document length. Options: None, \"linear\", \"sqrt\", \"log\". Default is None.</li> <li><code>norm</code>: Applies vector normalization. Options: None, \"l1\", \"l2\". Normalizes the resulting vectors (rows) to unit length. L1 normalization scales the term frequencies in each document so that the sum of the absolute values equals 1. This means each document vector is divided by the sum of its term frequencies, turning the values into proportions that sum to 1. L2 normalization scales the term frequencies so that the sum of the squares of the values equals 1 (i.e., the Euclidean norm is 1). This is useful for algorithms that are sensitive to the length of the document vectors, such as cosine similarity. Default is None.</li> </ul> <p>!!! note \"What Settings Should I Choose?</p> <pre><code>If you only want raw counts, just use the default settings. Changing an of the other settings will implement various types of weighting functions. The &lt;a href=\"https://github.com/chartbeat-labs/textacy/blob/f08ecbc46020f514b8cbb024778ec4f80456291f/src/textacy/representations/vectorizers.py#L163\" target=\"_blank\"&gt;Textacy source code&lt;/a&gt; provides helpful advice, which is partially reproduced here.\n\n&gt; In general, weights may consist of a local component (term frequency),\na global component (inverse document frequency), and a normalization\ncomponent (document length). Individual components may be modified:\nthey may have different scaling (e.g. tf vs. sqrt(tf)) or different behaviors\n(e.g. \"standard\" idf vs bm25's version). There are *many* possible weightings,\nand some may be better for particular use cases than others. When in doubt,\nthough, just go with something standard.\n\nOne of the most commonly-used weighting settings is &lt;a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\"&gt;term frequency-inverse document frequency (TF-IDF)&lt;/a&gt;, which measures the importance of a term to a document in a collection of documents, adjusted for the fact that some terms appear more frequently than others. The Lexos web app's implementation of TF-IDF has the following settings:\n\n- `tf_type=\"log\"`\n- `idf_type=\"smooth\"`\n- `norm=\"l2\"`\n</code></pre>"},{"location":"user_guide/the_document_term_matrix/#getting-term-counts-and-frequencies","title":"Getting Term Counts and Frequencies","text":"<p>Once you have generated your DTM, you can extract useful information from its properties:</p> <ul> <li><code>DTM.shape</code>: returns a tuple with the width and height of the matrix.</li> <li><code>DTM.sorted_terms_list</code>: Returns a sorted list of terms in the DTM.</li> <li><code>sorted_term_counts</code>: Returns a sorted dictionary of terms and their total counts across all documents in the DTM.</li> </ul> <p>Note</p> <p>By default, terms are sorted according to the rules of language used by your operating system. You can set the DTM to use a different sorting algorithm with the <code>alg</code> keyword, which takes a <code>natsorted.ns.LOCALE</code> object. For further information, see the natsort documentation.</p> <p>Perhaps the most useful method of the <code>DTM</code> class is <code>to_df()</code>, which converts the matrix to a pandas DataFrame for display or for further manipulation. As a DataFrame, the output can be modified using the full range of options available in the pandas API. However, <code>to_df()</code> provides parameters that can ease the process:</p> <ul> <li><code>by</code>: The term or terms to sort by.</li> <li><code>ascending</code>: Whether to sort by ascending values.</li> <li><code>as_percent</code>: Whether to convert counts to percentages.</li> <li><code>rounding</code>: The number of digits after the decimal point to include.</li> <li><code>transpose</code>: Whether to pivot the rows and columns in the matrix.</li> <li><code>sum</code>: Add a column showing the sum of each row.</li> <li><code>mean</code>: Add a column showing the mean of each row.</li> <li><code>median</code>: Add a column showing the median of each row.</li> </ul> <p>Important</p> <p>The <code>transpose</code> parameter is applied before the other parameters and may not work in tandem with them. If you need to transpose a DataFrame after applying <code>sum</code>, <code>mean</code>, or <code>median</code>, it is better to use the pandas <code>transpose()</code> method:</p> <pre><code># Transpose the DataFrame after creation\ndtm.df(sum=True).transpose()\n\n# Or use the alternative `T` property\ndtm.df(sum=True).T\n</code></pre>"},{"location":"user_guide/the_document_term_matrix/#visualising-the-dtm","title":"Visualising the DTM","text":"<p>Once a document-term matrix table has been generated as a pandas dataframe, it becomes possible to use any of the Pandas plotting methods to visualise the data. Here is a short example of a bar chart containing the top 20 terms in the DTM:</p> <pre><code># Get the first 20 rows of the DTM as a DataFrame sorted by sum\ndf = dtm.to_df(sum=True, by=\"Total\", ascending=False)[0:20]\n\n# Plot the DataFrame\ndf.Total.plot(\n    kind=\"bar\",\n    title=\"Top 20 Most Frequent Terms\",\n    xlabel=\"Terms\",\n    ylabel=\"Frequency\"\n)\n</code></pre> <p>See the Pandas <code>DataFrame.plot</code> documentation for the complete range of keywords.</p> <p>Note</p> <p>By default, <code>pandas.DataFrame.plot</code> uses the <code>matplotlib</code> plotting library. If you are familiar with <code>matplotlib</code>, you can use it directly by using the data from the DataFrame. The same goes for any other plotting library. In Lexos, interactive plots are frequently made using the Plotly library, for which Pandas has a backend. To use it, simply call the following code:</p> <pre><code>import pandas as pd\npd.options.plotting.backend = \"plotly\"\ndf.Total.plot(\n        kind=\"bar\",\n        title=\"Top 20 Most Frequent Terms\",\n        labels={\"index\": \"Terms\", \"value\": \"Frequency\"}\n)\n</code></pre> <p>Note that the keywords described in Pandas documentation apply only to the <code>matplotlib</code> backend. For Plotly, you will beed to consult the equivalent Plotly documentation to find the appropriate keywords (<code>labels</code>, in the example above).</p> <p>Lexos word clouds and bubble charts are also ideal for visualising DTMs. Word clouds can be generated for the entire DTM or for individual documents. Multiple word clouds arrange for comparison are referred to as multiclouds. For information on generating these and other visualizations, see the Visualization page.</p>"},{"location":"user_guide/the_document_term_matrix/#advanced-usage-with-scikit-learn-vectorizers","title":"Advanced Usage with <code>scikit-learn</code> Vectorizers","text":"<p>The popular machine-learning library <code>scikit-learn</code> (<code>sklearn</code>) provides its own vectorizer classes, such as <code>CountVectorizer</code> and <code>TfidfVectorizer</code>, which often form components of machine-learning pipelines. However, these tokenizers use simple regex patterns, rather than language models, to tokenize documents. In the example below, we'll show how you can use the scikit-learn's <code>CountVectorizer</code> as part of a pipeline for training a scikit-learn logistic regression model whilst still leveraging language-specific knowledge available in a document tokenised with Lexos.</p> <pre><code># Scikit-learn imports\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. We also need the Lexos `tokenizer` module\nfrom lexos.tokenizer import Tokenizer\n\n# 2. Create a Lexos Tokenizer instance\nlexos_tokenizer = Tokenizer(model=\"en_core_web_sm\")\n\n# 3. Define a custom tokenizer function to return lists of tokens\n# We'll get token lemmas instead of text just to show that we have\n# access to NLP capabilities.\ndef my_tokenizer(text, tokenizer=lexos_tokenizer):\n    return [token.lemma_ for token in tokenizer.make_doc(text)]\n\n# 4. Create a `CountVectorizer` instance\n# Notice that we supply `CountVectorizer`, passing it our own tokenizer.\n# `CountVectorizer` expects its own pattern for tokenizing, so we have to set\n# `token_pattern=None`, or it'll give us a warning.\ncount_vectorizer = CountVectorizer(tokenizer=my_tokenizer, token_pattern=None)\n\n# Define the steps of the scikit-learn pipeline\npipeline_steps = [\n    ('vectorizer', count_vectorizer),\n    ('logistic_regression', LogisticRegression())\n]\n\n# Create the pipeline\nmodel_pipeline = Pipeline(pipeline_steps)\n\n# Now, model_pipeline can be used like any other scikit-learn estimator\nmodel_pipeline.fit(X_train, y_train)\npredictions = model_pipeline.predict(X_test)\n</code></pre> <p>Only the steps numbered 1-4 are really important for showing how to construct a document-term-matrix (the rest is all specific to running the logistic regression pipeline).</p> <p>If you need to construct a Lexos <code>DTM</code> from documents tokenized with <code>scikit-learn</code>, you can follow the example below:</p> <pre><code># Set our custom tokenizer in CountVectorizer\n# Note that `CountVectorizer` expects `token_pattern=None`\n# when using a custom tokenizer.\nvectorizer = CountVectorizer(tokenizer=my_tokenizer, token_pattern=None)\n\n# Use CountVectorizer to fit and transform the documents\ndocument_term_matrix = vectorizer.fit_transform(docs)\n\n# Get a list of terms from the vectorizer\nterms_list = vectorizer.get_feature_names_out()\n\n# Create a new `DTM` instance\nnew_dtm = DTM(docs=docs, labels=labels)\n\n# Assign our scikit-learn matrix as a numpy array to the new instance\nnew_dtm.doc_term_matrix=document_term_matrix.toarray()\n\n# Assign the vocabulary terms to the `DTM`'s vectorizer\n# Note that the `_validate_vocabulary()` method returns a tuple,\n# so we take the first element.\nnew_dtm.vectorizer.vocabulary_terms = new_dtm.vectorizer._validate_vocabulary(\n    terms_list)[0]\n</code></pre> <p>This should enable you to access all properties and methods of the <code>DTM</code> class.</p>"},{"location":"user_guide/tokenizing_texts/","title":"Tokenizing Texts","text":"<p>Many computational methods of studying texts require the text to be split into smaller, countable units called tokens. These tokens can be words, phrases, or even characters, depending on the method being used. The process of splitting a text into tokens is called tokenization.</p> <p>A tokenized document can be defined as a text split into tokens. This can be represented by a simple list of token strings. However, each token may also be represented as dictionary in which the token string is stored along with additional annotations. Below, we will refer to these annotations as token attributes. Here is an example of a list of token dictionaries conntaining attributes to indicate the token's part of speech and whether or not it is a stop word.</p> <pre><code>tokenized_doc = [\n    {\"token_text\": \"The\", \"part_of_speech\": \"noun\", \"is_stopword\": True},\n    {\"token_text\": \"end\", \"part_of_speech\": \"noun\", \"is_stopword\": False}\n]\n</code></pre> <p>It is then a simple matter to iterate through the document and retrieve all the tokens that are not stopwords using a Python list comprehension.</p> <pre><code>for token in tokenized_doc:\n    if not token[\"is_stopword\"]: # If is_stopword is False\n        print(token)\n</code></pre> <p>Or you might want to save the tokens to a new list with a list comprehension:</p> <pre><code>non_stopwords = [\n    token for token in tokenized_doc\n    if not token[\"is_stopword\"]\n]\n</code></pre> <p>Many filtering procedures are easy to implement in this way.</p> <p>However, a list of dictionaries is not the only way to represent a tokenized document; it is used here purely to introduce the concept. The strategy employed by Lexos will be discussed further below.</p>"},{"location":"user_guide/tokenizing_texts/#language-models","title":"Language Models","text":"<p>The easiest method for splitting a text into tokens is to use a simple rule-based approach, such as splitting the text on whitespace. However, this method is not always sufficient, especially for languages with complex morphology or syntax or where whitespace is not used to separate words (typical of Chinese, Japanese, and Korean, for instance). In these cases, it is often necessary to use a more sophisticated approach that takes into account the language's grammar and structure.</p> <p>Lexos uses language models to automate the tokenization process. A language model is a statistical model that has been trained on a large corpus of texts in a specific language. It can be used to predict the likelihood of a sequence of words, which can help in identifying the boundaries between tokens. Language models can implement both rule-based and probabilistic strategies for separating document strings into tokens. The Lexos <code>tokenizer</code> module enables you to choose a language model appropriate to your data in order to split your texts into tokens.</p> <p>Note</p> <p>The <code>tokenizer</code> module is a big change for Lexos, as it formally separates tokenization from preprocessing. In the Lexos web app, users employ Scrubber to massage the text into shape using their implicit knowledge about the text's language. Tokenization then takes place by splitting the text according to a regular expression pattern (normally whitespace). By contrast, the Lexos <code>tokenizer</code> module uses a language model that formalizes the implicit rules and probabilities needed to tokenize the text. Because they have built-in explicit procedures appropriate to specific languages, language models can often do a better job of tokenization than the approach used in the Lexos web app.</p> <p>For languages such as Modern English, language models exist that can both split texts into tokens and automatically annotate tokens with attributes like parts of speech, lemmas, stop words, and other information. This procedure is often referred to as Natural Language Processing (NLP).</p> <p>If no language model exists for the text's language, it will only be possible to tokenize using general rules, and it will not be possible to add other annotations (at the tokenization stage). But new language models, including models for historical languages, are being produced all the time, and this is a growing area of interest in the Digital Humanities.</p> <p>There are some trade-offs to using language models. Because the algorithm does more than split strings, processing times can be greater. In addition, the tokenization procedure is not completely language agnostic. A language model is \"opinionated\" and it may overfit the data. At the same time, if no language model exists for the language being tokenized, the results may not be satisfactory. The Lexos strategy for handling this situation is described below.</p> <p>Using Other Tokenizers</p> <p>Many machine-learning tools \u2014 including the Lexos web app \u2014 deploy <code>scikit-learn</code>'s <code>CountVectorizer</code> (and similar) classes to perform tokenizations. Such tools combine the process of tokenizing and counting tokens, whereas the Lexos <code>tokenizer</code> module keeps them separate. Moreover, tools like <code>CountVectorizer</code> use simple regular expression patterns to divide texts into tokens and do not leverage the capabilities of a language model. If you need a tool like <code>CountVectorizer</code> to perform tokenization but still want to make use of the NLP capabilities of the Lexos Tokenizer, there is an example of how to do it on the Document-Term Matrix page.</p>"},{"location":"user_guide/tokenizing_texts/#spacy-docs","title":"SpaCy <code>Docs</code>","text":"<p>Lexos uses the spaCy Natural Language Processing (NLP) library for loading language models and tokenizing texts. Because spaCy has excellent documentation and fairly wide acceptance in the Digital Humanities community, it is a good tool to use under the bonnet. SpaCy has a growing number of language models for a variety of languages, as well as wrappers for loading models from other common NLP libraries such as Stanford Stanza.</p> <p>In spaCy, texts are parsed into spaCy <code>Doc</code> objects consisting of sequences of annotated tokens.</p> <p>Note</p> <p>In order to formalize the difference between a text string that has been scrubbed and one that has been tokenized, we refer wherever possible to the string as a \"text\" and to the tokenized <code>Doc</code> object as a \"document\" (or just \"doc\"). We continue to refer to the individual items as \"documents\" if we are not concerned with their data type.</p> <p>Each token is spaCy <code>Token</code> object which stores all the token's attributes.</p>"},{"location":"user_guide/tokenizing_texts/#creating-a-spacy-doc-object","title":"Creating a SpaCy <code>Doc</code> Object","text":"<p>The Lexos API wraps this procedure in the <code>Tokenizer.make_doc()</code> method:</p> <pre><code>from lexos.tokenizer import Tokenizer\n\ntokenizer = Tokenizer()\ndoc = tokenizer.make_doc(\"This is a test.\")\n</code></pre> <p>This returns a <code>Doc</code> object.</p> <p>By default the tokenizer uses spaCy's \"xx_sent_ud_sm\" language model, which has been trained for tokenization and sentence segmentation on multiple languages. This model performs statistical sentence segmentation and possesses general rules for token segmentation that work well for a variety of languages. The default model has been chosen to be as language-agnostic as possible, so it can be used for many languages without requiring a specific model. However, it is not guaranteed to work well for all languages.</p> <p>If you were making a document from a text in a language which rquired a more language-specific model, you would specify the model to be used. For instance, to use spaCy's small English model trained on web texts, instantiate the <code>Tokenizer</code> class and use the <code>model</code> keyword argument to specify the model (it must be installed in your Python environment):</p> <pre><code>tokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(\"This is a test.\")\n</code></pre> <p>Note</p> <p>The <code>tokenizer</code> module is a wrapper for the spaCy library, so you can also use spaCy directly to create a <code>Doc</code> object since spaCy is installed with Lexos. Lexos is designed to make it easier to work with spaCy's functionality, but it is not necessary to use the Lexos API to work with spaCy. The equivalent of the above using spaCy's API is</p> <pre><code>import spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a test.\")\n</code></pre> <p>In our documentation, we use the Lexos <code>Tokenizer</code> class because it automatically loads the default model.</p> <p>Be sure that the model you specify is installed in your Python environment. You can install spaCy models using the command line, for example:</p> <pre><code># Install the small French model\npython -m spacy download fr_core_news_sm\n</code></pre> <p>The <code>Tokenizer</code> class also has a <code>make_docs()</code> method to parse a list of texts into a list of spaCy docs.</p> <p>Important</p> <p>Tokenization with spaCy uses a lot of memory. For a small English-language model, the parser and named entity recognizer (NER) can require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the memory limit with the <code>max_length</code> parameter in <code>make_doc()</code> or <code>makes_docs()</code>. The limit is in number of characters (the default is set to 2,000,000 for Lexos), so you can check whether your inputs are too long by checking <code>len(text)</code>. If you are not using RAM-hungry pipeline components, you can disable or exclude them to avoid errors an increase efficiency (see the discussion on the spaCy pipeline below). In some cases, it may also be possible to cut the texts into segments before tokenization. See Cutting Documents for more information.</p>"},{"location":"user_guide/tokenizing_texts/#working-with-spacy-docs","title":"Working with SpaCy <code>Docs</code>","text":"<p>A list of individual tokens can be obtained by iterating over the spaCy doc:</p> <pre><code># Use a for loop to iterate over the tokens\nfor token in doc:\n    print(token.text, token.pos_, token.is_stop)\n\n# Get a list of tokens with a list comprehension\ntokens = [token.text for token in doc]\n</code></pre> <p>Here the <code>text</code> attribute stores the original text form of the token. Tokenizing texts into SpaCy docs is non-destructive because original text is preserved alongside the list of tokens and their attributes. You can access the original text of the entire doc by calling <code>doc.text</code> (assuming you have assigned the <code>Doc</code> object to the <code>doc</code> variable). Indeed, calling <code>doc.to_json()</code> will return a JSON representation which gives the start and end position of each token in the original text!</p> <p>As mentioned above, you can use a Python list comprehension to filter the the contents of the doc using information in the document's attributes. For instance:</p> <pre><code># Get a list of non-punctuation tokens\nnon_punct_tokens = [token.text for token in doc if not token.is_punct]\n</code></pre> <p>The example above leverages the built-in <code>is_punct</code> attribute to indicate whether the token is defined as (or predicted to be) a punctuation mark in the language model. SpaCy docs have a number of built-in attributes, which are described in the spaCy API reference.</p> <p>Note</p> <p>It is possible to extend spaCy's Doc object with its extension attribute. For instance, if you wanted to have an <code>is_fruit</code> attribute, you could create an extension and then access it using the underscore prefix, as shown below:</p> <pre><code># Indicate whether the token is labelled as fruit\nfor token in doc:\n    print(token._.is_fruit)\n</code></pre> <p>For information on creating custom extensions, see the spaCy documentation.</p>"},{"location":"user_guide/tokenizing_texts/#handling-stop-words","title":"Handling Stop Words","text":"<p>Stop words are tokens that are often filtered out in text processing because they do not carry significant meaning for the intended task. Examples include grammatical function words like \"the\", \"is\", \"in\", and \"and\". In many cases, it is useful to remove stop words from the text before performing further analysis, such as frequency counts or topic modeling. Many language models come with a predefined list of stop words that are commonly used in the language. These stop words are used to set the <code>is_stop</code> attribute for each stop word token to <code>True</code> when a document is tokenized. It is also possible to add stop words to or remove stop words from tokenizer using the <code>add_stopwords()</code> and <code>remove_stopwords()</code> method:</p> <pre><code>tokenizer = Tokenizer(model=\"en_core_web_sm\")\ntokenizer.add_stopwords([\"yes\", \"no\", \"maybe\"])\ntokenizer.remove_stopwords([\"and\", \"or\", \"for\"])\ndoc = tokenizer.make_doc(text)\n</code></pre> <p>A search of the web can often yield a list of stop words for the language you are working in, and you will often have to add stop words to obtain satisfactory results. If you are using a language model that does not have a predefined stop word list, you can use the <code>add_stopwords()</code> method to add your own stop words.</p> <p>Important</p> <p><code>add_stopwords()</code> and <code>remove_stopwords()</code> do not add or remove tokens from the <code>Doc</code>; instead, they modify the stop word list used to set the <code>is_stop</code> attribute of individual tokens to <code>True</code> or <code>False</code>. To get a list of tokens without stop words, you must filter them with something like <code>[token for token in doc if not token.is_stop]</code>. If you are producing a corpus of documents in which the documents will be processed by different models, it is most efficient to process the documents in batches, one batch for each model.</p>"},{"location":"user_guide/tokenizing_texts/#modifying-the-spacy-pipeline","title":"Modifying the SpaCy Pipeline","text":"<p>Once spaCy tokenizes a text, it normally passes the resulting document to a pipeline of functions to parse it for other features. Typically, these functions will perform actions such as part-of-speech tagging, labelling syntactic dependencies, and identifying named entities (named entity recognition, or NER). Processing times can be increased by disabling pipeline components if they are unavailable in the language model or not needed for the application's purposes. <code>make_doc()</code> and <code>make_docs()</code> will automatically run all pipeline components in the model unless they are disabled with the <code>disable</code> parameter.</p> <pre><code>doc = tokenizer.make_doc(text, disable=[\"parser\", \"ner\"])\n</code></pre> <p>Check the model's documentation for the names of the components it includes by default.</p> <p>It is also possible to include custom pipeline components, which can be inserted at any point in the pipeline order. Custom components are supplied with the <code>pipeline_components</code> parameter, which takes a dictionary containing the keyword \"custom\". The value is a list of dictionaries where each dictionary contains information about the component as described in spaCy's documentation.</p> <p>Note</p> <p>The <code>pipeline_components</code> dict also contains <code>disable</code> and <code>exclude</code> keywords. The values are lists of components which will be merged with any components supplied in the <code>disable</code> or <code>exclude</code> paramaters of <code>make_doc()</code> and <code>make_docs()</code>.</p> <p>The ability to add custom pipeline components is valuable for certain language- or application-specific scenarios. However, it also opens Lexos up to the wealth of third-part pipeline components available through the spaCy Universe.</p>"},{"location":"user_guide/tokenizing_texts/#custom-tokenizers","title":"Custom Tokenizers","text":"<p>Sometimes using a language model to perform tokenization is not appropriate or is overkill for the desired output. Lexos has two tokenizer classes that operate on strings and return lists of strings. They mostly illustrate how you can produce your own tokenizer class if required.</p> <p><code>SliceTokenizer</code> slices the text into tokens of <code>n</code> characters. The constructor takes two arguments: <code>n</code>, which is the number of characters  in each output token, and <code>drop_ws</code>, a modifier that controls whether to drop whitespace or keep it.</p> <pre><code>from lexos.tokenizer import SliceTokenizer\ntest_text = \"Cut me up into tiny pieces!\"\nslicer = SliceTokenizer(n=4, drop_ws=True)\nslices = slicer(test_text)\nprint(slices)\n</code></pre> <p><code>WhitespaceTokenizer</code> simply slices a text into tokens on whitespace, similarly to Python's built-in <code>split()</code> method.</p> <pre><code>from lexos.tokenizer import WhitespaceTokenizer\ntest_text = \"Split me up by whitespace!\"\nneatSlicer = WhitespaceTokenizer()\nslices = neatSlicer(test_text)\nprint(slices)\n</code></pre> <p>Note</p> <p>The NLP community are largely concerned with extracting linguistic features from text strings where whitespace is typically ignored. But scholars in the Humanities have pointed out that in the texts they study, whitespace itself can be meaningful. Lexos has an experimental <code>WhitespaceCounter</code> tokenizer that extends the <code>Tokenizer</code> class by counting runs of spaces and line breaks. See the Whitespace Counter documentation for more information.</p> <p>To use it, instantiate the <code>WhitespaceCounter</code> class and call it with a text string:</p> <pre><code>from lexos.tokenizer.whitespace_counter import WhitespaceCounter\n\nwc = WhitespaceCounter()\ntokens = wc(\"This  is   a\\n\\n test.\")\nfor token in tokens:\n    print(token, token._.width)\n</code></pre>"},{"location":"user_guide/tokenizing_texts/#generating-ngrams","title":"Generating Ngrams","text":"<p>Both texts and documents can be parsed into sequences of two or more tokens called ngrams. Many spaCy models can identify syntactic units such as noun chunks. These capabilities are not covered here since they are language specific. Instead, the section below describe how to obtain more general ngram sequences.</p> <p>The easiest method of obtaining ngrams from a text is to create a spaCy doc and then call the <code>ngrams_from_doc()</code> method:</p> <pre><code>import spacy\nfrom lexos.tokenizer.ngrams import Ngrams as ng\n\nnlp = spacy.load(\"xx_sent_ud_sm\")\ntext = \"The end is nigh.\"\ndoc = nlp(text)\n\nng = Ngrams()\n\nngrams = ng.from_doc(doc, size=2)\nfor ngram in ngrams:\n    print(ngram.text)\n# The end\n# end is\n# is nigh\n# nigh .\n</code></pre> <p>The <code>from_doc()</code> function yields a generator, so, if you wish to view it as a list, you need to call <code>list(ngrams)</code> on the output shown above. The size of the ngrams is specified by the <code>size</code> parameter, which defaults to 2. Setting it to 3, for instance, will result in the ngrams \"The end is\", \"end is nigh\", \"is nigh .\"</p> <p>Note</p> <p>The <code>from_doc()</code> function is a wrapper for the <code>textacy.extract.basics.ngrams</code> method, which is part of the Textacy library. You can call Textacy directly as shown below:</p> <pre><code>import textacy.extract.basics.ngrams as ng\nngrams = ng(doc, 2, min_freq=2)\n</code></pre> <p>The <code>min_freq</code> parameter removes ngrams that do not occur at least two times. This can cut down on the size of the generated ngrams. Textacy has a lot of additional options, which are documented in the Textacy API reference under <code>textacy.extract.basics.ngrams</code>. The Lexos <code>Ngrams.from_doc()</code> method accepts the same parameters as Textacy's method with a few additional options (see the API documentation).</p> <p>There is also a <code>Ngrams.from_docs()</code> method that accepts a list of <code>Doc</code> objects and returns a list of ngram generators.</p> <p>If you do not want to use a language model, the <code>Ngrams</code> class also accepts input data in the form of text strings with <code>Ngrams.from_text()</code> and <code>Ngrams.from_texts()</code>. By default, your text(s) will be processed using the <code>WhitespaceTokenizer</code> before the ngrams are generated, although you can swap it for another tokenizer. If you use the <code>SliceTokenizer</code>, you will produce character ngrams. For instance, the text \"Hello world\" will produce the bigrams \"He, el, lo, o , w, wo, or, rl, ld\". (You can also generate character ngrams by calling <code>SliceTokenizer</code> directly: <code>ngrams = SliceTokenizer(text, n=2)</code>). Note that your language model may not be able apply labels effectively to ngram tokens, so working with character ngrams is primarily useful if you are planning to work with the token forms only, or if the ngram size you use maps closely to character lengths of words in the language you are working in.</p> <p>If you have a list of pre-tokenized strings, you can use the <code>Ngrams.from_tokens()</code> method. For instance, <code>ngrams = ng.from_tokens([\"Hello\", \"world\", \"how\", \"are\", \"you\"], n=3)</code> will generate \"Hello world how, world how are, how are you\".</p> <p>In some cases, you may wish to generate a <code>Doc</code> object with ngrams as tokens. This can be done by calling spaCy's <code>Doc.from_docs()</code> method, which takes a list or tuple of ngrams and returns a new spaCy <code>Doc</code>:</p> <pre><code>from spacy.tokens import Doc\nnew_doc = Doc.from_docs(ngrams)\n</code></pre>"},{"location":"user_guide/topwords/","title":"Topwords Analysis","text":""},{"location":"user_guide/topwords/#overview","title":"Overview","text":"<p>The <code>lexos.topwords</code> module provides tools for identifying statistically significant words in a corpus by comparing target documents against background documents. It enables comparative text analysis to discover which terms best distinguish one set of texts from another.</p> <p>The <code>topwords</code> module helps you answer questions like:</p> <ul> <li>What words make Document A unique compared to all other documents?</li> <li>What terms distinguish Shakespeare's plays from Marlowe's plays?</li> <li>Which words are most characteristic of scientific papers versus news articles?</li> </ul> <p>A \"topword\" is a term that is highly ranked according to some statistical metric. Since there are numerous ways to measure statistical significance, Lexos provides three different Python classes for extracting topwords: <code>KeyTerms</code>, <code>ZTest</code>, <code>MannWhitney</code>.</p> <ul> <li><code>KeyTerms</code> extracts significant keywords from individual documents using graph-based ranking algorithms.</li> <li><code>ZTest</code> identifies statistically over-represented words in target documents compared to comparison documents by calculating their z-score.</li> <li><code>MannWhitney</code> implements the Mann-Whitney U test (also called the Wilcoxon rank-sum test) to compare two groups of documents to determine if they differ significantly.</li> </ul> <p>Note</p> <p>Each of these classes inherits from a basic <code>TopWords</code> class to define a common API for all topwords methods, creating a plugin-like architecture for additional methods to be added. Currently, this plugin structure is a bit limited. Each class is expected to have a <code>to_df()</code> method, but no other common elements are expected.</p> <p>There is also a <code>Compare</code> class which implements provides a workflow for compare documents across a corpus or comparing documents belonging to different classes. Any test that inherits from the base <code>TopWords</code> class can be used with <code>Compare</code>.</p> <p>Each of these classes is implemented as a submodule of <code>topwords</code>.</p>"},{"location":"user_guide/topwords/#submodules","title":"Submodules","text":""},{"location":"user_guide/topwords/#keyterms","title":"KeyTerms","text":"<p>The <code>KeyTerms</code> module extracts significant keywords from documents using graph-based ranking algorithms. Ranking algorithms build a graph where words are nodes and their co-occurrences create edges, then rank words by their importance in the network. Unlike statistical tests that compare document sets, <code>KeyTerms</code> analyzes individual documents to identify their most \"central\" or important terms.</p> <p>Common Use Cases:</p> <ol> <li>Document summarization - Extract key concepts from articles</li> <li>Metadata generation - Auto-generate tags for documents</li> <li>Topic identification - Understand what a document is about</li> <li>Search indexing - Identify important terms for search</li> <li>Content analysis - Analyze themes in qualitative data</li> </ol> <p>Available Algorithms:</p> Algorithm Description Best For <code>textrank</code> PageRank applied to words (default) General purpose, most documents <code>sgrank</code> Statistical selection, positional ranking Longer documents, academic papers <code>scake</code> Single candidate keyword extraction Short documents, tweets <code>yake</code> Yet Another Keyword Extractor Multilingual documents <p>Note</p> <p><code>KeyTerms</code> implements these algorithms using the Python Textacy library. Further documentation about their use can be found there.</p> <p>The <code>KeyTerms</code> class accepts either a string or a spaCy <code>Doc</code>. Here is an example:</p> <pre><code>from lexos.topwords.keyterms import KeyTerms\n\n# Basic usage with a single document\ntext = \"\"\"\nMachine learning is a subset of artificial intelligence that focuses on\nalgorithms that learn from data. Deep learning uses neural networks with\nmultiple layers to process complex patterns.\n\"\"\"\n\nkt = KeyTerms(\n    document=text,\n    method=\"textrank\",\n    topn=10,\n    model=\"xx_sent_ud_sm\",\n    ngrams=1,\n    normalize=\"lemma\"\n)\n\n# Access keyterms directly\nprint(kt.keyterms)\n# [('learning', 0.35), ('machine', 0.28), ('neural', 0.22), ...]\n\n# Get results as a pandas DataFrame\nresults_df = kt.to_df()\n</code></pre> <p>The code above shows the default settings. If you wish to use ngrams, you can set <code>ngrams=2</code> (or 3, 4, etc.). You can also count multiple ngrams with a tuple. For instance, <code>ngrams=(1, 2)</code> will count both single terms and bigrams. Note, however, that the <code>scake</code> method does not accept ngrams.</p> <p>The default <code>normalize</code> setting counts all variant forms of a lemma (if the language model supports lemma). You can also set <code>normalize=\"lower\"</code> to make counts case-insensitive. You can also turn off normalization with <code>normalize=None</code>.</p> <p>Note</p> <p>The <code>KeyTerms</code> class accepts a string or spaCy <code>Doc</code> (automatically converting strings to <code>Docs</code> using the chosen language model). However, it does not currently accept lists of strings or <code>Docs</code>. Since, strings are internally converted to <code>Docs</code>, you may find it more efficient to preprocess string data with the Lexos <code>tokenizer</code> module.</p>"},{"location":"user_guide/topwords/#ztest","title":"ZTest","text":"<p>Identifies statistically over-represented words in target documents compared to comparison documents using a two-proportion z-test.</p> <p>How it works:</p> <ul> <li>Calculates the proportion of each term in target vs. comparison documents</li> <li>Computes a z-score for the difference in proportions</li> <li>Returns terms with the highest z-scores, filtering out terms with a z-score of 0.0</li> </ul> <p>These terms are deemed to be the most distinctive of the target documents.</p> <p>Note</p> <p>There is a separate implementation of z-term calculation in the <code>corpus</code> module. If you are using <code>corpus</code>, you may find it easier to perform z-score testing directly in that module.</p> <p>Here is an example of how the <code>ZTest</code> class is used:</p> <pre><code>from lexos.topwords.ztest import ZTest\n\n# Basic usage with strings\ntarget = [\"This is Shakespeare's unique style.\", \"More Shakespeare text.\"]\ncomparison = [\"This is Marlowe's writing.\", \"More Marlowe text.\"]\n\n# Perform the test and return the 5 highest-ranked topwords\nztest = ZTest(target_docs=target, comparison_docs=comparison, topn=5)\n\n# Access topwords directly\nprint(ztest.topwords)\n# [('shakespeare', 2.45), ('unique', 1.89), ...]\n</code></pre> <p>By default, the topwords will be a list of tuples containing the terms and their z-scores. You can view the results in other formats using <code>to_dict()</code>, <code>to_list_of_dicts()</code>, or <code>to_df()</code> (returns a pandas DataFrame).</p> <p>By default, terms are counted in a case-sensitive manner. If you wish to count both lower and upper case forms as the same term, set <code>case_sensitive=False</code>.</p> <p>The class counts only single terms by default, but you can count ngrams by setting <code>ngrams=2</code> (or 3, 4, etc.). You can also count multiple ngrams with a tuple. For instance, <code>ngrams=(1, 2)</code> will count both single terms and bigrams.</p> <p>In the example above, we have used strings for documents, but you can also use spaCy <code>Doc</code> objects. Internally, strings are converted to spaCy <code>Docs</code> using the <code>xx_sent_ud_sm</code> model. You can change this to another model with the <code>model</code> keyword (e.g. <code>model=en_core_web_sm</code>). Note that converting strings to <code>Docs</code> can take some time, so you may want to do that in advance and then pass your <code>Docs</code> directly to <code>ZTest</code>.</p> <p>The use of spaCy <code>Docs</code> provides access to token attributes, it is possible to filter the tokens before they are counted by setting the <code>remove_stopwords</code>, <code>remove_punct</code>, and <code>remove_digits</code> keywords to <code>True</code>.</p>"},{"location":"user_guide/topwords/#mannwhitney","title":"MannWhitney","text":"<p>The Mann-Whitney U test (also called the Wilcoxon rank-sum test) is a statistical method that compares two groups to determine if they differ significantly. Unlike the z-test, it doesn't make assumptions about how the data is distributed.</p> <p>Instead of comparing proportions directly (like the z-test), the Mann-Whitney test:</p> <ol> <li>Ranks all term frequencies across both document sets</li> <li>Compares the ranks between target and comparison documents</li> <li>Calculates a U-statistic that measures how different the rankings are</li> <li>Provides a p-value indicating the probability the difference occurred by chance</li> </ol> <p>The <code>MannWhitney</code> class takes as its input a pandas DataFrame of term frequencies with docs in rows and terms in columns. Any filtering of your terms must be done in advance. The easiest way to produce the input DataFrames is with the Lexos DTM module. Here is an example:</p> <pre><code>from lexos.dtm import DTM\nfrom lexos.tokenizer import Tokenizer\nfrom lexos.topwords.mann_whitney import MannWhitney\n\ntexts = [\n    \"This is a sample text for testing.\",\n    \"Here is another example of a text to analyze.\",\n    \"This text is different from the others.\",\n    \"Yet another sample text for comparison.\",\n    \"This text is similar to the first one.\",\n    \"A completely different text for the analysis.\",\n]\n\n# Process the sample texts with spaCy to create documents\ndocs = tokenizer.make_docs(texts)\nlabels = [f\"Doc{i + 1}\" for i in range(len(docs))]\n\n# Create a Document-Term Matrix (DTM) using the sample documents\n# Limit to terms occurring in at least 2 documents\ndtm = DTM()\ndtm(docs=docs, labels=labels, min_df=2)\ndf = dtm.to_df(transpose=True)\n</code></pre> <p>In the example above, we'll split the DataFrame into target and comparison data based on whether the label has an even or odd number.</p> <pre><code>even_df = df[df.index.isin([\"Doc2\", \"Doc4\", \"Doc6\"])]\nodd_df = df[df.index.isin([\"Doc1\", \"Doc3\", \"Doc5\"])]\n\nmw = MannWhitney(target=even_df, comparison=odd_df)\nmw.to_df()\n</code></pre> <p>The output will show the terms ranked by their distinctiveness, along with their U statistic and p-value.</p> <p>The p-value is the probability that a test statistic is extreme or more extreme than the one observed, assuming that the two samples come from the same distribution. A small p-value (typically less than 0.05) indicates that the observed difference between the two samples is statistically significant, and we conclude that the two samples do not come from the same distribution.</p> <p>By default, the table displays the average frequency of terms in the control group along with the increase in frequency in the comparison group. This provides us with another view of how important the word is to the sample and its relative over- or under-usage in comparison to the other sample. You can suppress the average frequency and difference columns with <code>add_freq=False</code>.</p> <p>The following points provide a useful guide to interpreting the results:</p> <ul> <li>u_statistic: Higher values indicate the term appears more in target documents</li> <li>p_value: Lower values (&lt; 0.05) indicate statistically significant differences</li> <li>p &lt; 0.05: Statistically significant (95% confident)</li> <li>p &lt; 0.01: Highly significant (99% confident)</li> <li>p &lt; 0.001: Very highly significant (99.9% confident)</li> </ul>"},{"location":"user_guide/topwords/#when-to-use-mann-whitney-vs-z-test","title":"When to Use Mann-Whitney vs. Z-Test","text":"<p>There are a number of key differences between the two types of tests:</p> <ul> <li>Z-test assumes terms are normally distributed (that is, data has a rough bell curve shape with frequencies decreasing evenly on both sides of the central mean). Mann-Whitney makes no such assumption.</li> <li>Z-test is more powerful with large, well-behaved data; Mann-Whitney works better with small or irregular data.</li> <li>Z-test gives a z-score (can be positive or negative); Mann-Whitney gives a U-statistic and p-value. Lower p-values (&lt; 0.05) indicate more significant differences.</li> </ul> <p>Here are some rules of thumb for choosing a method:</p> <ul> <li>Large corpus (100+ documents) \u2192 Use <code>ZTest</code> for faster, more powerful results</li> <li>Small corpus (&lt; 30 documents) \u2192 Use <code>MannWhitney</code> for more reliable results</li> <li>Unsure about your data \u2192 Use <code>MannWhitney</code> to be safe</li> <li>Need fast computation \u2192 Use <code>ZTest</code> (it's computationally simpler)</li> </ul>"},{"location":"user_guide/topwords/#spacy-doc-extensions","title":"spaCy <code>Doc</code> Extensions","text":"<p>When spaCy <code>Doc</code> objects are used as input, the <code>ZTest</code> and <code>MannWhitney</code> classes automatically register a custom <code>._.topwords</code> attribute on each <code>Doc</code> (both target and comparison documents). The topwords are assign to this attribute and can be accessed directly form the <code>Doc</code>. For example:</p> <pre><code>ztest = ZTest(\n    target_docs=target_docs,\n    comparison_docs=background_docs\n)\n\n# Access results via the extension\nprint(target_docs[0]._.topwords)\n# [('distinctive', 2.5), ('unique', 2.1), ...]\n</code></pre>"},{"location":"user_guide/topwords/#comparison","title":"Comparison","text":"<p>The <code>Compare</code> class provides powerful methods for analyzing and comparing documents using statistical measures. This class wraps around <code>ZTest</code> or <code>MannWhitney</code> to enable three comparison strategies:</p> <ol> <li><code>document_to_corpus()</code> - Compare each document to all other documents. Use this method when you want to find what terms make each document unique.</li> <li><code>documents_to_classes()</code> - Compare each document to documents in other classes. Use this method when you want to find outliers or representative terms within classes.</li> <li><code>classes_to_classes()</code> - Compare entire classes to each other. Use this method when you want to find the signature vocabulary of particular categories.</li> </ol> <p>All methods support three output formats: <code>dict</code>, <code>dataframe</code>, and <code>list_of_dicts</code>.</p> <p>As a basic example, we will take four short texts. Although you can perform experiments with raw strings, they will generally be converted to spaCy <code>Doc</code> objects internally. So, for efficiency, we will preprocess the texts into spaCy <code>Docs</code>. We'll then create an instance of the <code>ZTest</code> class for our example. We provide it with no docs because these will be passed to it when we choose what we want to compare. The <code>ZTest</code> instance is our calculator. We can swap it out for other classes in the <code>topwords</code> module or with our own custom classes. Finally, we create an instance of the <code>Compare</code> class and pass it our calculator.</p> <pre><code>from lexos.tokenizer import Tokenizer\nfrom lexos.topwords.ztest import ZTest\nfrom lexos.topwords.compare import Compare\n\n# Load spaCy model\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\n\n# Prepare sample documents\ndocs = tokenizer.make_docs([\n    \"Dracula was a vampire who lived in Transylvania. He had sharp fangs and drank blood.\",\n    \"Frankenstein created a monster in his laboratory. The creature was terrifying and misunderstood.\",\n    \"Alice fell down the rabbit hole into Wonderland. She met the Cheshire Cat and Mad Hatter.\",\n    \"Peter Pan could fly and never wanted to grow up. He lived in Neverland with the Lost Boys.\"\n])\n\n# Create a calculator instance (ZTest in this example)\ncalculator = ZTest(target_docs=[], comparison_docs=[])\n\n# Create Compare instance\ncompare = Compare(calculator=calculator)\n</code></pre> <p>Note</p> <p>By creating the calculator class first, we are also able to configure it with any parameters relevant to the class. For instance, we might want to set the <code>case_sensitive</code> parameter with <code>calculator = ZTest(target_docs=[], comparison_docs=[], case_sensitive=False)</code>.</p> <p>We'll now go through the three methods of comparison.</p>"},{"location":"user_guide/topwords/#document_to_corpus","title":"<code>document_to_corpus()</code>","text":"<p>The <code>document_to_corpus()</code> method is used to find what terms make each document unique compared to all other documents in your corpus or collection.</p> <pre><code># Compare each document to the rest of the corpus\nresults = compare.document_to_corpus(docs)\n\n# Results structure (dict format):\n# {\n#     'Doc 1': [('vampire', 3.845), ('Transylvania', 3.845), ...],\n#     'Doc 2': [('monster', 3.621), ('laboratory', 3.621), ...],\n#     'Doc 3': [('Alice', 3.512), ('Wonderland', 3.512), ...],\n#     'Doc 4': [('fly', 3.412), ('Neverland', 3.412), ...]\n# }\n\nprint(results['Doc 1'])\n# [('vampire', 3.845), ('Transylvania', 3.845), ('blood', 3.845), ...]\n</code></pre> <p>By default, your docs will be named \"Doc 1\", \"Doc 2\", \"Doc 3\", etc. However, you can also provide your own custom labels in a separate <code>doc_labels</code> list (with the labels in the same order as the docs).</p> <pre><code># Provide custom document labels\ndoc_labels = [\"Dracula\", \"Frankenstein\", \"Alice\", \"Peter Pan\"]\n\nresults = compare.document_to_corpus(\n    corpus=docs,\n    doc_labels=doc_labels\n)\n\nprint(results['Dracula'])\n# [('vampire', 3.845), ('Transylvania', 3.845), ...]\n</code></pre> <p>Internally, the method creates a dict like <code>{\"doc_label\": label, \"doc\": doc}</code> which can be accessed with <code>compare.data</code>. Additionally, the results and keywords can be accessed with <code>compare.keywords</code> and <code>compare.keywords</code>.</p> <p>The default output format is a dict with your document labels as keys and the topwords as values. The format of the topwords will vary depending on which calculator you use, but it will generally be a list of tuples. You can modify the output to create a pandas DataFrame:</p> <pre><code>df = compare.document_to_corpus(\n    corpus=docs,\n    doc_labels=doc_labels,\n    output_format=\"dataframe\"\n)\n\nprint(df)\n#              vampire  Transylvania  blood  monster  laboratory  Alice  Wonderland  fly  Neverland\n# doc_label\n# Dracula        3.845         3.845  3.845      0.0         0.0    0.0         0.0  0.0        0.0\n# Frankenstein   0.000         0.000  0.000      3.621       3.621  0.0         0.0  0.0        0.0\n# Alice          0.000         0.000  0.000      0.0         0.0    3.512       3.512  0.0        0.0\n# Peter Pan      0.000         0.000  0.000      0.0         0.0    0.0         0.0  3.412      3.412\n</code></pre> <p>Additionally, you can produce a list of dicts:</p> <pre><code>list_results = compare.document_to_corpus(\n    corpus=docs,\n    doc_labels=doc_labels,\n    output_format=\"list_of_dicts\"\n)\n\nprint(list_results[:3])\n# [\n#     {'doc_label': 'Dracula', 'term': 'vampire', 'score': 3.845},\n#     {'doc_label': 'Dracula', 'term': 'Transylvania', 'score': 3.845},\n#     {'doc_label': 'Dracula', 'term': 'blood', 'score': 3.845}\n# ]\n</code></pre> <p>Once you have generated the results, calling a Class method like <code>document_to_corpus</code> with a different output format will use the cached data instead of re-calculating all the scores.</p>"},{"location":"user_guide/topwords/#document_to_classes","title":"<code>document_to_classes()</code>","text":"<p>The <code>documents_to_classes()</code> method is used to find what makes each document distinctive compared to documents in other categories. In addition to <code>docs</code> and <code>doc_labels</code>, it accepts a <code>class_labels</code> list that supplies categories for each document (class labels indices must correspond to document indices). The result is a dict as shown below.</p> <pre><code># Define class labels for each document\ndoc_labels = [\"Dracula\", \"Frankenstein\", \"Alice\", \"Peter Pan\"]\nclass_labels = [\"gothic\", \"gothic\", \"whimsy\", \"whimsy\"]\n\nresults = compare.documents_to_classes(\n    docs=docs,\n    doc_labels=doc_labels,\n    class_labels=class_labels\n)\n\n# Results structure (dict format):\n# {\n#     'Dracula': {\n#         'comparison_class': 'whimsy',\n#         'topwords': [('vampire', 4.123), ('Transylvania', 4.123), ...]\n#     },\n#     'Alice': {\n#         'comparison_class': 'gothic',\n#         'topwords': [('Wonderland', 3.956), ('rabbit', 3.956), ...]\n#     }\n# }\n\nprint(results['Dracula'])\n# {\n#     'comparison_class': 'whimsy',\n#     'topwords': [('vampire', 4.123), ('Transylvania', 4.123), ('blood', 4.012)]\n# }\n</code></pre> <p>Class labels are also added to the <code>data</code> attribute.</p> <p>When you call <code>documents_to_classes()</code>, each document is compared to all documents in other classes:</p> <ul> <li>\"Dracula\" (gothic) is compared to [\"Alice\", \"Peter Pan\"] (whimsy)</li> <li>\"Frankenstein\" (gothic) is compared to [\"Alice\", \"Peter Pan\"] (whimsy)</li> <li>\"Alice\" (whimsy) is compared to [\"Dracula\", \"Frankenstein\"] (gothic)</li> <li>\"Peter Pan\" (whimsy) is compared to [\"Dracula\", \"Frankenstein\"] (gothic)</li> </ul> <p>You can output the results in DataFrame format:</p> <pre><code>df = compare.documents_to_classes(\n    docs=docs,\n    doc_labels=doc_labels,\n    class_labels=class_labels,\n    output_format=\"dataframe\"\n)\n\nprint(df)\n#     doc_label comparison_class         term  score\n# 0     Dracula           whimsy      vampire  4.123\n# 1     Dracula           whimsy Transylvania  4.123\n# 2     Dracula           whimsy        blood  4.012\n# 3  Frankenstein        whimsy      monster  4.012\n# 4       Alice           gothic   Wonderland  3.956\n# 5       Alice           gothic       rabbit  3.956\n# 6   Peter Pan           gothic          fly  3.889\n</code></pre> <p>Likewise, you can generate a list of dicts.</p> <pre><code>list_results = compare.documents_to_classes(\n    docs=docs,\n    doc_labels=doc_labels,\n    class_labels=class_labels,\n    output_format=\"list_of_dicts\"\n)\n\nprint(list_results[:2])\n# [\n#     {'doc_label': 'Dracula', 'comparison_class': 'whimsy', 'term': 'vampire', 'score': 4.123},\n#     {'doc_label': 'Dracula', 'comparison_class': 'whimsy', 'term': 'Transylvania', 'score': 4.123}\n# ]\n</code></pre>"},{"location":"user_guide/topwords/#classes_to_classes","title":"<code>classes_to_classes()</code>","text":"<p>The <code>classes_to_classes()</code> method is used to find what what terms characterize entire categories/genres compared to other categories. It also takes a <code>class_labels</code> parameter.</p> <pre><code>results = compare.classes_to_classes(\n    docs=docs,\n    doc_labels=doc_labels,\n    class_labels=class_labels\n)\n\n# Results structure (dict format):\n# {\n#     'gothic': {\n#         'comparison_class': 'whimsy',\n#         'topwords': [('vampire', 5.234), ('monster', 5.123), ...]\n#     },\n#     'whimsy': {\n#         'comparison_class': 'gothic',\n#         'topwords': [('Wonderland', 5.112), ('fly', 4.998), ...]\n#     }\n# }\n\nprint(results['gothic'])\n# {\n#     'comparison_class': 'whimsy',\n#     'topwords': [('vampire', 5.234), ('monster', 5.123), ('dark', 4.987)]\n# }\n</code></pre> <p>Each class is treated as a unified group:</p> <ul> <li>\"gothic\" class: [\"Dracula\", \"Frankenstein\"] combined vs. [\"Alice\", \"Peter Pan\"] combined</li> <li>\"whimsy\" class: [\"Alice\", \"Peter Pan\"] combined vs. [\"Dracula\", \"Frankenstein\"] combined</li> </ul> <p>This is different from <code>documents_to_classes()</code> which compares individual documents.</p> <p>As with the other classes, you can output the results as a pandas DataFrame or as a list of dicts.</p> <pre><code>df = compare.classes_to_classes(\n    docs=docs,\n    doc_labels=doc_labels,\n    class_labels=class_labels,\n    output_format=\"dataframe\"\n)\n\nprint(df)\n#   class_label comparison_class        term  score\n# 0      gothic           whimsy     vampire  5.234\n# 1      gothic           whimsy     monster  5.123\n# 2      gothic           whimsy        dark  4.987\n# 3      whimsy           gothic  Wonderland  5.112\n# 4      whimsy           gothic         fly  4.998\n# 5      whimsy           gothic   Neverland  4.887\n\nlist_results = compare.classes_to_classes(\n    docs=docs,\n    doc_labels=doc_labels,\n    class_labels=class_labels,\n    output_format=\"list_of_dicts\"\n)\n\nprint(list_results[:2])\n# [\n#     {'class_label': 'gothic', 'comparison_class': 'whimsy', 'term': 'vampire', 'score': 5.234},\n#     {'class_label': 'gothic', 'comparison_class': 'whimsy', 'term': 'monster', 'score': 5.123}\n# ]\n</code></pre>"},{"location":"user_guide/topwords/#using-dictionary-input","title":"Using Dictionary Input","text":"<p>Instead of passing separate <code>doc_label</code> and <code>class_label</code> lists, you can pass documents as a list of dictionaries if you already have your data in the at format. Just pass the dictionary with the <code>docs</code> parameter:</p> <pre><code>docs = [\n    {\"doc\": docs[0], \"doc_label\": \"Dracula\", \"class_label\": \"gothic\"},\n    {\"doc\": docs[1], \"doc_label\": \"Frankenstein\", \"class_label\": \"gothic\"},\n    {\"doc\": docs[2], \"doc_label\": \"Alice\", \"class_label\": \"whimsy\"},\n    {\"doc\": docs[3], \"doc_label\": \"Peter Pan\", \"class_label\": \"whimsy\"}\n]\n\n# No need to specify doc_labels and class_labels separately\nresults = compare.documents_to_classes(docs=docs)\n</code></pre> <p>Note that only the <code>doc</code> and <code>doc_label</code> keys are used for <code>document_to_corpus()</code>. The <code>class_label</code> key (if present) will be ignored.</p> <p>If <code>doc_label</code> values are not available in the dict, <code>documents_to_classes()</code> and <code>classes_to_classes()</code> will supply \"Doc 1\", \"Doc 2\", \"Doc 3\", etc.</p>"},{"location":"user_guide/topwords/#using-spacy-doc-extensions","title":"Using spaCy <code>Doc</code> Extensions","text":"<p>If given a list of spaCy <code>Doc</code> objects, the <code>Compare</code> class will attempt to extract class values from custom extensions before trying other methods. For instance, if you supply the class label \"author\", <code>Compare</code> will first try to assign values for each <code>Doc</code> from its <code>._.author</code> extension. If that fails, the value \"author\" will be assigned as the class label for the doc.</p> <p>Note</p> <p>The class does not support nested dictionaries like <code>{\"metadata\": \"author\": \"Shakespeare\", \"language\": \"en\"}</code> If you have metadata in this form, you can convert it to an class instance. Here is a simple way to do this:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Metadata:\n    author: str\n    language: str\n\ndoc._.metadata = Metadata(\"Shakespeare\", \"en\")\n</code></pre> <p>You can now use dot notation for the nested attributes:</p> <pre><code>results = comparison.documents_to_classes(\n      docs=docs,\n      class_labels=[\"_.metadata.author\"]\n  )\n</code></pre>"},{"location":"user_guide/topwords/#helper-methods","title":"Helper Methods","text":"<p>The <code>Compare</code> class provides two helper methods for getting information about your data once it has been populated.</p>"},{"location":"user_guide/topwords/#get_class","title":"<code>get_class()</code>","text":"<p>This method takes a document label and returns the name of the class to which the document it belongs (if available).</p> <pre><code># First, run a comparison to populate the data\ncompare.documents_to_classes(docs, doc_labels, class_labels)\n\n# Get the class for a specific document\ndoc_class = compare.get_class(\"Dracula\")\nprint(doc_class)  # 'gothic'\n</code></pre>"},{"location":"user_guide/topwords/#get_docs_by_class","title":"<code>get_docs_by_class()</code>","text":"<p>This method returns a dict containing all documents grouped by class (the dictionary key). If you supply a <code>class_label</code> the output will be restricted to only documents with that label.</p> <pre><code># Get all documents grouped by class\ndocs_by_class = compare.get_docs_by_class()\nprint(docs_by_class.keys())  # dict_keys(['gothic', 'whimsy'])\n\n# Get documents for a specific class\ngothic_docs = compare.get_docs_by_class(class_label=\"gothic\")\nprint(gothic_docs)  # {'gothic': ['Dracula']}\n</code></pre>"},{"location":"user_guide/using_milestones/","title":"Using Milestones","text":"<p>Milestones are specified locations in the text that designate structural or sectional divisions. A milestone can be either a designated unit within the text or a placemarker inserted between sections of text. The Lexos <code>milestones</code> module provides methods for identifying milestone locations by searching for patterns you designate. There three separate classes for identifying milestones in different ways: <code>StringMilestones</code>, <code>TokenMilestones</code>, and <code>SpanMilestones</code>. We will look at each of these in turn.</p>"},{"location":"user_guide/using_milestones/#stringmilestones","title":"<code>StringMilestones</code>","text":"<p>The <code>StringMilestones</code> class is used for extracting and storing milestones in strings or spaCy Doc objects. It uses regular expressions to find patterns and returns their locations and text.</p> <p>Here is a basic example:</p> <pre><code># Import the StringMilestones class\nfrom lexos.milestones.string_milestones import StringMilestones\n\n# A sample doc\ndoc = \"The quick brown fox jumps over the lazy dog.\"\n\n# Create a String Milestones instance and search for the pattern \"quick\"\nmilestones = StringMilestones(doc=doc, patterns=\"quick\")\n\n# Print the start character, end character, and text of each milestone\nfor milestone in milestones:\n    print(milestone.start, milestone.end, milestone.text)\n\n# 4 9 quick\n</code></pre> <p>The <code>.spans</code> property returns a list of <code>StringSpan</code> objects that have <code>text</code>, <code>start</code>, and <code>end</code> attributes. As shown, above, you can iterate through the <code>Milestones</code> object direclt to return items from this list.</p> <p>You can use regex patterns to match more complex milestones. For instance, in the example below, we match the string \"Chapter\" followed by one or more digits.</p> <pre><code>text = \"Chapter 1\\nThis is the text of the first chapter.\\nChapter 2\\nThis is the text of the second chapter.\"\n\nmilestones = StringMilestones(doc=text, patterns=\"Chapter \\\\d+\")\n\n# Print the start character, end character, and text of each milestone\nfor milestone in milestones:\n    print(milestone.start, milestone.end, milestone.text)\n\n# 0 9 Chapter 1\n# 49 58 Chapter 2\n</code></pre> <p>By default, searches are case-sensitive. To ignore case, set <code>case_sensitive=False</code>:</p> <pre><code>text = \"Chapter 1\\nThis is the text of the first chapter.\\nChapter 2\\nThis is the text of the second chapter.\"\n\nmilestones = StringMilestones(doc=text, patterns=\"Chapter\", case_sensitive=False)\n\n# Print the start character, end character, and text of each milestone\nfor milestone in milestones:\n    print(milestone.start, milestone.end, milestone.text)\n\n# 0 7 Chapter\n# 40 47 chapter\n# 49 56 Chapter\n# 90 97 chapter\n</code></pre> <p>You can pass a list of patterns:</p> <pre><code>milestones = StringMilestones(doc=text, patterns=[\"This\", \"Chapter\"], case_sensitive=False)\n\nfor milestone in milestones:\n    print(milestone.start, milestone.end, milestone.text)\n\n# 0 7 Chapter\n#10 14 This\n#40 47 chapter\n#49 56 Chapter\n#59 63 This\n#90 97 chapter\n</code></pre>"},{"location":"user_guide/using_milestones/#updating-patterns-and-settings","title":"Updating Patterns and Settings","text":"<p>You can update the patterns or case sensitivity using the <code>.set()</code> method:</p> <pre><code>milestones.set(\"The\", case_sensitive=False)\nfor milestone in milestones:\n    print(milestone.start, milestone.end, milestone.text)\n\n# 18 21 the\n# 30 33 the\n# 67 70 the\n# 79 82 the\n</code></pre> <p>This will erase any previous settings.</p>"},{"location":"user_guide/using_milestones/#tokenmilestones","title":"<code>TokenMilestones</code>","text":"<p>The <code>TokenMilestones</code> class is used for extracting and storing milestones tokenized text, such as spaCy Doc objects. It differs from <code>StringMilestones</code> in that it matches against full tokens.</p> <p>Let's start by importing the Lexos <code>Tokenizer</code> class to create a spaCy Doc object. We'll also import the <code>TokenMilestones</code> class.</p> <pre><code># Import Tokenizer and TokenMilestones\nfrom lexos.tokenizer import Tokenizer\nfrom lexos.milestones.token_milestones import TokenMilestones\n\ntext = \"Chapter 1: Introduction. Chapter 2: Methods.\"\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(text)\n</code></pre> <p>Now, let's extract milestones for the word \"Chapter\".</p> <pre><code>milestones = TokenMilestones(doc=doc)\nmatches = milestones.get_matches(patterns=[\"Chapter\"])\nprint(matches)\n\n# [\"Chapter\", \"Chapter\"]\n</code></pre> <p>This functions as a quick check to make sure that the pattern as worked. By default, <code>get_matches</code> detects tokens that exactly match the pattern (although you can set <code>case_sensitive</code> to False). Other ways of matching will be discussed below.</p> <p>Unlike <code>StringMilestones</code> the output of <code>TokenMilestones</code> is stored in the original Doc object. To do this, you must pass the matches to the Doc object using the <code>set_milestones()</code> method.</p> <pre><code>milestones.set_milestones(matches)\n</code></pre> <p>The effect of this method is to create two custom extensions to the doc's tokens: <code>milestone_iob</code> and <code>milestone_label</code>. The former indicates whether the token is inside (\"I\"), outside (\"O\"), or at the beginning (\"B\") of a milestone. The <code>milestone_label</code> provides the complete text of the milestone for any \"B\" token.</p> <pre><code>milestones = TokenMilestones(doc=doc)\nmatches = milestones.get_matches(patterns=\"Chapter\")\nmilestones.set_milestones(matches)\nfor token in doc:\n    print(token.text, token._.milestone_iob, token._.milestone_label)\n\n# Chapter B Chapter\n# 1 O\n# : O\n# Introduction O\n# . O\n# Chapter B Chapter\n# 2 O\n# : O\n# Methods O\n# . O\n</code></pre> <p>Note</p> <p>The <code>milestone_iob</code> and <code>milestone_label</code> attributes are custom spaCy extensions, which need to be accessed with the <code>._.</code> prefix.</p> <p>In some cases, the milestone text itself is content that you do not wish to include in your analysis. For instance, you may have placed a milestone marker like <code>&lt;MILESTONE&gt;</code> at points in your original text, and you wish to identify divisions on either side of that marker. The <code>set_milestones</code> method provides a <code>start</code> parameter which allows you to choose to set the \"B\" tag \"before\" or \"after\" the matched token(s). So, if you are matching \"Chapter 2\", <code>start=before</code> would place the \"B\" attribute on the token before \"Chapter\", and <code>start=after</code> (the more likely choice) would place the \"B\" tag on the token after \"Chapter 2\". If, for instance, you were cutting the document into segments, you might want each segment to begin with content, rather than a chapter heading. If course, this would leave the chapter heading (the milestone) at the end of the previous segment. If you wish to remove the milestone tokens from the document completely, you can set <code>remove=True</code>.</p> <p>Note</p> <p>Note that the <code>remove</code> parameter creates a copy of the original Doc object with the milestone tokens removed. This means that the remaining tokens may not have the same indices in the copy as in the original.</p>"},{"location":"user_guide/using_milestones/#setting-the-search-mode","title":"Setting the Search Mode","text":"<p>As noted above, by default <code>TokenMilestones</code> searches for token strings that match the search pattern(s) exactly (allowing for the case-sensitivity setting). You can match multiple tokens by setting <code>mode=\"phrase\"</code> in <code>get_matches</code>.</p> <pre><code>milestones = TokenMilestones(doc=doc)\nmatches = milestones.get_matches(patterns=\"Chapter 1\", mode=\"phrase\")\nmilestones.set_milestones(matches)\nfor token in doc:\n    print(token.text, token._.milestone_iob, token._.milestone_label)\n\n# Chapter B Chapter\n# 1 I\n# : O\n# Introduction O\n# . O\n# Chapter O\n# 2 O\n# : O\n# Methods O\n# . O\n</code></pre> <p>You can also perform more nuanced matching using <code>mode=\"rule\"</code>. This allows you to submit rules using spaCy's powerful <code>Matcher</code> class. Here is an example:</p> <pre><code>pattern = [{\"TEXT\": \"Chapter\"}, {\"IS_DIGIT\": True}]\nmilestones = TokenMilestones(doc=doc)\nmatches = milestones.get_matches(patterns=[pattern], mode=\"rule\")\nmilestones.set_milestones(matches)\nfor token in doc:\n    print(token.text, token._.milestone_iob, token._.milestone_label)\n\n# Chapter B Chapter\n# 1 I\n# : O\n# Introduction O\n# . O\n# Chapter B Chapter\n# 2 I\n# : O\n# Methods O\n# . O\n</code></pre> <p>Note</p> <p>See spaCy's Rule-Based Matching documentation for a complete explanation of the syntax for formulating matching patterns. Note that your case-sensitivity setting may override any case handling in your search pattern(s).</p>"},{"location":"user_guide/using_milestones/#removing-and-resetting-milestones","title":"Removing and Resetting Milestones","text":"<p>You can remove previously set milestones with the <code>remove</code> method. For instance, the milestones set above can be removed as follows:</p> <pre><code>milestones.remove(matches)\nfor token in doc:\n    print(token.text, token._.milestone_iob, token._.milestone_label)\n\n# Chapter O\n# 1 O\n# : O\n# Introduction O\n# . O\n# Chapter O\n# 2 O\n# : O\n# Methods O\n# . O\n</code></pre> <p>If you wish to remove all milestones efficiently from a Doc object, simply call the <code>reset</code> method.</p>"},{"location":"user_guide/using_milestones/#spanmilestones","title":"<code>SpanMilestones</code>","text":"<p>Span milestones are used to group spans together for analysis or visualization. Span milestones differ from normal milestones in that milestones are \"invisible\" structural boundaries between spans or groups of spans (e.g. sentence or line breaks). Thus, instead of storing a list of patterns representing milestones, span milestones store the groups of spans themselves.</p> <p>There are three subclasses that inherit from <code>SpanMilestones</code>: <code>LineMilestones</code>, <code>SentenceMilestones</code>, and <code>CustomMilestones</code>. The <code>LineMilestones</code> class is the easiest to understand. It splits the text on line breaks and generates a list of spaCy <code>Span</code> objects. These can be accessed through the <code>spans</code> of both the <code>Milestones</code> and the <code>Doc</code> objects:</p> <pre><code>from lexos.milestones.span_milestones import LineMilestones\n\ntext = \"Chapter 1: Introduction.\\nChapter 2: Methods.\"\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(text)\nmilestones = LineMilestones(doc=doc)\nmilestones.set()\n\n# Print the text of the spans in a list\nprint(milestones.spans)\nprint(doc.spans)\n\n# [Chapter 1: Introduction., Chapter 2: Methods.]\n# {'milestones': [Chapter 1: Introduction., Chapter 2: Methods.]}\n</code></pre> <p>You can iterate through the <code>milestones.spans</code> list directly, as shown below:</p> <pre><code>for milestone in milestones:\n    print(milestone.start, milestone.end, milestone.text)\n\n# 0 5 Chapter 1: Introduction.\n# 6 11 Chapter 2: Methods.\n# 0 5 Chapter 1: Introduction.\n# 6 11 Chapter 2: Methods.\n</code></pre> <p>There is also a <code>to_list()</code> method, which returns a list of dictionaries providing additional indexing information, should you need it.</p> <pre><code>print(milestones.to_list())\n[{'text': 'Chapter 1: Introduction.', 'characters': 'Chapter 1: Introduction', 'start_token': 0, 'end_token': 5, 'start_char': 0, 'end_char': 23}, {'text': 'Chapter 2: Methods.', 'characters': 'Chapter 2: Methods', 'start_token': 6, 'end_token': 11, 'start_char': 25, 'end_char': 43}]\n</code></pre> <p>Note</p> <p>By default, the pattern used to identify line breaks is \"\\n\", but this can be customed with the <code>pattern</code> keyword when calling <code>set</code>. By default, all line breaks are not included in the milestone spans, but this can be disabled with <code>remove_linebreak= False</code>.</p> <p>The <code>SentenceMilestones</code> class works in a similar way:</p> <pre><code>from lexos.milestones.span_milestones import SentenceMilestones\n\ntext = \"This is sentence 1. This is sentence 2.\"\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(text)\nmilestones = SentenceMilestones(doc=doc)\nmilestones.set()\n\n# Print the text of the spans in a list\nprint(milestones.spans)\nprint(doc.spans)\nprint(list(doc.sents))\n\n# [This is sentence 1., This is sentence 12]\n# {'milestones': [This is sentence 1., This is sentence 2.]}\n# [This is sentence 1., This is sentence 12]\n</code></pre> <p>Note that the <code>Doc</code> object already has a <code>sents</code> attribute that contains a generator sentence spans. This is generated automatically if and only if your language model has a sentence segmenter. If it does not, you cannot use the <code>SentenceMilestones</code> class and will need to rely on the custom approach discussed below. See the spaCy documentation for further information on creating Doc objects with sentence segmentation in the pipeline.</p> <p>The <code>CustomMilestones</code> class can be used to generate milestones based on arbitrary spans. A good way to demonstrate this is to reproduce the sentence segments shown above.</p> <pre><code>from lexos.milestones.span_milestones import CustomMilestones\n\ntext = \"This is sentence 1. This is sentence 2.\"\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(text)\nspans = [doc[0:5], doc[5:10]]\nmilestones = CustomMilestones(doc=doc)\nmilestones.set(spans)\n\n# Print the text of the spans in a list\nprint(milestones.spans)\n\n# [This is sentence 1., This is sentence 2.]\n</code></pre> <p>Here we have manually set our spans to include the first and last five tokens, which happen to coincide with sentence boundaries. But we could easily create spans separated in other ways.</p> <p>Important</p> <p>Unlike the previous two classes, <code>CustomMilestones</code> requires you to pass a list of <code>Span</code> objects to the <code>set</code> method.</p>"},{"location":"user_guide/using_milestones/#additional-settings-and-methods","title":"Additional Settings and Methods","text":"<p>All three classes have additional <code>max_label_length</code> and <code>step</code> parameters. The <code>max_label_length</code> is the maximum number of characters in a token's <code>milestones_label</code> attribute (the default is 20). The <code>step</code> parameter takes an integer indicating the number of spans per item in the milestones list. For instance, if you wanted to have a milestone every tenth sentence, setting <code>step=10</code> would mean that every item in the <code>milestones.spans</code> list would consist of ten sentences. This parameter can similarly be used to group lines or custom spans.</p> <p>All three classes have a <code>reset</code> method, which will remove all spans from both the <code>Milestones</code> and <code>Doc</code> objects.</p>"},{"location":"user_guide/visualization/","title":"Visualization","text":""},{"location":"user_guide/visualization/#overview","title":"Overview","text":"<p>The Lexos <code>visualization</code> module provides a set of modular tools for visualizing the frequency of terms in textual data. Currently, the primary tool is the \"word cloud\", a cover term for a number of variant types of charts that display terms scaled according to their frequency. There are two basic types: traditional word clouds and packed circle charts, or bubble charts.</p> <p>There are two methods of generating word cloud variants: pure Python approaches that produce static images and Javascript versions that generate charts in the web browser with interactive features. Each of these will be discussed below.</p>"},{"location":"user_guide/visualization/#word-clouds","title":"Word Clouds","text":"<p>Word clouds display each submitted term in your text(s), scaled according to its frequency and laid out in a compact display so that you can easily \"eyeball\" which terms are most frequent. To produce a basic word cloud, import the <code>WordCloud</code> class and submit a simple text.</p> <pre><code>from lexos.visualization.cloud import WordCloud\n\ntext = \"This is a sample text to demonstrate how to produce a word cloud.\"\nwc = WordCloud(data=text, title=\"My Word Cloud\")\nwc.show()\n</code></pre> Basic word cloud <p>Notice that you can optionally supply a title with the <code>title</code> keyword. The last line (<code>wc.show()</code>) will display the word cloud.</p> <p>Unlike many other word cloud generators, Lexos prefers for you to pre-tokenise your data before you pass it to the <code>WordCloud</code> class, such as by converting it to a spaCy <code>Doc</code>. You can pass a spaCy <code>Doc</code> object directly, but it is better to convert it to a list of token strings. This allows you to perform other types of pre-processing, such as removing punctuation and stop words (you could also generate a list of token strings by some other means). In the example below, we convert our text to a spaCy <code>Doc</code>, filter out punctuation, stop words, and white space, then generate our word cloud.</p> <pre><code># Import the Lexos Tokenizer class\nfrom lexos.tokenizer import Tokenizer\n\n# Create an instance of the Tokenizer class and make a spaCy doc\ntokenizer = Tokenizer(model=\"en_core_web_sm\")\ndoc = tokenizer.make_doc(text)\n\n# Generate a filtered list of tokens\ntokens = [\n    token.text for token in doc\n    if not token.is_punct\n    and not token.is_stop\n    and not token.is_space\n]\n\n# Create a new word cloud\nwc = WordCloud(data=tokens, title=\"Word Cloud from Tokenized Doc\")\nwc.show()\n</code></pre> Word cloud from a pre-tokenized doc <p>That's much nicer! You can also pass a list of documents. By default, they will be merged into one document before the terms are counted. In the example below, we'll combine the document we created above with another document consisting of three addtional tokens. By using only the first 10 tokens of the first document, you'll be able to see whether the second document is being added.</p> <pre><code># Add some more tokens\nmultiple_docs = [\n    tokens[:10],\n    [\"Some\", \"additional\", \"tokens\"]\n]\n\n# Create and display a word cloud\nwc = WordCloud(\n    data=multiple_docs,\n    title=\"Additional Parameters\",\n    height=200,\n    width=200,\n    round=100,\n    limit=10\n)\nwc.show()\n</code></pre> Word cloud with additional parameters <p>Important</p> <p>The <code>limit</code> parameter will select the ten most common terms, not the first ten tokens in the token list.</p> <p>Under the hood, Lexos uses the Python <code>WordCloud</code> and <code>matplotlib</code> to create the word cloud. You can pass options to <code>WordCloud</code> with the <code>opts</code> keyword, the value of which should be a dictionary of options and their values. You can pass options to <code>matplotlib</code> with the <code>figure_opts</code> keyword, which also takes a dictionary. A full discussion of the available options is beyond the scope of this tutorial, and you are encouraged to consult the <code>WordCloud</code> and <code>matplotlib</code> documentation for ways to customise your word cloud. Here, we'll just provide a simple example showing how to change the background colour.</p> <pre><code># Define an options dictionary\nopts = {\"background_color\": \"lightblue\"}\n\n# Create the word cloud\nwc = WordCloud(data=text, title=\"My Blue Word Cloud\", opts=opts)\nwc.show()\n</code></pre> Word cloud with light blue background <p>Note</p> <p>Advanced users can manipulate the plot directly using <code>matplotlib</code>, as shown below:</p> <pre><code># Create a pyplot figure object with the specified options\nfig = plt.figure(**wc.figure_opts)\n\n# Modify the figure\nfig.set_facecolor(\"lightgreen\")\nfig.suptitle(\"My Light Green Word Cloud\")\n\n# Hide the axis lines and labels\nplt.axis(\"off\")\n\n# Create the image\n# The semicolon prevents display of the object in Jupyter notebooks, or you can add plt.show()\nplt.imshow(wc.cloud, interpolation=\"bilinear\");\n</code></pre> <p>To save your image, use the save method. The file type will be determined by your file suffix (<code>.png</code> or <code>.jpg</code>).</p> <pre><code># Change the file path to the location where you wish to save the image file\nwc.save(\"wordcloud.png\")\n\n# Or save as a jpg file\nwc.save(\"wordcloud.jpg\")\n</code></pre> <p>The <code>save</code> method accepts any arguments allowed by <code>matplotlib</code>'s <code>savefig</code> method:</p> <pre><code>plt.savefig(\"wordcloud.png\", dpi = 300)\n</code></pre>"},{"location":"user_guide/visualization/#bubble-charts","title":"Bubble Charts","text":"<p>Bubble charts (also known as packed circle charts or bubble visualisations) arrange terms into labelled circles, which can sometimes be easier to read than traditional word clouds. They are produced in a similar manner.</p> <pre><code>from lexos.visualization.cloud import WordCloud\n\ntext = \"This is a sample text to demonstrate how to produce a bubble chart.\"\nbc = BubbleChart(data=text, title=\"My Bubble Chart\")\nbc.show()\n</code></pre> Basic bubble chart <p>Bubble charts must have the same height and width, so the figure dimensions are controlled with the <code>figsize</code> keyword with the value in inches.</p> <pre><code>bc = BubbleChart(data=text, figsize=6.5, title=\"My Bubble Chart\")\n</code></pre> Basic bubble chart with parameters <p>Bubble charts do not have the <code>opts</code> and <code>figure_opts</code> keywords you can access in the <code>WordCloud</code> class. The two configuration options available are <code>bubble_spacing</code>, the spacing between bubbles (the default is 0.1) and <code>colors</code>, a list of hexidecimal codes designating the available bubble colours (the default is <code>[\"#5A69AF\", \"#579E65\", \"#F9C784\", \"#FC944A\", \"#F24C00\", \"#00B825\"]</code>).</p> <p>To save your image, use the save method. The file type will be determined by your file suffix (<code>.png</code> or <code>.jpg</code>).</p> <pre><code># Change the file path to the location where you wish to save the image file\nwc.save(\"bubble_chart.png\", dpi=300)\n\n# Or save as a jpg file\nwc.save(\"bubble_chart.jpg\")\n</code></pre> <p>As with word clouds, you can pass additional arguments to <code>matplotlib</code>'s <code>savefig</code> method.</p> <p>Note</p> <p>You can also manipulate the image in <code>matplotlib</code> after it has been generated. Here's an example showing how to change the title and plot size:</p> <pre><code># Import matplotlib\nimport matplotlib.pyplot as plt\n\n# Get the term counts from the BubbleChart instance\ndata = list(bc.counts.keys())\n\n# Create a new figure and axis, setting the figure size\nfig, ax = plt.subplots(subplot_kw=dict(aspect=\"equal\"), figsize=(10, 10))\n\n# Call the internal _plot method to create the bubble chart\nbc._plot(ax, data)\n\n# Turn off the axis lines and labels\nplt.axis(\"off\")\n\n# Recompute the data limits\nax.relim()\n\n# Autoscale the view limits using the data limits\nax.autoscale_view()\n\n# Add a new title\nax.set_title(\"A different title\")\n\n# Display the plot\nplt.show()\n</code></pre>"},{"location":"user_guide/visualization/#multiclouds","title":"Multiclouds","text":"<p>Multiclouds are grids of word clouds that allow you to compare the term counts in multiple documents. They are produced using the <code>MultiCloud</code> class. Each cloud in the grid is an individual instance of a <code>WordCloud</code>, so you can use the customisation parameters available for that class. Here is a short example:</p> <pre><code># Import the MultiCloud class\nfrom lexos.visualization.cloud import MultiCloud\n\ntexts = [\n    \"Natural language processing is a fascinating field that combines linguistics, computer science, and artificial intelligence.\",\n    \"Text analysis, sentiment analysis, and language modeling are key components of modern NLP systems.\",\n    \"Machine learning algorithms help computers understand and process human language effectively.\",\n    \"Natural language processing is a fascinating field that combines linguistics, computer science, and artificial intelligence.\",\n    \"Text analysis, sentiment analysis, and language modeling are key components of modern NLP systems.\",\n    \"Machine learning algorithms help computers understand and process human language effectively.\"\n]\n\n# Create and display a MultiCloud chart\nmc = MultiCloud(data=texts, ncols=3, round=150, title=\"Sample Multiclouds\")\nmc.show()\n</code></pre> Sample three-column multicloud chart with <code>round=150</code> <p>You can change the number of columns in the layout with the <code>ncols</code> parameter. The default is 3. The <code>height</code> and <code>width</code> parameters will be applied to each individual word cloud.</p> <p>The input is a list of texts (it can also be a list of token sublists), where each list item represents a document to be rendered as a word cloud.</p> <p>You will notice that by default the documents are labelled \"Doc 1\", \"Doc 2\", \"Doc 3\", etc. If you wish to use different titles for each word cloud, you can supply a list using the <code>labels</code> keyword.</p> <p>If you want to change the labels (\"Doc 1\", \"Doc 2\", etc.), you can pass a list to the <code>labels</code> parameter. For example:</p> <pre><code>labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\nmc = MultiCloud(data=texts, labels=labels, round=150, title=\"Sample Multiclouds\")\n</code></pre> <p>Note</p> <p>If you want to use <code>matplotlib</code> to modify the multicloud after it has been created, you can do it like this:</p> <pre><code># Create the MultiCloud chart\nmc = MultiCloud(data=texts, title=\"Starting Title\", round=150)\n\n# Draw the canvas\nmc.fig.canvas.draw()\n\n# Get the figure data as an image\nim = mc.fig.canvas.buffer_rgba()\n\n# Add the figure data to matplotlib pyplot instance\nplt.imshow(im, interpolation=\"bilinear\")\n\n# Change the title and size of the figure\nplt.suptitle(\"A Different Title\")\n\n# Turn off the axis lines and labels\nplt.axis(\"off\")\n\n# Save the figure\nplt.savefig(\"multiclouds.png\", dpi=300)\n\n# Display the plot\nplt.show()\n</code></pre>"},{"location":"user_guide/visualization/#types-of-data","title":"Types of Data","text":"<p>In the examples above, we have shown how <code>WordCloud</code>, <code>BubbleChart</code>, and <code>MultiCloud</code> first tokenise the text on whitespace if you use raw text strings. We have also seen that <code>WordCloud</code> and <code>BubbleChart</code> accept lists of token strings. For <code>MultiCloud</code>, you need to submit a list of lists where each list item at the top level is a document and each sublist is a list of tokens. For instance, here is a shortened version of the data we used to produce our multicloud above:</p> <pre><code>tokens = [\n    [\"Natural\", \"language\", \"processing\", \"is\", \"a\", \"fascinating\", \"field\", \".\"],\n    [\"Text\", \"analysis\", \"sentiment\", \"analysis\", \",\" \"and\", \"language\", \"modeling\"]\n]\n</code></pre> <p>Each sublist represents a separate document. By default, <code>WordCloud</code> and <code>BubbleChart</code> will merge these into a single document, whilst <code>MultiCloud</code> will generate an individual cloud for each document int he list. We will see below how you can modify this behaviour.</p> <p>If your data has already been tokenised into spaCy <code>Doc</code> objects, you can pass them directly to the visualisation classes (likewise, you can pass spaCy <code>Span</code> objects or lists of <code>Token</code> objects). However, a better approach is to use spaCy to pre-process your documents, such as by filtering punctuation and stop words, and then pass the tokens as lists of strings. Here is an example where we additionally convert the tokens to lower case.</p> <pre><code>tokens = [\n    token.lower_ for token in doc\n    if not token.is_punct and not token.is_stop\n]\n</code></pre> <p>This allows you to take advantage of spaCy's natural language processing functionality.</p> <p>Another scenario is where you might have pre-tokenised texts is if you have already generated a document-term matrix with the Lexos <code>dtm</code> module. The <code>WordCloud</code> and <code>BubbleChart</code> classes accept a Lexos <code>DTM</code> object, as well as a pandas DataFrame produced by the <code>DTM.to_df()</code> method.</p> <p>Finally, you may have a pre-generated list of term counts, such as is produced by the Python <code>collections.Counter</code> class:</p> <pre><code>from collections import Counter\n\ntokens = [\"this\", \"is\", \"a\", \"sample\", \"text\", \"to\", \"demonstrate\", \"how\", \"to\", \"produce\", \"a\", \"bubble\", \"chart\"]\ncounter = dict(Counter(tokens))\nprint(counter)\n# {'this': 1, 'is': 1, 'a': 2, 'sample': 1, 'text': 1, 'to': 2, 'demonstrate': 1, 'how': 1, 'produce': 1, 'bubble': 1, 'chart': 1}\n</code></pre> <p>You can pass this dictionary directly to the <code>data</code> parameter in the <code>WordCloud</code> and <code>BubbleChart</code> classes.</p>"},{"location":"user_guide/visualization/#limiting-the-number-of-documents","title":"Limiting the Number of Documents","text":"<p>If you pass a list of documents, a <code>DTM</code> object, or a pandas DataFrame, you may want to limit the chart to data from individual documents. You can do this by passing a list of document indexes (beginning with 0) to the <code>docs</code> keyword:</p> <pre><code>bc = BubbleChart(data=dtm, docs=[0, 2])\n</code></pre> <p>Only terms from the first and third documents in the document-term matrix will appear in the chart. The <code>docs</code> keyword is available in all three classes.</p>"},{"location":"user_guide/visualization/#making-interactive-word-clouds-with-d3js","title":"Making Interactive Word Clouds with D3.js","text":""},{"location":"user_guide/visualization/#generating-dynamic-images","title":"Generating Dynamic Images","text":"<p>The static images produced by <code>WordCloud</code>, <code>BubbleChart</code>, and <code>MultiCloud</code> are very good for presentations, but they have their limitations, especially for more cluttered data. Because of this, Lexos offers alternative versions that use the Javascript D3.js library. This allows you produce interactive features such as the ability to hover over the terms in your word cloud to see their counts. D3 visualisations are beautiful and useful for exploring data when static images are hard to read. They are also ideal for embedding in web applications.</p> <p>The cells below demonstrate how to generate D3 versions of word clouds and bubble charts.</p> <p>Important</p> <p>Because D3 is a Javascript library, it processes data into charts in the web browser. As a result, the charts will probably not display in a Jupyter notebook. Instead, you have to save your chart as an HTML page and open it separately in the web browser.</p> <p>For most system configurations, the Lexos D3 visualisations will automatically open a web browser when you generate your chart. If you do not wish to open the browser automatically, set <code>auto_open=False</code>.</p> <p>If your system does not have a default web browser set or saves temporary files in an unexpected location, the web browser may not open automatically or may open a blank page. In this circumstance, set the <code>auto_open</code> parameter to <code>False</code> and save the file. Then search for the file using your operating system and launch the file manually.</p> <p>To generate a D3 word cloud, follow the procedure below (noting the import):</p> <pre><code># Import the D3WordCloud class\nfrom lexos.visualization.d3_wordcloud import D3WordCloud\n\n# Generate and save a word cloud\nwc = D3WordCloud(data=text, width=300, height=300)\nwc.save(\"d3_wordcloud.html\")\n</code></pre> <p>Hover over the words to see further information.</p> <p>The <code>width</code> and <code>height</code> parameters are measured in pixels, and, as with the static image classes, you can set the <code>limit</code> and <code>docs</code> keywords.</p> <pre><code>wc = D3WordCloud(\n    data=docs,\n    docs=1,\n    title=\"30 Most Common Words in Doc 2\",\n    width=300,\n    height=300,\n    limit=30\n)\n</code></pre> <p>Note</p> <p>If you wish to generate a chart from a single doc in a list of docs, you can pass the doc index directly; it does not have to be in a list.</p> <p>Remember that, because docs are zero-indexed, the index 1 refers to the second document in the list.</p> <p>The <code>D3WordCloud</code> class provides a number of other parameters for customising the appearance of the word cloud:</p> <ul> <li><code>font</code>: The name of the font to use.</li> <li><code>spiral</code>: The spiral type to use for the word cloud, \"archimedean\" (the default) or \"rectangular\".</li> <li><code>scale</code>: The scale type to use for the word cloud, \"log\", \"sqrt\", or \"linear\".</li> <li><code>angle_count</code>: The number of angles to use for the word cloud.</li> <li><code>angle_from</code>: The starting angle for the word cloud. The default is -60\u00b0.</li> <li><code>angle_to</code>: The ending angle for the word cloud. The default is 60\u00b0.</li> <li><code>background_color</code>: The background color of the word cloud. The default is white.</li> <li><code>colorscale</code>: The name of a categorical d3 scale to use for the word cloud. The default is d3.scale.category20b\". For other colorscales, see the d3-scale documentation.</li> </ul> <p>Note</p> <p>The available options are based on the exceptional word cloud generator produced by Jason Davies.</p> <p>You can also generate a multicloud in D3:</p> <pre><code>from lexos.visualization.d3_wordcloud import D3MultiCloud\n\n# Create multi-cloud\nmc = D3MultiCloud(\n    data_sources=texts,\n    title=\"D3 Multiclouds\",\n    labels=None,\n    cloud_width=250,\n    cloud_height=250,\n    columns=2\n)\nmc.save(\"d3_multiclouds.html\")\n</code></pre> <p>All the customisation parameters listed above for <code>D3WordCloud</code> are available. Notice, however, that few minor differences. You input your data using the <code>data_sources</code> keyword. Since each source document can have its own title, you can supply these titles as a list with the <code>labels</code> parameter (if you do not provide this, generic titles \"Doc 1\", \"Doc 2\", etc. will be used). Likewise, you can specify the dimensions of individual clouds (in pixels) with the <code>cloud_width</code> and <code>cloud_height</code> parameters. Finally, you can set the number of columns in the layout.</p> <p>To generate a D3 bubble chart, you use the <code>D3BubbleChart</code> class:</p> <pre><code>from lexos.visualization.d3_bubbleviz import D3BubbleChart\n\nbc = D3BubbleChart(data=text, title=\"D3 Bubble Chart\")\nbc.save(\"d3_bubblechart.html)\n</code></pre> <p>Apart from the standard parameters, <code>D3BubbleChart</code> has two extra keywords for styling the chart.</p> <ul> <li><code>margin</code>: A dictionary with the keys \"top\", \"right\", \"bottom\", and \"left\", used to configure the margin around the chart in pixels.</li> <li><code>color</code>: The colour scheme for the chart, either the name D3 colour scheme or a list of custom colours. The default is \"schemeCategory10\". For other colour schemes, see the d3-scale documentation.</li> </ul>"},{"location":"user_guide/visualization/#customising-d3-visualisations","title":"Customising D3 Visualisations","text":"<p>D3 visualisations are standalone web pages, so they must be viewed in the browser. There are additional parameters for all three classes that allow you to choose whether to include the D3 Javascript in the web page (leading to a bigger file) or download it from the internet (which means it will only work if you have an internet connection). See the API documentation for usage. In most cases, it is safe to leave the default setting and include the D3 Javascript in the web page.</p> <p>The actual logic used to produce the visualisation is not loaded from the internet, and it is not minimised. This allows you to open the HTML file and modify the Javascript, as well as the CSS styling, if you are comfortable doing so.</p> <p>Developer's Note</p> <p>The visualisations are designed for display as web pages. However, if you are planning to incorporate them in an application, you may want to make more extensive changes to incorporate the charts into your layout. Each visualisation is produced from an HTML template, which is populated with variables passed from Python. You can design your own template appropriate for your application and specify the path to your template with the <code>template</code> parameter.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/","title":"Hierarchical Agglomerative Clustering","text":""},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#overview","title":"Overview","text":"<p>Hierarchical cluster analysis is a method grouping similar data points into a hierarchy of nested clusters. It builds a tree-like structure that shows the relationships between clusters, where closer clusters are more similar. This approach helps reveal patterns and relationships within datasets, especially complex ones, by visualizing data groupings at multiple levels of similarity.</p> <p>Hierarchical clustering may be agglomerative or divisive. Agglomerative clustering starts with each data point (typically a single document in Lexos) as a separate cluster and then merges them into larger clusters called clades. Divisive clustering takes the opposite approach, starting with all the data points in one cluster and splitting it. Currently, Lexos only provides an agglomerative algorithm.</p> <p>The the results of hierarchical cluster analysis produced by this approach are typically represented visually as a dendrogram, which shows the hierarchical relationshops between clusters and the distance (similarity) between them.</p> <p>In order to group documents into clusters, the algorithm relies on two important settings, a distance metric and a linkage criterion.</p> <p>The distance metric metric is a measure used to quantify the similarity or dissimilarity between data points or clusters. A simple way to understand this is that a term occurring once in document A and twice in document B will have a distance of 1 between the two documents (based on that term alone). Of course, there are more numerous different ways in which we could calculate difference, and these will be discussed in more detail below.</p> <p>The linkage criterion determines how the distance between clusters is calculated when merging. A simple way to think of this is to imagine two circles with dots in them representing the terms. The dots closest to the outer edge of the first circle will be closest to the dots closest to the outer edge of the second circle (in the direction where the circles are closest). We could use the short distance of these circles to select both to be merged into a single cluster at the next level of the hierarchy. But, equally, we could base our decision whether the merge them on the position of the dots in the centre. We have several other option to choose from, and these will be discussed in more detail below.</p> <p>With this knowledge, we can describe the clustering algorithm.</p> <ol> <li>We start with each document as its own cluster, or \"leaf\".</li> <li>We identify the two closest clusters based on our chosen distance metric and linkage criterion.</li> <li>We merge the two closest clusters into a single cluster. We then repeat steps 2 and 3 until all documen are in one cluster (the root of the tree).</li> <li>We plot a dendrogram to represent this hierarchical structure.</li> </ol> <p>An advantage of hierarchical clustering is that we do not need to choose the number of clusters in advance, and we can explore our clusters at different levels of granularity. However, our results can be sensitive to the choice of distance metric and linkage criterion. Further, the diagram represents a single cluster at the root level and a number of clusters equal to the number of documents at the leaf level. There may be more meaningful clusters on between hierarchy between these two extremes, but there is no clear method of determining a cut-off point (known as \"cutting the dendrogram\"). The discussion below will provide some guidance in dealing with these issues.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#how-to-perform-hierarchical-agglomerative-clustering","title":"How to Perform Hierarchical Agglomerative Clustering","text":"<p>To perform cluster analysis and generate a dendrogram, you will need document-term matrix produced by the DTM module. Then you simply import the Dendrogram class and feed it your DTM. You will also need a list of labels for the documents in your DTM object. In the example below, we will use the default settings for the distance metric and linkage criterion.</p> <pre><code># Import the Dendrogram class\nfrom lexos.cluster import Dendrogram\n\n# Create an instance of the Dendrogram object (feel free to change the parameters)\ndendrogram = Dendrogram(\n    dtm=dtm,\n    labels=labels,\n    metric=\"euclidean\",\n    method=\"average\",\n    orientation=\"top\",\n    # color_threshold=1.5, # Uncomment to color branches\n    title=\"My First Dendrogram\",\n    figsize=(10, 8),\n    show=True\n)\n\n# Show the dendrogram\ndendrogram.show()\n</code></pre> Sample dendrogram"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#dendrogram-settings","title":"Dendrogram Settings","text":"<p>When we create the <code>Dendrogram</code>, we need to tell it how to measure document similarity and how to connect those similarities into a tree. Here are the key parameters you can adjust:</p> <ul> <li><code>dtm</code>: This is our \"linguistic spreadsheet\" (<code>dtm</code>). See the notes below for the possible formats.</li> <li><code>metric</code>: This sets the distance metric, which tells the dendrogram how to measure the \"distance\" or dissimilarity between your documents. Shorter distances mean more similar documents. Options include <code>euclidean</code> (the default), <code>cosine</code>, and <code>cityblock</code>. For other options, see Choosing a Distance Metric below.</li> <li><code>method</code>: This sets the linkage criterion, which determines how individual documents (or existing clusters of documents) are joined together to form larger branches and clusters in the tree. Option <code>average</code> (the default), <code>single</code>, <code>complete</code>, and <code>ward</code>. For further information, see Choosing a Linkage Method below.</li> <li><code>labels</code>: This is simply the list of descriptive names for your documents (e.g., \"Poe\", \"Lippard\") that we defined earlier. These will appear as the leaves (endpoints) on your tree.</li> <li><code>orientation</code>: Controls the direction of the dendrogram. The default <code>\"top\"</code> orients the branches so that they extend downwards from root at the top. Other options are <code>\"bottom\"</code>, <code>\"left\"</code>, and <code>\"right\"</code>.</li> <li><code>color_threshold</code>: If set, branches with a distance below this threshold will be colored differently from those above it. This helps visualize clusters at a certain distance level. You can try a number like <code>1.0</code> or <code>1.5</code> to see its effect.</li> <li><code>show</code>: Controls whether the generated tree figure is displayed automatically. If <code>False</code>, the tree will not be displayed, but you display it later by calling <code>dendrogram.showfig()</code>. There are also methods that enable you to save it to a variable or file.</li> <li><code>title</code>: Adds a title to your dendrogram plot.</li> <li><code>figsize</code>: A tuple <code>(width, height)</code> in inches to set the size of the overall figure. For example, <code>(12, 8)</code> for a wider and taller plot.</li> </ul> <p>The easiest way to format your data for plotting is to generate a Lexos <code>DTM</code> instance and pass it to the <code>Dendrogram</code> class. However, <code>Dendrogram</code> also accepts two other formats:</p> <ol> <li>A Pandas DataFrame with document labels as row indexes and terms as column indexes (this is the equivalent of <code>DTM.to_df(transpose=True)</code>).</li> <li>A list of documents in which each document is a sublist containing the term counts. You can also pass equivalent numpy arrays. If you use data in this format, you will probably want to include a list of document labels using the <code>labels</code> keyword.</li> </ol>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#plotting-dendrograms-with-plotly","title":"Plotting Dendrograms with Plotly","text":"<p>The <code>Dendrogram</code> class uses Python's matplotlib library to produce static images. However, in very large dendrograms, there is a danger of leaf labels overlapping, making the plot unreadable. In this case, you can use the Plotly plotter, which provides the ability to pan and zoom around the dendrogram, making it more readable. The Plotly plotter is also ideal if you are including the dendrogram in a web app.</p> <p>To use the Plotly plotter, import the <code>PlotlyDendrogram</code> class, create an instance, and use it as above.</p> <pre><code># Import the PlotlyDendrogram class\nfrom lexos.cluster import PlotlyDendrogram\n\n# Create an instance of the PlotlyDendrogram object\ndendrogram = PlotlyDendrogram(\n    dtm=dtm,\n    labels=labels,\n    metric=\"euclidean\",\n    method=\"average\",\n    orientation=\"bottom\",\n    title=\"Document Similarity Dendrogram\",\n)\n\n# Show the dendrogram using Plotly\ndendrogram.show()\n</code></pre> Sample Plotly dendrogram <p>Note that the image above is a static representation and does not demonstrate Plotly's interactive features.</p> <p>Data should be passed to <code>PlotlyDendrogram</code> either as a Lexos <code>DTM</code> instance or using one of the other datatypes described for the <code>Dendrogram</code> class above.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#choosing-a-distance-metric","title":"Choosing a Distance Metric","text":"<p>One of the most important (and least well-documented) aspects of the hierarchical clustering method is the distance metric. Since we are representing texts as document vectors, it makes sense to define document similarity by comparing the vectors. One way to do this is to measure the distance between each pair of vectors. For example, if two vectors are visualized as lines in a triangle, the hypotenuse between these lines can be used as a measure of the distance between the two documents. This method of measuring how far apart two documents are is known as Euclidean distance. This is the default setting used by Lexos. It is good for general comparisons but can be sensitive to the overall length of documents (longer documents might naturally have larger term counts, increasing their \"distance\").</p> <p>Another common metric is cosine similarity. Imagine each document as an arrow pointing in a specific linguistic \"direction.\" Cosine similarity measures how much these arrows point in the same direction. If the angle at which they point is almost identical, the documents are very similar, even if one document is much longer than another. This is often an excellent choice for text analysis as it focuses on stylistic or thematic direction rather than raw word counts.</p> <p>Cityblock distance also called Manhattan distance is another common metric. Imagine moving on a city grid where you can only go along streets (no diagonal shortcuts). This distance is the sum of the absolute differences for each term between two documents. This metric is useful when the individual differences in term counts are important.</p> <p>Many other metrics are available (e.g., \"jaccard\", \"chebyshev\") from the Python scipy package, which Lexos runs under the hood. You can find a full list in the <code>SciPy documentation</code>.</p> <p>The table below provides some additional guidance on how to choose a distance metric.</p> Small Number of terms per segment Large Number of terms per segment Small Vocabulary Bray-Curtis, Hamming    (e.g. character dialogue) Euclidean, Chebyshev, Standardized Euclidean (e.g. chapters of books) Large Vocabulary Correlation, Jaccard, Squared Euclidean (e.g. non-epic poetry) Cosine, Manhattan, Canberra (e.g. comparing entire corpora)"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#choosing-a-linkage-method","title":"Choosing a Linkage Method","text":"<p>At each stage of the clustering process, a choice must be made about whether two clusters should be joined (and recall that a single document itself forms a cluster at the lowest level of the hierarchy). \"Average\" is the default, but you may choose other linkage methods by clicking the button.</p> <ul> <li>Average: This method is a compromise between single and complete linkage. It takes the average distance of the points in each cluster and uses the shortest average distance for deciding which cluster should be joined to the current one. When combining two clusters, this method considers the average distance between all pairs of documents in the two clusters. It tends to produce well-balanced clusters.</li> <li>Single: An intuitive means for doing this is to join the cluster containing a point (e.g. a term frequency) closest to the current cluster. This is known as single linkage, which joins clusters based on only a single point. In other words, clusters are joined based on the closest pair of documents between them. Single linkage does not take into account the rest of the points in the cluster, and the resulting dendrograms tend to have spread out clusters. This process is called \"chaining\". When this happens, where documents connect one after another, forming long, straggly branches.</li> <li>Complete: Complete linkage uses the opposite approach. It takes the two points furthest apart between the current cluster and the others. The cluster with the shortest distance to the current cluster is joined to it. Complete linkage thus takes into account all the points on the vector that come before the one with the maximum distance. It tends to produce compact, evenly distributed clusters, ensuring all documents within a cluster are relatively similar to each other.</li> <li>Weighted: The weighted average linkage performs the average linkage calculation but weights the distances based on the number of terms in the cluster. It, therefore, may be a good option when there is significant variation in the size of the documents under examination.</li> <li>Ward: This method aims to minimize the increase in \"variance\" (or spread) within clusters when they are merged. It tries to make clusters that are as \"tight\" and internally similar as possible. It often produces intuitive and well-structured clusters.</li> </ul> <p>Which linkage criterion you choose depends greatly on the variability of your data and your expectations of its likely cluster structure. The fact that it is very difficult to predict this in advance may explain why the \"compromise\" of average linkage is the best default.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#intepreting-dendrograms","title":"Intepreting Dendrograms","text":""},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#choosing-where-to-cut-the-dendrogram","title":"Choosing Where to Cut the Dendrogram","text":"<p>Hierarchical clustering is an exploratory technique, so it's often helpful to try different cut-off points and evaluate the resulting clusters. The height of the cut determines the number of clusters. A higher cut will result in fewer, larger clusters, while a lower cut will result in more, smaller clusters.</p> <p>The best way to cut a dendrogram will always depend on the specific dataset and the goals of the analysis. There's no single \"right\" way to do it. To determine where to cut a dendrogram for clustering, you can use visual cues like the longest vertical distance between nodes, or consider numerical criteria like the Silhouette score or Dunn's index, or even trial and error. The choice of cut-off point depends on how many clusters you want and the desired level of similarity within each cluster.</p> <p>Here is a procedure to use as a starting point:</p> <ol> <li>Identify the longest vertical distance: Look for the longest vertical line (distance) between merging nodes on the dendrogram. Cutting at this point often reveals a natural separation between clusters.</li> <li>Consider the overall structure: Observe how the data points are grouped at different height levels. You might choose a cut that separates well-defined, compact clusters or one that creates a few large clusters.</li> </ol> <p>Lexos does not offer any numerical criteria for evaluating the quality of hierarchical clusters. However, you can count the number of leaves at your cutoff point and use that as the k value in a k-means clustering analysis to provide comparative evidence.</p> <p>In addition, Lexos does not offer a method of drawing the dendrogram showing the cut. SciPy provides the <code>fcluster</code> method for cutting dendrograms, and we will hopefully implement it in the future. This Stack Overflow discussion provides information on how to add truncate mode.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#cluster-robustness","title":"Cluster Robustness","text":"<p>By cutting trying different distance metrics and linkage methods, as well as by cutting the dendrogram at different heights, you can evaluate the robustness of individual clusters. A \"robust\" cluster is one that persists, regardless of the setup criteria. If the cluster is sensitive to changes in the setup criteria, it is more likely to be a statistical artefact of those criteria, rather than a meaningful pattern. This, however, is a guideline, and its usefulness will depend on your data.</p> <p>Measuring Robustness with Bootstrap Consensus Trees</p> <p>One way to automate the process of assessing cluster robustness is to implement Bootstrap Consensus Trees, which perform clustering with multiple settings and record the most-consistent clusters. See the section on Bootstrap Consensus Trees below.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#clustermaps","title":"Clustermaps","text":"<p>A clustermap is a dendrogram attached to a heatmap, showing the relative similarity of documents using a colour scale. A clustermap combines the best of two worlds: hierarchical clustering (dendrograms) and pairwise similarity representation (heatmap).</p> <p>The dendrogram on the top shows the hierarchical clustering of your documents based on their content (the rows of your DTM). The dendrogram on the left shows the same clustering, but rotated. As with standalone dendrograms, shorter branches mean documents (or clusters) are more similar. The order of documents along the heatmap axes is determined by these dendrograms, grouping similar documents together.</p> <p>The heatmap grid visually represents the pairwise distances between your documents. Each cell at the intersection of a row and a column represents the distance between two documents. The color intensity on the heatmap will represent the distance between documents: typically, darker/different colors show greater distance (less similarity), while lighter/similar colors show shorter distance (more similarity). The diagonal of the heatmap will always be the same color, usually representing zero distance, as a document has zero distance from itself.</p> <p>Clustermaps can be useful for observing the following:</p> <ul> <li>Stylistic Groupings: Does the heatmap show a strong block of similarity among authors from the same literary period or movement?</li> <li>Thematic Cohesion: If your DTM focused on specific themes, do documents discussing similar themes cluster together?</li> <li>Influence or Divergence: You might see how a text aligns with or diverges from others, giving insights into authorship, genre, or evolution of style.</li> </ul> <p>Lexos can generate static clustermap images using the Python Seaborn library or dynamic images using Plotly.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#using-seaborn","title":"Using Seaborn\u00a4","text":"<p>The Python Seaborn visualization library has a clustermap function, which has somewhat limited functionality. Lexos wraps the Seaborn function in the <code>Clustermap</code> class to provide additional convenience features, such as the inclusion of titles. To generate a clustermap with Seaborn, use the following code:</p> <pre><code># Import the ClusterMap class\nfrom lexos.cluster import Clustermap\n\n# Create a ClusterMap object\ncm = Clustermap(dtm=dtm, title=\"My Clustermap\")\ncm.show()\n</code></pre> Sample clustermap <p>The <code>dtm</code> can be a Lexos <code>DTM</code> instance, a compatible Pandas DataFrame, or a list of lists of tokens. For the clustering parameters, see the advice in Choosing a Distance Metric and Choosing a Linkage Method above.</p> <p>Important</p> <p>Unlike <code>Dendrogram</code> and <code>PlotlyDendrogram</code>, <code>Clustermap</code> accepts a Pandas DataFrame formatted with documents as columns and terms as rows. This is the equivalent of <code>DTM.to_df()</code>.</p> <p>In addition to the <code>title</code>, <code>metric</code>, and <code>method</code> keywords, <code>Clustermap</code> takes the following parameters:</p> <ul> <li><code>labels</code>: A list of descriptive names for your documents. These will appear as the leaves (endpoints) on your tree. If not supplied, the labels from your Lexos <code>DTM</code> or Pandas DataFrame will be used.</li> <li><code>z_score</code>: Standardizes the values within each row (documents) or column (terms). If the value is set to <code>None</code>, the heatmap shows raw frequencies (or whatever your DTM contains). The setting <code>0</code> standardizes each row (document) by subtracting its mean and dividing by its standard deviation. This highlights how terms vary within a single document relative to that document's average term frequency. Useful for comparing patterns across documents regardless of their length. The setting <code>1</code> standardizes each column (term) by subtracting its mean and dividing by its standard deviation. This highlights how a single term's frequency varies across different documents relative to that term's average frequency. Useful for seeing which documents use a term more or less than average.</li> <li><code>standard_scale</code>: Similar to <code>z_score</code>, but scales to a specific range (usually 0 to 1). The setting <code>0</code> scales each row (document) so its minimum value is 0 and its maximum is 1. The setting <code>1</code> scales each column (term) so its minimum is 0 and its maximum is 1.</li> <li><code>cmap</code>: Sets the color scheme (colormap) for the heatmap. It determines which colors represent low values and which represent high values. The default setting \"vlag\" is a diverging colormap (red/blue), which is good for showing values around a center point (especially after <code>z_score</code> scaling). Other good general-purpose colormaps are \"viridis\" and \"coolwarm\". You can find listings of other <code>matplotlib</code> and <code>seaborn</code> colormaps online.</li> <li><code>hide_upper</code>: Setting the value to <code>True</code> removes the dendrogram above the heatmap. Useful if you are not interested in the clustering of columns/terms.</li> <li><code>hide_side</code>: Setting the value to <code>True</code> removes the dendrogram to the left of the heatmap. Useful if you are not interested in the clustering of rows/documents.</li> <li><code>row_cluster</code>: Perform clustering on rows (documents). Default is <code>True</code>. Along with <code>col_cluster</code>, this setting is useful if you have a specific ordering in mind for comparison, or if you've pre-computed a linkage.</li> <li><code>col_cluster</code>: Perform clustering on columns (terms). Default is <code>False</code>. If <code>False</code>, items will be displayed in their original order.</li> <li><code>row_colors</code>: Allows you to add colored strips alongside the rows, which can be used to visually group or categorize your documents. Provide a list of colors (e.g., <code>['red', 'blue', 'green']</code>). The list should match the number of documents/terms. You can also use a named <code>seaborn</code> palette (e.g., <code>\"husl\"</code>). Setting the value to \"default\" will use <code>seaborn.husl_palette(8, s=0.45)</code>. This setting is great for adding metadata! For example, if you have two categories of documents (e.g., \"male authors\" vs. \"female authors\"), you could assign a color to each category to see if your clustering aligns with these external factors.</li> <li><code>col_colors</code>:Allows you to add colored strips alongside the columns, which can be used to visually group or categorize your terms. Setting values are the same as for <code>row_colors</code>.</li> <li><code>title</code>: Adds a title to your dendrogram plot.</li> <li><code>figsize</code>: A tuple <code>(width, height)</code> in inches to set the size of the overall figure. For example, <code>(12, 8)</code> for a wider and taller plot.</li> </ul> <p>The <code>Clustermap</code> instance can be further customized with any  <code>Seaborn.clustermap</code> parameter.</p> <p>After you've generated your clustermap, you'll likely want to save it as an image for reports or presentations. The <code>save()</code> method lets you do this easily. Just provide a file path, and it'll save the image. You can specify different file formats by changing the extension (e.g., <code>.png</code>, <code>.jpg</code>, <code>.pdf</code>, <code>.svg</code>). The <code>save()</code> method wraps the <code>matplotlib</code> <code>savefig</code> function and accepts any of its keywords.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#using-plotly","title":"Using Plotly\u00a4","text":"<p>Plotly clustermaps are somewhat experimental and may or may not render plots that are as informative as Seaborn clustermaps. One advantage they have is that, instead of providing labels for each document at the bottom of the graph, they provide the document labels on the x- and y-axes, as well as the z (distance) score in the hovertext. This allows you to mouse over individual sections of the heatmap to see which documents are represented by that particular section, as well as the exact distance values.</p> <p>Plotly clustermaps are constructed in the same manner to Seaborn clustermaps with the same settings, so far as is possible in the Plotly library:</p> <pre><code># Import the PlotlyClustermap class\nfrom lexos.cluster import PlotlyClustermap\n\n# Create a PlotlyClustermap object\ncm = PlotlyClustermap(dtm=dtm, title=\"My Clustermap\")\n</code></pre> Sample Plotly clustermap <p>Note</p> <p>Note that the image above is a static representation and does not demonstrate Plotly's hover effects.</p> <p>All the options for Plotly dendrograms are available with the following differences:</p> <ul> <li><code>figsize</code> is measured in pixels.</li> <li><code>colorscale</code> is the name of a built-in Plotly colorscale. This is applied to the heatmap and converted internally to a list of colours to apply to the dendrograms.</li> </ul> <p>Two additional parameters, <code>hide_upper</code> and <code>hide_side</code> allow you to hide the individual dendrograms.</p> <p>Warning</p> <p>Note that panning and zooming can cause the heatmap and dendrograms to become unsynced. There is currently no way to maintain the syncing in pure Python. If you need to zoom in on particular sections of the plot, you may be able to achieve the effect you are looking for by saving the plot as an HTML file with the experimental <code>include_sync</code> parameter:</p> <pre><code>html = cm.to_html(include_sync=True)\nwith open(\"filename.html\", \"w\") as f:\n    f.write(html)\n</code></pre> <p>Open the HTML file in a web browser, and you may get the behaviour you need. See below for other options for saving your Plotly clustermaps.</p> <p>You have several option for saving your Plotly clustermaps. In the Plotly toolbar, there is a \"Download plot as png\" option to save the plot as a static <code>.png</code> file. You can also save the the image to a static file programmatically by calling <code>PlotlyClustermap.write_image()</code>. Just provide a file name (including the extension), and it will save the image. You can choose different file formats by changing the extension (e.g., <code>.png</code>, <code>.jpg</code>, <code>.pdf</code>, <code>.svg</code>). This is a wrapper around Plotly's <code>write_image()</code> function and accepts all the same arguments.</p> <p>Plotly figures are highly interactive when saved as HTML, allowing you to zoom, pan, and hover over data points in your saved file. If you wish to save your diagram as an HTML file, call <code>PlotlyClustermap.write_html()</code>. This is a wrapper around Plotly's <code>write_html()</code> function and accepts all the same arguments.</p> <p>Note that <code>write_image()</code> and <code>write_html()</code> have parallel <code>to_image()</code> and <code>to_html()</code> methods that allow you to assign the results to a variable, rather than saving to a file.</p>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#bootstrap-consensus-trees","title":"Bootstrap Consensus Trees","text":"<p>A Bootstrap Consensus Tree is particularly robust because it doesn't just build one tree. Instead, it builds many, many trees by randomly sampling portions of your DTM. It then finds the \"consensus\": the most consistently appearing relationships across all those individual trees.</p> <p>Generating bootrap consensus dendrograms involves submitting the same distance metric and linkage method parameters as regular dendrogram. However, there are a few additional parameters to set:</p> <ul> <li><code>dtm</code>: Unlike the other clustering modules, the <code>BCT</code> class accepts only an instance of a Lexos <code>DTM</code>.</li> <li><code>cutoff</code>: This is a confidence threshold. As mentioned, the BCT is built from many individual \"bootstrap\" trees. A <code>cutoff</code> of <code>0.5</code> (which means 50%) means that a specific grouping of documents (a branch on the tree) must appear in at least 50% of all the trees generated during the <code>iterations</code> to be considered reliable enough to show up in the final consensus tree. Higher <code>cutoff</code> values (e.g., 0.7 or 0.8) will result in a \"sparser\" tree, showing only the most robust and consistent relationships. Lower <code>cutoff</code> values (e.g., 0.3) will show more relationships, but some of these might be less statistically reliable.</li> <li><code>iterations</code>: This is the number of \"bootstrap resampling\" rounds. In each round, Lexos takes a random 80% sample of the terms (columns) from your DTM and builds a tree from that sample. More iterations (e.g., 100, 1000) makes the consensus tree more statistically reliable and representative of the underlying relationships in your texts, as it averages out more variations. However, it will take longer to compute. Fewer iterations (e.g., 10, 20) are good for quick testing or initial explorations. For final research results, <code>100</code> (the default in the <code>BCT</code> class) or higher is often recommended if computation time allows.</li> <li><code>replace</code>: This relates to how the terms are sampled during each iteration. Setting the value to \"with\" means a term column can be selected multiple times within a single 80% sample (allows for more randomness). The value \"without\" means each term column can only be selected once per 80% sample (more stable). This setting is generally suitable for DTMs as it ensures each unique term contributes uniquely within a sample.</li> <li><code>doc_labels</code>: This is simply the list of descriptive names for your documents (e.g., \"Poe\", \"Lippard\") that we defined earlier. These will appear as the leaves (endpoints) on your tree.</li> <li><code>text_color</code>: Sets the color for all text on the plot (axis labels, branch lengths, and document labels). You can use \"rgb(R, G, B)\" format. For example: <code>\"rgb(0, 0, 0)\"</code> (black) or <code>\"rgb(255, 0, 0)\"</code> (red).</li> <li><code>layout</code>: Sets the layout of the dendrogram, either \"rectangular\" (the default) or \"fan\".</li> </ul>"},{"location":"user_guide/cluster/hierarchical-agglomerative-clustering/#plotting-bootstrap-consensus-trees","title":"Plotting Bootstrap Consensus Trees","text":"<p>To create a bootstrap consensus tree with rectangular layout, use the following code, setting the parameters describe above as required:</p> <pre><code># Import the BCT class for Bootstrap Consensus Tree\nfrom lexos.cluster import BCT\n\n# Create an instance of the BCT object (feel free to adjust parameters)\nbct = BCT(\n    dtm=dtm,\n    metric=\"euclidean\",\n    method=\"average\",\n    cutoff=0.5,\n    iterations=10,\n    replace=\"without\",\n    labels=labels,\n    text_color=\"rgb(0, 0, 0)\",\n    layout=\"rectangular\",\n    title=\"Bootstrap Consensus Tree (Rectangular Layout)\"\n)\n\n# Show the figure\nbct.show()\n</code></pre> Sample Bootstrap Consensus Tree rectangular layout <p>To generate a diagram with a fan layout, set <code>layout=\"fan\"</code> (and adjust the <code>title</code> set above).</p> Sample Bootstrap Consensus Tree fan layout"},{"location":"user_guide/cluster/k-means/","title":"K-Means Cluster Analysis","text":""},{"location":"user_guide/cluster/k-means/#overview","title":"Overview","text":"<p>K-Means clustering partitions a set of documents into a number of groups or clusters in a way that minimizes the variation within clusters. The k refers to the number of partitions, so for example, if you wish to see how your documents might cluster into three (3) groups, you would set <code>k=3</code>. In fact, k-means clustering differs from hierarchical agglomerative clustering because you must begin by choosing the number of clusters into which you group your documents.</p> <p>The k-means algorithm works something like this:</p> <ol> <li>You decide on the number of clusters you wish to form.</li> <li>The algorithm computes a centroid for each cluster. The centroid is the center (mean point) of a cluster. The procedure for creating centroids at the very start can be varied and is discussed below.</li> <li>Assign each of your documents to the cluster with the nearest centroid.</li> <li>Repeat steps 2 and 3, thereby re-calculating the locations of centroids for the documents in each cluster and reassigning documents to the cluster with the closest center. The algorithm continues until no documents are reassigned to different clusters.</li> </ol>"},{"location":"user_guide/cluster/k-means/#how-to-perform-k-means-clustering","title":"How to Perform K-Means Clustering","text":"<p>Lexos requires that you choose in advance a value for k, that is, how many groups you want to cluster your documents into.</p> <p>To perform a simple k-means analysis with the default settings, start by constructing a Document-Term Matrix (DTM) as discussed in The Document-Term Matrix. The import the <code>KMeans</code> class and instantiate it with the DTM. You can then run the analysis with your chosen k value:</p> <pre><code># Import KMeans\nfrom lexos.cluster import KMeans\n\n# Assuming you have your DTM saved to the dtm variable\nkmeans = KMeans(dtm=dtm, k=4)\n</code></pre> <p>Pre-configuring your k-means settings can be valuable in helping you to produce meaningful results. Lexos provides a number of options for configuring the k-means procedure.</p> <ul> <li><code>k</code>: The number of clusters to create.</li> <li><code>init</code>: This is the initialization strategy, which can be \"k-means++\" or \"random\". \"k-means++\" selects initial cluster centers using a weighted probability distribution to speed up convergence. This can help can help to constrain the initial placement of the centroids. The \"random\" option chooses k observations at random from the data to serve as the initial centroids. The default is \"k-means++\".</li> <li><code>max_iter</code>: The maximum number of iterations of the k-means algorithm for a single run. The default is 300.</li> <li><code>n_init</code>: The number of times (N) the k-means algorithm will be run with different centroid seeds (the tolerance for convergence). The final results will be the best output of those N consecutive runs. The default is 10.</li> <li><code>tol</code> The relative tolerance with respect to inertia to declare convergence. The default is 0.0001.</li> <li><code>random_state</code>: A number to use as the initial seed to insure that the results are reproducible. The default is 42.</li> </ul> <p>The easiest way to format your data for plotting is to generate a Lexos <code>DTM</code> instance and pass it to the <code>Dendrogram</code> class. However, <code>Dendrogram</code> also accepts two other formats:</p> <ol> <li>A Pandas DataFrame with document labels as column indexes and terms as row indexes (this is the equivalent of <code>DTM.to_df()</code>).</li> <li>A list of documents in which each document is a sublist containing the term counts. You can also pass equivalent numpy arrays. If you use data in this format, you will probably want to include a list of document labels using the <code>labels</code> keyword.</li> </ol> <p>There is no obvious way to choose the number of clusters, but some strategies will be discussed below. The k-means procedure can be very sensitive to how you have constructed your DTM, for instance, whether you have performed normalization or restricted it to only the most frequesnt terms. The procedure is also very sensitive to the position of the initial centroid seeds, although employing the \"k-means++\" setting of the <code>init</code> parameter helps to constrain this placement.</p> <p>You can play with these settings to determine which one provide you with the best results.</p>"},{"location":"user_guide/cluster/k-means/#how-to-choose-the-number-of-clusters-k","title":"How to Choose the Number of Clusters (k)?","text":"<p>A mathematical metric known as the elbow method is commonly used to decide the optimal value of k. This method involves trying numerous settings of k, running k-means on each, and the sum of squared distances of data points to their cluster centroids. These within-cluster sum of squares (WCSS) values are plotted and the \"elbow\" is the point where the rate of decrease between k settings slows down. Lexos can produce an elbow plot like the one below to allow you to identify the \"elbow\".</p> <p>We can generate an elbow plot with the <code>elbow_plot()</code> method, submitting a range between 1 and 10 clusters to evaluate.</p> <pre><code>kmeans.elbow_plot(k_range=range(1, 10))\n</code></pre> Sample elbow plot <p>The following points can help you to interpret the elbow plot.</p> <ul> <li>The x-axis shows the number of clusters (<code>k</code>) we tried.</li> <li>The y-axis shows the inertia (or within-cluster sum of squares), which measures how compact the clusters are.</li> <li>Lower values of inertia mean tighter, more defined clusters.</li> <li>The \"elbow\" is where the curve sharply changes direction \u2014 it\u2019s the point beyond which adding more clusters doesn't significantly reduce inertia. This is the point of diminishing returns in decreasing WCSS.</li> </ul> <p>In this example, the elbow occurs at <code>k=4</code>, meaning that 4 clusters is a good balance between under- and over-clustering. It is thus a good candidate for the optimal number of clusters.</p> <p>Limitations of the Elbow Method</p> <p>Although the elbow method has a mathematical basis, the elbow point may not always be clear, and interpreting it is still subjective. For some datasets, it may not work well. It can be helpful to perform hierarchical agglomerative clustering before performing k-means clustering, as the resulting dendrogram may suggest a certain number of clusters that is likely to produce meaningful results. Mathematical metrics like the elbow method are not a substitute for human knowledge about the texts being considered.</p>"},{"location":"user_guide/cluster/k-means/#visualizing-k-means-clusters","title":"Visualizing K-Means Clusters","text":"<p>Lexos provides three methods of visualizing the results of a k-means cluster analysis. In each case, Lexos first applies PCA (Principal Component Analysis) to reduce the dimensions of the data so it can be viewed in a 2D or 3D graph.</p> <ul> <li>2D-Scatter: k-means viewed as a traditional 2D scatter plot with each cluster as a data point</li> <li>3D-Scatter: k-means viewed as a traditional 3D scatter plot with each cluster as a data point</li> <li>Voronoi: This is the default method of visualization which identifies a centroid in each cluster (a black X) and draws a trapezoidal polygon around it. Your documents are plotted as colored dots based on which cluster they belong to. This may be helpful in allowing you to see which points fall into which cluster and how close they are to the centroid.</li> </ul> <p>To generate a plot, use the <code>scatter()</code> or <code>voronoi()</code> methods as shown below:</p>"},{"location":"user_guide/cluster/k-means/#generate-a-2d-scatter-plot","title":"Generate a 2D-Scatter Plot","text":"<pre><code>kmeans.scatter(dim=2, title=\"KMeans Clustering 2D Plot\", show=True)\n</code></pre> Sample KMeans Clustering 2D Plot <p>The <code>show=True</code> automatically displays the plot. In some cases, you may wish to save it to a variable or file for display later. In that case, set <code>show=False</code>.</p>"},{"location":"user_guide/cluster/k-means/#generate-a-3d-scatter-plot","title":"Generate a 3D-Scatter Plot","text":"<pre><code>kmeans.scatter(dim=3, title=\"KMeans Clustering 3D Plot\", show=True)\n</code></pre> Sample KMeans Clustering 3D Plot"},{"location":"user_guide/cluster/k-means/#generate-a-voronoi-diagram","title":"Generate a Voronoi Diagram","text":"<pre><code>kmeans.voronoi(title=\"KMeans Clustering Voronoi Diagram\" show=True)\n</code></pre> Sample KMeans Clustering Voronoi diagram <p>When considering visualizations of k-means clusters, we recommend that you think of each of your documents as represented by a single (x, y) point on a two-dimensional coordinate plane. In this view, a cluster is a collection of documents (points) that are close to one another and together form a group. Assigning documents to a specific cluster amounts to determining which cluster \"center\" is closest to your document.</p> <p>All three types of visualizations are generated using the Python Plotly library. If you pan over the graph, you'll notice that a menu appears in the top right corner. This menu provides the following options:</p> <ul> <li>Download Plot as a PNG: This button allows you to download the image as a .png file. See below for methods of downloading in other formats.</li> <li>Zoom: This option allows you to click and drag to zoom in to a specific part of the graph.</li> <li>Pan: This option will change the click and drag function to panning across the graph.</li> <li>Zoom in and Zoom out: These will automatically zoom to the center of the graph.</li> <li>Auto-scale and Reset Axis: These options will zoom all the way out with the axis reset to fit the window</li> <li>Show closest data on hover: If you hover over a data point, this option will show you the value of the data point.</li> <li>Compare data on hover: If you hover over a data point, this option will show you the value of the data point and it's corresponding x-axis value.</li> <li>When you enable the 3D scatter plot, there are other options that function essentially the same as for a 2D graph that will allow you to view the 3D plot at different angles.</li> </ul> <p>You can also save the images programmatically using <code>kmeans.save()</code>. The image type will be detected automatically by the file extension in the <code>path</code> parameter. You can also set <code>html=True</code> to save the image as an HTML file. Here are some examples:</p> <pre><code>kmeans.save(path=\"myimage.png\") # Saves as a .png file\nkmeans.save(path=\"myimage.html\", html=True) # Saves as an HTML file\n</code></pre> <p>Under the hood, <code>save()</code> calls Plotly's <code>write_image()</code> and <code>write_html()</code>, and it will accept any keywords taken by those methods.</p> <p>You can also call <code>to_csv()</code> to export your data to a CSV file of PCA coordinates and cluster labels. <code>Kmeans.to_csv()</code> accepts any parameter taken by the Pandas<code>to_csv()</code> method.</p>"},{"location":"user_guide/corpus/","title":"Corpus","text":""},{"location":"user_guide/corpus/#overview","title":"Overview","text":"<p>The Lexos Corpus module is used to manage, search, and analyze text collections. Whilst you can easily pass documents loaded from files or assigned in memory to any Lexos tool, the Corpus module provides useful ways of managing your documents, especially for larger collections. Think of a corpus as a smart filing cabinet for your texts. Each document in your corpus is wrapped in a <code>Record</code> object \u2014 a container that holds not just the text itself, but also metadata (like author or date) and optional linguistic analysis.</p> <p>The corpus module allows you to serialize and de-serialize your records to disk, generate statistics about your documents, and activate and de-active records for analysis, and filter and search your documents.</p>"},{"location":"user_guide/corpus/#records","title":"Records","text":"<p>Let's start with the foundation: the <code>Record</code>. A Record is simply a document with some metadata attached.</p> <pre><code>from lexos.corpus import Record\n\nrecord = Record(\n    name=\"my_first_doc\",\n    content=\"This is the text of my document.\",\n    meta={\"author\": \"Jane\", \"date\": \"2025-01-15\"}\n)\n</code></pre> <p>Under the hood, a Record can store plain text, a parsed spaCy Doc (for NLP analysis), or both. It also handles serialization \u2014 saving and loading from disk or database.</p> <p>record = corpus.get(name=[\"saying_1\", \"fable_1\"])</p> <p>For more information, see Working with Records.</p>"},{"location":"user_guide/corpus/#creating-a-corpus","title":"Creating a Corpus","text":"<p>A corpus is a collection of records. The simplest way to create a file-based corpus using the <code>Corpus</code> class.</p> <pre><code>from lexos.corpus import Corpus\n\ncorpus = Corpus(corpus_dir=\"my_collection\", name=\"My Texts\")\n\n# Add documents\ncorpus.add(\n    content=\"The quick brown fox jumps over the lazy dog.\",\n    name=\"fable_1\",\n    metadata={\"type\": \"fable\", \"year\": 1900}\n)\n\ncorpus.add(\n    content=\"A journey of a thousand miles begins with a single step.\",\n    name=\"saying_1\",\n    metadata={\"type\": \"saying\", \"author\": \"Lao Tzu\"}\n)\n\nprint(f\"Corpus has {corpus.num_docs} documents\")\n</code></pre> <p>Your documents are now stored as <code>Record</code> objects in the <code>my_collection</code> directory. Each record is saved as a file, and metadata is tracked in a central index. When you create a corpus, a file called <code>corpus_metadata.json</code> is created in the <code>my_collection</code> directory. This file contains metadata about your corpus which can be used as a form of \"ground truth\" for the corpus.</p>"},{"location":"user_guide/corpus/#loading-files-from-disk","title":"Loading Files from Disk","text":"<p>For loading multiple files into your corpus efficiently, use the <code>add_from_files()</code> method. This method provides memory-efficient streaming of files with parallel processing. For 100 files, <code>add_from_files()</code> is typically 1.4x faster than loading files individually. Performance gains increase with larger file counts (1000+ files). It works with plain text files as well as PDFs, DOCX, and ZIP archives.</p> <pre><code>from lexos.corpus import Corpus\n\ncorpus = Corpus(corpus_dir=\"my_collection\", name=\"My Texts\")\n\n# Load all text files from a directory\ncorpus.add_from_files(\n    paths=\"path/to/text/files\",\n    max_workers=4,\n    show_progress=True,\n    name_template=\"doc_{index:03d}\",\n    metadata={\"collection\": \"my_texts\", \"year\": 2025}\n)\n\nprint(f\"Loaded {corpus.num_docs} documents\")\n</code></pre> <p>Key Parameters:</p> <ul> <li>paths (str | Path): Path to directory or file(s) to load</li> <li>max_workers (int): Number of parallel workers (default: 4)</li> <li>worker_strategy (str): Worker optimization strategy - <code>\"auto\"</code>, <code>\"io_bound\"</code>, <code>\"cpu_bound\"</code>, or <code>\"balanced\"</code> (default: <code>\"auto\"</code>)</li> <li>batch_size (int): Files per batch for progress tracking (default: 50)</li> <li>show_progress (bool): Display progress bars (default: True)</li> <li>name_template (str): Template for naming records with placeholders:</li> <li><code>{filename}</code>: Full filename with extension</li> <li><code>{stem}</code>: Filename without extension</li> <li><code>{index}</code>: Sequential number (use <code>:03d</code> for zero-padding)</li> <li>is_active (bool): Set records as active/inactive (default: True)</li> <li>model (str): Language model for NLP parsing (optional)</li> <li>extensions (list[str]): File extensions to include (default: common text formats)</li> <li>metadata (dict): Metadata to add to all loaded records</li> <li>id_type (str): ID generation method - <code>\"uuid4\"</code> or <code>\"integer\"</code> (default: <code>\"uuid4\"</code>)</li> </ul> <p>Example with Custom Naming:</p> <pre><code># Load files with custom naming based on filename\ncorpus.add_from_files(\n    paths=\"documents\",\n    name_template=\"{stem}\",  # Use filename without extension\n    metadata={\"source\": \"project_alpha\"}\n)\n\n# Or use sequential numbering\ncorpus.add_from_files(\n    paths=\"documents\",\n    name_template=\"doc_{index:03d}\",  # doc_001, doc_002, etc.\n)\n</code></pre>"},{"location":"user_guide/corpus/#accessing-your-documents","title":"Accessing Your Documents","text":"<p>You can access corpus records with the <code>get()</code> method by passing the record's <code>id</code> or <code>name</code>.</p> <pre><code># Get a specific record by id\nrecord = corpus.get(id=\"1\")\n\n# Get multiple records by name\nrecords = corpus.get(name=[\"fable_1\", \"saying_1\"])  # returns a list\n\n# Get all records\nall_records = list(corpus.records.values())\n</code></pre>"},{"location":"user_guide/corpus/#displaying-a-corpus","title":"Displaying a Corpus","text":"<p>You can generate a pandas DataFrame of your corpus for easy inspection as a table.</p> <pre><code>df = corpus.to_df()\nprint(df)\n</code></pre>"},{"location":"user_guide/corpus/#filtering-and-querying","title":"Filtering and Querying","text":"<p>You can filter records by metadata:</p> <pre><code># Find all fables\nfables = corpus.filter_records(type=\"fable\")\n</code></pre>"},{"location":"user_guide/corpus/#the-sqlite-backend","title":"The SQLite Backend","text":"<p>For larger projects\u2014hundreds or thousands of documents\u2014use the SQLite backend. This stores everything in a fast, searchable database:</p> <pre><code>from lexos.corpus.sqlite import SQLiteCorpus\n\ncorpus = SQLiteCorpus(\n    corpus_dir=\"my_collection\",\n    sqlite_path=\"corpus.db\",\n    name=\"My Texts\",\n    use_sqlite=True\n)\n\n# Add documents just like before\ncorpus.add(\n    content=\"The digital revolution transformed society.\",\n    name=\"article_1\",\n    metadata={\"source\": \"tech_journal\", \"topic\": \"technology\"}\n)\n\n# Sync file-based records to the database\ncorpus.sync()\n</code></pre> <p>Note</p> <p>The <code>sqlite_path</code> is the path to the location where you want to save the database. SQLiteCorpus maintains both file storage (for backup) and database storage (for fast queries). Syncing keeps them in sync. If you wish to use only a database without separate file storage, set <code>sqlite_only=True</code>.</p>"},{"location":"user_guide/corpus/#full-text-search","title":"Full-Text Search","text":"<p>The SQLite backend supports powerful full-text search:</p> <pre><code># Search for keywords\nresults = corpus.search(\"digital OR technology\")\n\nfor record in results:\n    print(f\"{record.name}: {record.preview}\")\n</code></pre>"},{"location":"user_guide/corpus/#advanced-filtering","title":"Advanced Filtering","text":"<p>Filter by multiple criteria:</p> <pre><code># Find long documents that have been parsed with NLP\nlong_parsed = corpus.filter_records(\n    is_parsed=True,\n    min_tokens=500\n)\n\n# Find inactive records (useful for archiving)\narchived = corpus.filter_records(is_active=False)\n</code></pre>"},{"location":"user_guide/corpus/#corpus-statistics","title":"Corpus Statistics","text":"<p>You can generate statistics to help you understand your data by calling the <code>get_stats()</code> method.</p> <pre><code>stats = corpus.get_stats()\n</code></pre> <p>This gives you insights like:</p> <ul> <li>Total records and active records \u2013 How many documents do you have?</li> <li>Token counts \u2013 Total words and average words per document</li> <li>Vocabulary size \u2013 How many unique words?</li> <li>Lexical diversity \u2013 Measures like type-token ratio (TTR) that show vocabulary richness</li> <li>Hapax legomena \u2013 Words appearing only once (often interesting!)</li> <li>Document length distribution \u2013 Min, max, average document length</li> </ul> <p>Example output:</p> <pre><code>{\n    'total_records': 100,\n    'active_records': 98,\n    'total_tokens': 15234,\n    'avg_tokens_per_record': 152,\n    'vocabulary_size': 3421,\n    'hapax_ratio': 0.45,\n    'lexical_diversity': {\n        'ttr': 0.22,\n        'rttr': 4.5\n    },\n    'doc_length': {\n        'min': 10,\n        'max': 2341,\n    all_records = list(corpus.records.values())\n        'std': 205\n    }\n}\n</code></pre> <p>Under the hood, the <code>get_stats()</code> method creates an instance of the <code>CorpusStats</code> class. For further details, see the separate sections below.</p>"},{"location":"user_guide/corpus/#generating-corpus-statistics-with-corpusstats","title":"Generating Corpus Statistics with <code>CorpusStats</code>","text":"<p>The <code>corpus_stats</code> module provides comprehensive statistical analysis of your corpus. It calculates everything from basic counts to advanced lexical diversity measures, helping you understand the structure and quality of your text collection.</p> <p>You do not technically need to create a corpus to use it. All you neeed is a list of tuples where each tuple contains:</p> <ul> <li>ID: A unique identifier for the document</li> <li>Label: A human-readable name</li> <li>Tokens: A list of tokens (words or other units)</li> </ul> <p>You can then pass this tuple to the <code>CorpusStats</code> class, as shown below:</p> <pre><code>from lexos.corpus.corpus_stats import CorpusStats\n\n# Prepare your documents\ndocs = [\n    (\"doc1\", \"First Document\", [\"the\", \"quick\", \"brown\", \"fox\"]),\n    (\"doc2\", \"Second Document\", [\"the\", \"lazy\", \"dog\", \"sleeps\"]),\n    (\"doc3\", \"Third Document\", [\"a\", \"quick\", \"dog\", \"runs\"])\n]\n\n# Create the statistics object\nstats = CorpusStats(docs=docs)\n</code></pre> <p>Note</p> <p>CorpusStats creates a Lexos Document-Term Matrix (DTM) internally. You can configure the DTM vectorizer with parameters like <code>min_df</code>, <code>max_df</code>, and <code>max_n_terms</code> to filter rare or common terms.</p> <pre><code># Filter terms appearing in at least 2 documents\nstats = CorpusStats(docs=docs, min_df=2)\n</code></pre> <p>See the Lexos <code>dtm</code> module documentation for further details.</p>"},{"location":"user_guide/corpus/#document-statistics-table","title":"Document Statistics Table","text":"<p>The <code>doc_stats_df</code> property gives you a pandas DataFrame with statistics for each document:</p> <pre><code>df = stats.doc_stats_df\nprint(df)\n</code></pre> <p>Example output:</p> Documents hapax_legomena total_tokens total_terms vocabulary_density hapax_dislegomena First Document 3 4 4 100.00 0 Second Document 3 4 4 100.00 0 Third Document 2 4 4 100.00 0 <ul> <li>hapax_legomena: Words appearing exactly once in this document</li> <li>total_tokens: Total word count</li> <li>total_terms: Number of unique words</li> <li>vocabulary_density: (unique words / total words) \u00d7 100</li> <li>hapax_dislegomena: Words appearing exactly twice</li> </ul>"},{"location":"user_guide/corpus/#basic-corpus-statistics","title":"Basic Corpus Statistics","text":"<p>Get the mean and standard deviation across all documents:</p> <pre><code>mean_tokens = stats.mean\nstd_dev = stats.standard_deviation\n\nprint(f\"Average document length: {mean_tokens:.1f} tokens\")\nprint(f\"Standard deviation: {std_dev:.1f}\")\n</code></pre>"},{"location":"user_guide/corpus/#finding-outliers","title":"Finding Outliers","text":"<p>Identify documents that are unusually short or long using two methods:</p> <p>1. Interquartile Range (IQR) Method:</p> <pre><code># Get IQR-based outliers\noutliers = stats.get_iqr_outliers()\nfor doc_id, doc_name in outliers:\n    print(f\"Outlier: {doc_name} (ID: {doc_id})\")\n\n# Get the IQR bounds\nlower, upper = stats.iqr_bounds\nprint(f\"Normal range: {lower:.0f} to {upper:.0f} tokens\")\n</code></pre> <p>2. Standard Deviation Method:</p> <pre><code># Documents more than 2 standard deviations from mean\noutliers = stats.get_std_outliers()\n</code></pre>"},{"location":"user_guide/corpus/#distribution-analysis","title":"Distribution Analysis","text":"<p>Understand how document lengths are distributed:</p> <pre><code>dist_stats = stats.distribution_stats\n\nprint(f\"Skewness: {dist_stats['skewness']:.2f}\")\nprint(f\"Kurtosis: {dist_stats['kurtosis']:.2f}\")\nprint(f\"Is normally distributed: {dist_stats['is_normal']}\")\nprint(f\"Coefficient of variation: {dist_stats['coefficient_of_variation']:.2f}\")\n</code></pre> <p>Interpreting the metrics:</p> <ul> <li>Skewness: Negative means more short docs, positive means more long docs, ~0 is balanced</li> <li>Kurtosis: Positive means more extreme values, negative means flatter distribution</li> <li>Is normal: Whether the distribution resembles a bell curve (Shapiro-Wilk test, p &gt; 0.05)</li> <li>Coefficient of variation: Relative variability (std/mean); lower means more consistent document lengths</li> </ul>"},{"location":"user_guide/corpus/#percentile-analysis","title":"Percentile Analysis","text":"<p>Get detailed percentile breakdowns:</p> <pre><code>percentiles = stats.percentiles\n\nprint(f\"5th percentile: {percentiles['percentile_5']:.0f} tokens\")\nprint(f\"Median: {percentiles['percentile_50']:.0f} tokens\")\nprint(f\"95th percentile: {percentiles['percentile_95']:.0f} tokens\")\nprint(f\"Range: {percentiles['range']:.0f} tokens\")\n</code></pre> <p>This tells you, for example, that 95% of documents are shorter than the 95th percentile value.</p>"},{"location":"user_guide/corpus/#text-diversity-statistics","title":"Text Diversity Statistics","text":"<p>Analyze vocabulary richness across your corpus:</p> <pre><code>diversity = stats.text_diversity_stats\n\nprint(f\"Mean Type-Token Ratio: {diversity['mean_ttr']:.3f}\")\nprint(f\"Corpus-level TTR: {diversity['corpus_ttr']:.3f}\")\nprint(f\"Hapax legomena ratio: {diversity['corpus_hapax_ratio']:.3f}\")\nprint(f\"Total hapax words: {diversity['total_hapax']}\")\n</code></pre> <p>Key metrics:</p> <ul> <li>Type-Token Ratio (TTR): Unique words / total words (higher = more diverse vocabulary)</li> <li>Hapax ratio: Proportion of words appearing once (indicates vocabulary growth potential)</li> <li>Dislegomena ratio: Proportion of words appearing twice</li> </ul>"},{"location":"user_guide/corpus/#advanced-lexical-diversity","title":"Advanced Lexical Diversity","text":"<p>Beyond simple TTR, get more sophisticated diversity measures:</p> <pre><code>adv_diversity = stats.advanced_lexical_diversity\n\nprint(f\"Mean CTTR: {adv_diversity['mean_cttr']:.3f}\")\nprint(f\"Mean RTTR: {adv_diversity['mean_rttr']:.3f}\")\nprint(f\"Diversity coefficient of variation: {adv_diversity['diversity_coefficient_variation']:.3f}\")\n</code></pre> <ul> <li>CTTR (Corrected TTR): Types / \u221a(2 \u00d7 tokens) - less sensitive to text length</li> <li>RTTR (Root TTR): Types / \u221atokens - another length-adjusted measure</li> <li>Log TTR: log(types) / log(tokens) - logarithmic scaling</li> <li>Diversity CV: How much diversity varies across documents</li> </ul> <p>Note</p> <p>These measures correct for the fact that longer texts naturally have lower TTR, making them more suitable for comparing texts of different lengths.</p>"},{"location":"user_guide/corpus/#zipfs-law-analysis","title":"Zipf's Law Analysis","text":"<p>Test whether your corpus follows Zipf's law (a power law distribution where word frequency is inversely proportional to rank):</p> <pre><code>zipf = stats.zipf_analysis\n\nprint(f\"Zipf slope: {zipf['zipf_slope']:.3f}\")  # Should be close to -1 for ideal Zipf\nprint(f\"R-squared: {zipf['r_squared']:.3f}\")\nprint(f\"Follows Zipf's law: {zipf['follows_zipf']}\")\nprint(f\"Goodness of fit: {zipf['zipf_goodness_of_fit']}\")\n</code></pre> <ul> <li>A slope near -1 and high R\u00b2 indicates the corpus follows natural language patterns</li> <li>\"Excellent\" or \"good\" fit suggests the corpus is representative of natural text</li> <li>Poor fit might indicate specialized vocabulary, small corpus, or unusual text types</li> </ul>"},{"location":"user_guide/corpus/#corpus-quality-metrics","title":"Corpus Quality Metrics","text":"<p>Assess whether your corpus is balanced and sufficiently sampled:</p> <pre><code>quality = stats.corpus_quality_metrics\n\n# Document length balance\nbalance = quality['document_length_balance']\nprint(f\"Length balance: {balance['classification']}\")\nprint(f\"Coefficient of variation: {balance['coefficient_variation']:.3f}\")\n\n# Vocabulary coverage\ncoverage = quality['corpus_coverage']\nprint(f\"Total unique terms: {coverage['unique_terms']}\")\nprint(f\"Coverage ratio: {coverage['coverage_ratio']:.4f}\")\n\n# Sampling adequacy\nrichness = quality['vocabulary_richness']\nprint(f\"Sampling adequacy: {richness['sampling_adequacy']}\")\nprint(f\"Hapax ratio: {richness['hapax_ratio']:.3f}\")\n</code></pre> <p>Interpreting quality metrics:</p> <p>Balance classifications:</p> <ul> <li><code>very_balanced</code>: CV &lt; 0.2 - documents are very similar in length</li> <li><code>balanced</code>: CV &lt; 0.4 - reasonable consistency</li> <li><code>moderately_unbalanced</code>: CV &lt; 0.6 - some variation</li> <li><code>highly_unbalanced</code>: CV \u2265 0.6 - wide variation in document lengths</li> </ul> <p>Sampling adequacy:</p> <ul> <li><code>excellent</code>: &lt; 10% hapax words - vocabulary is well covered</li> <li><code>good</code>: 10-30% hapax - adequate sampling</li> <li><code>adequate</code>: 30-50% hapax - borderline; more data may help</li> <li><code>insufficient</code>: \u2265 50% hapax - too many rare words; need more documents</li> </ul>"},{"location":"user_guide/corpus/#comparing-groups","title":"Comparing Groups","text":"<p>Compare statistics between two subsets of your corpus:</p> <pre><code># Compare documents from two different authors\nresults = stats.compare_groups(\n    group1_labels=[\"doc1\", \"doc3\"],\n    group2_labels=[\"doc2\"],\n    metric=\"total_tokens\",\n    test_type=\"mann_whitney\"  # or \"t_test\" or \"welch_t\"\n)\n\nprint(f\"Group 1 mean: {results['group1_mean']:.1f}\")\nprint(f\"Group 2 mean: {results['group2_mean']:.1f}\")\nprint(f\"P-value: {results['p_value']:.4f}\")\nprint(f\"Effect size: {results['effect_size']:.3f} ({results['effect_size_interpretation']})\")\nprint(f\"Significant: {results['is_significant']}\")\n</code></pre> <p>Test types:</p> <ul> <li>mann_whitney: Non-parametric test (doesn't assume normal distribution)</li> <li>t_test: Assumes equal variances and normal distribution</li> <li>welch_t: Doesn't assume equal variances</li> </ul> <p>Note</p> <p>Effect sizes tell you the magnitude of difference. A \"large\" effect size means the difference is substantial, even if groups overlap; \"small\" means the difference is detectable but modest.</p>"},{"location":"user_guide/corpus/#bootstrap-confidence-intervals","title":"Bootstrap Confidence Intervals","text":"<p>Estimate confidence intervals for any metric using resampling:</p> <pre><code>ci = stats.bootstrap_confidence_interval(\n    metric=\"total_tokens\",\n    confidence_level=0.95,\n    n_bootstrap=1000\n)\n\nprint(f\"Original mean: {ci['original_mean']:.1f}\")\nprint(f\"95% CI: [{ci['ci_lower']:.1f}, {ci['ci_upper']:.1f}]\")\nprint(f\"Margin of error: \u00b1{ci['margin_of_error']:.1f}\")\n</code></pre> <p>This tells you the range where the true mean likely falls, accounting for sampling variability.</p>"},{"location":"user_guide/corpus/#visualization","title":"Visualization","text":"<p>Create boxplots to visualize document length distribution:</p> <p>Seaborn (static):</p> <pre><code>stats.plot(column=\"total_tokens\", type=\"seaborn_boxplot\", title=\"Document Lengths\")\n</code></pre> <p>Plotly (interactive):</p> <pre><code>stats.plot(column=\"total_tokens\", type=\"plotly_boxplot\", title=\"Document Lengths\")\n</code></pre> <p>You can plot any column from <code>doc_stats_df</code>, such as:</p> <ul> <li><code>total_tokens</code></li> <li><code>total_terms</code></li> <li><code>vocabulary_density</code></li> <li><code>hapax_legomena</code></li> </ul>"},{"location":"user_guide/corpus/working_with_records/","title":"Working with Individual Records","text":""},{"location":"user_guide/corpus/working_with_records/#overview","title":"Overview","text":"<p>The <code>record</code> module provides the <code>Record</code> class, which is the building block for every document in your corpus. Each <code>Record</code> wraps your text (or a parsed spaCy Doc) and metadata and offers a suite of methods for serialization, statistics, and manipulation.</p>"},{"location":"user_guide/corpus/working_with_records/#creating-a-record","title":"Creating a Record","text":"<p>You can create a record from plain text or a spaCy Doc, and you can attach any metadata you like:</p> <pre><code>from lexos.corpus import Record\n\nrecord = Record(\n    name=\"example_doc\",\n    content=\"The quick brown fox jumps over the lazy dog.\",\n    meta={\"author\": \"Jane\", \"year\": 2025}\n)\n</code></pre> <p>You can pass a <code>data_source</code> pointing to the original location (filepath or url) where the data came from. The <code>meta</code> parameter is for passing a dictionary of more arbitrary metadata items.</p> <p>If you already have a spaCy <code>Doc</code> object, just pass it to the record using the <code>content</code> parameter \u2014 the record's <code>is_parsed</code> property will be set automatically. Using the <code>model</code> parameter to identify the spaCy language model used is useful for serialization and deserialization.</p> <pre><code>record = Record(\n    name=\"parsed_doc\",\n    content=doc,\n    meta={\"author\": \"Jane\"},\n    model=\"en_core_web_sm\"\n)\n</code></pre> <p>If your <code>Doc</code> object has spaCy custom extensions, provide a list of those extensions with the <code>extensions</code> parameter.</p> <p>When you create a <code>Record</code> object, it is automatically assigned an <code>id</code>, and its <code>is_active</code> attribute is set to <code>True</code>. The <code>id</code> is a UUID value (by default) and you can override it by passing your own <code>id</code> value. When <code>is_active=True</code>, the record is assumed to be available for analysis. Some applications may want to keep records in a corpus, enabling or disabling them as needed for specific purposes. The <code>is_active</code> attribute does not affect the record on its own.</p> <p>The following properties can be inspected from a <code>Record</code> object:</p> <ul> <li>is_parsed: Returns <code>True</code> if the content is a spaCy Doc.</li> <li>preview: Returns a short preview of the record\u2019s text.</li> <li>terms: Returns a Counter of terms (requires parsed content).</li> <li>tokens: Returns a list of tokens (requires parsed content).</li> <li>text: Returns the text of the record (works for both string and parsed Doc).</li> </ul> <p>For instance, for the \"parsed_doc\" record above, calling <code>record.is_parsed</code> will return <code>True</code>.</p> <p>The <code>set()</code> method is a helper method to set attributes of the <code>Record</code>. For instance, <code>record.set(is_active=False)</code> will set its <code>is_active</code> attribute to <code>False</code>.</p>"},{"location":"user_guide/corpus/working_with_records/#serialization-disk-operations","title":"Serialization &amp; Disk Operations","text":"<p>The following methods are used to serialize and deserialize <code>Record</code> objects to bytes or to disk:</p> <ul> <li>to_bytes(extensions=[], include_hash=True): Serialize the record to bytes (for saving or transmitting). If you have a spaCy <code>Doc</code> with custom extensions, you need to add the names of the extensions in a list. By default, a data integrity hash is included in the serialization, but you can remove it with <code>include_hash=False</code>.</li> <li>from_bytes(bytestring, model=None, model_cache=None, verify_hash=True): Load a record from bytes, optionally verifying data integrity. If you have a spaCy <code>Doc</code>, you should specify the language model used. If you have multiple docs</li> <li>to_disk(path, extensions=None): Save the record to disk.</li> <li>from_disk(path, model=None, model_cache=None): Load the record from disk.</li> </ul> <p>When deserializing a <code>Record</code> that contains a parsed spaCy <code>Doc</code>, you need the correct spaCy model to reconstruct the <code>Doc</code> object. <code>LexosModelCache</code> provides a way to retrieve and reuse these models. If you pass a <code>model_cache</code> to methods like <code>from_bytes</code> or <code>from_disk</code>, Lexos will use it to get the correct spaCy vocabulary for reconstructing the <code>Doc</code>. This is especially useful when working with many records or large corpora parsed with different models by making the process faster and more memory-efficient. Here is an example:</p> <pre><code>from lexos.corpus.utils import LexosModelCache\n\n# Create a cache and load models as needed\nmodel_cache = LexosModelCache()\ndoc_model = model_cache.get_model(\"en_core_web_sm\")  # Loads and caches the model\n\n# When loading a Record from bytes or disk:\nrecord.from_bytes(bytestring, model=\"en_core_web_sm\", model_cache=model_cache)\n</code></pre> <p>If you use spaCy Docs, any custom token attributes and extensions are preserved during serialization</p>"},{"location":"user_guide/corpus/working_with_records/#statistical-data","title":"Statistical Data","text":"<p><code>Record</code> objects provide methods for exposing a number of useful types of statistical information about the documents they contain:</p> <ul> <li>num_terms(): Returns the number of unique terms (words) in the record (requires parsed content).</li> <li>num_tokens(): Returns the total number of tokens (words) in the record (requires parsed content).</li> <li>vocab_density(): Returns the ratio of unique terms to total tokens (requires parsed content).</li> <li>most_common_terms(n=None): Returns the n most common terms as a list of (term, count) tuples.</li> <li>least_common_terms(n=None): Returns the n least common terms as a list of (term, count) tuples.</li> </ul>"},{"location":"user_guide/corpus/working_with_records/#example-workflow","title":"Example Workflow","text":"<pre><code>import spacy\nfrom lexos.corpus import Record\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Lexos makes text analysis easy.\")\n\nrecord = Record(name=\"demo\", content=doc, meta={\"topic\": \"NLP\"})\n\nprint(\"Preview:\", record.preview)\nprint(\"Tokens:\", record.tokens)\nprint(\"Most common terms:\", record.most_common_terms(3))\n\n# Save to disk\nrecord.to_disk(\"demo_record.lexos\")\n\n# Load from disk\nloaded = Record()\nloaded.from_disk(\"demo_record.lexos\", model=\"en_core_web_sm\")\nprint(\"Loaded text:\", loaded.text)\n</code></pre>"},{"location":"user_guide/corpus/working_with_records/#technical-notes","title":"Technical Notes","text":"<ul> <li>UUIDs: Each record gets a unique ID by default. You can use integers if you prefer.</li> <li>spaCy Integration: If you use spaCy Docs, custom token attributes and extensions are preserved during serialization.</li> <li>Data Integrity: When serializing, a hash is included to help detect corruption or incomplete writes.</li> <li>Metadata: Any metadata you attach is sanitized for JSON compatibility.</li> </ul>"},{"location":"user_guide/scrubber/internal_components/","title":"Internal Components","text":"<p>Using Scrubber components is made easier by an understanding of the underlying architecture and resources used by the individual functions. This page provides some additional for information to help you understand Scrubber's inner workings.</p> <p>Developer's Note</p> <p>Many of the functions and resources in Scrubber are built on top of the preprocessing functions in the Python Textacy library, although sometimes with modifications. Textacy is installed with Lexos, so it can also be called directly where that is useful.</p>"},{"location":"user_guide/scrubber/internal_components/#the-registry","title":"The Registry","text":"<p>Lexos ships with a registry of default functions which can be imported and referenced by name for easy access. Custom functions can be added to the registry by registering them as in the following example:</p> <pre><code>#  Define the custom function\ndef title_case(text: str) -&gt; str:\n    \"\"\"Our custom function to convert text to title case.\"\"\"\n    return text.title()\n\n# Register the custom function\nscrubber_components.register(\"title_case\", func=title_case)\n</code></pre> <p>The registry is implemented using the Python catalogue library. See the <code>catalogue</code> documentation for further details about how to work with the registry.</p>"},{"location":"user_guide/scrubber/internal_components/#resources","title":"Resources","text":"<p>Many Scrubber functions use internal resources such as mappings of Unicode code points, regex values, or even helper functions. Inspecting the code in the API Documentation can help you understand whether what Scrubber is doing under the hood is your desired behaviour or whether you need to modify Scrubber with a custom function.</p>"},{"location":"user_guide/scrubber/normalize/","title":"Normalize","text":""},{"location":"user_guide/scrubber/normalize/#overview","title":"Overview","text":"<p>The <code>normalize</code> component of Scrubber is a submodule containing functions for manipulating text into a standardized form. This includes functions for converting text to lower case, removing whitespace, and more. This page offers an overview of the component functions. For a fuller description of their usage, see the API documentation.</p> <p>A classic problem in text processing is that the same text string can be represented in many different ways. For example, the single quotation mark, or apostrophe, may be straight (') or curly (\u2018 or \u2019), depending on the surrounding characters. A text in the wild may contain either one or a mixture of the two, which may not be desirable for a target publication. However, it may also have significant downstream consequences such as breaking XML code or influencing token counts used in computational analysis. The <code>normalize</code> component contains a functions that allow you to convert variants like these into a single desired (normalized) form.</p> <p>Here is a list of the functions available in the <code>normalize</code> component:</p> <ul> <li><code>bullet_points</code>: Converts all types of bullet points to a hyphen (-).</li> <li><code>hyphenated_words</code>: Joins words split by a hyphen across line breaks.</li> <li><code>lower_case</code>: Converts all text to lower case.</li> <li><code>quotation_marks</code>: Converts all types of single and double quotation marks to straight quotes.</li> <li><code>repeating_chars</code>: Truncates repeating characters to a specified maximum length.</li> <li><code>unicode</code>: Converts Unicode characters in <code>text</code> into canonical forms (for instance, by changing \"\u00e9\" to plain ASCII \"e\").</li> <li><code>whitespace</code>: Strips leading and trailing whitespace from text and replaces multiple spaces with a single space.</li> </ul>"},{"location":"user_guide/scrubber/normalize/#example-using-direct-import","title":"Example Using Direct Import","text":"<pre><code>from lexos.scrubber.normalize import lower_case, whitespace\n\ntext = \"  This is a Sample Text with   Irregular Whitespace.  \"\n\nnormalized_text = whitespace(lower_case(text))\nprint(normalized_text)\n# \"this is a sample text with irregular whitespace.\n</code></pre>"},{"location":"user_guide/scrubber/normalize/#example-using-in-a-pipeline","title":"Example Using in a Pipeline","text":"<p>The example below shows how the components can used in a Scrubber pipeline.</p> <pre><code>from lexos.scrubber import Scrubber\nfrom lexos.scrubber.normalize import lower_case, whitespace\n\nscrubber = Scrubber()\nscrubber.add_pipe([lower_case, whitespace])\n\ntext = \"  This is a Sample Text with   Irregular Whitespace.  \"\n\nnormalized_text = scrubber.scrub(text)\nprint(normalized_text)\n# this is a sample text with irregular whitespace.\n</code></pre>"},{"location":"user_guide/scrubber/remove/","title":"Remove","text":""},{"location":"user_guide/scrubber/remove/#overview","title":"Overview","text":"<p>The <code>remove</code> component of Scrubber is a submodule containing functions for components removing strings and string patterns from text. This includes functions for removing diacritics, punctuation marks, digits, HTML and XML markup tags, and other items that are typically removed from texts prior to analysis. This page offers an overview of the component functions. For a fuller description of their usage, see the API documentation.</p> <p>Note</p> <p>The functions in the <code>remove</code> component can often be used to clean up texts prior to some other use, frequently because the content to be removed is undersirable for a downstream task. Note that this approach can be heavy handed because it depends on identifying string patterns and replacing them without regard to context. An alternative approach is to apply a language model to tokenize the text and then filter the tokens. This approach is discussed further in Tokenizing Texts.</p> <p>Here is a list of the functions available in the <code>remove</code> component:</p> <ul> <li><code>accents</code>: Removes accented characters or replaces them with ASCII equivalents (similar to <code>normalize.unicode</code>).</li> <li><code>brackets</code>: Removes square, curly, or round brackets (parentheses) from the text.</li> <li><code>digits</code>: Removes digits from the text.</li> <li><code>new_lines</code>: Removes all line-breaking spaces from the text.</li> <li><code>pattern</code>: Truncates repeating characters to a specified maximum length.</li> <li><code>project_gutenberg_headers</code>: Attempts to remove Project Gutenberg headers and footers from the text.</li> <li><code>punctuation</code>: Removes all punctuation marks from textRemoves tabs from the text.</li> <li><code>tabs</code>: Removes all punctuation marks from text (if you want to replace tabs with a single space, use <code>normalize.whitespace</code> instead).</li> <li><code>tags</code>: Removes all HTML and XML markup tags from text. See the <code>tags</code> component for a more nuanced approach to removing tags.</li> </ul> <p>Many of these functions have parameters that allow you to specify how the function should behave. For example, the <code>accents</code> function has a <code>replace</code> parameter that allows you to specify whether accented characters should be replaced with their ASCII equivalents or simply removed. The <code>punctuation</code> function has a <code>keep</code> parameter that allows you to specify which punctuation marks should be retained in the text. See the API documentation for more details on the parameters available for each function.</p>"},{"location":"user_guide/scrubber/remove/#example-using-direct-import","title":"Example Using Direct Import","text":"<pre><code>from lexos.scrubber.remove import punctuation, digits\n\ntext = \"Hello, world! 1234\"\n\nscrubbed_text = punctuation(digits(text))\nprint(scrubbed_text)\n# \"Hello world \"\n</code></pre>"},{"location":"user_guide/scrubber/remove/#example-using-in-a-pipeline","title":"Example Using in a Pipeline","text":"<p>The example below shows how the components can used in a Scrubber pipeline.</p> <pre><code>from lexos.scrubber import Scrubber\nfrom lexos.scrubber.remove import punctuation, digits\n\nscrubber = Scrubber()\nscrubber.add_pipe([punctuation, digits])\n\ntext = \"Hello, world! 1234\"\n\nscrubbed_text = scrubber.scrub(text)\nprint(scrubbed_text)\n# \"Hello world \"\n</code></pre>"},{"location":"user_guide/scrubber/replace/","title":"Replace","text":""},{"location":"user_guide/scrubber/replace/#overview","title":"Overview","text":"<p>The <code>replace</code> component of Scrubber is a submodule containing functions for components replacing strings and string patterns from text. This includes functions for replacing diacritics, punctuation marks, digits, HTML and XML markup tags, and other items that are typically removed from texts prior to analysis. This page offers an overview of the component functions. For a fuller description of their usage, see the API documentation.</p> <p>Here is a list of the functions available in the <code>replace</code> component:</p> <ul> <li><code>currency_symbols</code>: Replaces all currency symbols with a common symbol.</li> <li><code>digits</code>: Replaces all digits with a common symbol.</li> <li><code>emails</code>: Replaces all emails with a common symbol.</li> <li><code>emojis</code>: Replaces all emojis with a common symbol.</li> <li><code>hashtags</code>: Replaces all hashtags with a common symbol.</li> <li><code>pattern</code>: Replaces all examples of a string pattern or patterns  with a common symbol.</li> <li><code>phone_numbers</code>: Replaces all phone numbers with a common symbol.</li> <li><code>punctuation</code>: Replaces punctuation marks with whitespace.</li> <li><code>special_characters</code>: Replaces special characters following a dictionary of rules with an option to handle HTML and XML entities.</li> <li><code>urls</code>: Replaces all urls with a common symbol.</li> <li><code>user_handles</code>: Replaces all Twitter-style user handles with a common symbol.</li> </ul> <p>Each function has parameters that allow you to specify how the function should behave. For example, the <code>pattern</code> function allows you to specify a regular expression pattern to match and replace in the text. The <code>punctuation</code> function allows you to specify which punctuation marks should be replaced with whitespace. The functions that replace patterns with a common symbol allow you to specify the common symbol, such as \"EMAIL\". See the API documentation for more details on the parameters available for each function.</p> <p>Note</p> <p>The <code>replace</code> component can often perform the same or similar actions on text, so it is up to the user to decide which is appropriate. For instance, both components have <code>digits</code>, <code>pattern</code>, and <code>punctuation</code> functions. In order to disambiguate the function calls, it can be helpful to import these functions with aliases:</p> <pre><code>from lexos.scrubber.remove import punctuation as remove_punctuation\nfrom lexos.scrubber.replace import punctuation as replace_punctuation\n</code></pre>"},{"location":"user_guide/scrubber/replace/#example-using-direct-import","title":"Example Using Direct Import","text":"<pre><code>from lexos.scrubber.replace import punctuation, digits\n\ntext = \"Hello, world! 12\"\nscrubbed_text = punctuation(digits(text))\n\nprint(scrubbed_text)\n# Hello  world  _DIGIT__DIGIT_\n</code></pre>"},{"location":"user_guide/scrubber/replace/#example-using-in-a-pipeline","title":"Example Using in a Pipeline","text":"<p>The example below shows how the components can used in a Scrubber pipeline.</p> <pre><code>from lexos.scrubber import Scrubber\nfrom lexos.scrubber.replace import punctuation, re-digits\n\nscrubber = Scrubber()\nscrubber.add_pipe([punctuation, digits])\n\ntext = \"Hello, world! 12\"\nscrubbed_text = scrubber.scrub(text)\n\nprint(scrubbed_text)\n# Hello  world  _DIGIT__DIGIT_\n</code></pre>"},{"location":"user_guide/scrubber/tags/","title":"Tags","text":""},{"location":"user_guide/scrubber/tags/#overview","title":"Overview","text":"<p>The <code>tag</code> component of Scrubber is a submodule containing functions for transforming HTML and XML content (if you simply wish to remove all tags, using <code>remove.tags</code> is more efficient). These markup languages wrap content into elements indicated by angular brackets. Each element can further contain attributes, the value of which is contained within quotation marks. For example, the markup <code>&lt;span id=\"1\"&gt;John Smith&lt;/span&gt;</code> indicates that the content \"John Smith\" is a <code>span</code> element with an <code>id</code> attribute, the value of which is \"1\". The functions in Scrubber's tag component allow you to manipulate the elements, attributes, and content by choosing a selector (usually the name of an element) and providing filters if only certain occurrences of this selector should be changed. This allows for some fairly nuanced transformations to be applied to the markup. For instance, in many cases Scrubber could be used to transform XML content into HTML markup for presentation on the web. This page offers an overview of the component functions. For a fuller description of their usage, see the API documentation.</p> <p>Here is a list of the functions available in the <code>replace</code> component:</p> <ul> <li><code>remove_attribute</code>: Removes all instances of an attribute in a specified element.</li> <li><code>remove_comments</code>: Removes all comments from the text.</li> <li><code>remove_doctype</code>: Removes the HTML document type declaration or XML declaration from the text.</li> <li><code>remove_element</code>: Removes a specified HTML or XML element from the text, including the tag's content.</li> <li><code>remove_tag</code>: Remove a tag from the text but retain the tag's content.</li> <li><code>replace_attribute</code>: Replaces the value of an attribute with another value.</li> <li><code>replace_tag</code>: Replaces a tag with anther tag.</li> </ul> <p>Each function has parameters that allow you to specify how the function should behave. For example, you can specify which elements to target, whether to include or exclude certain attributes, and how to handle the content of the elements. The functions can be used individually or in combination to achieve complex transformations on your text. See the API documentation for more details on the parameters available for each function.</p> <p>In order to keep the functions simple, each function call can perform only a single transformation. To perform multiple transformations, it is necessary to call functions multiple times (or construct a pipeline that calls the functions iteratively). You may have to get to know your text's markup fairly well or inspect the results after each transformation in order to achieve the desired effects. When calling <code>tags</code> functions multiple times, the order in which the functions are called can make a considerable difference in the output.</p> <p>Note</p> <p>Under the hood, Lexos uses the Python BeautifulSoup library to parse and transform the string before returning the output as a new string.</p>"},{"location":"user_guide/scrubber/tags/#removing-elements-and-replacing-tag-names","title":"Removing Elements and Replacing Tag Names","text":"<pre><code>from lexos.scrubber.tags import remove_element, replace_tag\n\ntext = \"&lt;p&gt;Hello World&lt;/p&gt;&lt;span&gt;Hello again!&lt;/span&gt;\"\n\n# Remove &lt;p&gt;\nscrubbed_text = remove_element(text, selector=\"p\")\n\n# Replace &lt;span&gt; with &lt;p&gt;\nscrubbed_text = replace_tag(scrubbed_text, selector=\"span\", replacement=\"p\")\n\nprint(scrubbed_text)\n# &lt;p&gt;Hello again!&lt;/p&gt;\n</code></pre> <p>Note</p> <p>Setting <code>selector=None</code> will target all elements in the document.</p> <p>The <code>remove_element</code> function removes the element and all of its content. The <code>replace_tags</code> function replaces an element's tag with a different tag. Both functions have a <code>mode</code> parameter set to \"html\" by default. To parse the text as \"xml\", set <code>mode=\"xml\"</code>. You can also set the <code>matcher_type</code>. By default, it is \"exact\", which means that the selector \"p\" will match all <code>p</code> elements. However, setting <code>matcher_type=regex</code> uses regular expressions to perform the matching. In this example, using the selector \"p\" will also target <code>span</code> elements (or any other elements with tag names containing the letter p).</p> <p>Elements can also be targeted based on their attributes by setting the <code>attribute</code> and <code>attribute_value</code> parameters:</p> <pre><code>text = \"&lt;p id=\"1\"&gt;Hello World&lt;/p&gt;&lt;p id=\"2\"&gt;Hello again!&lt;/p&gt;\"\n\nscrubbed_text = remove_element(text, selector=\"p\", attribute=\"id\", attribute_value=\"1)\n\nprint(scrubbed_text)\n# &lt;p id=\"2\"&gt;Hello again!&lt;/p&gt;\n</code></pre> <p>The <code>replace_tag</code> function also has a boolean <code>preserve_attributes</code> parameter, which allows you to choose whether or not to keep an elements attributes when changing the tag name.</p>"},{"location":"user_guide/scrubber/tags/#removing-and-replacing-attributes","title":"Removing and Replacing Attributes","text":"<p>To remove or change the attribute values of an element, you can use the <code>remove_attribute</code> and <code>replace_attribute</code> functions.</p> <pre><code>from lexos.scrubber.tags import remove_attribute, replace_attribute\n\ntext = '&lt;div class=\"main\"&gt;Text&lt;/div&gt;'\n\nscrubbed_text = remove_attribute(text, selector=\"div\", attribute=\"class\")\n\nprint(scrubbed_text)\n# &lt;div&gt;Text&lt;/div&gt;\n</code></pre> <p>Additionally, you can filter which attributes to target with the <code>attribute_filter</code> and <code>attribute_value</code> parameters. In the example below, we remove the <code>class</code> attribute only when it has the value \"remove\":</p> <pre><code>text = '&lt;div class=\"keep\"&gt;Text&lt;/div&gt;&lt;div class=\"remove\"&gt;Text&lt;/div&gt;'\n\nscrubbed_text = remove_attribute(text, selector=\"div\", attribute_filter=\"class\", attribute_value=\"remove\")\n\nprint(scrubbed_text)\n# &lt;div class=\"keep\"&gt;Text&lt;/div&gt;&lt;div&gt;Text&lt;/div&gt;\n</code></pre> <p>The <code>replace_attribute</code> function is somewhat more complex. It has the following parameters:</p> <ul> <li><code>old_attribute</code>: The name of the attribute to replace</li> <li><code>new_attribute</code>: The name of the new attribute (or same name if only changing the value)</li> <li><code>attribute_value</code>: Only replace attributes with this specific value</li> <li><code>replace_value</code>: The new value to use (keeps original value if <code>None</code>)</li> <li><code>attribute_filter</code>: Optional attribute name to filter elements</li> <li><code>filter_value</code>: Optional value for the attribute filter</li> </ul> <p>Here are some examples of how to use these parameters:</p> <pre><code># Replace class attribute with data-type, keeping the value\ntext = '&lt;div class=\"main\"&gt;Text&lt;/div&gt;'\nreplace_attribute(text, \"div\", \"class\", \"data-type\")\n# &lt;div data-type=\"main\"&gt;Text&lt;/div&gt;\n\n# Replace class=\"info\" with class=\"highlight\"\ntext = '&lt;p class=\"info\"&gt;Text&lt;/p&gt;&lt;p class=\"data\"&gt;More&lt;/p&gt;'\nreplace_attribute(text, \"p\", \"class\", \"class\", filter_value=\"info\", replace_value=\"highlight\")\n# &lt;p class=\"highlight\"&gt;Text&lt;/p&gt;&lt;p class=\"data\"&gt;More&lt;/p&gt;\n\n# Only replace attributes on elements with a specific attribute value\ntext = '&lt;div class=\"main\" id=\"content\"&gt;Text&lt;/div&gt;&lt;div class=\"sidebar\"&gt;Side&lt;/div&gt;'\nreplace_attribute(text, \"div\", \"class\", \"role\", attribute_filter=\"id\", filter_value=\"content\")\n# &lt;div role=\"main\" id=\"content\"&gt;Text&lt;/div&gt;&lt;div class=\"sidebar\"&gt;Side&lt;/div&gt;\n</code></pre> <p>As with <code>remove_element</code> and <code>replace_tag</code>, both functions allow you can use the <code>mode</code> and <code>matcher_type</code> parameters to perform more complex parsing of your document</p>"},{"location":"user_guide/scrubber/tags/#using-tags-functions-in-a-scrubber-pipeline","title":"Using Tags Functions in a Scrubber Pipeline","text":"<p>The example below shows how the components can used in a Scrubber pipeline. The partial functions are created for clarity so that you can see what each function does. In practice, you could also use the functions directly in the pipeline without creating partials.</p> <pre><code>from functools import partial\nfrom lexos.scrubber import Scrubber\nfrom lexos.scrubber.replace import remove_element, replace_tag\n\n# Create partial functions for specific transformations\nremove_p_by_class_value = partial(remove_element, attribute=\"class\", value=\"remove\")\nreplace_span = partial(\"span\", \"p\")\n\n# Create Scrubber pipeline and add the partial functions\nscrubber = Scrubber()\nscrubber.add_pipe([remove_p_by_class_value, replace_span])\n\n# Pass the text through the pipeline and print the scrubbed result\ntext = \"&lt;p&gt;Hello world&lt;/p&gt;&lt;p class=\"remove\"&gt;Hello World&lt;/p&gt;&lt;p class=\"change\"&gt;Hello, world!&lt;/p&gt;\"\nscrubbed_text = scrubber.scrub(text)\nprint(scrubbed_text)\n# &lt;span&gt;Hello world&lt;/span&gt;&lt;span class=\"change\"&gt;Hello, world!&lt;/span&gt;\n</code></pre>"},{"location":"user_guide/topic_modeling/dfr_browser2/","title":"DFR Browser 2","text":""},{"location":"user_guide/topic_modeling/dfr_browser2/#overview","title":"Overview","text":"<p>DFR Browser 2 is web-based topic modelling browser that provides interactive visualizations and analysis tools for exploring topic models generated by MALLET. DFR Browser 2 is based on Andrew Goldstone's original dfr-browser. It reproduces all the major functionality of the original, but with an entirely new architecture and additional features. For full documentation, see the DFR Browser 2 repository.</p> <p>The <code>dfr_browser2</code> module provides a small helper class <code>Browser</code> that automates the steps required to prepare and open a a DFR Browser 2 distribution. This helper is designed to be used programmatically from Python and can be used to produce a small static browser bundle. It performs the following functions:</p> <ul> <li>Validates that all required MALLET output files exist</li> <li>Auto-generates <code>topic_coords.csv</code> from <code>topic-state.gz</code> if the coordinates file is missing</li> <li>Copies a template DFR Browser 2 folder into a working browser folder</li> <li>Copies all MALLET output and metadata files into the browser's <code>data/</code>, along with optional files (diagnostics.xml) if present</li> <li>Copies an optional file containing the documents used to generate the topic model</li> <li>Manages configuration settings for the browser</li> <li>Checks port availability before starting the server</li> <li>Starts a simple HTTP server to serve the browser and opens it in a web browser</li> </ul> <p>If you have not yet generated a MALLET topic model, it is recommended that you start with the MALLET tutorial.</p>"},{"location":"user_guide/topic_modeling/dfr_browser2/#browser-class","title":"Browser Class","text":"<p>To create a simple instance of a browser, you need to supply a path to the directory where you mallet files are located and a path to a directory where you want to save the browser:</p> <pre><code>from lexos.topic_modeling.dfr_browser2 import Browser\n\nb = Browser(\n    mallet_files_path=\"/path/to/mallet_files\",\n    browser_path=\"/tmp/dfr_browser_output\",  # optional (temporary folder created if omitted)\n)\n\nb.serve()\n</code></pre> <p>Calling the <code>serve()</code> method will start a localhost server and open the browser in your system's default web browser. The server runs in a subprocess, so you can continue working in your Python session while the browser is running. You can also start the server from the commmand line by running the <code>server.py</code> script in your DFR Browser 2 folder.</p> <p></p> <p>Note</p> <p>DFR Browser 2 must be served from a server; otherwise it will not have full functionality. The <code>serve()</code> method checks if the specified port is available before starting the server. If the port is already in use, you'll receive a helpful error message with instructions on how to find and terminate the conflicting process, or you can specify a different port using the <code>port</code> parameter. The default port is 8000, which may conflict with Jupyter notebooks or other local servers.</p> <p>Tip</p> <p>If you're running this in a Jupyter notebook, the browser will open in a new tab. You can stop the server by calling <code>b.stop_server()</code> or by restarting the kernel.</p> <p>The example below demonstrates a fuller set of options:</p> <pre><code>b = Browser(\n    mallet_files_path=\"/path/to/mallet_files\",\n    browser_path=\"/tmp/dfr_browser_output\",  # optional (temporary folder created if omitted)\n    data_path=\"/path/to/docs.txt\",  # optional\n    template_path=\"/path/to/dfr_browser2/template\",\n    filename_map={\"doc-topics.txt\": \"doc-topic.txt\"},\n    config={\"application\": {\"name\": \"My Browser\"}},\n    port=5000\n)\n</code></pre> <p>The <code>data_path</code> is the path to your original training data file. It should be a tab-separated file with 2 columns per line (ID, content) or 3 columns per line (ID, label, content) \u2014 lines are validated during initialization and copied to <code>data/docs.txt</code>.</p> <p>Because DFR Browser 2 is a separate package, Lexos may not have the latest distribution. If this is the case, you can download the latest version from the DFR Browser 2 repository and set the <code>template_path</code> to this version.</p> <p>The <code>Browser</code> class assumes canonical names like <code>doc-topic.txt</code> for your input files. You can rename your files to the canonical names or map the current names of your files to the canonical ones with <code>filename_map</code>. See below for further information on using this parameter.</p> <p>Every instance of DFR Browser 2 has a <code>config.json</code> file containing the browser configuration. You can pass configuration values as a dictionary to this file using the <code>Browser</code> class with the <code>config</code> parameter. For discussion of the configuration options, see the DFR Browser 2 repository.</p> <p>As mentioned earlier, you can also set the port on which the browser is served with the <code>port</code> parameter.</p>"},{"location":"user_guide/topic_modeling/dfr_browser2/#typical-usage-flow","title":"Typical Usage Flow","text":"<ul> <li>Construct the <code>Browser</code> object \u2014 initialization checks required files, auto-generates missing files (like <code>topic_coords.csv</code>), copies the template, copies files into <code>data/</code>, validates TSVs, and writes <code>config.json</code>.</li> <li>Start serving using <code>b.serve()</code> \u2014 this checks port availability, runs a subprocess HTTP server, and opens the browser in your default web browser (or prints instructions if a web browser cannot be opened).</li> <li>Continue working or stop the server using <code>b.stop_server()</code> when done.</li> </ul>"},{"location":"user_guide/topic_modeling/dfr_browser2/#required-mallet-files-and-alternate-filenames","title":"Required MALLET Files and Alternate Filenames","text":"<p>The Browser checks a small set of required files in <code>mallet_files_path</code> and supports some alternate names. Canonical names are:</p> <ul> <li><code>metadata.csv</code> (required)</li> <li><code>topic-keys.txt</code> (required)</li> <li><code>doc-topic.txt</code> (or <code>doc-topics.txt</code>) (required) \u2014 synonyms for the documents-to-topic mapping</li> <li><code>topic-state.gz</code> (or <code>state.gz</code>) (required) \u2014 supports <code>state.gz</code> as an alternate name</li> <li><code>topic_coords.csv</code> (or <code>topic-coords.csv</code>) (optional) \u2014 coordinates for topic display; auto-generated from topic-state.gz if missing</li> <li><code>diagnostics.xml</code> (optional) \u2014 MALLET diagnostics file, copied if present</li> </ul> <p>Behavior notes:</p> <ul> <li>Auto-generation: If <code>topic_coords.csv</code> is missing, the Browser will automatically generate it from <code>topic-state.gz</code> using Jensen-Shannon divergence and multidimensional scaling (MDS). This ensures the browser has topic coordinates for visualization even if they weren't pre-computed.</li> <li>If you supply a <code>filename_map</code>, keys are considered the original filenames found in <code>mallet_files_path</code> and values are destination names to use inside <code>data/</code>.</li> <li><code>filename_map</code> is flexible \u2014 if you reverse the mapping (i.e., use the destination as key and the source as value), Browser attempts to detect and correct the mapping.</li> <li>For canonicalization, where both <code>doc-topic.txt</code> and <code>doc-topics.txt</code> are present, Browser will deduplicate and select the canonical destination <code>doc-topic.txt</code>.</li> <li>Optional files like <code>diagnostics.xml</code> are automatically copied if they exist in the MALLET output directory.</li> </ul>"},{"location":"user_guide/topic_modeling/dfr_browser2/#example-filename_map","title":"Example: <code>filename_map</code>","text":"<ul> <li>Standard mapping when you want <code>doc-topics.txt</code> to be called <code>doc-topic.txt</code> in <code>data/</code>:</li> </ul> <pre><code>Browser(..., filename_map={\"doc-topics.txt\": \"doc-topic.txt\"})\n</code></pre> <ul> <li>You can also specify a partial map to only rename some files:</li> </ul> <pre><code>Browser(..., filename_map={\"topic-keys.txt\": \"custom-topic-keys.txt\"})\n</code></pre> <ul> <li>If your mapping was reversed (the key was the destination), <code>Browser</code> will attempt to handle that by checking if the key or value exists in the <code>mallet_files_path</code> and swapping behavior as necessary.</li> </ul>"},{"location":"user_guide/topic_modeling/dfr_browser2/#data_path-tsv-validation","title":"<code>data_path</code> TSV Validation","text":"<p>If <code>data_path</code> is provided, <code>Browser</code> will ensure it is a non-directory file and validate each row. Each non-empty row must contain exactly 2 or 3 tab-separated columns. If any row fails this validation a <code>ValueError</code> is raised.</p> <p>Browser will copy the data file to <code>data/docs.txt</code> and update the config accordingly.</p>"},{"location":"user_guide/topic_modeling/dfr_browser2/#merging-configjson","title":"Merging <code>config.json</code>","text":"<p><code>Browser</code> reads the template <code>config.json</code> (if it exists) and then merges the <code>config</code> provided by the caller. The merge rules are:</p> <ol> <li>User-supplied <code>config</code> keys are preserved and take precedence \u2014 Browser will not overwrite keys in <code>self.config</code></li> <li>Browser sets <code>*_file</code> values (e.g. <code>doc_topic_file</code>, <code>topic_keys_file</code>, <code>topic_state_file</code>, etc.) to the files that were copied into <code>data/</code> \u2014 but only when the user did not specify those keys in <code>self.config</code></li> <li>Template values are used only where not overridden by the user or the copying process</li> </ol> <p>The merged <code>config.json</code> is written to <code>browser_path/config.json</code> and the merged config is saved back to <code>b.config</code> so it is available in-memory. Assigning <code>b.config</code> later (either by <code>b.config = {...}</code> or using <code>config_browser()</code>) also writes the new config to disk.</p> <p>To explicitly update the config programmatically after the <code>Browser</code> instance has been initialized, you can call:</p> <pre><code>b.config_browser({\"application\": {\"name\": \"New Title\"}})\n</code></pre> <p>This sets <code>b.config</code>, which triggers a write to the <code>json.config</code> file.</p>"},{"location":"user_guide/topic_modeling/dfr_browser2/#version-behavior","title":"Version Behavior","text":"<p>Calling <code>Browser.version</code> will return the DFR Browser 2 version number. The <code>Browser</code> has a class-level <code>BROWSER_VERSION</code> used as a default. If the template or the user-specified <code>config</code> has an <code>application.version</code>, that value is returned by the property <code>Browser.version</code>. Otherwise, <code>BROWSER_VERSION</code> is returned.</p> <p>The property is intentionally defensive \u2014 if the <code>config.json</code> file is malformed, it will simply fall back to the default <code>BROWSER_VERSION</code>.</p>"},{"location":"user_guide/topic_modeling/dfr_browser2/#troubleshooting-tips","title":"Troubleshooting &amp; Tips","text":"<ul> <li>If you get <code>FileNotFoundError: Missing required mallet files</code>, check <code>mallet_files_path</code> for expected files and confirm names or provide a <code>filename_map</code> to rename or canonicalize source files.</li> <li>If you get <code>RuntimeError: Server failed to start</code> with a port conflict message, either use a different port by passing <code>port=XXXX</code> to <code>serve()</code>, or follow the instructions in the error message to terminate the conflicting process.</li> <li>If <code>topic_coords.csv</code> is missing, it will be automatically generated from <code>topic-state.gz</code>. This may take a few seconds for large topic models.</li> <li>If a <code>config.json</code> key is missing or incorrect, verify whether you set the config key in <code>config</code> (user-specified configs override template values) or whether Browser wrote a copied/data path into the merged <code>config.json</code>.</li> <li>Use <code>Browser.BROWSER_VERSION</code> or <code>b.version</code> to inspect or assert the configured version.</li> <li>To stop a running server, call <code>b.stop_server()</code> or interrupt the Python process.</li> </ul>"},{"location":"user_guide/topic_modeling/mallet/","title":"Topic Modeling with MALLET","text":""},{"location":"user_guide/topic_modeling/mallet/#overview","title":"Overview","text":"<p>Topic modeling is a statistical method for discovering abstract themes or \"topics\" within a collection of documents. MALLET is a mature tool for topic modeling used widely in the Humanities. It is a Java package that needs to be installed separately from Lexos. The Lexos <code>mallet</code> module provides a straightforward wrapper for running MALLET, managing outputs, and creating visualizations of your topic model.</p> <p>For more on topic modeling and installing MALLET, see Shawn Graham, Scott Weingart, and Ian Milligan's tutorial Getting Started with Topic Modeling and MALLET.</p> <p>The Lexos <code>mallet</code> module integrates Maria Antoniak's Litte Mallet Wrapper functions with a slightly simplified API that manages file paths. For more advanced methods of exploring a topic model, see the Lexos integration of DFR Browser 2.</p>"},{"location":"user_guide/topic_modeling/mallet/#import-the-mallet-class-from-the-mallet-module","title":"Import the <code>Mallet</code> class from the <code>mallet</code> Module","text":"<p>First, import the <code>Mallet</code> class and helper functions from the Lexos <code>mallet</code> module.</p> <pre><code>from lexos.topic_modeling.mallet import Mallet\nfrom IPython.display import display\n</code></pre>"},{"location":"user_guide/topic_modeling/mallet/#check-mallet-installation","title":"Check Mallet Installation","text":"<p>Verify that MALLET is installed and accessible by calling the MALLET binary.</p> <p>If you are using a Jupyter notebook, you can configure the path to your MALLET binary and run</p> <pre><code>mallet_binary_path = \"/path/to/mallet\"\n!$mallet_binary_path\n</code></pre> <p>If you receive a list of commands, MALLET is installed and the path is correct.</p> <p>Or on the command line, type the path to your MALLET binary and hit Enter. You should see the same list of commands.</p>"},{"location":"user_guide/topic_modeling/mallet/#load-your-data","title":"Load Your Data","text":"<p>Your data must take the form of a list of strings or spaCy <code>Doc</code> objects. Example data:</p> <pre><code>sample_docs = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Never jump over the lazy dog quickly.\",\n    \"A fast brown fox leaps over sleeping dogs.\",\n    \"Dogs are great pets for families.\",\n    \"Foxes are wild animals found in forests.\"\n]\n</code></pre> <p>You will see the command sent to MALLET in the output.</p>"},{"location":"user_guide/topic_modeling/mallet/#reading-directories","title":"Reading Directories","text":"<p>The <code>read_dirs()</code> function will read all text files in a directory (or list of directories) into a list of strings. Each file will be treated as a separate document. Note that the order of the documents in the list is important, as it will be used for document indexes in the topic model.</p> <p>In the examples below, we will use the <code>sample_data</code> folder distributed with MALLET, which contains text from 12 Wikipedia articles.</p> <pre><code>from lexos.topic_modeling.mallet import read_dirs\ncorpus_dir = \"sample_data\"\ndocs = read_dirs(corpus_dir)\nfor doc in docs[:5]:\n    print(f\"- {doc[0:100]}...\")\n</code></pre>"},{"location":"user_guide/topic_modeling/mallet/#reading-from-a-file","title":"Reading from a File","text":"<p>You can also load documents from a single file using the <code>read_file()</code> function. Here each line in the file will be treated as a separate document. Again, the order of documents will be used for document indexes in the topic model.</p> <p>Note: Technically, MALLET expects the tab-delimited file where the first column is an index, the second is an optional label, and the third is the document text itself. The <code>read_file()</code> function accepts files in this format, as well as files containing only texts.</p> <pre><code>from lexos.topic_modeling.mallet import read_file\ncorpus_file = \"sample_data.txt\"\ndocs = read_file(corpus_file)\nfor doc in docs[:5]:\n    print(f\"- {doc[0:100]}...\")\n</code></pre>"},{"location":"user_guide/topic_modeling/mallet/#train-a-topic-model","title":"Train a Topic Model","text":"<p>You are now ready to train a topic model. This involves three steps:</p> <ol> <li>Create a topic model involves three steps illustrated below. First, create a <code>Mallet</code> instance, providing it with the path to a directory to save the model and, if required (see above), the path to your Mallet binary file.</li> <li>Next, import your data with the <code>import_data()</code> method.</li> <li>Finally, train the topic model.</li> </ol> <p>The model <code>metadata</code> property returns a dictionary containing paths to output files and model statistics.</p> <p>Start by creating a <code>Mallet</code> instance:</p> <pre><code>model_dir = \"mallet_model\"\npath_to_mallet = \"/path/to/your/mallet/binary\"\nmallet_model = Mallet(model_dir=model_dir, path_to_mallet=path_to_mallet)\n</code></pre> <p>Now import your training data with <code>import_data</code>.</p> <pre><code>mallet_model.import_data(training_data=docs)\n</code></pre> <p>You can configure the following parameters:</p> <ul> <li><code>keep_sequence</code>: Keep the token sequence. Default is <code>True</code>.</li> <li><code>preserve_case</code>: Preserve case. Default is <code>True</code>.</li> <li><code>remove_stopwords</code>: Remove stopwords. Default is <code>True</code>.</li> <li><code>training_ids</code>: A list of integers indicating the IDs of the documents you want to import. If <code>None</code>, all documents in your training data will be imported.</li> </ul> <p>When you train a model, MALLET creates a \"pipe\" file that records the sequence of data processing steps (such as tokenization, stopword removal, case normalization, etc.). This file is saved in your model directory with the extension <code>.mallet</code>.</p> <p>If you later want to import new documents for inference or further modeling, you can use <code>use_pipe_from</code> with the path to the <code>.mallet</code> file. This guarantees consistency between training and inference, so your new documents are handled identically to your training set.</p> <p>Finally, train your model:</p> <pre><code>mallet_model.train(num_topics=20, num_iterations=100, verbose=True)\n</code></pre> <p>If <code>verbose</code> is set to <code>True</code>, you will see something like:</p> <pre><code>\u2714 Training topics...\nMallet LDA: 20 topics, 5 topic bits, 11111 topic mask\nData loaded.\nmax tokens: 147\ntotal tokens: 1245\n&lt;10&gt; LL/token: -9.11285\n&lt;20&gt; LL/token: -8.87062\n&lt;30&gt; LL/token: -8.71832\n&lt;40&gt; LL/token: -8.61369\n\n0       0.25    average Test energy Hill energies ended innings batsman day predictions accurate holds neutron\nproperties predict Dulong\u2013Petit classical mechanics statistical handicapper\n1       0.25    back Gilbert year actors drama Greek productions England movie accomplishments romance Actress\ngraduating retiring apex habitat introduction continent mainland commonly\n\n...\n\nTotal time: 0 seconds\n\n\u2714 Complete\n</code></pre> <p>This is a display of the state of your model after each iteration. It can be quite long, so it may be truncated in a notebook environment. If you are not interested in observing this output, you can set <code>verbose=False</code>.</p> <p>The <code>train()</code> method takes the following parameters:</p> <ul> <li><code>num_topics</code>: The number of topics to train. The default is 20.</li> <li><code>num_iterations</code>: The number of iterations to train for. The default is 100.</li> <li><code>optimize_interval</code>: The interval at which to optimize the model. The default is 10.</li> <li><code>verbose</code>: Whether to print the MALLET output showing the state at different iterations. The default is <code>True</code>.</li> <li><code>path_to_inferencer</code>: Optional output filename for saving a trained inferencer object (see below). If not provided, defaults to <code>model_dir/inferencer.mallet</code>.</li> </ul>"},{"location":"user_guide/topic_modeling/mallet/#after-training","title":"After Training","text":"<p>After training, you can inspect various model properties:</p> <ul> <li><code>mallet_model.metadata</code>: Returns a dictionary of information about the model, especially the paths and commands that were used to generate the model.</li> <li><code>mallet_model.topic_keys</code>: A list of lists where each sublist is the topic keys for a given topic.</li> <li><code>mallet_model.distributions</code>: A list of lists where each inner list is the topic distribution for a single document: how much each topic contributes to the document (sums to 1).</li> <li><code>mallet_model.num_docs</code>: The number of documents used to generate the model.</li> <li><code>mallet_model.vocab_size</code>: The number of unique terms used by the trained model.</li> <li><code>mallet_model.mean_num_tokens</code>: The mean number of tokens per document.</li> </ul> <p>These properties allow you to inspect the model, analyze results, and use outputs for further processing.</p>"},{"location":"user_guide/topic_modeling/mallet/#display-topics-and-top-words","title":"Display Topics and Top Words","text":"<p>Once you have created your model, you can display the discovered topics and their top words using <code>get_keys()</code>. This method takes the following parameters:</p> <ul> <li><code>num_topics</code>: The number of topics to get keys for. If <code>None</code>, get keys for all topics.</li> <li><code>topics</code>: A list of topic indices to get keys for. If <code>None</code>, get keys for all topics.</li> <li><code>num_keys</code>: The number of key terms to output for each topic.</li> <li><code>as_df</code>: Whether to return the result as a pandas DataFrame instead of a string. The default is <code>True</code>.</li> </ul> <pre><code>mallet_model.get_keys(as_df=True)\n</code></pre> Topic Label Keywords 0 0 0.25 Test cricket Hill South top-grossing year runs team batsman played 1 1 0.25 film Gilbert equilibrium addition considered original News movie Alvida Kabhi 2 2 0.25 London time Needham series critical men Grant's Grant values forms 3 3 0.25 died return run Paris Davis lobby traveled governor brigade Commonwealth's 4 4 0.25 Edward survived Richard invaded Dil born collisional originated exosphere\u2014corona aerodynamic 5 5 0.25 back general Thespis Greek Sullivan practice allegiance swore captured Bragg's 6 6 0.25 Zinta role Indian acting Kehna Award Filmfare earned films performances 7 7 0.25 Thylacine Tiger extinct pouch found related Tasmania marsupial Thylacinus stability 8 8 0.25 Tasmanian modern reported Devil species century teenage cricketers helping discord 9 9 0.25 Norway king including journalist spent husband details Saga figure subsequently 10 10 0.25 Hindi Naa actress innings drama Punjab Kings Wadia Ness boyfriend 11 11 0.25 Yard National wilderness standards Parks Service movement invasion interrupted Governor 12 12 0.25 rings system ring dust moons narrow Uranian particles discovered Uranus 13 13 0.25 Echo Sunderland World paper Thomas Storey East newspaper boycott Triangular 14 14 0.25 Gunnhild life Orkney Erik mother outbreak Telugu specific formula episodes 15 15 0.25 Union Gen battle Confederates War line position Maj launched Beauregard 16 16 0.25 death States United numerous Tennessee American national neutrality park late 17 17 0.25 years Australian career record including England ended scored involved worked 18 18 0.25 Hawes Confederate Kentucky Army Battle ceremony Commonwealth Virginia Whig fighting 19 19 0.25 equipartition theorem average energy kinetic system effects stars law classical"},{"location":"user_guide/topic_modeling/mallet/#display-the-top-documents-in-each-topic","title":"Display the Top Documents in Each Topic","text":"<p>You can display the discovered topics and their top words using <code>get_top_docs()</code>. This method takes the following parameters:</p> <ul> <li><code>topic</code>: The topic number to display.</li> <li><code>n</code>: The number of top documents to return.</li> <li><code>metadata</code>: A Dataframe with the metadata in the same order as the training data. This can include information such as document labels.</li> <li><code>as_str</code>: Whether to return the result as a string instead of a pandas DataFrame. The default is <code>False</code>.</li> </ul> <pre><code>mallet_model.get_top_docs(topic=0, n=10)\n</code></pre> Doc ID Distribution Document 8 0.435096 Clem Hill (1877\u20131945) was an Australian cricketer who played 49 Test matches as a specialist batsman between 1896 and 1912. He captained the Australian team in ten Tests, winning five and losing five. A prolific run scorer, Hill scored 3,412 runs in Test cricket\u2014a world record at the time of his retirement\u2014at an average of 39.21 per innings, including seven centuries. In 1902, Hill was the first batsman to make 1,000 Test runs in a calendar year, a feat that would not be repeated for 45 years. His innings of 365 scored against New South Wales for South Australia in 1900\u201301 was a Sheffield Shield record for 27 years. His Test cricket career ended in controversy after he was involved in a brawl with cricket administrator and fellow Test selector Peter McAlister in 1912. He was one of the \"Big Six\", a group of leading Australian cricketers who boycotted the 1912 Triangular Tournament in England when the players were stripped of the right to appoint the tour manager. The boycott effectively ended his Test career. After retiring from cricket, Hill worked in the horse racing industry as a stipendiary steward and later as a handicapper for races including the Caulfield Cup. 10 0.12037 Preity Zinta (born 1975) is an Indian film actress. She has appeared in Hindi films of Bollywood, as well as Telugu and English-language movies. After graduating with a degree in criminal psychology, Zinta made her acting debut in Dil Se in 1998 followed by a role in Soldier the same year. These performances earned her a Filmfare Best Female Debut Award, and she was later recognised for her role as a teenage single mother in Kya Kehna (2000). She subsequently played a variety of character types, and in doing so has been credited with changing the image of a Hindi film heroine. Zinta received her first Filmfare Best Actress Award in 2003 for her performance in the drama Kal Ho Naa Ho. She went on to play the lead female role in two consecutive annual top-grossing films in India: the science fiction film Koi... Mil Gaya, her biggest commercial success, and the star-crossed romance Veer-Zaara, which earned her critical acclaim. She was later noted for her portrayal of independent, modern Indian women in Salaam Namaste and Kabhi Alvida Naa Kehna, top-grossing productions in overseas markets. These accomplishments have established her as a leading actress of Hindi cinema. In addition to movie acting, Zinta has written a series of columns for BBC News Online South Asia, is a regular stage performer, and along with boyfriend Ness Wadia she is a co-owner of the Indian Premier League cricket team Kings XI Punjab. 3 0.00431034 Elizabeth Needham (died 3 May 1731), also known as Mother Needham, was an English procuress and brothel-keeper of 18th-century London, who has been identified as the bawd greeting Moll Hackabout in the first plate of William Hogarth\\'s series of satirical etchings, A Harlot\\'s Progress. Although Needham was notorious in London at the time, little is recorded of her life, and no genuine portraits of her survive. Her house was the most exclusive in London and her customers came from the highest strata of fashionable society, but she eventually crossed the moral reformers of the day and died as a result of the severe treatment she received after being sentenced to stand in the pillory. 5 0.00294118 Gunnhild konungam\u00f3\u00f0ir (mother of kings) or Gunnhild Gormsd\u00f3ttir[1] (c. 910  \u2013  c. 980) was the wife of Erik Bloodaxe (king of Norway 930\u201334, \"king\" of Orkney c. 937\u201354, and king of J\u00f3rv\u00edk 948\u201349 and 952\u201354). Gunnhild is a prominent figure in many Norse sagas, including Fagrskinna, Egil\\'s Saga, Njal\\'s Saga, and Heimskringla. Many of the details of her life are disputed, including her parentage. Gunnhild lived during a time of great change in Norway. Her father-in-law Harald Fairhair had recently united much of Norway under his rule. Shortly after his death, Gunnhild and her husband were overthrown and exiled. She spent much of the rest of her life in exile in Orkney, Jorvik and Denmark. A number of her many children with Erik became co-rulers of Norway in the late tenth century. What details of her life are known come largely from Icelandic sources; because the Icelanders were generally hostile to her and her husband, scholars regard some of the episodes reported in them as suspect. 2 0.0025 Thespis is an operatic extravaganza that was the first collaboration between dramatist W. S. Gilbert and composer Arthur Sullivan. It was never published, and most of the music is now lost. However, Gilbert and Sullivan went on to become one of the most famous and successful partnerships in Victorian England, creating a string of comic opera hits, including H.M.S. Pinafore, The Pirates of Penzance and The Mikado, that continue to be popular. Thespis premi\u00e8red in London at the Gaiety Theatre on 26 December 1871. Like many productions at that theatre, it was written in a broad, burlesque style, considerably different from Gilbert and Sullivan\\'s later works. It was a modest success\u2014for a Christmas entertainment of the time\u2014and closed on 8 March 1872, after a run of 63 performances. It was advertised as \"An entirely original Grotesque Opera in Two Acts\". The story follows an acting troupe headed by Thespis, the legendary Greek father of the drama, who temporarily trade places with the gods on Mount Olympus, who have grown elderly and ignored. The actors turn out to be comically inept rulers. Having seen the ensuing mayhem down below, the angry gods return, sending the actors back to Earth as \"eminent tragedians, whom no one ever goes to see.\" 9 0.0025 The equipartition theorem is a formula from statistical mechanics that relates the temperature of a system with its average energies. The original idea of equipartition was that, in thermal equilibrium, energy is shared equally among its various forms; for example, the average kinetic energy in the translational motion of a molecule should equal the average kinetic energy in its rotational motion. Like the virial theorem, the equipartition theorem gives the total average kinetic and potential energies for a system at a given temperature, from which the system\\'s heat capacity can be computed. However, equipartition also gives the average values of individual components of the energy. It can be applied to any classical system in thermal equilibrium, no matter how complicated. The equipartition theorem can be used to derive the classical ideal gas law, and the Dulong\u2013Petit law for the specific heat capacities of solids. It can also be used to predict the properties of stars, even white dwarfs and neutron stars, since it holds even when relativistic effects are considered. Although the equipartition theorem makes very accurate predictions in certain conditions, it becomes inaccurate when quantum effects are significant, namely at low enough temperatures. 0 0.00247525 The Sunderland Echo is an evening provincial newspaper serving the Sunderland, South Tyneside and East Durham areas of North East England. The newspaper was founded by Samuel Storey, Edward Backhouse, Edward Temperley Gourley, Charles Palmer, Richard Ruddock, Thomas Glaholm and Thomas Scott Turnbull in 1873, as the Sunderland Daily Echo and Shipping Gazette. Designed to provide a platform for the Radical views held by Storey and his partners, it was also Sunderland\\'s first local daily paper. The inaugural edition of the Echo was printed in Press Lane, Sunderland on 22 December 1873; 1,000 copies were produced and sold for a halfpenny each. The Echo survived intense competition in its early years, as well as the depression of the 1930s and two World Wars. Sunderland was heavily bombed in the Second World War and, although the Echo building was undamaged, it was forced to print its competitor\\'s paper under wartime rules. It was during this time that the paper\\'s format changed, from a broadsheet to its current tabloid layout, because of national newsprint shortages. 7 0.00227273 The rings of Uranus were discovered on March 10, 1977, by James L. Elliot, Edward W. Dunham, and Douglas J. Mink. Two additional rings were discovered in 1986 by the Voyager 2 spacecraft, and two outer rings were found in 2003\u20132005 by the Hubble Space Telescope. A number of faint dust bands and incomplete arcs may exist between the main rings. The rings are extremely dark\u2014the Bond albedo of the rings\\' particles does not exceed 2%. They are likely composed of water ice with the addition of some dark radiation-processed organics. The majority of Uranus\\'s rings are opaque and only a few kilometres wide. The ring system contains little dust overall; it consists mostly of large bodies 0.2\u201320 m in diameter. The relative lack of dust in the ring system is due to aerodynamic drag from the extended Uranian exosphere\u2014corona. The rings of Uranus are thought to be relatively young, at not more than 600 million years. The mechanism that confines the narrow rings is not well understood. The Uranian ring system probably originated from the collisional fragmentation of a number of moons that once existed around the planet. After colliding, the moons broke up into numerous particles, which survived as narrow and optically dense rings only in strictly confined zones of maximum stability. 11 0.00227273 Richard Hawes (1797\u20131877) was a United States Representative from Kentucky and the second Confederate Governor of Kentucky. Originally a Whig, Hawes became a Democrat following the dissolution of the Whig party in the 1850s. At the outbreak of the American Civil War, Hawes was a supporter of Kentucky\\'s doctrine of armed neutrality. When the Commonwealth\\'s neutrality was breached in September 1861, Hawes fled to Virginia and enlisted as a brigade commissary under Confederate general Humphrey Marshall. He was elected Confederate governor of the Commonwealth following the late George W. Johnson\\'s death at the Battle of Shiloh. Hawes and the Confederate government traveled with Braxton Bragg\\'s Army of Tennessee, and when Bragg invaded Kentucky in October 1862, he captured Frankfort and held an inauguration ceremony for Hawes. The ceremony was interrupted, however, by forces under Union general Don Carlos Buell, and the Confederates were driven from the Commonwealth following the Battle of Perryville. Hawes relocated to Virginia, where he continued to lobby President Jefferson Davis to attempt another invasion of Kentucky. Following the war, he returned to his home in Paris, Kentucky, swore an oath of allegiance to the Union, and was allowed to return to his law practice. 6 0.00215517 Robert Sterling Yard (1861\u20131945) was an American writer, journalist and wilderness activist. Yard graduated from Princeton University and spent the first twenty years of his career as a journalist, editor and publisher. In 1915 he was recruited by his friend Stephen Mather to help publicize the need for an independent national park agency. Their numerous publications were part of a movement that resulted in legislative support for a National Park Service in 1916. Yard served as head of the National Parks Educational Committee for several years after its conception, but tension within the NPS led him to concentrate on non-government initiatives. He became executive secretary of the National Parks Association in 1919. Yard worked to promote the national parks as well as educate Americans about their use. Creating high standards based on aesthetic ideals for park selection, he also opposed commercialism and industrialization of what he called \"America\\'s masterpieces\". These standards caused discord with his peers. After helping to establish a relationship between the NPA and the United States Forest Service, Yard later became involved in the protection of wilderness areas. In 1935 he became one of the eight founding members of The Wilderness Society and acted as its first president from 1937 until his death eight years later. Yard is now considered an important figure in the modern wilderness movement."},{"location":"user_guide/topic_modeling/mallet/#display-the-topic-term-probabilities","title":"Display the Topic Term Probabilities","text":"<p>You can display the the term distribution for a given topic with <code>get_topic_term_probabilities()</code>. This method takes the following parameters:</p> <ul> <li><code>topics</code>: The topic number (or list of topic numbers) to display. If None, get the probabilities for all topics.</li> <li><code>n</code>: The number of key terms to display.</li> <li><code>as_df</code>: Whether to return the result as a string instead of a pandas DataFrame. The default is <code>False</code>.</li> </ul> <pre><code>mallet_model.get_topic_term_probabilities(topics=[0, 1], n=10, as_df=True)\n</code></pre> Topic Term Probability 0 0 Test 0.0856613 1 0 Hill 0.0571551 2 0 cricket 0.0571551 3 0 South 0.0429019 4 0 played 0.0286488 5 0 batsman 0.0286488 6 0 team 0.0286488 7 0 runs 0.0286488 8 0 year 0.0286488 9 0 top-grossing 0.0286488 10 1 Gilbert 0.042299 11 1 film 0.042299 12 1 original 0.0282462 13 1 considered 0.0282462 14 1 addition 0.0282462 15 1 equilibrium 0.0282462 16 1 intense 0.0141934 17 1 collaboration 0.0141934 18 1 composer 0.0141934 19 1 published 0.0141934"},{"location":"user_guide/topic_modeling/mallet/#visualizing-topic-probabilities-by-category-with-boxplots","title":"Visualizing Topic Probabilities by Category with Boxplots","text":"<p>The <code>plot_categories_by_topic_boxplots()</code> method lets you visualize how topic probabilities are distributed across different categories (e.g., genres, labels, or other groupings). This is useful for understanding which topics are most associated with which categories in your data.</p> <p>The function takes a number of parameters that allow you to choose your topics and categories, as well as to customize the appearance of the box plots.</p> <ul> <li><code>categories</code>: List of category labels for each document (must match the order of your training data).</li> <li><code>topics</code>: Topic index or list of indices to plot. If <code>None</code>, all topics are plotted.</li> <li><code>output_path</code>: Path to save the figure. If <code>None</code>, the plot is shown but not saved.</li> <li><code>target_labels</code>: List of unique category labels to include. If <code>None</code>, all categories are included.</li> <li><code>num_keys</code>: Number of top keywords to display in the plot title.</li> <li><code>figsize</code>: Size of the figure (tuple, e.g., <code>(8, 6)</code>).</li> <li><code>font_scale</code>: Font scaling for the plot.</li> <li><code>color</code>: Color for the boxplots (matplotlib color name or object).</li> <li><code>show</code>: Whether to display the plot (<code>True</code>) or just return the figure object (<code>False</code>).</li> <li><code>title</code>: Custom title for the plot. If not provided, a default is used.</li> <li><code>overlay</code>: How to display individual data points (<code>'strip'</code>, <code>'swarm'</code>, or <code>'none'</code>).</li> <li><code>overlay_kws</code>: Dictionary of keyword arguments for the overlay plot (e.g., point size, color).</li> </ul> <p>Overlay advice:</p> <ul> <li>Use <code>'strip'</code> (default) for most cases, especially when you have a moderate number of documents per category. It shows individual points with jitter for visibility.</li> <li>Use <code>'swarm'</code> when you have a small number of documents and want to avoid overlapping points; it arranges points to minimize overlap.</li> <li>Use <code>'none'</code> if you only want to see the boxplot summary and not individual data points (useful for large datasets).</li> </ul> <p>The cell below will run a basic example.</p> <pre><code>categories = [\"People\", \"Concepts\", \"People\", \"People\", \"People\", \"Battles\", \"Texts\", \"Texts\", \"Animals\", \"Planets\", \"People\", \"People\"]\nmallet_model.plot_categories_by_topic_boxplots(categories)\n</code></pre> <p></p>"},{"location":"user_guide/topic_modeling/mallet/#visualizing-topic-category-associations-with-a-heatmap","title":"Visualizing Topic-Category Associations with a Heatmap","text":"<p>The <code>plot_categories_by_topics_heatmap()</code> method creates a heatmap showing how topics are distributed across different categories. This is useful for quickly spotting which topics are most associated with which categories, especially when you have many topics or categories. It takes the following parameters:</p> <ul> <li><code>categories</code>: List of category labels for each document (must match the order of your training data).</li> <li><code>output_path</code>: Path to save the figure. If <code>None</code>, the plot is shown but not saved.</li> <li><code>target_labels</code>: List of unique category labels to include. If <code>None</code>, all categories are included.</li> <li><code>num_keys</code>: Number of top keywords to display in the topic labels.</li> <li><code>figsize</code>: Size of the figure (tuple, e.g., <code>(10, 8)</code>).</li> <li><code>font_scale</code>: Font scaling for the plot.</li> <li><code>cmap</code>: Colormap for the heatmap (e.g., <code>\"rocket_r\"</code>, <code>\"viridis\"</code>, or any matplotlib colormap).</li> <li><code>show</code>: Whether to display the plot (<code>True</code>) or just return the figure object (<code>False</code>).</li> <li><code>title</code>: Custom title for the plot. If not provided, a default is used.</li> </ul> <pre><code>categories = [\"People\", \"Concepts\", \"People\", \"People\", \"People\", \"Battles\", \"Texts\", \"Texts\", \"Animals\", \"Planets\", \"People\", \"People\"]\nmallet_model.plot_categories_by_topics_heatmap(\n    categories=categories,\n    num_keys=3,\n    figsize=(8, 6),\n    font_scale=1,\n    cmap=\"viridis\",\n    show=True,\n    title=\"Topic-Category Heatmap\"\n)\n</code></pre> <p></p> <p>Note</p> <p>If you make the figure size too small, some topic labels may be omitted. You can mitigate this by reducting the font scale.</p>"},{"location":"user_guide/topic_modeling/mallet/#visualizing-topics-with-word-clouds","title":"Visualizing Topics with Word Clouds","text":"<p>The <code>topic_clouds()</code> method in the Mallet class generates word clouds for each topic, providing a visual summary of the most important terms per topic. This is useful for quickly understanding the main themes captured by your model.</p> <p>Parameters:</p> <ul> <li><code>topics</code>: (int or list[int], optional) Topics to include. If None, all topics are shown.</li> <li><code>max_terms</code>: (int, optional) Maximum number of keywords per topic cloud (default: 30).</li> <li><code>figsize</code>: (tuple, optional) Size of the overall figure (default: (10, 10)).</li> <li><code>output_path</code>: (str, optional) If provided, saves the figure to this path.</li> <li><code>show</code>: (bool, optional) If True, displays the figure; if False, returns the matplotlib Figure object.</li> <li><code>round_mask</code>: (bool|int|str, optional) Whether to use a circular mask for the clouds (True/False or integer radius).</li> <li><code>title</code>: (str, optional) Title for the figure.</li> <li><code>**kwargs</code>: Additional keyword arguments for customization (see below).</li> </ul> <p>Customization:</p> <ul> <li>Pass <code>opts</code> in <code>**kwargs</code> to control word cloud appearance (e.g., background color, colormap). Accepts arguments for the Python wordcloud package.</li> <li>Pass <code>figure_opts</code> in <code>**kwargs</code> to control figure-level options using <code>matplotlib</code> (e.g., facecolor).</li> </ul> <pre><code>mallet_model.topic_clouds(show=True)\n</code></pre> <p></p>"},{"location":"user_guide/topic_modeling/mallet/#visualizing-topic-trends-over-time","title":"Visualizing Topic Trends Over Time","text":"<p>The <code>plot_topics_over_time()</code> method in the Mallet class allows you to visualize how the probability of a specific topic changes across a sequence of documents, such as those ordered by time or another variable. This is useful for exploring temporal or sequential patterns in your corpus.</p> <p>Parameters:</p> <ul> <li><code>times</code>: (list) Sequence of time points or other ordering variable, one per document.</li> <li><code>topic_index</code>: (int) The topic to plot (0-based index).</li> <li><code>topic_distributions</code>: (list[list[float]], optional) Topic distributions per document. If None, uses the model's distributions.</li> <li><code>topic_keys</code>: (list[list[str]], optional) Topic keys. If None, uses the model's keys.</li> <li><code>output_path</code>: (str, optional) If provided, saves the figure to this path.</li> <li><code>figsize</code>: (tuple, optional) Size of the figure (default: (7, 2.5)).</li> <li><code>font_scale</code>: (float, optional) Seaborn font scale (default: 1.2).</li> <li><code>color</code>: (str, optional) Line color (default: \"cornflowerblue\").</li> <li><code>show</code>: (bool, optional) If True, displays the figure; if False, returns the matplotlib Figure object.</li> <li><code>title</code>: (str, optional) Title for the figure. If not supplied, uses topic keywords.</li> </ul> <p>Note:</p> <ul> <li>The <code>times</code> list must be the same length as the number of documents.</li> </ul> <pre><code>times = [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\nmallet_model.plot_topics_over_time(times=times, topic_index=0, show=True)\n</code></pre> <p></p>"},{"location":"user_guide/topic_modeling/mallet/#advanced-infer-topics-for-new-documents","title":"Advanced: Infer Topics for New Documents","text":"<p>Sometimes you want train a model and then feed it new documents after training. To help you do this, Lexos creates an inferencer file when you initially train the model. It will automatically be saved as <code>inferencer.mallet</code> in your model's folder, but you can use the <code>path_to_inferencer</code> parameter if you want to save its somewhere else.</p> <pre><code>import os\nnew_docs = [\n    \"A fox runs quickly through the forest.\",\n    \"Dogs are loyal and friendly pets.\"\n]\npipe_file = os.path.join(mallet_model.model_dir, \"training_data.mallet\")\noutput_path = os.path.join(mallet_model.model_dir, \"new_doc_topics.txt\")\npath_to_inferencer = mallet_model.metadata['path_to_inferencer']\ninferred_topics = mallet_model.infer(\n    new_docs,\n    path_to_inferencer=path_to_inferencer,\n    use_pipe_from=pipe_file,\n    output_path=output_path\n)\nfor i, dist in enumerate(inferred_topics):\n    print(f\"Document {i}: {dist}\\n\")\n</code></pre> <p>This will output</p> <pre><code>First Two Distributions:\nDocument 0: [0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.20833333333333334, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664, 0.041666666666666664]\n\nDocument 1: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n</code></pre> <p>Next, use the inferencer to infer new topic distributions. You will need to define the following paths:</p> <ul> <li><code>pipe_file</code>: Path to the <code>.mallet</code> pipe file created during training. Ensures new documents are processed identically to training data.</li> <li><code>output_path</code>: Where to save the topic distributions for the new documents (as a text file).</li> <li>path_to_inferencer`: Path to the inferencer file created during training. This is used to infer topics for new documents.</li> </ul> <p>In the code below, we use the default paths that should have been created in your model folder.</p> <pre><code>all_distributions = mallet_model.distributions + inferred_topics\ncategories_training = [\"People\", \"Concepts\", \"People\", \"People\", \"People\", \"Battles\", \"Texts\", \"Texts\", \"Animals\", \"Planets\", \"People\", \"People\"]\ncategories_new = [\"Animals\", \"Animals\"]\nall_categories = categories_training + categories_new\n</code></pre> <p>Now we can use any of the visualization methods. For instance, here are boxplots for the combined distributions:</p> <pre><code>mallet_model.plot_categories_by_topic_boxplots(\n    topics=3,\n    categories=all_categories,\n    topic_distributions=all_distributions,  # Pass the combined distributions\n    show=True\n)\n</code></pre> <p></p> <p>The code below will produce heatmap and topic over time visualizations.</p> <pre><code>mallet_model.plot_categories_by_topics_heatmap(\n    all_categories,\n    topic_distributions=all_distributions,\n    num_keys=3,\n    figsize=(8, 6),\n    font_scale=1,\n    cmap=\"viridis\",\n    show=True,\n    title=\"Topic-Category Heatmap (Combined Distributions)\"\n)\n</code></pre> <p></p> <pre><code>times = [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]\nmallet_model.plot_topics_over_time(\n    times=times,\n    topic_index=0,\n    topic_distributions=all_distributions,\n    title=\"Topic 0 Trend (Combined Distributions)\",\n    color=\"blue\",\n    figsize=(10, 3),\n    show=True\n)\n</code></pre> <p></p>"}]}