{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00a4 The Lexos API is a library of methods for programmatically implementing and extending the functionality in the Lexos text analysis tool. Eventually, the web app will be rewritten to use the API directly. The goal of this alpha stage of development is to reproduce (and in some cases extend) the functionality of the current web app. For the moment, much of the thinking behind the API's architecture is explained in the Tutorial . Current Status: v0.0.1-alpha Features \u00a4 Loads texts from a variety of sources. Manages a corpus of texts. Performs text pre-processing (\"scrubbing\") and splitting (\"cutting\"). Performs tokenization and trains language models using spaCy . Creates assorted visualizations of term vectors. Generates topic models and topic model visualizations using MALLET and dfr-browser . An expanded set of features is planned for the future.","title":"Home"},{"location":"#introduction","text":"The Lexos API is a library of methods for programmatically implementing and extending the functionality in the Lexos text analysis tool. Eventually, the web app will be rewritten to use the API directly. The goal of this alpha stage of development is to reproduce (and in some cases extend) the functionality of the current web app. For the moment, much of the thinking behind the API's architecture is explained in the Tutorial . Current Status: v0.0.1-alpha","title":"Introduction"},{"location":"#features","text":"Loads texts from a variety of sources. Manages a corpus of texts. Performs text pre-processing (\"scrubbing\") and splitting (\"cutting\"). Performs tokenization and trains language models using spaCy . Creates assorted visualizations of term vectors. Generates topic models and topic model visualizations using MALLET and dfr-browser . An expanded set of features is planned for the future.","title":"Features"},{"location":"installation/","text":"Installation \u00a4 Installing Python and Anaconda \u00a4 We recommend installing using the Anaconda Python distribution, even if you already have a version of Python installed on your computer. The Lexos installation process has been tested using Anaconda, and we have run into some issues trying to do it without Anaconda in the past. Visit the Anaconda Distribution page to download the Anaconda installer for your operating system. Once it has finished downloading, run the Anaconda Installer and follow the instructions. To verify Python has been installed correctly, open the Anaconda prompt and enter `python -V`. The response should be a Python version number, e.g. \"Python 3.9.12\". Installing the Lexos API \u00a4 To install the Lexos API, run pip install lexos To update to the latest version, use pip install -U lexos This will install the Lexos API and all of its dependencies. Downloading the Default Language Model (Required) \u00a4 Before using Lexos, you will want to install its default language model: python -m spacy download xx_sent_ud_sm For information on how Lexos uses language models, see Tokenizing Texts . Downloading Additional Language Models (Optional) \u00a4 This is a minimal model that performs sentence and token segmentation for a variety of languages. If you want a model for a specific language, such as English, download it by providing the name of the model: python -m spacy download en_core_web_sm If you are working in another language or need a larger language model, you can download instructions for additional models from the spaCy models page.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#installing-python-and-anaconda","text":"We recommend installing using the Anaconda Python distribution, even if you already have a version of Python installed on your computer. The Lexos installation process has been tested using Anaconda, and we have run into some issues trying to do it without Anaconda in the past. Visit the Anaconda Distribution page to download the Anaconda installer for your operating system. Once it has finished downloading, run the Anaconda Installer and follow the instructions. To verify Python has been installed correctly, open the Anaconda prompt and enter `python -V`. The response should be a Python version number, e.g. \"Python 3.9.12\".","title":"Installing Python and Anaconda"},{"location":"installation/#installing-the-lexos-api","text":"To install the Lexos API, run pip install lexos To update to the latest version, use pip install -U lexos This will install the Lexos API and all of its dependencies.","title":"Installing the Lexos API"},{"location":"installation/#downloading-the-default-language-model-required","text":"Before using Lexos, you will want to install its default language model: python -m spacy download xx_sent_ud_sm For information on how Lexos uses language models, see Tokenizing Texts .","title":"Downloading the Default Language Model (Required)"},{"location":"installation/#downloading-additional-language-models-optional","text":"This is a minimal model that performs sentence and token segmentation for a variety of languages. If you want a model for a specific language, such as English, download it by providing the name of the model: python -m spacy download en_core_web_sm If you are working in another language or need a larger language model, you can download instructions for additional models from the spaCy models page.","title":"Downloading Additional Language Models (Optional)"},{"location":"api/","text":"API \u00a4 A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: cluster A module for cluster analysis. corpus Manages a corpus of documents. cutter Splits documents into segments. DTM Creates a document-term matrix. io A set of functions for handling input-output processes. language_model A method of training and packaging language models for use with with the Lexos tokenizer or external tools. scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. utils A set of utility functions shared by multiple modules. visualization A set of functions for visualising data generated from documents.","title":"Overview"},{"location":"api/#api","text":"A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: cluster A module for cluster analysis. corpus Manages a corpus of documents. cutter Splits documents into segments. DTM Creates a document-term matrix. io A set of functions for handling input-output processes. language_model A method of training and packaging language models for use with with the Lexos tokenizer or external tools. scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. utils A set of utility functions shared by multiple modules. visualization A set of functions for visualising data generated from documents.","title":"API"},{"location":"api/utils/","text":"Utils \u00a4 This module contains helper functions used by multiple modules. lexos . utils . _decode_bytes ( raw_bytes ) \u00a4 Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: Type Description str The decoded string. Source code in lexos\\utils.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def _decode_bytes ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: The decoded string. \"\"\" if isinstance ( raw_bytes , bytes ): try : decoded_str = _try_decode_bytes_ ( raw_bytes ) except ( UnicodeDecodeError , TypeError ): raise LexosException ( \"Chardet failed to detect encoding of your \" \"file. Please make sure your file is in \" \"utf-8 encoding.\" ) else : decoded_str = raw_bytes # Normalize line breaks # \"\\r\\n\" -> \"\\n\" if \" \\r\\n \" in decoded_str [: constants . MIN_NEWLINE_DETECT ]: decoded_str = decoded_str . replace ( \" \\r \" , \"\" ) # \"\\r\" -> \"\\n\" if \" \\r \" in decoded_str [: constants . MIN_NEWLINE_DETECT ]: decoded_str = decoded_str . replace ( \" \\r \" , \" \\n \" ) return decoded_str lexos . utils . _try_decode_bytes_ ( raw_bytes ) \u00a4 Try to decode raw bytes (helper function for decode_bytes(). Parameters: Name Type Description Default raw_bytes bytes The bytes you want to decode to string. required Returns: Type Description str A decoded string. Source code in lexos\\utils.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def _try_decode_bytes_ ( raw_bytes : bytes ) -> str : \"\"\"Try to decode raw bytes (helper function for decode_bytes(). Args: raw_bytes (bytes): The bytes you want to decode to string. Returns: A decoded string. \"\"\" # Detect the encoding with only the first couple of bytes encoding_detect = chardet . detect ( raw_bytes [: constants . MIN_ENCODING_DETECT ]) # Get the encoding encoding_type = encoding_detect [ \"encoding\" ] if encoding_type is None : encoding_detect = chardet . detect ( raw_bytes ) encoding_type = encoding_detect [ \"encoding\" ] try : # Try to decode the string using the original encoding decoded_string = raw_bytes . decode ( encoding_type ) except ( UnicodeDecodeError , TypeError ): # Try UnicodeDammit if chardet didn't work if encoding_type == \"ascii\" : dammit = UnicodeDammit ( raw_bytes , [ \"iso-8859-1\" , \"iso-8859-15\" , \"windows-1252\" ] ) else : dammit = UnicodeDammit ( raw_bytes ) decoded_string = dammit . unicode_markup return decoded_string lexos . utils . ensure_list ( item ) \u00a4 Ensure string is converted to a Path. Parameters: Name Type Description Default item Any Anything. required Returns: Type Description List The item inside a list if it is not already a list. Source code in lexos\\utils.py 22 23 24 25 26 27 28 29 30 31 32 33 def ensure_list ( item : Any ) -> List : \"\"\"Ensure string is converted to a Path. Args: item (Any): Anything. Returns: The item inside a list if it is not already a list. \"\"\" if not isinstance ( item , list ): item = [ item ] return item lexos . utils . ensure_path ( path ) \u00a4 Ensure string is converted to a Path. Parameters: Name Type Description Default path Any Anything. If string, it's converted to Path. required Returns: Type Description Any Path or original argument. Source code in lexos\\utils.py 36 37 38 39 40 41 42 43 44 45 46 47 48 def ensure_path ( path : Any ) -> Any : \"\"\"Ensure string is converted to a Path. Args: path (Any): Anything. If string, it's converted to Path. Returns: Path or original argument. \"\"\" if isinstance ( path , str ): return Path ( path . replace ( \" \\\\ \" , \"/\" )) else : return path lexos . utils . get_encoding ( input_string ) \u00a4 Use chardet to return the encoding type of a string. Parameters: Name Type Description Default input_string bytes A bytestring. required Returns: Type Description str The string's encoding type. Source code in lexos\\utils.py 217 218 219 220 221 222 223 224 225 226 227 228 def get_encoding ( input_string : bytes ) -> str : \"\"\"Use chardet to return the encoding type of a string. Args: input_string (bytes): A bytestring. Returns: The string's encoding type. \"\"\" encoding_detect = chardet . detect ( input_string [: constants . MIN_ENCODING_DETECT ]) encoding_type = encoding_detect [ \"encoding\" ] return encoding_type lexos . utils . get_github_raw_paths ( path , user = None , repo = None , branch = None ) \u00a4 Get raw paths to files in a GitHub directory. Parameters: Name Type Description Default path Union [ Path , str ] The path to the directory. required user Optional [ str ] The user name of the GitHub repository. None repo Optional [ str ] The repository name of the GitHub repository. None branch Optional [ str ] The branch of the GitHub repository. None Returns: Name Type Description list list A list of raw download paths. Source code in lexos\\utils.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def get_github_raw_paths ( path : Union [ Path , str ], user : Optional [ str ] = None , repo : Optional [ str ] = None , branch : Optional [ str ] = None , ) -> list : \"\"\"Get raw paths to files in a GitHub directory. Args: path (Union[Path, str]): The path to the directory. user (Optional[str]): The user name of the GitHub repository. repo (Optional[str]): The repository name of the GitHub repository. branch (Optional[str]): The branch of the GitHub repository. Returns: list: A list of raw download paths. \"\"\" path = str ( path ) if not user or not repo or not branch : try : prefix , suffix = path . split ( \"tree\" ) prefix = prefix . split ( \"/\" ) prefix = [ x for x in prefix if x != \"\" ] user = prefix [ - 2 ] repo = prefix [ - 1 ] suffix = suffix . split ( \"/\" ) suffix = [ x for x in suffix if x != \"\" ] branch = suffix [ 0 ] except ValueError : sample = ( \"https://github.com/ {user} / {repository} /tree/ {branch} / {path_from_root} \" ) raise ValueError ( f \"Invalid GitHub path. Use the format { sample } .\" ) relpath = path . split ( f \"tree/ { branch } /\" )[ 1 ] api_path = f \"https://api.github.com/repos/ { user } / { repo } /contents/ { relpath } \" r = requests . get ( api_path ) return [ path [ \"download_url\" ] for path in r . json ()] lexos . utils . get_paths ( path ) \u00a4 Get a list paths in a directory. Parameters: Name Type Description Default path Union [ Path , str ] The path to the directory. required Returns: Name Type Description list list A list of file paths. Source code in lexos\\utils.py 51 52 53 54 55 56 57 58 59 60 def get_paths ( path : Union [ Path , str ]) -> list : \"\"\"Get a list paths in a directory. Args: path (Union[Path, str]): The path to the directory. Returns: list: A list of file paths. \"\"\" return list ( Path ( path ) . glob ( \"**/*\" )) lexos . utils . is_dir ( filepath ) \u00a4 Check if a path corresponds to a directory. Source code in lexos\\utils.py 102 103 104 def is_dir ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a path corresponds to a directory.\"\"\" return ensure_path ( filepath ) . is_dir () lexos . utils . is_github_dir ( filepath ) \u00a4 Check if a path corresponds to a directory on GitHub. Source code in lexos\\utils.py 107 108 109 110 111 112 def is_github_dir ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a path corresponds to a directory on GitHub.\"\"\" if \"github.com\" in str ( filepath ): if ensure_path ( filepath ) . suffix == \"\" : return True return False lexos . utils . is_docx ( filepath ) \u00a4 Check if a file is a docx. Source code in lexos\\utils.py 115 116 117 def is_docx ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a file is a docx.\"\"\" return str ( filepath ) . endswith ( \".docx\" ) lexos . utils . is_file ( filepath ) \u00a4 Check if a path corresponds to a file. Source code in lexos\\utils.py 120 121 122 def is_file ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a path corresponds to a file.\"\"\" return ensure_path ( filepath ) . is_file () lexos . utils . is_pdf ( filepath ) \u00a4 Check if a file is a pdf. Source code in lexos\\utils.py 125 126 127 def is_pdf ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a file is a pdf.\"\"\" return str ( filepath ) . endswith ( \".pdf\" ) lexos . utils . is_url ( s ) \u00a4 Check if string is a URL. Source code in lexos\\utils.py 130 131 132 133 134 135 136 137 138 139 140 141 142 def is_url ( s : Union [ Path , str ]) -> bool : \"\"\"Check if string is a URL.\"\"\" s = str ( s ) return bool ( re . match ( r \"(https?|ftp)://\" # protocol r \"(\\w+(\\-\\w+)*\\.)?\" # host (optional) r \"((\\w+(\\-\\w+)*)\\.(\\w+))\" # domain r \"(\\.\\w+)*\" # top-level domain (optional, can have > 1) r \"([\\w\\-\\._\\~/]*)*(?<!\\.)\" , # path, params, anchors, etc. (optional) s , ) ) lexos . utils . normalize ( raw_bytes ) \u00a4 Normalise a string to LexosFile format. Parameters: Name Type Description Default raw_bytes bytes The input bytestring. required Returns: Type Description str Normalised version of the input string. Source code in lexos\\utils.py 231 232 233 234 235 236 237 238 239 240 241 def normalize ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Normalise a string to LexosFile format. Args: raw_bytes (bytes): The input bytestring. Returns: Normalised version of the input string. \"\"\" s = _decode_bytes ( raw_bytes ) return s lexos . utils . normalize_file ( filepath , destination_dir = '.' ) \u00a4 Normalise a file to LexosFile format and save the file. Parameters: Name Type Description Default filepath Union [ Path , str ] The path to the input file. required destination_dir Union [ Path , str ] The path to the directory where the files. will be saved. '.' Source code in lexos\\utils.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def normalize_file ( filepath : Union [ Path , str ], destination_dir : Union [ Path , str ] = \".\" ) -> None : \"\"\"Normalise a file to LexosFile format and save the file. Args: filepath (Union[Path, str]): The path to the input file. destination_dir (Union[Path, str]): The path to the directory where the files. will be saved. \"\"\" # filepath = ensure_path(filepath) filepath = Path ( filepath ) destination_dir = ensure_path ( destination_dir ) with open ( filepath , \"rb\" ) as f : doc = f . read () with open ( destination_dir / Path ( filepath . name ), \"w\" ) as f : f . write ( normalize ( doc )) lexos . utils . normalize_files ( filepaths , destination_dir = '.' ) \u00a4 Normalise a list of files to LexosFile format and save the files. Parameters: Name Type Description Default filepaths List [ Union [ Path , str ]] The list of paths to input files. required destination_dir Union [ Path , str ] The path to the directory where the files. will be saved. '.' Source code in lexos\\utils.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def normalize_files ( filepaths : List [ Union [ Path , str ]], destination_dir : Union [ Path , str ] = \".\" ) -> None : \"\"\"Normalise a list of files to LexosFile format and save the files. Args: filepaths (List[Union[Path, str]]): The list of paths to input files. destination_dir (Union[Path, str]): The path to the directory where the files. will be saved. \"\"\" for filepath in filepaths : filepath = ensure_path ( filepath ) with open ( filepath , \"rb\" ) as f : doc = f . read () with open ( destination_dir / filepath . name , \"w\" ) as f : f . write ( normalize ( doc )) lexos . utils . normalize_strings ( strings ) \u00a4 Normalise a list of strings to LexosFile format. Parameters: Name Type Description Default strings List [ Union [ bytes , str ]] The list of input strings. required Returns: Type Description List [ str ] A list of normalised versions of the input strings. Source code in lexos\\utils.py 244 245 246 247 248 249 250 251 252 253 254 255 256 def normalize_strings ( strings : List [ str ]) -> List [ str ]: \"\"\"Normalise a list of strings to LexosFile format. Args: strings (List[Union[bytes, str]]): The list of input strings. Returns: A list of normalised versions of the input strings. \"\"\" normalized_strings = [] for s in strings : normalized_strings . append ( normalize ( s )) return normalized_strings lexos . utils . to_collection ( val , val_type , col_type ) \u00a4 Validate and cast a value or values to a collection. Parameters: Name Type Description Default val object Value or values to validate and cast. required val_type type Type of each value in collection, e.g. int or (str, bytes) . required col_type type Type of collection to return, e.g. tuple or set . required Returns: Type Description Collection [ AnyVal ] Collection of type col_type with values all of type val_type . Raises: Type Description TypeError An invalid value was passed. Source code in lexos\\utils.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def to_collection ( val : Union [ AnyVal , Collection [ AnyVal ]], val_type : Union [ Type [ Any ], Tuple [ Type [ Any ], ... ]], col_type : Type [ Any ], ) -> Collection [ AnyVal ]: \"\"\"Validate and cast a value or values to a collection. Args: val (object): Value or values to validate and cast. val_type (type): Type of each value in collection, e.g. ``int`` or ``(str, bytes)``. col_type (type): Type of collection to return, e.g. ``tuple`` or ``set``. Returns: Collection of type ``col_type`` with values all of type ``val_type``. Raises: TypeError: An invalid value was passed. \"\"\" if val is None : return [] if isinstance ( val , val_type ): return col_type ([ val ]) elif isinstance ( val , ( tuple , list , set , frozenset )): if not all ( isinstance ( v , val_type ) for v in val ): raise TypeError ( f \"not all values are of type { val_type } \" ) return col_type ( val ) else : # TODO: use standard error message, maybe? raise TypeError ( f \"values must be { val_type } or a collection thereof, not { type ( val ) } \" ) lexos . utils . unzip_archive ( archive_path , extract_dir ) \u00a4 Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python . Parameters: Name Type Description Default archive_path str The path to the archive file to be unzipped. required extract_dir str The path to folder where the archive will be extracted. required Source code in lexos\\utils.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def unzip_archive ( archive_path : str , extract_dir : str ): \"\"\"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python. Args: archive_path (str): The path to the archive file to be unzipped. extract_dir (str): The path to folder where the archive will be extracted. \"\"\" zf = zipfile . ZipFile ( archive_path , \"r\" ) progress = Progress () with progress : for file in progress . track ( zf . infolist (), description = \"Processing...\" ): zf . extract ( file , path = extract_dir ) sleep ( 0.1 ) lexos . utils . zip_folder ( source_dir , archive_file ) \u00a4 Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Parameters: Name Type Description Default source_dir Path The path to the source directory. required archive_file Path The path to the archive file to be created (including file extension). required Source code in lexos\\utils.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def zip_folder ( source_dir : Path , archive_file : Path ): \"\"\"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Args: source_dir (Path): The path to the source directory. archive_file (Path): The path to the archive file to be created (including file extension). \"\"\" progress = Progress () with zipfile . ZipFile ( archive_file , mode = \"w\" , compression = zipfile . ZIP_DEFLATED , compresslevel = 7 ) as zip : files = list ( source_dir . rglob ( \"*\" )) with progress : for file in progress . track ( files , description = \"Processing...\" ): relative_path = file . relative_to ( source_dir ) zip . write ( file , arcname = relative_path ) sleep ( 0.1 )","title":"Utils"},{"location":"api/utils/#utils","text":"This module contains helper functions used by multiple modules.","title":"Utils"},{"location":"api/utils/#lexos.utils._decode_bytes","text":"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: Type Description str The decoded string. Source code in lexos\\utils.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def _decode_bytes ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: The decoded string. \"\"\" if isinstance ( raw_bytes , bytes ): try : decoded_str = _try_decode_bytes_ ( raw_bytes ) except ( UnicodeDecodeError , TypeError ): raise LexosException ( \"Chardet failed to detect encoding of your \" \"file. Please make sure your file is in \" \"utf-8 encoding.\" ) else : decoded_str = raw_bytes # Normalize line breaks # \"\\r\\n\" -> \"\\n\" if \" \\r\\n \" in decoded_str [: constants . MIN_NEWLINE_DETECT ]: decoded_str = decoded_str . replace ( \" \\r \" , \"\" ) # \"\\r\" -> \"\\n\" if \" \\r \" in decoded_str [: constants . MIN_NEWLINE_DETECT ]: decoded_str = decoded_str . replace ( \" \\r \" , \" \\n \" ) return decoded_str","title":"_decode_bytes()"},{"location":"api/utils/#lexos.utils._try_decode_bytes_","text":"Try to decode raw bytes (helper function for decode_bytes(). Parameters: Name Type Description Default raw_bytes bytes The bytes you want to decode to string. required Returns: Type Description str A decoded string. Source code in lexos\\utils.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def _try_decode_bytes_ ( raw_bytes : bytes ) -> str : \"\"\"Try to decode raw bytes (helper function for decode_bytes(). Args: raw_bytes (bytes): The bytes you want to decode to string. Returns: A decoded string. \"\"\" # Detect the encoding with only the first couple of bytes encoding_detect = chardet . detect ( raw_bytes [: constants . MIN_ENCODING_DETECT ]) # Get the encoding encoding_type = encoding_detect [ \"encoding\" ] if encoding_type is None : encoding_detect = chardet . detect ( raw_bytes ) encoding_type = encoding_detect [ \"encoding\" ] try : # Try to decode the string using the original encoding decoded_string = raw_bytes . decode ( encoding_type ) except ( UnicodeDecodeError , TypeError ): # Try UnicodeDammit if chardet didn't work if encoding_type == \"ascii\" : dammit = UnicodeDammit ( raw_bytes , [ \"iso-8859-1\" , \"iso-8859-15\" , \"windows-1252\" ] ) else : dammit = UnicodeDammit ( raw_bytes ) decoded_string = dammit . unicode_markup return decoded_string","title":"_try_decode_bytes_()"},{"location":"api/utils/#lexos.utils.ensure_list","text":"Ensure string is converted to a Path. Parameters: Name Type Description Default item Any Anything. required Returns: Type Description List The item inside a list if it is not already a list. Source code in lexos\\utils.py 22 23 24 25 26 27 28 29 30 31 32 33 def ensure_list ( item : Any ) -> List : \"\"\"Ensure string is converted to a Path. Args: item (Any): Anything. Returns: The item inside a list if it is not already a list. \"\"\" if not isinstance ( item , list ): item = [ item ] return item","title":"ensure_list()"},{"location":"api/utils/#lexos.utils.ensure_path","text":"Ensure string is converted to a Path. Parameters: Name Type Description Default path Any Anything. If string, it's converted to Path. required Returns: Type Description Any Path or original argument. Source code in lexos\\utils.py 36 37 38 39 40 41 42 43 44 45 46 47 48 def ensure_path ( path : Any ) -> Any : \"\"\"Ensure string is converted to a Path. Args: path (Any): Anything. If string, it's converted to Path. Returns: Path or original argument. \"\"\" if isinstance ( path , str ): return Path ( path . replace ( \" \\\\ \" , \"/\" )) else : return path","title":"ensure_path()"},{"location":"api/utils/#lexos.utils.get_encoding","text":"Use chardet to return the encoding type of a string. Parameters: Name Type Description Default input_string bytes A bytestring. required Returns: Type Description str The string's encoding type. Source code in lexos\\utils.py 217 218 219 220 221 222 223 224 225 226 227 228 def get_encoding ( input_string : bytes ) -> str : \"\"\"Use chardet to return the encoding type of a string. Args: input_string (bytes): A bytestring. Returns: The string's encoding type. \"\"\" encoding_detect = chardet . detect ( input_string [: constants . MIN_ENCODING_DETECT ]) encoding_type = encoding_detect [ \"encoding\" ] return encoding_type","title":"get_encoding()"},{"location":"api/utils/#lexos.utils.get_github_raw_paths","text":"Get raw paths to files in a GitHub directory. Parameters: Name Type Description Default path Union [ Path , str ] The path to the directory. required user Optional [ str ] The user name of the GitHub repository. None repo Optional [ str ] The repository name of the GitHub repository. None branch Optional [ str ] The branch of the GitHub repository. None Returns: Name Type Description list list A list of raw download paths. Source code in lexos\\utils.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def get_github_raw_paths ( path : Union [ Path , str ], user : Optional [ str ] = None , repo : Optional [ str ] = None , branch : Optional [ str ] = None , ) -> list : \"\"\"Get raw paths to files in a GitHub directory. Args: path (Union[Path, str]): The path to the directory. user (Optional[str]): The user name of the GitHub repository. repo (Optional[str]): The repository name of the GitHub repository. branch (Optional[str]): The branch of the GitHub repository. Returns: list: A list of raw download paths. \"\"\" path = str ( path ) if not user or not repo or not branch : try : prefix , suffix = path . split ( \"tree\" ) prefix = prefix . split ( \"/\" ) prefix = [ x for x in prefix if x != \"\" ] user = prefix [ - 2 ] repo = prefix [ - 1 ] suffix = suffix . split ( \"/\" ) suffix = [ x for x in suffix if x != \"\" ] branch = suffix [ 0 ] except ValueError : sample = ( \"https://github.com/ {user} / {repository} /tree/ {branch} / {path_from_root} \" ) raise ValueError ( f \"Invalid GitHub path. Use the format { sample } .\" ) relpath = path . split ( f \"tree/ { branch } /\" )[ 1 ] api_path = f \"https://api.github.com/repos/ { user } / { repo } /contents/ { relpath } \" r = requests . get ( api_path ) return [ path [ \"download_url\" ] for path in r . json ()]","title":"get_github_raw_paths()"},{"location":"api/utils/#lexos.utils.get_paths","text":"Get a list paths in a directory. Parameters: Name Type Description Default path Union [ Path , str ] The path to the directory. required Returns: Name Type Description list list A list of file paths. Source code in lexos\\utils.py 51 52 53 54 55 56 57 58 59 60 def get_paths ( path : Union [ Path , str ]) -> list : \"\"\"Get a list paths in a directory. Args: path (Union[Path, str]): The path to the directory. Returns: list: A list of file paths. \"\"\" return list ( Path ( path ) . glob ( \"**/*\" ))","title":"get_paths()"},{"location":"api/utils/#lexos.utils.is_dir","text":"Check if a path corresponds to a directory. Source code in lexos\\utils.py 102 103 104 def is_dir ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a path corresponds to a directory.\"\"\" return ensure_path ( filepath ) . is_dir ()","title":"is_dir()"},{"location":"api/utils/#lexos.utils.is_github_dir","text":"Check if a path corresponds to a directory on GitHub. Source code in lexos\\utils.py 107 108 109 110 111 112 def is_github_dir ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a path corresponds to a directory on GitHub.\"\"\" if \"github.com\" in str ( filepath ): if ensure_path ( filepath ) . suffix == \"\" : return True return False","title":"is_github_dir()"},{"location":"api/utils/#lexos.utils.is_docx","text":"Check if a file is a docx. Source code in lexos\\utils.py 115 116 117 def is_docx ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a file is a docx.\"\"\" return str ( filepath ) . endswith ( \".docx\" )","title":"is_docx()"},{"location":"api/utils/#lexos.utils.is_file","text":"Check if a path corresponds to a file. Source code in lexos\\utils.py 120 121 122 def is_file ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a path corresponds to a file.\"\"\" return ensure_path ( filepath ) . is_file ()","title":"is_file()"},{"location":"api/utils/#lexos.utils.is_pdf","text":"Check if a file is a pdf. Source code in lexos\\utils.py 125 126 127 def is_pdf ( filepath : Union [ Path , str ]) -> bool : \"\"\"Check if a file is a pdf.\"\"\" return str ( filepath ) . endswith ( \".pdf\" )","title":"is_pdf()"},{"location":"api/utils/#lexos.utils.is_url","text":"Check if string is a URL. Source code in lexos\\utils.py 130 131 132 133 134 135 136 137 138 139 140 141 142 def is_url ( s : Union [ Path , str ]) -> bool : \"\"\"Check if string is a URL.\"\"\" s = str ( s ) return bool ( re . match ( r \"(https?|ftp)://\" # protocol r \"(\\w+(\\-\\w+)*\\.)?\" # host (optional) r \"((\\w+(\\-\\w+)*)\\.(\\w+))\" # domain r \"(\\.\\w+)*\" # top-level domain (optional, can have > 1) r \"([\\w\\-\\._\\~/]*)*(?<!\\.)\" , # path, params, anchors, etc. (optional) s , ) )","title":"is_url()"},{"location":"api/utils/#lexos.utils.normalize","text":"Normalise a string to LexosFile format. Parameters: Name Type Description Default raw_bytes bytes The input bytestring. required Returns: Type Description str Normalised version of the input string. Source code in lexos\\utils.py 231 232 233 234 235 236 237 238 239 240 241 def normalize ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Normalise a string to LexosFile format. Args: raw_bytes (bytes): The input bytestring. Returns: Normalised version of the input string. \"\"\" s = _decode_bytes ( raw_bytes ) return s","title":"normalize()"},{"location":"api/utils/#lexos.utils.normalize_file","text":"Normalise a file to LexosFile format and save the file. Parameters: Name Type Description Default filepath Union [ Path , str ] The path to the input file. required destination_dir Union [ Path , str ] The path to the directory where the files. will be saved. '.' Source code in lexos\\utils.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def normalize_file ( filepath : Union [ Path , str ], destination_dir : Union [ Path , str ] = \".\" ) -> None : \"\"\"Normalise a file to LexosFile format and save the file. Args: filepath (Union[Path, str]): The path to the input file. destination_dir (Union[Path, str]): The path to the directory where the files. will be saved. \"\"\" # filepath = ensure_path(filepath) filepath = Path ( filepath ) destination_dir = ensure_path ( destination_dir ) with open ( filepath , \"rb\" ) as f : doc = f . read () with open ( destination_dir / Path ( filepath . name ), \"w\" ) as f : f . write ( normalize ( doc ))","title":"normalize_file()"},{"location":"api/utils/#lexos.utils.normalize_files","text":"Normalise a list of files to LexosFile format and save the files. Parameters: Name Type Description Default filepaths List [ Union [ Path , str ]] The list of paths to input files. required destination_dir Union [ Path , str ] The path to the directory where the files. will be saved. '.' Source code in lexos\\utils.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def normalize_files ( filepaths : List [ Union [ Path , str ]], destination_dir : Union [ Path , str ] = \".\" ) -> None : \"\"\"Normalise a list of files to LexosFile format and save the files. Args: filepaths (List[Union[Path, str]]): The list of paths to input files. destination_dir (Union[Path, str]): The path to the directory where the files. will be saved. \"\"\" for filepath in filepaths : filepath = ensure_path ( filepath ) with open ( filepath , \"rb\" ) as f : doc = f . read () with open ( destination_dir / filepath . name , \"w\" ) as f : f . write ( normalize ( doc ))","title":"normalize_files()"},{"location":"api/utils/#lexos.utils.normalize_strings","text":"Normalise a list of strings to LexosFile format. Parameters: Name Type Description Default strings List [ Union [ bytes , str ]] The list of input strings. required Returns: Type Description List [ str ] A list of normalised versions of the input strings. Source code in lexos\\utils.py 244 245 246 247 248 249 250 251 252 253 254 255 256 def normalize_strings ( strings : List [ str ]) -> List [ str ]: \"\"\"Normalise a list of strings to LexosFile format. Args: strings (List[Union[bytes, str]]): The list of input strings. Returns: A list of normalised versions of the input strings. \"\"\" normalized_strings = [] for s in strings : normalized_strings . append ( normalize ( s )) return normalized_strings","title":"normalize_strings()"},{"location":"api/utils/#lexos.utils.to_collection","text":"Validate and cast a value or values to a collection. Parameters: Name Type Description Default val object Value or values to validate and cast. required val_type type Type of each value in collection, e.g. int or (str, bytes) . required col_type type Type of collection to return, e.g. tuple or set . required Returns: Type Description Collection [ AnyVal ] Collection of type col_type with values all of type val_type . Raises: Type Description TypeError An invalid value was passed. Source code in lexos\\utils.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def to_collection ( val : Union [ AnyVal , Collection [ AnyVal ]], val_type : Union [ Type [ Any ], Tuple [ Type [ Any ], ... ]], col_type : Type [ Any ], ) -> Collection [ AnyVal ]: \"\"\"Validate and cast a value or values to a collection. Args: val (object): Value or values to validate and cast. val_type (type): Type of each value in collection, e.g. ``int`` or ``(str, bytes)``. col_type (type): Type of collection to return, e.g. ``tuple`` or ``set``. Returns: Collection of type ``col_type`` with values all of type ``val_type``. Raises: TypeError: An invalid value was passed. \"\"\" if val is None : return [] if isinstance ( val , val_type ): return col_type ([ val ]) elif isinstance ( val , ( tuple , list , set , frozenset )): if not all ( isinstance ( v , val_type ) for v in val ): raise TypeError ( f \"not all values are of type { val_type } \" ) return col_type ( val ) else : # TODO: use standard error message, maybe? raise TypeError ( f \"values must be { val_type } or a collection thereof, not { type ( val ) } \" )","title":"to_collection()"},{"location":"api/utils/#lexos.utils.unzip_archive","text":"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python . Parameters: Name Type Description Default archive_path str The path to the archive file to be unzipped. required extract_dir str The path to folder where the archive will be extracted. required Source code in lexos\\utils.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def unzip_archive ( archive_path : str , extract_dir : str ): \"\"\"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python. Args: archive_path (str): The path to the archive file to be unzipped. extract_dir (str): The path to folder where the archive will be extracted. \"\"\" zf = zipfile . ZipFile ( archive_path , \"r\" ) progress = Progress () with progress : for file in progress . track ( zf . infolist (), description = \"Processing...\" ): zf . extract ( file , path = extract_dir ) sleep ( 0.1 )","title":"unzip_archive()"},{"location":"api/utils/#lexos.utils.zip_folder","text":"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Parameters: Name Type Description Default source_dir Path The path to the source directory. required archive_file Path The path to the archive file to be created (including file extension). required Source code in lexos\\utils.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def zip_folder ( source_dir : Path , archive_file : Path ): \"\"\"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Args: source_dir (Path): The path to the source directory. archive_file (Path): The path to the archive file to be created (including file extension). \"\"\" progress = Progress () with zipfile . ZipFile ( archive_file , mode = \"w\" , compression = zipfile . ZIP_DEFLATED , compresslevel = 7 ) as zip : files = list ( source_dir . rglob ( \"*\" )) with progress : for file in progress . track ( files , description = \"Processing...\" ): relative_path = file . relative_to ( source_dir ) zip . write ( file , arcname = relative_path ) sleep ( 0.1 )","title":"zip_folder()"},{"location":"api/cluster/","text":"Cluster \u00a4 Cluster is a module that performs various types of cluster analysis. Currently, the only implementation is hierarchical agglomerative clustering, which is implemented throught the Dendrogram class. lexos.cluster.dendrogram.Dendrogram \u00a4 Dendrogram. Typical usage: from lexos.cluster.dendrogram import Dendrogram dendrogram = Dendrogram ( dtm , show = True ) or dendrogram = Dendrogram ( dtm , show = False ) dendrogram . fig Source code in lexos\\cluster\\dendrogram.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Dendrogram : \"\"\"Dendrogram. Typical usage: ```python from lexos.cluster.dendrogram import Dendrogram dendrogram = Dendrogram(dtm, show=True) or dendrogram = Dendrogram(dtm, show=False) dendrogram.fig ``` \"\"\" def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , color_threshold : str = None , get_leaves : bool = True , orientation : str = \"top\" , count_sort : Union [ bool , str ] = None , distance_sort : Union [ bool , str ] = None , show_leaf_counts : bool = False , no_plot : bool = False , no_labels : bool = False , leaf_rotation : int = 90 , leaf_font_size : int = None , leaf_label_func : Callable = None , show_contracted : bool = False , link_color_func : Callable = None , ax = None , above_threshold_color : str = \"C0\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . color_threshold = color_threshold self . get_leaves = get_leaves self . orientation = orientation self . count_sort = count_sort self . distance_sort = distance_sort self . show_leaf_counts = show_leaf_counts self . no_plot = no_plot self . no_labels = no_labels self . leaf_rotation = leaf_rotation self . leaf_font_size = leaf_font_size self . leaf_label_func = leaf_label_func self . show_contracted = show_contracted self . link_color_func = link_color_func self . ax = ax self . above_threshold_color = above_threshold_color self . title = title self . figsize = figsize self . show = show # Get the dtm table self . dtm_table = dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . dtm_table . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . dtm_table = self . dtm_table . set_index ( \"terms\" ) . T # Build the dendrogram self . build () def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Create the distance and linkage matrixes for matplotlib X = pdist ( self . dtm_table , metric = self . metric ) Z = sch . linkage ( X , self . method ) fig , ax = plt . subplots ( figsize = self . figsize ) if self . title : plt . title ( self . title ) sch . dendrogram ( Z , labels = self . labels , truncate_mode = self . truncate_mode , color_threshold = self . color_threshold , get_leaves = self . get_leaves , orientation = self . orientation , count_sort = self . count_sort , distance_sort = self . distance_sort , show_leaf_counts = self . show_leaf_counts , no_plot = self . no_plot , no_labels = self . no_labels , leaf_rotation = self . leaf_rotation , leaf_font_size = self . leaf_font_size , leaf_label_func = self . leaf_label_func , show_contracted = self . show_contracted , link_color_func = self . link_color_func , ax = self . ax , above_threshold_color = self . above_threshold_color , ) self . fig = fig if not self . show : plt . close () def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `Dendrogram.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig __init__ ( dtm , labels = None , metric = 'euclidean' , method = 'average' , truncate_mode = None , color_threshold = None , get_leaves = True , orientation = 'top' , count_sort = None , distance_sort = None , show_leaf_counts = False , no_plot = False , no_labels = False , leaf_rotation = 90 , leaf_font_size = None , leaf_label_func = None , show_contracted = False , link_color_func = None , ax = None , above_threshold_color = 'C0' , title = None , figsize = ( 10 , 10 ), show = False ) \u00a4 Initialise the Dendrogram. Source code in lexos\\cluster\\dendrogram.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , color_threshold : str = None , get_leaves : bool = True , orientation : str = \"top\" , count_sort : Union [ bool , str ] = None , distance_sort : Union [ bool , str ] = None , show_leaf_counts : bool = False , no_plot : bool = False , no_labels : bool = False , leaf_rotation : int = 90 , leaf_font_size : int = None , leaf_label_func : Callable = None , show_contracted : bool = False , link_color_func : Callable = None , ax = None , above_threshold_color : str = \"C0\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . color_threshold = color_threshold self . get_leaves = get_leaves self . orientation = orientation self . count_sort = count_sort self . distance_sort = distance_sort self . show_leaf_counts = show_leaf_counts self . no_plot = no_plot self . no_labels = no_labels self . leaf_rotation = leaf_rotation self . leaf_font_size = leaf_font_size self . leaf_label_func = leaf_label_func self . show_contracted = show_contracted self . link_color_func = link_color_func self . ax = ax self . above_threshold_color = above_threshold_color self . title = title self . figsize = figsize self . show = show # Get the dtm table self . dtm_table = dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . dtm_table . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . dtm_table = self . dtm_table . set_index ( \"terms\" ) . T # Build the dendrogram self . build () build () \u00a4 Build a dendrogram. Source code in lexos\\cluster\\dendrogram.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Create the distance and linkage matrixes for matplotlib X = pdist ( self . dtm_table , metric = self . metric ) Z = sch . linkage ( X , self . method ) fig , ax = plt . subplots ( figsize = self . figsize ) if self . title : plt . title ( self . title ) sch . dendrogram ( Z , labels = self . labels , truncate_mode = self . truncate_mode , color_threshold = self . color_threshold , get_leaves = self . get_leaves , orientation = self . orientation , count_sort = self . count_sort , distance_sort = self . distance_sort , show_leaf_counts = self . show_leaf_counts , no_plot = self . no_plot , no_labels = self . no_labels , leaf_rotation = self . leaf_rotation , leaf_font_size = self . leaf_font_size , leaf_label_func = self . leaf_label_func , show_contracted = self . show_contracted , link_color_func = self . link_color_func , ax = self . ax , above_threshold_color = self . above_threshold_color , ) self . fig = fig if not self . show : plt . close () savefig ( filename ) \u00a4 Show the figure if it is hidden. Parameters: Name Type Description Default filename str The name of the file to save. required Source code in lexos\\cluster\\dendrogram.py 125 126 127 128 129 130 131 def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) showfig () \u00a4 Show the figure if it is hidden. This is a helper method. You can also reference the figure using Dendrogram.fig . This will generally display in a Jupyter notebook. Source code in lexos\\cluster\\dendrogram.py 133 134 135 136 137 138 139 140 def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `Dendrogram.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig","title":"Cluster"},{"location":"api/cluster/#cluster","text":"Cluster is a module that performs various types of cluster analysis. Currently, the only implementation is hierarchical agglomerative clustering, which is implemented throught the Dendrogram class.","title":"Cluster"},{"location":"api/cluster/#lexos.cluster.dendrogram.Dendrogram","text":"Dendrogram. Typical usage: from lexos.cluster.dendrogram import Dendrogram dendrogram = Dendrogram ( dtm , show = True ) or dendrogram = Dendrogram ( dtm , show = False ) dendrogram . fig Source code in lexos\\cluster\\dendrogram.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Dendrogram : \"\"\"Dendrogram. Typical usage: ```python from lexos.cluster.dendrogram import Dendrogram dendrogram = Dendrogram(dtm, show=True) or dendrogram = Dendrogram(dtm, show=False) dendrogram.fig ``` \"\"\" def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , color_threshold : str = None , get_leaves : bool = True , orientation : str = \"top\" , count_sort : Union [ bool , str ] = None , distance_sort : Union [ bool , str ] = None , show_leaf_counts : bool = False , no_plot : bool = False , no_labels : bool = False , leaf_rotation : int = 90 , leaf_font_size : int = None , leaf_label_func : Callable = None , show_contracted : bool = False , link_color_func : Callable = None , ax = None , above_threshold_color : str = \"C0\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . color_threshold = color_threshold self . get_leaves = get_leaves self . orientation = orientation self . count_sort = count_sort self . distance_sort = distance_sort self . show_leaf_counts = show_leaf_counts self . no_plot = no_plot self . no_labels = no_labels self . leaf_rotation = leaf_rotation self . leaf_font_size = leaf_font_size self . leaf_label_func = leaf_label_func self . show_contracted = show_contracted self . link_color_func = link_color_func self . ax = ax self . above_threshold_color = above_threshold_color self . title = title self . figsize = figsize self . show = show # Get the dtm table self . dtm_table = dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . dtm_table . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . dtm_table = self . dtm_table . set_index ( \"terms\" ) . T # Build the dendrogram self . build () def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Create the distance and linkage matrixes for matplotlib X = pdist ( self . dtm_table , metric = self . metric ) Z = sch . linkage ( X , self . method ) fig , ax = plt . subplots ( figsize = self . figsize ) if self . title : plt . title ( self . title ) sch . dendrogram ( Z , labels = self . labels , truncate_mode = self . truncate_mode , color_threshold = self . color_threshold , get_leaves = self . get_leaves , orientation = self . orientation , count_sort = self . count_sort , distance_sort = self . distance_sort , show_leaf_counts = self . show_leaf_counts , no_plot = self . no_plot , no_labels = self . no_labels , leaf_rotation = self . leaf_rotation , leaf_font_size = self . leaf_font_size , leaf_label_func = self . leaf_label_func , show_contracted = self . show_contracted , link_color_func = self . link_color_func , ax = self . ax , above_threshold_color = self . above_threshold_color , ) self . fig = fig if not self . show : plt . close () def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `Dendrogram.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig","title":"Dendrogram"},{"location":"api/cluster/#lexos.cluster.dendrogram.Dendrogram.__init__","text":"Initialise the Dendrogram. Source code in lexos\\cluster\\dendrogram.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , color_threshold : str = None , get_leaves : bool = True , orientation : str = \"top\" , count_sort : Union [ bool , str ] = None , distance_sort : Union [ bool , str ] = None , show_leaf_counts : bool = False , no_plot : bool = False , no_labels : bool = False , leaf_rotation : int = 90 , leaf_font_size : int = None , leaf_label_func : Callable = None , show_contracted : bool = False , link_color_func : Callable = None , ax = None , above_threshold_color : str = \"C0\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . color_threshold = color_threshold self . get_leaves = get_leaves self . orientation = orientation self . count_sort = count_sort self . distance_sort = distance_sort self . show_leaf_counts = show_leaf_counts self . no_plot = no_plot self . no_labels = no_labels self . leaf_rotation = leaf_rotation self . leaf_font_size = leaf_font_size self . leaf_label_func = leaf_label_func self . show_contracted = show_contracted self . link_color_func = link_color_func self . ax = ax self . above_threshold_color = above_threshold_color self . title = title self . figsize = figsize self . show = show # Get the dtm table self . dtm_table = dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . dtm_table . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . dtm_table = self . dtm_table . set_index ( \"terms\" ) . T # Build the dendrogram self . build ()","title":"__init__()"},{"location":"api/cluster/#lexos.cluster.dendrogram.Dendrogram.build","text":"Build a dendrogram. Source code in lexos\\cluster\\dendrogram.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Create the distance and linkage matrixes for matplotlib X = pdist ( self . dtm_table , metric = self . metric ) Z = sch . linkage ( X , self . method ) fig , ax = plt . subplots ( figsize = self . figsize ) if self . title : plt . title ( self . title ) sch . dendrogram ( Z , labels = self . labels , truncate_mode = self . truncate_mode , color_threshold = self . color_threshold , get_leaves = self . get_leaves , orientation = self . orientation , count_sort = self . count_sort , distance_sort = self . distance_sort , show_leaf_counts = self . show_leaf_counts , no_plot = self . no_plot , no_labels = self . no_labels , leaf_rotation = self . leaf_rotation , leaf_font_size = self . leaf_font_size , leaf_label_func = self . leaf_label_func , show_contracted = self . show_contracted , link_color_func = self . link_color_func , ax = self . ax , above_threshold_color = self . above_threshold_color , ) self . fig = fig if not self . show : plt . close ()","title":"build()"},{"location":"api/cluster/#lexos.cluster.dendrogram.Dendrogram.savefig","text":"Show the figure if it is hidden. Parameters: Name Type Description Default filename str The name of the file to save. required Source code in lexos\\cluster\\dendrogram.py 125 126 127 128 129 130 131 def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename )","title":"savefig()"},{"location":"api/cluster/#lexos.cluster.dendrogram.Dendrogram.showfig","text":"Show the figure if it is hidden. This is a helper method. You can also reference the figure using Dendrogram.fig . This will generally display in a Jupyter notebook. Source code in lexos\\cluster\\dendrogram.py 133 134 135 136 137 138 139 140 def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `Dendrogram.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig","title":"showfig()"},{"location":"api/corpus/","text":"Corpus \u00a4 The Corpus module consists of a Corpus class that helps you manage assets in your workflow and serialise them to disk for later use. It is strictly optional; you may find is sufficient to load your documents into memory with a Loader or to manage your corpus assets independently through a different application. lexos.corpus.Corpus \u00a4 Bases: BaseModel The main Corpus model. Source code in lexos\\corpus\\__init__.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 class Corpus ( BaseModel ): \"\"\"The main Corpus model.\"\"\" corpus_dir : str = \"corpus\" description : str = None docs : Dict [ int , object ] = {} ids : List [ int ] = [] meta : List [ dict ] = [] name : str = None names : List [ int ] = [] num_active_docs : int = 0 num_docs : int = 0 num_terms : int = 0 num_tokens : int = 0 terms : set = set () class Config : \"\"\"Config for the Corpus class.\"\"\" arbitrary_types_allowed = True extra = \"allow\" def __init__ ( self , ** data ): \"\"\"Initialise the Corpus.\"\"\" super () . __init__ ( ** data ) # for field in self.__fields_set__: # self.meta[field] = getattr(self, field) Path ( f \" { self . corpus_dir } /docs\" ) . mkdir ( parents = True , exist_ok = True ) srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . json ()) print ( \"Corpus created.\" ) def __repr__ ( self ): \"\"\"Return a string representation of the Corpus.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ } field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] rep = f \"Corpus( { ', ' . join ( sorted ( field_list )) } )\" return rep def add ( self , content : Union [ object , str ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , metadata : dict = None , cache : bool = False , ): \"\"\"Add a document the Corpus. Args: content (Union[object, str]): A text string or a spaCy document. name (str): A name for the document. is_parsed (bool): Whether or not the document is parsed. is_active (bool): Whether or not the document is active. model (str): The name of the language model used to parse the document (optional). metadata (dict): A dict containing any metadata. cache (bool): Whether or not to cache the record. \"\"\" # If the doc is a string, make a spaCy doc with untokenised text if not is_parsed : nlp = spacy . blank ( \"xx\" ) nlp . tokenizer = NullTokenizer ( nlp . vocab ) content = nlp ( content ) # Create a Record unique_name = self . _ensure_unique_name ( name ) unique_filename = self . _ensure_unique_filename ( unique_name ) record = Record ( content = content , name = unique_name , filename = unique_filename , id = self . _get_new_id (), is_active = is_active , is_parsed = is_parsed , model = model , ) # Add arbitrary metadata properties if metadata : for k , v in metadata . items (): record . set ( k , v ) # Add the record to the Corpus self . _add_to_corpus ( record , cache = cache ) def add_docs ( self , docs : List [ dict ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , cache : bool = False , ): \"\"\"Add multiple docs to the corpus. Args: docs (List[dict]): A list of dicts containing texts or docs to add, plus metadata. name (str): The name of the record. is_parsed (bool): Whether the record has been parsed. is_active (bool): Whether the record is active. model (str): The name of the model used to parse the record. cache (bool): Whether to cache the record. Note: Each doc in `docs` should have a `content` field containing doc text or spaCy doc and may have any number of additional metadata fields. If \"name\", \"is_parsed\", \"is_active\", and \"model\" are not specified, the defaults will be used. \"\"\" for doc in docs : content = doc [ \"content\" ] if \"name\" in doc : name = doc [ \"name\" ] if \"is_parsed\" in doc : is_parsed = doc [ \"is_parsed\" ] if \"is_active\" in doc : is_active = doc [ \"is_active\" ] if \"model\" in doc : model = doc [ \"model\" ] for item in [ \"content\" , \"name\" , \"is_parsed\" , \"is_active\" , \"model\" ]: if item in doc . keys (): del doc [ item ] self . add ( content = content , name = name , is_parsed = is_parsed , is_active = is_active , model = model , metadata = doc , cache = cache , ) def add_record ( self , record : object , cache : bool = False ): \"\"\"Add a Record the Corpus. Args: record (object): A Record object. cache (bool): Whether or not to cache the record. \"\"\" record . name = self . _ensure_unique_name ( record . name ) if record . filename in os . listdir ( f \" { self . corpus_dir } /docs\" ): record . filename = self . _ensure_unique_filename ( record . name ) self . _add_to_corpus ( record , cache = cache ) def add_records ( self , records : List [ object ], cache : bool = False ): \"\"\"Add multiple docs to the corpus. Args: records (List[objct]): A list of Record objects. cache (bool): Whether or not to cache the record. \"\"\" for record in records : self . add_record ( record , cache = cache ) def get ( self , id ): \"\"\"Get a record from the Corpus by ID. Tries to get the record from memory; otherwise loads it from file. Args: id (int): A document id from the Corpus records. \"\"\" # If the id is in the Corpus cache, return the record if id in self . docs . keys (): return self . docs [ id ] # Otherwise, load the record from file else : id_idx = self . ids . index ( id ) filename = self . meta [ id_idx ][ \"filename\" ] return self . _from_disk ( filename = filename ) def get_records ( self , ids : List [ int ] = None , query : \"str\" = None ) -> Callable : \"\"\"Get multiple records using a list of ids or a pandas query. Args: ids (List[int]): A list of record ids to retrieve query (str): A query string parsable by pandas.DataFrame.query Yields: Callable: A generator containing the docs matching the ids or query \"\"\" if not ids and not query : ids = self . ids if query : # Use pandas to query the dataframe metadata table = self . records_table () . query ( query ) ids = table . id . values . tolist () for id in ids : yield self . get ( id ) def get_term_counts ( self ) -> Callable : \"\"\"Get a Counter with the Corpus term counts. Returns: A collections.Counter \"\"\" return sum ([ self . get ( id ) . terms for id in self . ids ], Counter ()) def meta_table ( self , drop : List [ str ] = [ \"docs\" , \"meta\" , \"terms\" ]) -> pd . DataFrame : \"\"\"Display Corpus metadata, one attribute per row, in a dataframe. Args: drop (List[str]): A list of of rows to drop from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" reduced = { k : v for k , v in self . dict () . items () if k not in drop } df = pd . DataFrame . from_dict ( reduced , orient = \"index\" , columns = [ \"Corpus\" ]) df . sort_index ( axis = 0 , inplace = True ) return df def records_table ( self , columns : List [ str ] = [ \"id\" , \"name\" , \"filename\" , \"num_tokens\" , \"num_terms\" , \"is_active\" , \"is_parsed\" , ], exclude : List [ str ] = None , ) -> pd . DataFrame : \"\"\"Display each document, one per row, in a dataframe. Args: columns (List[str]): A list of of columns to include in the table. exclude (List[str]): A list of columns to exclude from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" df = pd . DataFrame . from_records ( self . meta , columns = columns , exclude = exclude ) df . fillna ( \"\" , inplace = True ) return df def remove ( self , id : int ): \"\"\"Remove a Corpus record by id. Args: id (int): A record id. \"\"\" if not id : raise ValueError ( \"Please supply a record ID.\" ) else : id_idx = self . ids . index ( id ) # Get the record entry in Corpus.meta record = next ( item for item in self . meta if item [ \"id\" ] == id ) # Remove the record file os . remove ( record . filename ) # Remove the id and docs entries del self . ids [ id_idx ] if id in self . docs . keys (): del self . docs [ id ] del self . meta [ id_idx ] # Decrement the token/term/record counts self . num_tokens = self . num_tokens - record . num_tokens () self . num_terms = self . num_terms - record . num_terms () # Save the Corpus metadata srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . dict ()) def remove_records ( self , ids : List [ int ]): \"\"\"Remove multiple records from the corpus. Args: ids (List[int]): A list of record ids to remove. \"\"\" for id in ids : self . remove ( id ) def set ( self , id : int , ** props ): \"\"\"Set a property or properties of a record in the Corpus. Args: id (int): A document id. **props (dict): The dict containing any other properties to set. \"\"\" record = self . get ( id ) old_filename = record . filename for k , v in props . items (): record . set ( k , v ) if record . filename not in os . listdir ( f \" { self . corpus_dir } /docs\" ): self . save ( record , record . filename ) if os . path . isfile ( old_filename ): os . remove ( old_filename ) # Update corpus table data here update = { \"id\" : record . id , \"name\" : record . name , \"filename\" : record . filename , \"num_terms\" : record . num_terms (), \"num_tokens\" : record . num_tokens (), \"is_active\" : record . is_active , \"is_parsed\" : record . is_parsed , } self . meta = [{ ** d , ** update } for d in self . meta ] else : raise ValueError ( f \"A file with the name ` { record . filename } ` already exists.\" ) def _add_to_corpus ( self , record : object , cache : bool = False ): \"\"\"Add a record to the Corpus. Args: record (object): A Record doc. \"\"\" # Update corpus records table meta = record . dict () del meta [ \"content\" ] meta [ \"num_tokens\" ] = record . num_tokens () meta [ \"num_terms\" ] = record . num_terms () self . meta . append ( meta ) # Save the record to disk self . _to_disk ( record , record . filename ) # Update the Corpus ids and names self . ids . append ( record . id ) self . names . append ( record . name ) # Update the Corpus cache if cache : self . docs [ record . id ] = record # Update the Corpus statistics self . num_docs += 1 if record . is_active : self . num_active_docs += 1 self . num_tokens += record . num_tokens () for term in list ( record . terms ): self . terms . add ( term ) self . num_terms = len ( self . terms ) def _ensure_unique_name ( self , name : str = None ) -> str : \"\"\"Ensure that no names are duplicated in the Corpus. Args: name (str): The record name. Returns: A string. \"\"\" if not name : name = str ( uuid . uuid1 ()) if name in self . names : name = f \" { name } _ { uuid . uuid1 () } \" return name def _ensure_unique_filename ( self , name : str = None ) -> str : \"\"\"Ensure that no filenames are duplicated in the Corpus. Args: name (str): The record name (on which the filename will be based). Returns: A filepath. \"\"\" if not name : name = str ( uuid . uuid1 ()) if name in self . names : name = f \" { name } _ { self . id } \" docs_dir = f \" { self . corpus_dir } /docs\" filepath = f \" { docs_dir } / { name } .pkl\" try : assert filepath not in os . listdir ( docs_dir ) return filepath except AssertionError : raise AssertionError ( \"Could not make a unique filepath. Try changing the record name.\" ) def _from_disk ( self , filename ) -> object : \"\"\"Deserialise a record file from disk. Args: filename: The full path to the record file. Returns: Record: A Corpus record \"\"\" with open ( filename , \"rb\" ) as f : return pickle . load ( f ) def _get_new_id ( self ) -> int : \"\"\"Get the highest id in the ids list. Returns: int: An id. \"\"\" if self . ids == []: return 1 else : return max ( self . ids ) + 1 def _to_disk ( self , record , filename ): \"\"\"Serialise a record file to disk. Args: filename: The full path to the record file. \"\"\" with open ( filename , \"wb\" ) as f : pickle . dump ( record , f ) Config \u00a4 Config for the Corpus class. Source code in lexos\\corpus\\__init__.py 139 140 141 142 143 class Config : \"\"\"Config for the Corpus class.\"\"\" arbitrary_types_allowed = True extra = \"allow\" __init__ ( ** data ) \u00a4 Initialise the Corpus. Source code in lexos\\corpus\\__init__.py 145 146 147 148 149 150 151 152 def __init__ ( self , ** data ): \"\"\"Initialise the Corpus.\"\"\" super () . __init__ ( ** data ) # for field in self.__fields_set__: # self.meta[field] = getattr(self, field) Path ( f \" { self . corpus_dir } /docs\" ) . mkdir ( parents = True , exist_ok = True ) srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . json ()) print ( \"Corpus created.\" ) __repr__ () \u00a4 Return a string representation of the Corpus. Source code in lexos\\corpus\\__init__.py 154 155 156 157 158 159 def __repr__ ( self ): \"\"\"Return a string representation of the Corpus.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ } field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] rep = f \"Corpus( { ', ' . join ( sorted ( field_list )) } )\" return rep add ( content , name = None , is_parsed = False , is_active = True , model = None , metadata = None , cache = False ) \u00a4 Add a document the Corpus. Parameters: Name Type Description Default content Union [ object , str ] A text string or a spaCy document. required name str A name for the document. None is_parsed bool Whether or not the document is parsed. False is_active bool Whether or not the document is active. True model str The name of the language model used to parse the document (optional). None metadata dict A dict containing any metadata. None cache bool Whether or not to cache the record. False Source code in lexos\\corpus\\__init__.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def add ( self , content : Union [ object , str ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , metadata : dict = None , cache : bool = False , ): \"\"\"Add a document the Corpus. Args: content (Union[object, str]): A text string or a spaCy document. name (str): A name for the document. is_parsed (bool): Whether or not the document is parsed. is_active (bool): Whether or not the document is active. model (str): The name of the language model used to parse the document (optional). metadata (dict): A dict containing any metadata. cache (bool): Whether or not to cache the record. \"\"\" # If the doc is a string, make a spaCy doc with untokenised text if not is_parsed : nlp = spacy . blank ( \"xx\" ) nlp . tokenizer = NullTokenizer ( nlp . vocab ) content = nlp ( content ) # Create a Record unique_name = self . _ensure_unique_name ( name ) unique_filename = self . _ensure_unique_filename ( unique_name ) record = Record ( content = content , name = unique_name , filename = unique_filename , id = self . _get_new_id (), is_active = is_active , is_parsed = is_parsed , model = model , ) # Add arbitrary metadata properties if metadata : for k , v in metadata . items (): record . set ( k , v ) # Add the record to the Corpus self . _add_to_corpus ( record , cache = cache ) add_docs ( docs , name = None , is_parsed = False , is_active = True , model = None , cache = False ) \u00a4 Add multiple docs to the corpus. Parameters: Name Type Description Default docs List [ dict ] A list of dicts containing texts or docs to add, plus metadata. required name str The name of the record. None is_parsed bool Whether the record has been parsed. False is_active bool Whether the record is active. True model str The name of the model used to parse the record. None cache bool Whether to cache the record. False Note Each doc in docs should have a content field containing doc text or spaCy doc and may have any number of additional metadata fields. If \"name\", \"is_parsed\", \"is_active\", and \"model\" are not specified, the defaults will be used. Source code in lexos\\corpus\\__init__.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def add_docs ( self , docs : List [ dict ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , cache : bool = False , ): \"\"\"Add multiple docs to the corpus. Args: docs (List[dict]): A list of dicts containing texts or docs to add, plus metadata. name (str): The name of the record. is_parsed (bool): Whether the record has been parsed. is_active (bool): Whether the record is active. model (str): The name of the model used to parse the record. cache (bool): Whether to cache the record. Note: Each doc in `docs` should have a `content` field containing doc text or spaCy doc and may have any number of additional metadata fields. If \"name\", \"is_parsed\", \"is_active\", and \"model\" are not specified, the defaults will be used. \"\"\" for doc in docs : content = doc [ \"content\" ] if \"name\" in doc : name = doc [ \"name\" ] if \"is_parsed\" in doc : is_parsed = doc [ \"is_parsed\" ] if \"is_active\" in doc : is_active = doc [ \"is_active\" ] if \"model\" in doc : model = doc [ \"model\" ] for item in [ \"content\" , \"name\" , \"is_parsed\" , \"is_active\" , \"model\" ]: if item in doc . keys (): del doc [ item ] self . add ( content = content , name = name , is_parsed = is_parsed , is_active = is_active , model = model , metadata = doc , cache = cache , ) add_record ( record , cache = False ) \u00a4 Add a Record the Corpus. Parameters: Name Type Description Default record object A Record object. required cache bool Whether or not to cache the record. False Source code in lexos\\corpus\\__init__.py 253 254 255 256 257 258 259 260 261 262 263 def add_record ( self , record : object , cache : bool = False ): \"\"\"Add a Record the Corpus. Args: record (object): A Record object. cache (bool): Whether or not to cache the record. \"\"\" record . name = self . _ensure_unique_name ( record . name ) if record . filename in os . listdir ( f \" { self . corpus_dir } /docs\" ): record . filename = self . _ensure_unique_filename ( record . name ) self . _add_to_corpus ( record , cache = cache ) add_records ( records , cache = False ) \u00a4 Add multiple docs to the corpus. Parameters: Name Type Description Default records List [ objct ] A list of Record objects. required cache bool Whether or not to cache the record. False Source code in lexos\\corpus\\__init__.py 265 266 267 268 269 270 271 272 273 def add_records ( self , records : List [ object ], cache : bool = False ): \"\"\"Add multiple docs to the corpus. Args: records (List[objct]): A list of Record objects. cache (bool): Whether or not to cache the record. \"\"\" for record in records : self . add_record ( record , cache = cache ) get ( id ) \u00a4 Get a record from the Corpus by ID. Tries to get the record from memory; otherwise loads it from file. Parameters: Name Type Description Default id int A document id from the Corpus records. required Source code in lexos\\corpus\\__init__.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def get ( self , id ): \"\"\"Get a record from the Corpus by ID. Tries to get the record from memory; otherwise loads it from file. Args: id (int): A document id from the Corpus records. \"\"\" # If the id is in the Corpus cache, return the record if id in self . docs . keys (): return self . docs [ id ] # Otherwise, load the record from file else : id_idx = self . ids . index ( id ) filename = self . meta [ id_idx ][ \"filename\" ] return self . _from_disk ( filename = filename ) get_records ( ids = None , query = None ) \u00a4 Get multiple records using a list of ids or a pandas query. Parameters: Name Type Description Default ids List [ int ] A list of record ids to retrieve None query str A query string parsable by pandas.DataFrame.query None Yields: Name Type Description Callable Callable A generator containing the docs matching the ids or query Source code in lexos\\corpus\\__init__.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def get_records ( self , ids : List [ int ] = None , query : \"str\" = None ) -> Callable : \"\"\"Get multiple records using a list of ids or a pandas query. Args: ids (List[int]): A list of record ids to retrieve query (str): A query string parsable by pandas.DataFrame.query Yields: Callable: A generator containing the docs matching the ids or query \"\"\" if not ids and not query : ids = self . ids if query : # Use pandas to query the dataframe metadata table = self . records_table () . query ( query ) ids = table . id . values . tolist () for id in ids : yield self . get ( id ) get_term_counts () \u00a4 Get a Counter with the Corpus term counts. Returns: Type Description Callable A collections.Counter Source code in lexos\\corpus\\__init__.py 311 312 313 314 315 316 317 def get_term_counts ( self ) -> Callable : \"\"\"Get a Counter with the Corpus term counts. Returns: A collections.Counter \"\"\" return sum ([ self . get ( id ) . terms for id in self . ids ], Counter ()) meta_table ( drop = [ 'docs' , 'meta' , 'terms' ]) \u00a4 Display Corpus metadata, one attribute per row, in a dataframe. Parameters: Name Type Description Default drop List [ str ] A list of of rows to drop from the table. ['docs', 'meta', 'terms'] Returns: Type Description pd . DataFrame pd.DataFrame: A pandas dataframe containing the table. Source code in lexos\\corpus\\__init__.py 319 320 321 322 323 324 325 326 327 328 329 330 331 def meta_table ( self , drop : List [ str ] = [ \"docs\" , \"meta\" , \"terms\" ]) -> pd . DataFrame : \"\"\"Display Corpus metadata, one attribute per row, in a dataframe. Args: drop (List[str]): A list of of rows to drop from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" reduced = { k : v for k , v in self . dict () . items () if k not in drop } df = pd . DataFrame . from_dict ( reduced , orient = \"index\" , columns = [ \"Corpus\" ]) df . sort_index ( axis = 0 , inplace = True ) return df records_table ( columns = [ 'id' , 'name' , 'filename' , 'num_tokens' , 'num_terms' , 'is_active' , 'is_parsed' ], exclude = None ) \u00a4 Display each document, one per row, in a dataframe. Parameters: Name Type Description Default columns List [ str ] A list of of columns to include in the table. ['id', 'name', 'filename', 'num_tokens', 'num_terms', 'is_active', 'is_parsed'] exclude List [ str ] A list of columns to exclude from the table. None Returns: Type Description pd . DataFrame pd.DataFrame: A pandas dataframe containing the table. Source code in lexos\\corpus\\__init__.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 def records_table ( self , columns : List [ str ] = [ \"id\" , \"name\" , \"filename\" , \"num_tokens\" , \"num_terms\" , \"is_active\" , \"is_parsed\" , ], exclude : List [ str ] = None , ) -> pd . DataFrame : \"\"\"Display each document, one per row, in a dataframe. Args: columns (List[str]): A list of of columns to include in the table. exclude (List[str]): A list of columns to exclude from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" df = pd . DataFrame . from_records ( self . meta , columns = columns , exclude = exclude ) df . fillna ( \"\" , inplace = True ) return df remove ( id ) \u00a4 Remove a Corpus record by id. Parameters: Name Type Description Default id int A record id. required Source code in lexos\\corpus\\__init__.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def remove ( self , id : int ): \"\"\"Remove a Corpus record by id. Args: id (int): A record id. \"\"\" if not id : raise ValueError ( \"Please supply a record ID.\" ) else : id_idx = self . ids . index ( id ) # Get the record entry in Corpus.meta record = next ( item for item in self . meta if item [ \"id\" ] == id ) # Remove the record file os . remove ( record . filename ) # Remove the id and docs entries del self . ids [ id_idx ] if id in self . docs . keys (): del self . docs [ id ] del self . meta [ id_idx ] # Decrement the token/term/record counts self . num_tokens = self . num_tokens - record . num_tokens () self . num_terms = self . num_terms - record . num_terms () # Save the Corpus metadata srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . dict ()) remove_records ( ids ) \u00a4 Remove multiple records from the corpus. Parameters: Name Type Description Default ids List [ int ] A list of record ids to remove. required Source code in lexos\\corpus\\__init__.py 384 385 386 387 388 389 390 391 def remove_records ( self , ids : List [ int ]): \"\"\"Remove multiple records from the corpus. Args: ids (List[int]): A list of record ids to remove. \"\"\" for id in ids : self . remove ( id ) set ( id , ** props ) \u00a4 Set a property or properties of a record in the Corpus. Parameters: Name Type Description Default id int A document id. required **props dict The dict containing any other properties to set. {} Source code in lexos\\corpus\\__init__.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def set ( self , id : int , ** props ): \"\"\"Set a property or properties of a record in the Corpus. Args: id (int): A document id. **props (dict): The dict containing any other properties to set. \"\"\" record = self . get ( id ) old_filename = record . filename for k , v in props . items (): record . set ( k , v ) if record . filename not in os . listdir ( f \" { self . corpus_dir } /docs\" ): self . save ( record , record . filename ) if os . path . isfile ( old_filename ): os . remove ( old_filename ) # Update corpus table data here update = { \"id\" : record . id , \"name\" : record . name , \"filename\" : record . filename , \"num_terms\" : record . num_terms (), \"num_tokens\" : record . num_tokens (), \"is_active\" : record . is_active , \"is_parsed\" : record . is_parsed , } self . meta = [{ ** d , ** update } for d in self . meta ] else : raise ValueError ( f \"A file with the name ` { record . filename } ` already exists.\" ) lexos.corpus.NullTokenizer \u00a4 Pass the text back as a spaCy Doc with the text as single token. Source code in lexos\\corpus\\__init__.py 30 31 32 33 34 35 36 37 38 39 40 41 class NullTokenizer : \"\"\"Pass the text back as a spaCy Doc with the text as single token.\"\"\" def __init__ ( self , vocab ): \"\"\"Initialise the tokeniser.\"\"\" self . vocab = vocab def __call__ ( self , text ): \"\"\"Return the text as a single token.\"\"\" words = [ text ] spaces = [ False ] return Doc ( self . vocab , words = words , spaces = spaces ) __call__ ( text ) \u00a4 Return the text as a single token. Source code in lexos\\corpus\\__init__.py 37 38 39 40 41 def __call__ ( self , text ): \"\"\"Return the text as a single token.\"\"\" words = [ text ] spaces = [ False ] return Doc ( self . vocab , words = words , spaces = spaces ) __init__ ( vocab ) \u00a4 Initialise the tokeniser. Source code in lexos\\corpus\\__init__.py 33 34 35 def __init__ ( self , vocab ): \"\"\"Initialise the tokeniser.\"\"\" self . vocab = vocab lexos.corpus.Record \u00a4 Bases: BaseModel The main Record model. Source code in lexos\\corpus\\__init__.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 class Record ( BaseModel ): \"\"\"The main Record model.\"\"\" content : spacy . tokens . Doc = None filename : str = None id : int = 1 is_active : bool = True is_parsed : bool = False model : str = None name : str = None class Config : \"\"\"Config for the Record class.\"\"\" arbitrary_types_allowed = True extra = \"allow\" def __repr__ ( self ): \"\"\"Return a string representation of the record.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ if field != \"content\" } properties = { k : getattr ( self , k ) for k in self . get_properties ()} fields = { ** fields , ** properties } fields [ \"terms\" ] = \"Counter()\" fields [ \"text\" ] = self . preview fields [ \"tokens\" ] = f '[ { \", \" . join ([ t . text for t in self . content [ 0 : 5 ]]) } ...]' field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] + [ f \"content= { self . preview } \" ] return f \"Record( { ', ' . join ( sorted ( field_list )) } )\" @classmethod def get_properties ( cls ): \"\"\"Return a list of the properties of the Record class.\"\"\" return [ prop for prop in cls . __dict__ if isinstance ( cls . __dict__ [ prop ], property ) ] @property def preview ( self ): \"\"\"Return a preview of the record text.\"\"\" return f \" { self . content . text [ 0 : 50 ] } ...\" @property def terms ( self ): \"\"\"Return the terms in the record.\"\"\" return Counter ([ t . text for t in self . content ]) @property def text ( self ): \"\"\"Return the text of the record.\"\"\" return [ t . text for t in self . content ] @property def tokens ( self ): \"\"\"Return the tokens in the record.\"\"\" return self . content def num_terms ( self ): \"\"\"Return the number of terms.\"\"\" return len ( self . terms ) def num_tokens ( self ): \"\"\"Return the number of tokens.\"\"\" return len ( self . content ) def save ( self ): \"\"\"Serialise the record to disk.\"\"\" with open ( self . filename , \"wb\" ) as f : pickle . dump ( self , f ) def set ( self , k , v ): \"\"\"Set a record property.\"\"\" setattr ( self , k , v ) Config \u00a4 Config for the Record class. Source code in lexos\\corpus\\__init__.py 55 56 57 58 59 class Config : \"\"\"Config for the Record class.\"\"\" arbitrary_types_allowed = True extra = \"allow\" __repr__ () \u00a4 Return a string representation of the record. Source code in lexos\\corpus\\__init__.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __repr__ ( self ): \"\"\"Return a string representation of the record.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ if field != \"content\" } properties = { k : getattr ( self , k ) for k in self . get_properties ()} fields = { ** fields , ** properties } fields [ \"terms\" ] = \"Counter()\" fields [ \"text\" ] = self . preview fields [ \"tokens\" ] = f '[ { \", \" . join ([ t . text for t in self . content [ 0 : 5 ]]) } ...]' field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] + [ f \"content= { self . preview } \" ] return f \"Record( { ', ' . join ( sorted ( field_list )) } )\" get_properties () classmethod \u00a4 Return a list of the properties of the Record class. Source code in lexos\\corpus\\__init__.py 78 79 80 81 82 83 @classmethod def get_properties ( cls ): \"\"\"Return a list of the properties of the Record class.\"\"\" return [ prop for prop in cls . __dict__ if isinstance ( cls . __dict__ [ prop ], property ) ] num_terms () \u00a4 Return the number of terms. Source code in lexos\\corpus\\__init__.py 105 106 107 def num_terms ( self ): \"\"\"Return the number of terms.\"\"\" return len ( self . terms ) num_tokens () \u00a4 Return the number of tokens. Source code in lexos\\corpus\\__init__.py 109 110 111 def num_tokens ( self ): \"\"\"Return the number of tokens.\"\"\" return len ( self . content ) preview () property \u00a4 Return a preview of the record text. Source code in lexos\\corpus\\__init__.py 85 86 87 88 @property def preview ( self ): \"\"\"Return a preview of the record text.\"\"\" return f \" { self . content . text [ 0 : 50 ] } ...\" save () \u00a4 Serialise the record to disk. Source code in lexos\\corpus\\__init__.py 113 114 115 116 def save ( self ): \"\"\"Serialise the record to disk.\"\"\" with open ( self . filename , \"wb\" ) as f : pickle . dump ( self , f ) set ( k , v ) \u00a4 Set a record property. Source code in lexos\\corpus\\__init__.py 118 119 120 def set ( self , k , v ): \"\"\"Set a record property.\"\"\" setattr ( self , k , v ) terms () property \u00a4 Return the terms in the record. Source code in lexos\\corpus\\__init__.py 90 91 92 93 @property def terms ( self ): \"\"\"Return the terms in the record.\"\"\" return Counter ([ t . text for t in self . content ]) text () property \u00a4 Return the text of the record. Source code in lexos\\corpus\\__init__.py 95 96 97 98 @property def text ( self ): \"\"\"Return the text of the record.\"\"\" return [ t . text for t in self . content ] tokens () property \u00a4 Return the tokens in the record. Source code in lexos\\corpus\\__init__.py 100 101 102 103 @property def tokens ( self ): \"\"\"Return the tokens in the record.\"\"\" return self . content","title":"Corpus"},{"location":"api/corpus/#corpus","text":"The Corpus module consists of a Corpus class that helps you manage assets in your workflow and serialise them to disk for later use. It is strictly optional; you may find is sufficient to load your documents into memory with a Loader or to manage your corpus assets independently through a different application.","title":"Corpus"},{"location":"api/corpus/#lexos.corpus.Corpus","text":"Bases: BaseModel The main Corpus model. Source code in lexos\\corpus\\__init__.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 class Corpus ( BaseModel ): \"\"\"The main Corpus model.\"\"\" corpus_dir : str = \"corpus\" description : str = None docs : Dict [ int , object ] = {} ids : List [ int ] = [] meta : List [ dict ] = [] name : str = None names : List [ int ] = [] num_active_docs : int = 0 num_docs : int = 0 num_terms : int = 0 num_tokens : int = 0 terms : set = set () class Config : \"\"\"Config for the Corpus class.\"\"\" arbitrary_types_allowed = True extra = \"allow\" def __init__ ( self , ** data ): \"\"\"Initialise the Corpus.\"\"\" super () . __init__ ( ** data ) # for field in self.__fields_set__: # self.meta[field] = getattr(self, field) Path ( f \" { self . corpus_dir } /docs\" ) . mkdir ( parents = True , exist_ok = True ) srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . json ()) print ( \"Corpus created.\" ) def __repr__ ( self ): \"\"\"Return a string representation of the Corpus.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ } field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] rep = f \"Corpus( { ', ' . join ( sorted ( field_list )) } )\" return rep def add ( self , content : Union [ object , str ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , metadata : dict = None , cache : bool = False , ): \"\"\"Add a document the Corpus. Args: content (Union[object, str]): A text string or a spaCy document. name (str): A name for the document. is_parsed (bool): Whether or not the document is parsed. is_active (bool): Whether or not the document is active. model (str): The name of the language model used to parse the document (optional). metadata (dict): A dict containing any metadata. cache (bool): Whether or not to cache the record. \"\"\" # If the doc is a string, make a spaCy doc with untokenised text if not is_parsed : nlp = spacy . blank ( \"xx\" ) nlp . tokenizer = NullTokenizer ( nlp . vocab ) content = nlp ( content ) # Create a Record unique_name = self . _ensure_unique_name ( name ) unique_filename = self . _ensure_unique_filename ( unique_name ) record = Record ( content = content , name = unique_name , filename = unique_filename , id = self . _get_new_id (), is_active = is_active , is_parsed = is_parsed , model = model , ) # Add arbitrary metadata properties if metadata : for k , v in metadata . items (): record . set ( k , v ) # Add the record to the Corpus self . _add_to_corpus ( record , cache = cache ) def add_docs ( self , docs : List [ dict ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , cache : bool = False , ): \"\"\"Add multiple docs to the corpus. Args: docs (List[dict]): A list of dicts containing texts or docs to add, plus metadata. name (str): The name of the record. is_parsed (bool): Whether the record has been parsed. is_active (bool): Whether the record is active. model (str): The name of the model used to parse the record. cache (bool): Whether to cache the record. Note: Each doc in `docs` should have a `content` field containing doc text or spaCy doc and may have any number of additional metadata fields. If \"name\", \"is_parsed\", \"is_active\", and \"model\" are not specified, the defaults will be used. \"\"\" for doc in docs : content = doc [ \"content\" ] if \"name\" in doc : name = doc [ \"name\" ] if \"is_parsed\" in doc : is_parsed = doc [ \"is_parsed\" ] if \"is_active\" in doc : is_active = doc [ \"is_active\" ] if \"model\" in doc : model = doc [ \"model\" ] for item in [ \"content\" , \"name\" , \"is_parsed\" , \"is_active\" , \"model\" ]: if item in doc . keys (): del doc [ item ] self . add ( content = content , name = name , is_parsed = is_parsed , is_active = is_active , model = model , metadata = doc , cache = cache , ) def add_record ( self , record : object , cache : bool = False ): \"\"\"Add a Record the Corpus. Args: record (object): A Record object. cache (bool): Whether or not to cache the record. \"\"\" record . name = self . _ensure_unique_name ( record . name ) if record . filename in os . listdir ( f \" { self . corpus_dir } /docs\" ): record . filename = self . _ensure_unique_filename ( record . name ) self . _add_to_corpus ( record , cache = cache ) def add_records ( self , records : List [ object ], cache : bool = False ): \"\"\"Add multiple docs to the corpus. Args: records (List[objct]): A list of Record objects. cache (bool): Whether or not to cache the record. \"\"\" for record in records : self . add_record ( record , cache = cache ) def get ( self , id ): \"\"\"Get a record from the Corpus by ID. Tries to get the record from memory; otherwise loads it from file. Args: id (int): A document id from the Corpus records. \"\"\" # If the id is in the Corpus cache, return the record if id in self . docs . keys (): return self . docs [ id ] # Otherwise, load the record from file else : id_idx = self . ids . index ( id ) filename = self . meta [ id_idx ][ \"filename\" ] return self . _from_disk ( filename = filename ) def get_records ( self , ids : List [ int ] = None , query : \"str\" = None ) -> Callable : \"\"\"Get multiple records using a list of ids or a pandas query. Args: ids (List[int]): A list of record ids to retrieve query (str): A query string parsable by pandas.DataFrame.query Yields: Callable: A generator containing the docs matching the ids or query \"\"\" if not ids and not query : ids = self . ids if query : # Use pandas to query the dataframe metadata table = self . records_table () . query ( query ) ids = table . id . values . tolist () for id in ids : yield self . get ( id ) def get_term_counts ( self ) -> Callable : \"\"\"Get a Counter with the Corpus term counts. Returns: A collections.Counter \"\"\" return sum ([ self . get ( id ) . terms for id in self . ids ], Counter ()) def meta_table ( self , drop : List [ str ] = [ \"docs\" , \"meta\" , \"terms\" ]) -> pd . DataFrame : \"\"\"Display Corpus metadata, one attribute per row, in a dataframe. Args: drop (List[str]): A list of of rows to drop from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" reduced = { k : v for k , v in self . dict () . items () if k not in drop } df = pd . DataFrame . from_dict ( reduced , orient = \"index\" , columns = [ \"Corpus\" ]) df . sort_index ( axis = 0 , inplace = True ) return df def records_table ( self , columns : List [ str ] = [ \"id\" , \"name\" , \"filename\" , \"num_tokens\" , \"num_terms\" , \"is_active\" , \"is_parsed\" , ], exclude : List [ str ] = None , ) -> pd . DataFrame : \"\"\"Display each document, one per row, in a dataframe. Args: columns (List[str]): A list of of columns to include in the table. exclude (List[str]): A list of columns to exclude from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" df = pd . DataFrame . from_records ( self . meta , columns = columns , exclude = exclude ) df . fillna ( \"\" , inplace = True ) return df def remove ( self , id : int ): \"\"\"Remove a Corpus record by id. Args: id (int): A record id. \"\"\" if not id : raise ValueError ( \"Please supply a record ID.\" ) else : id_idx = self . ids . index ( id ) # Get the record entry in Corpus.meta record = next ( item for item in self . meta if item [ \"id\" ] == id ) # Remove the record file os . remove ( record . filename ) # Remove the id and docs entries del self . ids [ id_idx ] if id in self . docs . keys (): del self . docs [ id ] del self . meta [ id_idx ] # Decrement the token/term/record counts self . num_tokens = self . num_tokens - record . num_tokens () self . num_terms = self . num_terms - record . num_terms () # Save the Corpus metadata srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . dict ()) def remove_records ( self , ids : List [ int ]): \"\"\"Remove multiple records from the corpus. Args: ids (List[int]): A list of record ids to remove. \"\"\" for id in ids : self . remove ( id ) def set ( self , id : int , ** props ): \"\"\"Set a property or properties of a record in the Corpus. Args: id (int): A document id. **props (dict): The dict containing any other properties to set. \"\"\" record = self . get ( id ) old_filename = record . filename for k , v in props . items (): record . set ( k , v ) if record . filename not in os . listdir ( f \" { self . corpus_dir } /docs\" ): self . save ( record , record . filename ) if os . path . isfile ( old_filename ): os . remove ( old_filename ) # Update corpus table data here update = { \"id\" : record . id , \"name\" : record . name , \"filename\" : record . filename , \"num_terms\" : record . num_terms (), \"num_tokens\" : record . num_tokens (), \"is_active\" : record . is_active , \"is_parsed\" : record . is_parsed , } self . meta = [{ ** d , ** update } for d in self . meta ] else : raise ValueError ( f \"A file with the name ` { record . filename } ` already exists.\" ) def _add_to_corpus ( self , record : object , cache : bool = False ): \"\"\"Add a record to the Corpus. Args: record (object): A Record doc. \"\"\" # Update corpus records table meta = record . dict () del meta [ \"content\" ] meta [ \"num_tokens\" ] = record . num_tokens () meta [ \"num_terms\" ] = record . num_terms () self . meta . append ( meta ) # Save the record to disk self . _to_disk ( record , record . filename ) # Update the Corpus ids and names self . ids . append ( record . id ) self . names . append ( record . name ) # Update the Corpus cache if cache : self . docs [ record . id ] = record # Update the Corpus statistics self . num_docs += 1 if record . is_active : self . num_active_docs += 1 self . num_tokens += record . num_tokens () for term in list ( record . terms ): self . terms . add ( term ) self . num_terms = len ( self . terms ) def _ensure_unique_name ( self , name : str = None ) -> str : \"\"\"Ensure that no names are duplicated in the Corpus. Args: name (str): The record name. Returns: A string. \"\"\" if not name : name = str ( uuid . uuid1 ()) if name in self . names : name = f \" { name } _ { uuid . uuid1 () } \" return name def _ensure_unique_filename ( self , name : str = None ) -> str : \"\"\"Ensure that no filenames are duplicated in the Corpus. Args: name (str): The record name (on which the filename will be based). Returns: A filepath. \"\"\" if not name : name = str ( uuid . uuid1 ()) if name in self . names : name = f \" { name } _ { self . id } \" docs_dir = f \" { self . corpus_dir } /docs\" filepath = f \" { docs_dir } / { name } .pkl\" try : assert filepath not in os . listdir ( docs_dir ) return filepath except AssertionError : raise AssertionError ( \"Could not make a unique filepath. Try changing the record name.\" ) def _from_disk ( self , filename ) -> object : \"\"\"Deserialise a record file from disk. Args: filename: The full path to the record file. Returns: Record: A Corpus record \"\"\" with open ( filename , \"rb\" ) as f : return pickle . load ( f ) def _get_new_id ( self ) -> int : \"\"\"Get the highest id in the ids list. Returns: int: An id. \"\"\" if self . ids == []: return 1 else : return max ( self . ids ) + 1 def _to_disk ( self , record , filename ): \"\"\"Serialise a record file to disk. Args: filename: The full path to the record file. \"\"\" with open ( filename , \"wb\" ) as f : pickle . dump ( record , f )","title":"Corpus"},{"location":"api/corpus/#lexos.corpus.Corpus.Config","text":"Config for the Corpus class. Source code in lexos\\corpus\\__init__.py 139 140 141 142 143 class Config : \"\"\"Config for the Corpus class.\"\"\" arbitrary_types_allowed = True extra = \"allow\"","title":"Config"},{"location":"api/corpus/#lexos.corpus.Corpus.__init__","text":"Initialise the Corpus. Source code in lexos\\corpus\\__init__.py 145 146 147 148 149 150 151 152 def __init__ ( self , ** data ): \"\"\"Initialise the Corpus.\"\"\" super () . __init__ ( ** data ) # for field in self.__fields_set__: # self.meta[field] = getattr(self, field) Path ( f \" { self . corpus_dir } /docs\" ) . mkdir ( parents = True , exist_ok = True ) srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . json ()) print ( \"Corpus created.\" )","title":"__init__()"},{"location":"api/corpus/#lexos.corpus.Corpus.__repr__","text":"Return a string representation of the Corpus. Source code in lexos\\corpus\\__init__.py 154 155 156 157 158 159 def __repr__ ( self ): \"\"\"Return a string representation of the Corpus.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ } field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] rep = f \"Corpus( { ', ' . join ( sorted ( field_list )) } )\" return rep","title":"__repr__()"},{"location":"api/corpus/#lexos.corpus.Corpus.add","text":"Add a document the Corpus. Parameters: Name Type Description Default content Union [ object , str ] A text string or a spaCy document. required name str A name for the document. None is_parsed bool Whether or not the document is parsed. False is_active bool Whether or not the document is active. True model str The name of the language model used to parse the document (optional). None metadata dict A dict containing any metadata. None cache bool Whether or not to cache the record. False Source code in lexos\\corpus\\__init__.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def add ( self , content : Union [ object , str ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , metadata : dict = None , cache : bool = False , ): \"\"\"Add a document the Corpus. Args: content (Union[object, str]): A text string or a spaCy document. name (str): A name for the document. is_parsed (bool): Whether or not the document is parsed. is_active (bool): Whether or not the document is active. model (str): The name of the language model used to parse the document (optional). metadata (dict): A dict containing any metadata. cache (bool): Whether or not to cache the record. \"\"\" # If the doc is a string, make a spaCy doc with untokenised text if not is_parsed : nlp = spacy . blank ( \"xx\" ) nlp . tokenizer = NullTokenizer ( nlp . vocab ) content = nlp ( content ) # Create a Record unique_name = self . _ensure_unique_name ( name ) unique_filename = self . _ensure_unique_filename ( unique_name ) record = Record ( content = content , name = unique_name , filename = unique_filename , id = self . _get_new_id (), is_active = is_active , is_parsed = is_parsed , model = model , ) # Add arbitrary metadata properties if metadata : for k , v in metadata . items (): record . set ( k , v ) # Add the record to the Corpus self . _add_to_corpus ( record , cache = cache )","title":"add()"},{"location":"api/corpus/#lexos.corpus.Corpus.add_docs","text":"Add multiple docs to the corpus. Parameters: Name Type Description Default docs List [ dict ] A list of dicts containing texts or docs to add, plus metadata. required name str The name of the record. None is_parsed bool Whether the record has been parsed. False is_active bool Whether the record is active. True model str The name of the model used to parse the record. None cache bool Whether to cache the record. False Note Each doc in docs should have a content field containing doc text or spaCy doc and may have any number of additional metadata fields. If \"name\", \"is_parsed\", \"is_active\", and \"model\" are not specified, the defaults will be used. Source code in lexos\\corpus\\__init__.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def add_docs ( self , docs : List [ dict ], name : str = None , is_parsed : bool = False , is_active : bool = True , model : str = None , cache : bool = False , ): \"\"\"Add multiple docs to the corpus. Args: docs (List[dict]): A list of dicts containing texts or docs to add, plus metadata. name (str): The name of the record. is_parsed (bool): Whether the record has been parsed. is_active (bool): Whether the record is active. model (str): The name of the model used to parse the record. cache (bool): Whether to cache the record. Note: Each doc in `docs` should have a `content` field containing doc text or spaCy doc and may have any number of additional metadata fields. If \"name\", \"is_parsed\", \"is_active\", and \"model\" are not specified, the defaults will be used. \"\"\" for doc in docs : content = doc [ \"content\" ] if \"name\" in doc : name = doc [ \"name\" ] if \"is_parsed\" in doc : is_parsed = doc [ \"is_parsed\" ] if \"is_active\" in doc : is_active = doc [ \"is_active\" ] if \"model\" in doc : model = doc [ \"model\" ] for item in [ \"content\" , \"name\" , \"is_parsed\" , \"is_active\" , \"model\" ]: if item in doc . keys (): del doc [ item ] self . add ( content = content , name = name , is_parsed = is_parsed , is_active = is_active , model = model , metadata = doc , cache = cache , )","title":"add_docs()"},{"location":"api/corpus/#lexos.corpus.Corpus.add_record","text":"Add a Record the Corpus. Parameters: Name Type Description Default record object A Record object. required cache bool Whether or not to cache the record. False Source code in lexos\\corpus\\__init__.py 253 254 255 256 257 258 259 260 261 262 263 def add_record ( self , record : object , cache : bool = False ): \"\"\"Add a Record the Corpus. Args: record (object): A Record object. cache (bool): Whether or not to cache the record. \"\"\" record . name = self . _ensure_unique_name ( record . name ) if record . filename in os . listdir ( f \" { self . corpus_dir } /docs\" ): record . filename = self . _ensure_unique_filename ( record . name ) self . _add_to_corpus ( record , cache = cache )","title":"add_record()"},{"location":"api/corpus/#lexos.corpus.Corpus.add_records","text":"Add multiple docs to the corpus. Parameters: Name Type Description Default records List [ objct ] A list of Record objects. required cache bool Whether or not to cache the record. False Source code in lexos\\corpus\\__init__.py 265 266 267 268 269 270 271 272 273 def add_records ( self , records : List [ object ], cache : bool = False ): \"\"\"Add multiple docs to the corpus. Args: records (List[objct]): A list of Record objects. cache (bool): Whether or not to cache the record. \"\"\" for record in records : self . add_record ( record , cache = cache )","title":"add_records()"},{"location":"api/corpus/#lexos.corpus.Corpus.get","text":"Get a record from the Corpus by ID. Tries to get the record from memory; otherwise loads it from file. Parameters: Name Type Description Default id int A document id from the Corpus records. required Source code in lexos\\corpus\\__init__.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def get ( self , id ): \"\"\"Get a record from the Corpus by ID. Tries to get the record from memory; otherwise loads it from file. Args: id (int): A document id from the Corpus records. \"\"\" # If the id is in the Corpus cache, return the record if id in self . docs . keys (): return self . docs [ id ] # Otherwise, load the record from file else : id_idx = self . ids . index ( id ) filename = self . meta [ id_idx ][ \"filename\" ] return self . _from_disk ( filename = filename )","title":"get()"},{"location":"api/corpus/#lexos.corpus.Corpus.get_records","text":"Get multiple records using a list of ids or a pandas query. Parameters: Name Type Description Default ids List [ int ] A list of record ids to retrieve None query str A query string parsable by pandas.DataFrame.query None Yields: Name Type Description Callable Callable A generator containing the docs matching the ids or query Source code in lexos\\corpus\\__init__.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def get_records ( self , ids : List [ int ] = None , query : \"str\" = None ) -> Callable : \"\"\"Get multiple records using a list of ids or a pandas query. Args: ids (List[int]): A list of record ids to retrieve query (str): A query string parsable by pandas.DataFrame.query Yields: Callable: A generator containing the docs matching the ids or query \"\"\" if not ids and not query : ids = self . ids if query : # Use pandas to query the dataframe metadata table = self . records_table () . query ( query ) ids = table . id . values . tolist () for id in ids : yield self . get ( id )","title":"get_records()"},{"location":"api/corpus/#lexos.corpus.Corpus.get_term_counts","text":"Get a Counter with the Corpus term counts. Returns: Type Description Callable A collections.Counter Source code in lexos\\corpus\\__init__.py 311 312 313 314 315 316 317 def get_term_counts ( self ) -> Callable : \"\"\"Get a Counter with the Corpus term counts. Returns: A collections.Counter \"\"\" return sum ([ self . get ( id ) . terms for id in self . ids ], Counter ())","title":"get_term_counts()"},{"location":"api/corpus/#lexos.corpus.Corpus.meta_table","text":"Display Corpus metadata, one attribute per row, in a dataframe. Parameters: Name Type Description Default drop List [ str ] A list of of rows to drop from the table. ['docs', 'meta', 'terms'] Returns: Type Description pd . DataFrame pd.DataFrame: A pandas dataframe containing the table. Source code in lexos\\corpus\\__init__.py 319 320 321 322 323 324 325 326 327 328 329 330 331 def meta_table ( self , drop : List [ str ] = [ \"docs\" , \"meta\" , \"terms\" ]) -> pd . DataFrame : \"\"\"Display Corpus metadata, one attribute per row, in a dataframe. Args: drop (List[str]): A list of of rows to drop from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" reduced = { k : v for k , v in self . dict () . items () if k not in drop } df = pd . DataFrame . from_dict ( reduced , orient = \"index\" , columns = [ \"Corpus\" ]) df . sort_index ( axis = 0 , inplace = True ) return df","title":"meta_table()"},{"location":"api/corpus/#lexos.corpus.Corpus.records_table","text":"Display each document, one per row, in a dataframe. Parameters: Name Type Description Default columns List [ str ] A list of of columns to include in the table. ['id', 'name', 'filename', 'num_tokens', 'num_terms', 'is_active', 'is_parsed'] exclude List [ str ] A list of columns to exclude from the table. None Returns: Type Description pd . DataFrame pd.DataFrame: A pandas dataframe containing the table. Source code in lexos\\corpus\\__init__.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 def records_table ( self , columns : List [ str ] = [ \"id\" , \"name\" , \"filename\" , \"num_tokens\" , \"num_terms\" , \"is_active\" , \"is_parsed\" , ], exclude : List [ str ] = None , ) -> pd . DataFrame : \"\"\"Display each document, one per row, in a dataframe. Args: columns (List[str]): A list of of columns to include in the table. exclude (List[str]): A list of columns to exclude from the table. Returns: pd.DataFrame: A pandas dataframe containing the table. \"\"\" df = pd . DataFrame . from_records ( self . meta , columns = columns , exclude = exclude ) df . fillna ( \"\" , inplace = True ) return df","title":"records_table()"},{"location":"api/corpus/#lexos.corpus.Corpus.remove","text":"Remove a Corpus record by id. Parameters: Name Type Description Default id int A record id. required Source code in lexos\\corpus\\__init__.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def remove ( self , id : int ): \"\"\"Remove a Corpus record by id. Args: id (int): A record id. \"\"\" if not id : raise ValueError ( \"Please supply a record ID.\" ) else : id_idx = self . ids . index ( id ) # Get the record entry in Corpus.meta record = next ( item for item in self . meta if item [ \"id\" ] == id ) # Remove the record file os . remove ( record . filename ) # Remove the id and docs entries del self . ids [ id_idx ] if id in self . docs . keys (): del self . docs [ id ] del self . meta [ id_idx ] # Decrement the token/term/record counts self . num_tokens = self . num_tokens - record . num_tokens () self . num_terms = self . num_terms - record . num_terms () # Save the Corpus metadata srsly . write_json ( f \" { self . corpus_dir } /corpus_meta.json\" , self . dict ())","title":"remove()"},{"location":"api/corpus/#lexos.corpus.Corpus.remove_records","text":"Remove multiple records from the corpus. Parameters: Name Type Description Default ids List [ int ] A list of record ids to remove. required Source code in lexos\\corpus\\__init__.py 384 385 386 387 388 389 390 391 def remove_records ( self , ids : List [ int ]): \"\"\"Remove multiple records from the corpus. Args: ids (List[int]): A list of record ids to remove. \"\"\" for id in ids : self . remove ( id )","title":"remove_records()"},{"location":"api/corpus/#lexos.corpus.Corpus.set","text":"Set a property or properties of a record in the Corpus. Parameters: Name Type Description Default id int A document id. required **props dict The dict containing any other properties to set. {} Source code in lexos\\corpus\\__init__.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def set ( self , id : int , ** props ): \"\"\"Set a property or properties of a record in the Corpus. Args: id (int): A document id. **props (dict): The dict containing any other properties to set. \"\"\" record = self . get ( id ) old_filename = record . filename for k , v in props . items (): record . set ( k , v ) if record . filename not in os . listdir ( f \" { self . corpus_dir } /docs\" ): self . save ( record , record . filename ) if os . path . isfile ( old_filename ): os . remove ( old_filename ) # Update corpus table data here update = { \"id\" : record . id , \"name\" : record . name , \"filename\" : record . filename , \"num_terms\" : record . num_terms (), \"num_tokens\" : record . num_tokens (), \"is_active\" : record . is_active , \"is_parsed\" : record . is_parsed , } self . meta = [{ ** d , ** update } for d in self . meta ] else : raise ValueError ( f \"A file with the name ` { record . filename } ` already exists.\" )","title":"set()"},{"location":"api/corpus/#lexos.corpus.NullTokenizer","text":"Pass the text back as a spaCy Doc with the text as single token. Source code in lexos\\corpus\\__init__.py 30 31 32 33 34 35 36 37 38 39 40 41 class NullTokenizer : \"\"\"Pass the text back as a spaCy Doc with the text as single token.\"\"\" def __init__ ( self , vocab ): \"\"\"Initialise the tokeniser.\"\"\" self . vocab = vocab def __call__ ( self , text ): \"\"\"Return the text as a single token.\"\"\" words = [ text ] spaces = [ False ] return Doc ( self . vocab , words = words , spaces = spaces )","title":"NullTokenizer"},{"location":"api/corpus/#lexos.corpus.NullTokenizer.__call__","text":"Return the text as a single token. Source code in lexos\\corpus\\__init__.py 37 38 39 40 41 def __call__ ( self , text ): \"\"\"Return the text as a single token.\"\"\" words = [ text ] spaces = [ False ] return Doc ( self . vocab , words = words , spaces = spaces )","title":"__call__()"},{"location":"api/corpus/#lexos.corpus.NullTokenizer.__init__","text":"Initialise the tokeniser. Source code in lexos\\corpus\\__init__.py 33 34 35 def __init__ ( self , vocab ): \"\"\"Initialise the tokeniser.\"\"\" self . vocab = vocab","title":"__init__()"},{"location":"api/corpus/#lexos.corpus.Record","text":"Bases: BaseModel The main Record model. Source code in lexos\\corpus\\__init__.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 class Record ( BaseModel ): \"\"\"The main Record model.\"\"\" content : spacy . tokens . Doc = None filename : str = None id : int = 1 is_active : bool = True is_parsed : bool = False model : str = None name : str = None class Config : \"\"\"Config for the Record class.\"\"\" arbitrary_types_allowed = True extra = \"allow\" def __repr__ ( self ): \"\"\"Return a string representation of the record.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ if field != \"content\" } properties = { k : getattr ( self , k ) for k in self . get_properties ()} fields = { ** fields , ** properties } fields [ \"terms\" ] = \"Counter()\" fields [ \"text\" ] = self . preview fields [ \"tokens\" ] = f '[ { \", \" . join ([ t . text for t in self . content [ 0 : 5 ]]) } ...]' field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] + [ f \"content= { self . preview } \" ] return f \"Record( { ', ' . join ( sorted ( field_list )) } )\" @classmethod def get_properties ( cls ): \"\"\"Return a list of the properties of the Record class.\"\"\" return [ prop for prop in cls . __dict__ if isinstance ( cls . __dict__ [ prop ], property ) ] @property def preview ( self ): \"\"\"Return a preview of the record text.\"\"\" return f \" { self . content . text [ 0 : 50 ] } ...\" @property def terms ( self ): \"\"\"Return the terms in the record.\"\"\" return Counter ([ t . text for t in self . content ]) @property def text ( self ): \"\"\"Return the text of the record.\"\"\" return [ t . text for t in self . content ] @property def tokens ( self ): \"\"\"Return the tokens in the record.\"\"\" return self . content def num_terms ( self ): \"\"\"Return the number of terms.\"\"\" return len ( self . terms ) def num_tokens ( self ): \"\"\"Return the number of tokens.\"\"\" return len ( self . content ) def save ( self ): \"\"\"Serialise the record to disk.\"\"\" with open ( self . filename , \"wb\" ) as f : pickle . dump ( self , f ) def set ( self , k , v ): \"\"\"Set a record property.\"\"\" setattr ( self , k , v )","title":"Record"},{"location":"api/corpus/#lexos.corpus.Record.Config","text":"Config for the Record class. Source code in lexos\\corpus\\__init__.py 55 56 57 58 59 class Config : \"\"\"Config for the Record class.\"\"\" arbitrary_types_allowed = True extra = \"allow\"","title":"Config"},{"location":"api/corpus/#lexos.corpus.Record.__repr__","text":"Return a string representation of the record. Source code in lexos\\corpus\\__init__.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __repr__ ( self ): \"\"\"Return a string representation of the record.\"\"\" fields = { field : getattr ( self , field ) for field in self . __fields_set__ if field != \"content\" } properties = { k : getattr ( self , k ) for k in self . get_properties ()} fields = { ** fields , ** properties } fields [ \"terms\" ] = \"Counter()\" fields [ \"text\" ] = self . preview fields [ \"tokens\" ] = f '[ { \", \" . join ([ t . text for t in self . content [ 0 : 5 ]]) } ...]' field_list = [ f \" { k } = { v } \" for k , v in fields . items ()] + [ f \"content= { self . preview } \" ] return f \"Record( { ', ' . join ( sorted ( field_list )) } )\"","title":"__repr__()"},{"location":"api/corpus/#lexos.corpus.Record.get_properties","text":"Return a list of the properties of the Record class. Source code in lexos\\corpus\\__init__.py 78 79 80 81 82 83 @classmethod def get_properties ( cls ): \"\"\"Return a list of the properties of the Record class.\"\"\" return [ prop for prop in cls . __dict__ if isinstance ( cls . __dict__ [ prop ], property ) ]","title":"get_properties()"},{"location":"api/corpus/#lexos.corpus.Record.num_terms","text":"Return the number of terms. Source code in lexos\\corpus\\__init__.py 105 106 107 def num_terms ( self ): \"\"\"Return the number of terms.\"\"\" return len ( self . terms )","title":"num_terms()"},{"location":"api/corpus/#lexos.corpus.Record.num_tokens","text":"Return the number of tokens. Source code in lexos\\corpus\\__init__.py 109 110 111 def num_tokens ( self ): \"\"\"Return the number of tokens.\"\"\" return len ( self . content )","title":"num_tokens()"},{"location":"api/corpus/#lexos.corpus.Record.preview","text":"Return a preview of the record text. Source code in lexos\\corpus\\__init__.py 85 86 87 88 @property def preview ( self ): \"\"\"Return a preview of the record text.\"\"\" return f \" { self . content . text [ 0 : 50 ] } ...\"","title":"preview()"},{"location":"api/corpus/#lexos.corpus.Record.save","text":"Serialise the record to disk. Source code in lexos\\corpus\\__init__.py 113 114 115 116 def save ( self ): \"\"\"Serialise the record to disk.\"\"\" with open ( self . filename , \"wb\" ) as f : pickle . dump ( self , f )","title":"save()"},{"location":"api/corpus/#lexos.corpus.Record.set","text":"Set a record property. Source code in lexos\\corpus\\__init__.py 118 119 120 def set ( self , k , v ): \"\"\"Set a record property.\"\"\" setattr ( self , k , v )","title":"set()"},{"location":"api/corpus/#lexos.corpus.Record.terms","text":"Return the terms in the record. Source code in lexos\\corpus\\__init__.py 90 91 92 93 @property def terms ( self ): \"\"\"Return the terms in the record.\"\"\" return Counter ([ t . text for t in self . content ])","title":"terms()"},{"location":"api/corpus/#lexos.corpus.Record.text","text":"Return the text of the record. Source code in lexos\\corpus\\__init__.py 95 96 97 98 @property def text ( self ): \"\"\"Return the text of the record.\"\"\" return [ t . text for t in self . content ]","title":"text()"},{"location":"api/corpus/#lexos.corpus.Record.tokens","text":"Return the tokens in the record. Source code in lexos\\corpus\\__init__.py 100 101 102 103 @property def tokens ( self ): \"\"\"Return the tokens in the record.\"\"\" return self . content","title":"tokens()"},{"location":"api/cutter/","text":"Cutter \u00a4 Cutter is a module that divides files, texts, or documents into segments using separate classes (with cute codenames). Ginsu splits pre-tokenized documents into shorter segments. Machete splits raw text strings into shorter segments. Filesplit splits binary files into shorter files. Ginsu acts as a more precise cutter, using language-based tokenization, which provides greater accuracy in exchange for longer processing times. Machete , on the other hand, allows for faster processing in exchange for precision. Filesplit (aka \"Chainsaw\") allows files to be split before they are loaded, but potentially in the middle of linguistically significant units. A separate Milestones class is used to populate pre-tokenized texts with milestones for cutting.","title":"Cutter"},{"location":"api/cutter/#cutter","text":"Cutter is a module that divides files, texts, or documents into segments using separate classes (with cute codenames). Ginsu splits pre-tokenized documents into shorter segments. Machete splits raw text strings into shorter segments. Filesplit splits binary files into shorter files. Ginsu acts as a more precise cutter, using language-based tokenization, which provides greater accuracy in exchange for longer processing times. Machete , on the other hand, allows for faster processing in exchange for precision. Filesplit (aka \"Chainsaw\") allows files to be split before they are loaded, but potentially in the middle of linguistically significant units. A separate Milestones class is used to populate pre-tokenized texts with milestones for cutting.","title":"Cutter"},{"location":"api/cutter/filesplit/","text":"Filesplit \u00a4 The Filesplit class allows the user to cut binary files into numbered file segments with the format filename_1.txt , filename_2.txt , etc. It would typically be used as a first step in a workflow if a large file needs to be divided into many smaller files. The class is a fork of Ram Prakash Jayapalan's filesplit module with a few minor tweaks. The most important is that the split function takes a sep argument to allow the user to specify the separator between the filename and number in each generated file. lexos.cutter.filesplit.Filesplit \u00a4 Filesplit class. Source code in lexos\\cutter\\filesplit.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 class Filesplit : \"\"\"Filesplit class.\"\"\" def __init__ ( self ) -> None : \"\"\"Constructor. \"\"\" self . log = logging . getLogger ( __name__ ) . getChild ( self . __class__ . __name__ ) self . man_filename = \"fs_manifest.csv\" self . _buffer_size = 1000000 # 1 MB def __process_split ( self , fi : IO , fo : IO , split_size : int , carry_over : Optional [ str ], newline : bool = False , output_encoding : str = None , include_header : bool = False , header : str = None , ) -> Tuple : \"\"\"Split. the incoming stream. Args: fi (IO): Input-file like object that implements read() and readline() method. fo (IO): File-like object that implements write() method. split_size (int): File split size in bytes. newline (bool): When True, splits at newline on top of bytes. output_encoding (str): Split file encoding. include_header (bool): When True, first line is treated as header and each split receives the header. This flag is dependant on newline flag to be set to True as well. carry_over (str): Any carry over bytes to the next file. header (str): Header from the file if any. Returns: tuple: carry_over, output_size, header \"\"\" buffer_size = ( split_size if split_size < self . _buffer_size else self . _buffer_size ) buffer = 0 if not newline : while True : if carry_over : fo . write ( carry_over ) buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None continue chunk = fi . read ( buffer_size ) if not chunk : break chunk_size = ( len ( chunk ) if not output_encoding else len ( chunk . encode ( output_encoding )) ) if buffer + chunk_size <= split_size : fo . write ( chunk ) buffer += chunk_size else : carry_over = chunk break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , None else : if carry_over : if header : fo . write ( header ) fo . write ( carry_over ) if header : buffer += ( len ( carry_over ) + len ( header ) if not output_encoding else len ( carry_over . encode ( output_encoding )) + len ( header . encode ( output_encoding )) ) else : buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None for line in fi : if include_header and not header : header = line line_size = ( len ( line ) if not output_encoding else len ( line . encode ( output_encoding )) ) if buffer + line_size <= split_size : fo . write ( line ) buffer += line_size else : carry_over = line break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , header def split ( self , file : str , split_size : int , sep : str = \"_\" , output_dir : str = \".\" , callback : Callable = None , ** kwargs , ) -> None : \"\"\"Splits the file into chunks based on the newline char in the file. By default uses binary mode. Args: file (str): Path to the source file. split_size (int): File split size in bytes. sep (str): Separator to be used in the file name. output_dir (str): Output dir to write the split files. callback (Callable): Callback function [func (str, long, long)] that accepts three arguments - full file path to the destination, size of the file in bytes and line count. \"\"\" start_time = time . time () self . log . info ( \"Starting file split process...\" ) newline = kwargs . get ( \"newline\" , False ) include_header = kwargs . get ( \"include_header\" , False ) # If include_header is provided, default newline flag to True # as this should apply only to structured file. if include_header : newline = True encoding = kwargs . get ( \"encoding\" , None ) split_file_encoding = kwargs . get ( \"split_file_encoding\" , None ) f = ntpath . split ( file )[ 1 ] filename , ext = os . path . splitext ( f ) fi , man = None , None # Split file encoding cannot be specified without specifying # encoding which is required to read the file in text mode. if split_file_encoding and not encoding : raise ValueError ( \"`encoding` needs to be specified \" \"when providing `split_file_encoding`.\" ) try : # Determine the splits based off bytes when newline is set to False. # If newline is True, split only at newline considering the bytes # as well. if encoding and not split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) elif encoding and split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = file , mode = \"rb\" ) # Create file handler for the manifest file man_file = os . path . join ( output_dir , self . man_filename ) man = open ( file = man_file , mode = \"w+\" , encoding = \"utf-8\" ) # Create man file csv dict writer object man_writer = csv . DictWriter ( f = man , fieldnames = [ \"filename\" , \"filesize\" , \"encoding\" , \"header\" ] ) # Write man file header man_writer . writeheader () split_counter , carry_over , header = 1 , \"\" , None while carry_over is not None : split_file = os . path . join ( output_dir , f \" { filename }{ sep }{ split_counter }{ ext } \" ) fo = None try : if encoding and not split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = encoding ) elif encoding and split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = split_file_encoding ) else : fo = open ( file = split_file , mode = \"wb+\" ) carry_over , output_size , header = self . __process_split ( fi = fi , fo = fo , split_size = split_size , newline = newline , output_encoding = split_file_encoding , carry_over = carry_over , include_header = include_header , header = header , ) if callback : callback ( split_file , output_size ) # Write to manifest file di = { \"filename\" : ntpath . split ( split_file )[ 1 ], \"filesize\" : output_size , \"encoding\" : encoding , \"header\" : True if header else None , } man_writer . writerow ( di ) split_counter += 1 finally : if fo : fo . close () finally : if fi : fi . close () if man : man . close () run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" ) def merge ( self , input_dir : str , sep : str = \"_\" , output_file : str = None , manifest_file : str = None , callback : Callable = None , cleanup : bool = False , ) -> None : \"\"\"Merge the split files based off manifest file. Args: input_dir (str): Directory containing the split files and manifest file sep (str): Separator used in the file names. output_file (str): Final merged output file path. If not provided, the final merged filename is derived from the split filename and placed in the same input dir. manifest_file (str): Path to the manifest file. If not provided, the process will look for the file within the input_dir. callback (Callable): Callback function [func (str, long)] that accepts 2 arguments - path to destination, size of the file in bytes. cleanup (bool): If True, all the split files and the manifest file will be deleted after the merge, leaving behind the merged file. Raises: FileNotFoundError: If missing manifest and split files. NotADirectoryError: If input path is not a directory. \"\"\" start_time = time . time () self . log . info ( \"Starting file merge process...\" ) if not os . path . isdir ( input_dir ): raise NotADirectoryError ( \"Input directory is not a valid directory.\" ) manifest_file = ( os . path . join ( input_dir , self . man_filename ) if not manifest_file else manifest_file ) if not os . path . exists ( manifest_file ): raise FileNotFoundError ( \"Unable to locate manifest file.\" ) fo = None clear_output_file = True header_set = False try : # Read from manifest every split and merge to single file with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : encoding = line . get ( \"encoding\" , None ) header_avail = line . get ( \"header\" , None ) # Derive output filename from split file if output file # not provided if not output_file : f , ext = ntpath . splitext ( line . get ( \"filename\" )) output_filename = \"\" . join ([ f . rsplit ({ sep }, 1 )[ 0 ], ext ]) output_file = os . path . join ( input_dir , output_filename ) # Clear output file present before merging. This should # happen only once during beginning of merge if clear_output_file : if os . path . exists ( output_file ): os . remove ( output_file ) clear_output_file = False # Create write file handle based on the encoding from # man file if not fo : if encoding : fo = open ( file = output_file , mode = \"a\" , encoding = encoding ) else : fo = open ( file = output_file , mode = \"ab\" ) # Open the split file in read more and write contents to the # output file try : input_file = os . path . join ( input_dir , line . get ( \"filename\" )) if encoding : fi = open ( file = input_file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = input_file , mode = \"rb\" ) # Skip header if the flag is set to True if header_set : next ( fi ) for line in fi : if header_avail and not header_set : header_set = True fo . write ( line ) finally : if fi : fi . close () finally : if fo : fo . close () # Clean up files if required if cleanup : # Clean up split files with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : f = os . path . join ( input_dir , line . get ( \"filename\" )) if os . path . exists ( f ): os . remove ( f ) # Clean up man file if os . path . exists ( manifest_file ): os . remove ( manifest_file ) # Call the callback function with path and file size if callback : callback ( output_file , os . stat ( output_file ) . st_size ) run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" ) __init__ () \u00a4 Constructor. Source code in lexos\\cutter\\filesplit.py 22 23 24 25 26 def __init__ ( self ) -> None : \"\"\"Constructor. \"\"\" self . log = logging . getLogger ( __name__ ) . getChild ( self . __class__ . __name__ ) self . man_filename = \"fs_manifest.csv\" self . _buffer_size = 1000000 # 1 MB __process_split ( fi , fo , split_size , carry_over , newline = False , output_encoding = None , include_header = False , header = None ) \u00a4 Split. the incoming stream. Parameters: Name Type Description Default fi IO Input-file like object that implements read() and readline() required method. fo (IO): File-like object that implements write() method. split_size (int): File split size in bytes. newline (bool): When True, splits at newline on top of bytes. output_encoding (str): Split file encoding. include_header (bool): When True, first line is treated as header and each split receives the header. This flag is dependant on newline flag to be set to True as well. carry_over (str): Any carry over bytes to the next file. header (str): Header from the file if any. Returns: Name Type Description tuple Tuple carry_over, output_size, header Source code in lexos\\cutter\\filesplit.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __process_split ( self , fi : IO , fo : IO , split_size : int , carry_over : Optional [ str ], newline : bool = False , output_encoding : str = None , include_header : bool = False , header : str = None , ) -> Tuple : \"\"\"Split. the incoming stream. Args: fi (IO): Input-file like object that implements read() and readline() method. fo (IO): File-like object that implements write() method. split_size (int): File split size in bytes. newline (bool): When True, splits at newline on top of bytes. output_encoding (str): Split file encoding. include_header (bool): When True, first line is treated as header and each split receives the header. This flag is dependant on newline flag to be set to True as well. carry_over (str): Any carry over bytes to the next file. header (str): Header from the file if any. Returns: tuple: carry_over, output_size, header \"\"\" buffer_size = ( split_size if split_size < self . _buffer_size else self . _buffer_size ) buffer = 0 if not newline : while True : if carry_over : fo . write ( carry_over ) buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None continue chunk = fi . read ( buffer_size ) if not chunk : break chunk_size = ( len ( chunk ) if not output_encoding else len ( chunk . encode ( output_encoding )) ) if buffer + chunk_size <= split_size : fo . write ( chunk ) buffer += chunk_size else : carry_over = chunk break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , None else : if carry_over : if header : fo . write ( header ) fo . write ( carry_over ) if header : buffer += ( len ( carry_over ) + len ( header ) if not output_encoding else len ( carry_over . encode ( output_encoding )) + len ( header . encode ( output_encoding )) ) else : buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None for line in fi : if include_header and not header : header = line line_size = ( len ( line ) if not output_encoding else len ( line . encode ( output_encoding )) ) if buffer + line_size <= split_size : fo . write ( line ) buffer += line_size else : carry_over = line break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , header merge ( input_dir , sep = '_' , output_file = None , manifest_file = None , callback = None , cleanup = False ) \u00a4 Merge the split files based off manifest file. Parameters: Name Type Description Default input_dir str Directory containing the split files and manifest file required sep str Separator used in the file names. '_' output_file str Final merged output file path. If not provided, the final merged filename is derived from the split filename and placed in the same input dir. None manifest_file str Path to the manifest file. If not provided, the process will look for the file within the input_dir. None callback Callable Callback function [func (str, long)] that accepts 2 arguments - path to destination, size of the file in bytes. None cleanup bool If True, all the split files and the manifest file will be deleted after the merge, leaving behind the merged file. False Raises: Type Description FileNotFoundError If missing manifest and split files. NotADirectoryError If input path is not a directory. Source code in lexos\\cutter\\filesplit.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def merge ( self , input_dir : str , sep : str = \"_\" , output_file : str = None , manifest_file : str = None , callback : Callable = None , cleanup : bool = False , ) -> None : \"\"\"Merge the split files based off manifest file. Args: input_dir (str): Directory containing the split files and manifest file sep (str): Separator used in the file names. output_file (str): Final merged output file path. If not provided, the final merged filename is derived from the split filename and placed in the same input dir. manifest_file (str): Path to the manifest file. If not provided, the process will look for the file within the input_dir. callback (Callable): Callback function [func (str, long)] that accepts 2 arguments - path to destination, size of the file in bytes. cleanup (bool): If True, all the split files and the manifest file will be deleted after the merge, leaving behind the merged file. Raises: FileNotFoundError: If missing manifest and split files. NotADirectoryError: If input path is not a directory. \"\"\" start_time = time . time () self . log . info ( \"Starting file merge process...\" ) if not os . path . isdir ( input_dir ): raise NotADirectoryError ( \"Input directory is not a valid directory.\" ) manifest_file = ( os . path . join ( input_dir , self . man_filename ) if not manifest_file else manifest_file ) if not os . path . exists ( manifest_file ): raise FileNotFoundError ( \"Unable to locate manifest file.\" ) fo = None clear_output_file = True header_set = False try : # Read from manifest every split and merge to single file with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : encoding = line . get ( \"encoding\" , None ) header_avail = line . get ( \"header\" , None ) # Derive output filename from split file if output file # not provided if not output_file : f , ext = ntpath . splitext ( line . get ( \"filename\" )) output_filename = \"\" . join ([ f . rsplit ({ sep }, 1 )[ 0 ], ext ]) output_file = os . path . join ( input_dir , output_filename ) # Clear output file present before merging. This should # happen only once during beginning of merge if clear_output_file : if os . path . exists ( output_file ): os . remove ( output_file ) clear_output_file = False # Create write file handle based on the encoding from # man file if not fo : if encoding : fo = open ( file = output_file , mode = \"a\" , encoding = encoding ) else : fo = open ( file = output_file , mode = \"ab\" ) # Open the split file in read more and write contents to the # output file try : input_file = os . path . join ( input_dir , line . get ( \"filename\" )) if encoding : fi = open ( file = input_file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = input_file , mode = \"rb\" ) # Skip header if the flag is set to True if header_set : next ( fi ) for line in fi : if header_avail and not header_set : header_set = True fo . write ( line ) finally : if fi : fi . close () finally : if fo : fo . close () # Clean up files if required if cleanup : # Clean up split files with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : f = os . path . join ( input_dir , line . get ( \"filename\" )) if os . path . exists ( f ): os . remove ( f ) # Clean up man file if os . path . exists ( manifest_file ): os . remove ( manifest_file ) # Call the callback function with path and file size if callback : callback ( output_file , os . stat ( output_file ) . st_size ) run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" ) split ( file , split_size , sep = '_' , output_dir = '.' , callback = None , ** kwargs ) \u00a4 Splits the file into chunks based on the newline char in the file. By default uses binary mode. Parameters: Name Type Description Default file str Path to the source file. required split_size int File split size in bytes. required sep str Separator to be used in the file name. '_' output_dir str Output dir to write the split files. '.' callback Callable Callback function [func (str, long, long)] that accepts three arguments - full file path to the destination, size of the file in bytes and line count. None Source code in lexos\\cutter\\filesplit.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def split ( self , file : str , split_size : int , sep : str = \"_\" , output_dir : str = \".\" , callback : Callable = None , ** kwargs , ) -> None : \"\"\"Splits the file into chunks based on the newline char in the file. By default uses binary mode. Args: file (str): Path to the source file. split_size (int): File split size in bytes. sep (str): Separator to be used in the file name. output_dir (str): Output dir to write the split files. callback (Callable): Callback function [func (str, long, long)] that accepts three arguments - full file path to the destination, size of the file in bytes and line count. \"\"\" start_time = time . time () self . log . info ( \"Starting file split process...\" ) newline = kwargs . get ( \"newline\" , False ) include_header = kwargs . get ( \"include_header\" , False ) # If include_header is provided, default newline flag to True # as this should apply only to structured file. if include_header : newline = True encoding = kwargs . get ( \"encoding\" , None ) split_file_encoding = kwargs . get ( \"split_file_encoding\" , None ) f = ntpath . split ( file )[ 1 ] filename , ext = os . path . splitext ( f ) fi , man = None , None # Split file encoding cannot be specified without specifying # encoding which is required to read the file in text mode. if split_file_encoding and not encoding : raise ValueError ( \"`encoding` needs to be specified \" \"when providing `split_file_encoding`.\" ) try : # Determine the splits based off bytes when newline is set to False. # If newline is True, split only at newline considering the bytes # as well. if encoding and not split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) elif encoding and split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = file , mode = \"rb\" ) # Create file handler for the manifest file man_file = os . path . join ( output_dir , self . man_filename ) man = open ( file = man_file , mode = \"w+\" , encoding = \"utf-8\" ) # Create man file csv dict writer object man_writer = csv . DictWriter ( f = man , fieldnames = [ \"filename\" , \"filesize\" , \"encoding\" , \"header\" ] ) # Write man file header man_writer . writeheader () split_counter , carry_over , header = 1 , \"\" , None while carry_over is not None : split_file = os . path . join ( output_dir , f \" { filename }{ sep }{ split_counter }{ ext } \" ) fo = None try : if encoding and not split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = encoding ) elif encoding and split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = split_file_encoding ) else : fo = open ( file = split_file , mode = \"wb+\" ) carry_over , output_size , header = self . __process_split ( fi = fi , fo = fo , split_size = split_size , newline = newline , output_encoding = split_file_encoding , carry_over = carry_over , include_header = include_header , header = header , ) if callback : callback ( split_file , output_size ) # Write to manifest file di = { \"filename\" : ntpath . split ( split_file )[ 1 ], \"filesize\" : output_size , \"encoding\" : encoding , \"header\" : True if header else None , } man_writer . writerow ( di ) split_counter += 1 finally : if fo : fo . close () finally : if fi : fi . close () if man : man . close () run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" )","title":"Filesplit"},{"location":"api/cutter/filesplit/#filesplit","text":"The Filesplit class allows the user to cut binary files into numbered file segments with the format filename_1.txt , filename_2.txt , etc. It would typically be used as a first step in a workflow if a large file needs to be divided into many smaller files. The class is a fork of Ram Prakash Jayapalan's filesplit module with a few minor tweaks. The most important is that the split function takes a sep argument to allow the user to specify the separator between the filename and number in each generated file.","title":"Filesplit"},{"location":"api/cutter/filesplit/#lexos.cutter.filesplit.Filesplit","text":"Filesplit class. Source code in lexos\\cutter\\filesplit.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 class Filesplit : \"\"\"Filesplit class.\"\"\" def __init__ ( self ) -> None : \"\"\"Constructor. \"\"\" self . log = logging . getLogger ( __name__ ) . getChild ( self . __class__ . __name__ ) self . man_filename = \"fs_manifest.csv\" self . _buffer_size = 1000000 # 1 MB def __process_split ( self , fi : IO , fo : IO , split_size : int , carry_over : Optional [ str ], newline : bool = False , output_encoding : str = None , include_header : bool = False , header : str = None , ) -> Tuple : \"\"\"Split. the incoming stream. Args: fi (IO): Input-file like object that implements read() and readline() method. fo (IO): File-like object that implements write() method. split_size (int): File split size in bytes. newline (bool): When True, splits at newline on top of bytes. output_encoding (str): Split file encoding. include_header (bool): When True, first line is treated as header and each split receives the header. This flag is dependant on newline flag to be set to True as well. carry_over (str): Any carry over bytes to the next file. header (str): Header from the file if any. Returns: tuple: carry_over, output_size, header \"\"\" buffer_size = ( split_size if split_size < self . _buffer_size else self . _buffer_size ) buffer = 0 if not newline : while True : if carry_over : fo . write ( carry_over ) buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None continue chunk = fi . read ( buffer_size ) if not chunk : break chunk_size = ( len ( chunk ) if not output_encoding else len ( chunk . encode ( output_encoding )) ) if buffer + chunk_size <= split_size : fo . write ( chunk ) buffer += chunk_size else : carry_over = chunk break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , None else : if carry_over : if header : fo . write ( header ) fo . write ( carry_over ) if header : buffer += ( len ( carry_over ) + len ( header ) if not output_encoding else len ( carry_over . encode ( output_encoding )) + len ( header . encode ( output_encoding )) ) else : buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None for line in fi : if include_header and not header : header = line line_size = ( len ( line ) if not output_encoding else len ( line . encode ( output_encoding )) ) if buffer + line_size <= split_size : fo . write ( line ) buffer += line_size else : carry_over = line break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , header def split ( self , file : str , split_size : int , sep : str = \"_\" , output_dir : str = \".\" , callback : Callable = None , ** kwargs , ) -> None : \"\"\"Splits the file into chunks based on the newline char in the file. By default uses binary mode. Args: file (str): Path to the source file. split_size (int): File split size in bytes. sep (str): Separator to be used in the file name. output_dir (str): Output dir to write the split files. callback (Callable): Callback function [func (str, long, long)] that accepts three arguments - full file path to the destination, size of the file in bytes and line count. \"\"\" start_time = time . time () self . log . info ( \"Starting file split process...\" ) newline = kwargs . get ( \"newline\" , False ) include_header = kwargs . get ( \"include_header\" , False ) # If include_header is provided, default newline flag to True # as this should apply only to structured file. if include_header : newline = True encoding = kwargs . get ( \"encoding\" , None ) split_file_encoding = kwargs . get ( \"split_file_encoding\" , None ) f = ntpath . split ( file )[ 1 ] filename , ext = os . path . splitext ( f ) fi , man = None , None # Split file encoding cannot be specified without specifying # encoding which is required to read the file in text mode. if split_file_encoding and not encoding : raise ValueError ( \"`encoding` needs to be specified \" \"when providing `split_file_encoding`.\" ) try : # Determine the splits based off bytes when newline is set to False. # If newline is True, split only at newline considering the bytes # as well. if encoding and not split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) elif encoding and split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = file , mode = \"rb\" ) # Create file handler for the manifest file man_file = os . path . join ( output_dir , self . man_filename ) man = open ( file = man_file , mode = \"w+\" , encoding = \"utf-8\" ) # Create man file csv dict writer object man_writer = csv . DictWriter ( f = man , fieldnames = [ \"filename\" , \"filesize\" , \"encoding\" , \"header\" ] ) # Write man file header man_writer . writeheader () split_counter , carry_over , header = 1 , \"\" , None while carry_over is not None : split_file = os . path . join ( output_dir , f \" { filename }{ sep }{ split_counter }{ ext } \" ) fo = None try : if encoding and not split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = encoding ) elif encoding and split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = split_file_encoding ) else : fo = open ( file = split_file , mode = \"wb+\" ) carry_over , output_size , header = self . __process_split ( fi = fi , fo = fo , split_size = split_size , newline = newline , output_encoding = split_file_encoding , carry_over = carry_over , include_header = include_header , header = header , ) if callback : callback ( split_file , output_size ) # Write to manifest file di = { \"filename\" : ntpath . split ( split_file )[ 1 ], \"filesize\" : output_size , \"encoding\" : encoding , \"header\" : True if header else None , } man_writer . writerow ( di ) split_counter += 1 finally : if fo : fo . close () finally : if fi : fi . close () if man : man . close () run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" ) def merge ( self , input_dir : str , sep : str = \"_\" , output_file : str = None , manifest_file : str = None , callback : Callable = None , cleanup : bool = False , ) -> None : \"\"\"Merge the split files based off manifest file. Args: input_dir (str): Directory containing the split files and manifest file sep (str): Separator used in the file names. output_file (str): Final merged output file path. If not provided, the final merged filename is derived from the split filename and placed in the same input dir. manifest_file (str): Path to the manifest file. If not provided, the process will look for the file within the input_dir. callback (Callable): Callback function [func (str, long)] that accepts 2 arguments - path to destination, size of the file in bytes. cleanup (bool): If True, all the split files and the manifest file will be deleted after the merge, leaving behind the merged file. Raises: FileNotFoundError: If missing manifest and split files. NotADirectoryError: If input path is not a directory. \"\"\" start_time = time . time () self . log . info ( \"Starting file merge process...\" ) if not os . path . isdir ( input_dir ): raise NotADirectoryError ( \"Input directory is not a valid directory.\" ) manifest_file = ( os . path . join ( input_dir , self . man_filename ) if not manifest_file else manifest_file ) if not os . path . exists ( manifest_file ): raise FileNotFoundError ( \"Unable to locate manifest file.\" ) fo = None clear_output_file = True header_set = False try : # Read from manifest every split and merge to single file with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : encoding = line . get ( \"encoding\" , None ) header_avail = line . get ( \"header\" , None ) # Derive output filename from split file if output file # not provided if not output_file : f , ext = ntpath . splitext ( line . get ( \"filename\" )) output_filename = \"\" . join ([ f . rsplit ({ sep }, 1 )[ 0 ], ext ]) output_file = os . path . join ( input_dir , output_filename ) # Clear output file present before merging. This should # happen only once during beginning of merge if clear_output_file : if os . path . exists ( output_file ): os . remove ( output_file ) clear_output_file = False # Create write file handle based on the encoding from # man file if not fo : if encoding : fo = open ( file = output_file , mode = \"a\" , encoding = encoding ) else : fo = open ( file = output_file , mode = \"ab\" ) # Open the split file in read more and write contents to the # output file try : input_file = os . path . join ( input_dir , line . get ( \"filename\" )) if encoding : fi = open ( file = input_file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = input_file , mode = \"rb\" ) # Skip header if the flag is set to True if header_set : next ( fi ) for line in fi : if header_avail and not header_set : header_set = True fo . write ( line ) finally : if fi : fi . close () finally : if fo : fo . close () # Clean up files if required if cleanup : # Clean up split files with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : f = os . path . join ( input_dir , line . get ( \"filename\" )) if os . path . exists ( f ): os . remove ( f ) # Clean up man file if os . path . exists ( manifest_file ): os . remove ( manifest_file ) # Call the callback function with path and file size if callback : callback ( output_file , os . stat ( output_file ) . st_size ) run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" )","title":"Filesplit"},{"location":"api/cutter/filesplit/#lexos.cutter.filesplit.Filesplit.__init__","text":"Constructor. Source code in lexos\\cutter\\filesplit.py 22 23 24 25 26 def __init__ ( self ) -> None : \"\"\"Constructor. \"\"\" self . log = logging . getLogger ( __name__ ) . getChild ( self . __class__ . __name__ ) self . man_filename = \"fs_manifest.csv\" self . _buffer_size = 1000000 # 1 MB","title":"__init__()"},{"location":"api/cutter/filesplit/#lexos.cutter.filesplit.Filesplit.__process_split","text":"Split. the incoming stream. Parameters: Name Type Description Default fi IO Input-file like object that implements read() and readline() required method. fo (IO): File-like object that implements write() method. split_size (int): File split size in bytes. newline (bool): When True, splits at newline on top of bytes. output_encoding (str): Split file encoding. include_header (bool): When True, first line is treated as header and each split receives the header. This flag is dependant on newline flag to be set to True as well. carry_over (str): Any carry over bytes to the next file. header (str): Header from the file if any. Returns: Name Type Description tuple Tuple carry_over, output_size, header Source code in lexos\\cutter\\filesplit.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def __process_split ( self , fi : IO , fo : IO , split_size : int , carry_over : Optional [ str ], newline : bool = False , output_encoding : str = None , include_header : bool = False , header : str = None , ) -> Tuple : \"\"\"Split. the incoming stream. Args: fi (IO): Input-file like object that implements read() and readline() method. fo (IO): File-like object that implements write() method. split_size (int): File split size in bytes. newline (bool): When True, splits at newline on top of bytes. output_encoding (str): Split file encoding. include_header (bool): When True, first line is treated as header and each split receives the header. This flag is dependant on newline flag to be set to True as well. carry_over (str): Any carry over bytes to the next file. header (str): Header from the file if any. Returns: tuple: carry_over, output_size, header \"\"\" buffer_size = ( split_size if split_size < self . _buffer_size else self . _buffer_size ) buffer = 0 if not newline : while True : if carry_over : fo . write ( carry_over ) buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None continue chunk = fi . read ( buffer_size ) if not chunk : break chunk_size = ( len ( chunk ) if not output_encoding else len ( chunk . encode ( output_encoding )) ) if buffer + chunk_size <= split_size : fo . write ( chunk ) buffer += chunk_size else : carry_over = chunk break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , None else : if carry_over : if header : fo . write ( header ) fo . write ( carry_over ) if header : buffer += ( len ( carry_over ) + len ( header ) if not output_encoding else len ( carry_over . encode ( output_encoding )) + len ( header . encode ( output_encoding )) ) else : buffer += ( len ( carry_over ) if not output_encoding else len ( carry_over . encode ( output_encoding )) ) carry_over = None for line in fi : if include_header and not header : header = line line_size = ( len ( line ) if not output_encoding else len ( line . encode ( output_encoding )) ) if buffer + line_size <= split_size : fo . write ( line ) buffer += line_size else : carry_over = line break # Set the carry_over to None if there is no carry_over available if not carry_over : carry_over = None return carry_over , buffer , header","title":"__process_split()"},{"location":"api/cutter/filesplit/#lexos.cutter.filesplit.Filesplit.merge","text":"Merge the split files based off manifest file. Parameters: Name Type Description Default input_dir str Directory containing the split files and manifest file required sep str Separator used in the file names. '_' output_file str Final merged output file path. If not provided, the final merged filename is derived from the split filename and placed in the same input dir. None manifest_file str Path to the manifest file. If not provided, the process will look for the file within the input_dir. None callback Callable Callback function [func (str, long)] that accepts 2 arguments - path to destination, size of the file in bytes. None cleanup bool If True, all the split files and the manifest file will be deleted after the merge, leaving behind the merged file. False Raises: Type Description FileNotFoundError If missing manifest and split files. NotADirectoryError If input path is not a directory. Source code in lexos\\cutter\\filesplit.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def merge ( self , input_dir : str , sep : str = \"_\" , output_file : str = None , manifest_file : str = None , callback : Callable = None , cleanup : bool = False , ) -> None : \"\"\"Merge the split files based off manifest file. Args: input_dir (str): Directory containing the split files and manifest file sep (str): Separator used in the file names. output_file (str): Final merged output file path. If not provided, the final merged filename is derived from the split filename and placed in the same input dir. manifest_file (str): Path to the manifest file. If not provided, the process will look for the file within the input_dir. callback (Callable): Callback function [func (str, long)] that accepts 2 arguments - path to destination, size of the file in bytes. cleanup (bool): If True, all the split files and the manifest file will be deleted after the merge, leaving behind the merged file. Raises: FileNotFoundError: If missing manifest and split files. NotADirectoryError: If input path is not a directory. \"\"\" start_time = time . time () self . log . info ( \"Starting file merge process...\" ) if not os . path . isdir ( input_dir ): raise NotADirectoryError ( \"Input directory is not a valid directory.\" ) manifest_file = ( os . path . join ( input_dir , self . man_filename ) if not manifest_file else manifest_file ) if not os . path . exists ( manifest_file ): raise FileNotFoundError ( \"Unable to locate manifest file.\" ) fo = None clear_output_file = True header_set = False try : # Read from manifest every split and merge to single file with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : encoding = line . get ( \"encoding\" , None ) header_avail = line . get ( \"header\" , None ) # Derive output filename from split file if output file # not provided if not output_file : f , ext = ntpath . splitext ( line . get ( \"filename\" )) output_filename = \"\" . join ([ f . rsplit ({ sep }, 1 )[ 0 ], ext ]) output_file = os . path . join ( input_dir , output_filename ) # Clear output file present before merging. This should # happen only once during beginning of merge if clear_output_file : if os . path . exists ( output_file ): os . remove ( output_file ) clear_output_file = False # Create write file handle based on the encoding from # man file if not fo : if encoding : fo = open ( file = output_file , mode = \"a\" , encoding = encoding ) else : fo = open ( file = output_file , mode = \"ab\" ) # Open the split file in read more and write contents to the # output file try : input_file = os . path . join ( input_dir , line . get ( \"filename\" )) if encoding : fi = open ( file = input_file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = input_file , mode = \"rb\" ) # Skip header if the flag is set to True if header_set : next ( fi ) for line in fi : if header_avail and not header_set : header_set = True fo . write ( line ) finally : if fi : fi . close () finally : if fo : fo . close () # Clean up files if required if cleanup : # Clean up split files with open ( file = manifest_file , mode = \"r\" , encoding = \"utf-8\" ) as man_fh : man_reader = csv . DictReader ( f = man_fh ) for line in man_reader : f = os . path . join ( input_dir , line . get ( \"filename\" )) if os . path . exists ( f ): os . remove ( f ) # Clean up man file if os . path . exists ( manifest_file ): os . remove ( manifest_file ) # Call the callback function with path and file size if callback : callback ( output_file , os . stat ( output_file ) . st_size ) run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" )","title":"merge()"},{"location":"api/cutter/filesplit/#lexos.cutter.filesplit.Filesplit.split","text":"Splits the file into chunks based on the newline char in the file. By default uses binary mode. Parameters: Name Type Description Default file str Path to the source file. required split_size int File split size in bytes. required sep str Separator to be used in the file name. '_' output_dir str Output dir to write the split files. '.' callback Callable Callback function [func (str, long, long)] that accepts three arguments - full file path to the destination, size of the file in bytes and line count. None Source code in lexos\\cutter\\filesplit.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def split ( self , file : str , split_size : int , sep : str = \"_\" , output_dir : str = \".\" , callback : Callable = None , ** kwargs , ) -> None : \"\"\"Splits the file into chunks based on the newline char in the file. By default uses binary mode. Args: file (str): Path to the source file. split_size (int): File split size in bytes. sep (str): Separator to be used in the file name. output_dir (str): Output dir to write the split files. callback (Callable): Callback function [func (str, long, long)] that accepts three arguments - full file path to the destination, size of the file in bytes and line count. \"\"\" start_time = time . time () self . log . info ( \"Starting file split process...\" ) newline = kwargs . get ( \"newline\" , False ) include_header = kwargs . get ( \"include_header\" , False ) # If include_header is provided, default newline flag to True # as this should apply only to structured file. if include_header : newline = True encoding = kwargs . get ( \"encoding\" , None ) split_file_encoding = kwargs . get ( \"split_file_encoding\" , None ) f = ntpath . split ( file )[ 1 ] filename , ext = os . path . splitext ( f ) fi , man = None , None # Split file encoding cannot be specified without specifying # encoding which is required to read the file in text mode. if split_file_encoding and not encoding : raise ValueError ( \"`encoding` needs to be specified \" \"when providing `split_file_encoding`.\" ) try : # Determine the splits based off bytes when newline is set to False. # If newline is True, split only at newline considering the bytes # as well. if encoding and not split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) elif encoding and split_file_encoding : fi = open ( file = file , mode = \"r\" , encoding = encoding ) else : fi = open ( file = file , mode = \"rb\" ) # Create file handler for the manifest file man_file = os . path . join ( output_dir , self . man_filename ) man = open ( file = man_file , mode = \"w+\" , encoding = \"utf-8\" ) # Create man file csv dict writer object man_writer = csv . DictWriter ( f = man , fieldnames = [ \"filename\" , \"filesize\" , \"encoding\" , \"header\" ] ) # Write man file header man_writer . writeheader () split_counter , carry_over , header = 1 , \"\" , None while carry_over is not None : split_file = os . path . join ( output_dir , f \" { filename }{ sep }{ split_counter }{ ext } \" ) fo = None try : if encoding and not split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = encoding ) elif encoding and split_file_encoding : fo = open ( file = split_file , mode = \"w+\" , encoding = split_file_encoding ) else : fo = open ( file = split_file , mode = \"wb+\" ) carry_over , output_size , header = self . __process_split ( fi = fi , fo = fo , split_size = split_size , newline = newline , output_encoding = split_file_encoding , carry_over = carry_over , include_header = include_header , header = header , ) if callback : callback ( split_file , output_size ) # Write to manifest file di = { \"filename\" : ntpath . split ( split_file )[ 1 ], \"filesize\" : output_size , \"encoding\" : encoding , \"header\" : True if header else None , } man_writer . writerow ( di ) split_counter += 1 finally : if fo : fo . close () finally : if fi : fi . close () if man : man . close () run_time = round (( time . time () - start_time ) / 60 ) self . log . info ( f \"Process complete.\" ) self . log . info ( f \"Run time(m): { run_time } \" )","title":"split()"},{"location":"api/cutter/ginsu/","text":"Ginsu \u00a4 The Ginsu class allows the user to cut documents pre-tokenized with spaCy. Documents can be split into a pre-determined number of segments, based on the number of tokens, or based on tokens defined as milestones. lexos.cutter.ginsu.Ginsu \u00a4 Codename Ginsu. https://www.youtube.com/watch?v=Sv_uL1Ar0oM . Note: Does not work on wood or watermelons. To do Allow the user to set token._.is_milestone on the fly. Source code in lexos\\cutter\\ginsu.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 class Ginsu : \"\"\"Codename Ginsu. https://www.youtube.com/watch?v=Sv_uL1Ar0oM. Note: Does not work on wood or watermelons. To do: - Allow the user to set token._.is_milestone on the fly. \"\"\" def __init__ ( self , config : dict = None ): \"\"\"Initialize the class.\"\"\" self . config = config def _chunk_doc ( self , doc : list , n : int = 1000 ) -> Callable : \"\"\"Yield successive n-sized chunks from a spaCy doc by a fixed number of tokens. Args: docs (list): A list of spaCy docs. n (int): The number of tokens to split on. Returns: list: A list of spaCy docs. \"\"\" for i in range ( 0 , len ( doc ), n ): yield doc [ i : i + n ] . as_doc () def _create_overlapping_segments ( self , segments : List [ spacy . tokens . doc . Doc ], overlap : int , ) -> List [ spacy . tokens . doc . Doc ]: \"\"\"Create overlapping segments. Args: segments (List[spacy.tokens.doc.Doc]): A list of spaCy docs. overlap (int): The number of tokens to overlap. Returns: List[spacy.tokens.doc.Doc]: A list of spaCy docs. \"\"\" overlapped_segs = [] for i , seg in enumerate ( segments ): if i == 0 : # Get the first overlap tokens from the second segment overlapped_segs . append ( Doc . from_docs ([ seg , segments [ i + 1 ][: overlap ] . as_doc ()]) ) else : if i < len ( segments ) - 1 : # Get the last overlap tokens from the previous segment # and the first from the next segment overlapped_segs . append ( Doc . from_docs ( [ segments [ i - 1 ][ - overlap :] . as_doc (), seg , segments [ i + 1 ][: overlap ] . as_doc (), ] ) ) else : # Get the last overlap tokens from the previous segment overlapped_segs . append ( Doc . from_docs ([ segments [ i - 1 ][ - overlap :] . as_doc (), seg ]) ) return overlapped_segs def _get_milestone_result ( self , attr : str , token : object , value : Union [ str , tuple ] ) -> bool : \"\"\"Test a token for a match. If value is a tuple, it must have the form `(pattern, operator)`, where pattern is the string or regex pattern to match, and operator is the method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: attr (str): The spaCy token attribute to test. token (object): The token to test. value (Union[str, tuple]): The value to test. Returns: bool: Whether the token matches the query. \"\"\" if attr == \"is_milestone\" : if token . _ . is_milestone == True : return True else : return False elif isinstance ( value , str ) or isinstance ( value , bool ): if getattr ( token , attr ) == value : return True else : return False elif isinstance ( value , tuple ): pattern = value [ 0 ] operator = value [ 1 ] if operator == \"in\" : if getattr ( token , attr ) in pattern : return True else : return False elif operator == \"not_in\" : if getattr ( token , attr ) not in pattern : return True else : return False elif operator == \"starts_with\" : if getattr ( token , attr ) . startswith ( pattern ): return True else : return False elif operator == \"ends_with\" : if getattr ( token , attr ) . endswith ( pattern ): return True else : return False elif operator == \"re_match\" : if re . match ( pattern , getattr ( token , attr )): return True else : return False elif operator == \"re_search\" : if re . search ( pattern , getattr ( token , attr )): return True else : return False def _matches_milestone ( self , token : object , milestone : Union [ dict , list , str ] ) -> bool : \"\"\"Test a token for a match. Args: token (object): The token to test. milestone (Union[dict, str]): A variable representing the value(s) to be matched. Returns: bool: Whether the token matches the query. \"\"\" if isinstance ( milestone , str ): if token . text == milestone : return True else : return False elif isinstance ( milestone , list ): if token . text in milestone : return True else : return False elif isinstance ( milestone , dict ): return self . _parse_milestone_dict ( token , milestone ) def _parse_milestone_dict ( self , token , milestone_dict ): \"\"\"Parse a milestone dictionary and get results for each criterion. Key-value pairs in `milestone_dict` will be interpreted as token attributes and their values. If the value is given as a tuple, it must have the form `(pattern, operator)`, where the pattern is the string or regex pattern to match, and the operator is the matching method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: token (object): The token to test. milestone_dict (dict): A dict in the format given above. Returns: bool: Whether the token matches the query. \"\"\" # Get lists and_ = milestone_dict . get ( \"and\" , {}) or_ = milestone_dict . get ( \"or\" , {}) and_valid = True or_valid = False # Iterate through the and_ list for query_dict in and_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token fails to satisfy all criteria if self . _get_milestone_result ( attr , token , value ): and_valid = True else : and_valid = False # Iterate through the or_ list for query_dict in or_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token satisfies at least one criterion if self . _get_milestone_result ( attr , token , value ): or_valid = True # Determine if there is a match with \"and\" and \"or\" if and_valid and or_valid : is_match = True elif and_valid and not or_valid : is_match = True elif not and_valid and or_valid : is_match = True else : is_match = False # Handle keywords other than \"and\" and \"or\" for attr , value in milestone_dict . items (): if attr not in [ \"and\" , \"or\" ]: if self . _get_milestone_result ( attr , token , value ): is_match = True else : is_match = False # Return the result return is_match def _split_doc ( self , doc : spacy . tokens . doc . Doc , n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Split a spaCy doc into chunks by a fixed number of tokens. Args: doc (spacy.tokens.doc.Doc): A spaCy doc. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of spaCy docs. \"\"\" segments = list ( self . _chunk_doc ( doc , n )) # Apply the merge threshold if len ( segments [ - 1 ]) < n * merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single doc segments [ - 1 ] = Doc . from_docs ([ segments [ - 1 ], last_seg ]) if overlap : return self . _create_overlapping_segments ( segments , overlap ) else : return segments def _splitn_doc ( self , doc : spacy . tokens . doc . Doc , n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Get a specific number of sequential segments from a spaCy doc. Args: doc (spacy.tokens.doc.Doc): A spaCy doc. n (int): The number of segments to create. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of spaCy doc segments. Note: For this implementation, see https://stackoverflow.com/a/54802737. See `split()` for more information on the validation model. \"\"\" # Validate input try : model = SplitModel ( docs = doc , n = n , merge_threshold = merge_threshold , overlap = overlap ) except Exception as e : raise LexosException ( e ) # Get the number of tokens per segment (d) and the remaining tokens (r) d , r = divmod ( len ( doc ), model . n ) # Get the segments segments = [] for i in range ( model . n ): index = ( d + 1 ) * ( i if i < r else r ) + d * ( 0 if i < r else i - r ) segments . append ( doc [ index : index + ( d + 1 if i < r else d )] . as_doc ()) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single doc segments [ - 1 ] = Doc . from_docs ([ segments [ - 1 ], last_seg ]) if overlap : segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in segments ] # Convert the list of list segments to a list of spaCy doc segments segmented_doc = [] for segment in segments : if isinstance ( segment , spacy . tokens . doc . Doc ): segmented_doc . append ( segment ) else : segmented_doc . append ( segment . as_doc ()) return segmented_doc def _split_doc_on_milestones ( self , doc : spacy . tokens . doc . Doc , milestone : Union [ dict , str ], preserve_milestones : bool = True , ): \"\"\"Split document on a milestone. Args: doc (spacy.tokens.doc.Doc): The document to be split. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. \"\"\" segments = [] indices = [ i for i , x in enumerate ( doc ) if self . _matches_milestone ( x , milestone ) ] for start , end in zip ([ 0 , * indices ], [ * indices , len ( doc )]): if preserve_milestones : segments . append ( doc [ start : end ] . as_doc ()) else : segments . append ( doc [ start + 1 : end ] . as_doc ()) return segments def merge ( self , segments : List [ spacy . tokens . doc . Doc ]) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[spacy.tokens.doc.Doc]): The list of segments to merge. Returns: spacy.tokens.doc.Doc: The merged doc. \"\"\" return Doc . from_docs ( segments ) def split ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , ) -> List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]]: \"\"\"Split spaCy docs into chunks by a fixed number of tokens. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: List[Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]]: A list of spaCy docs (segments) for the input doc or a list of segment lists for multiple docs. Note: `n`, `merge_threshold`, and `overlap` are referenced from the validated model in case Pydantic has coerced them into the expected data types. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments def splitn ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Get a specific number of sequential segments from a spaCy doc or docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of segments to create. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of lists with where the inner list is the resulting segments for each doc. Note: For this implementation, see https://stackoverflow.com/a/54802737. See `split()` for more information on the validation model. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _splitn_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _splitn_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments def split_on_milestones ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], preserve_milestones : bool = True , ): \"\"\"Split document on a milestone. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): The document(s) to be split. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. \"\"\" # Validate input try : _ = SplitMilestoneModel ( docs = docs , milestone = milestone , preserve_milestones = preserve_milestones ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc_on_milestones ( docs , milestone , preserve_milestones ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc_on_milestones ( doc , milestone , preserve_milestones ) ) return all_segments __init__ ( config = None ) \u00a4 Initialize the class. Source code in lexos\\cutter\\ginsu.py 55 56 57 def __init__ ( self , config : dict = None ): \"\"\"Initialize the class.\"\"\" self . config = config merge ( segments ) \u00a4 Merge a list of segments into a single string. Parameters: Name Type Description Default segments List [ spacy . tokens . doc . Doc ] The list of segments to merge. required Returns: Type Description str spacy.tokens.doc.Doc: The merged doc. Source code in lexos\\cutter\\ginsu.py 374 375 376 377 378 379 380 381 382 383 def merge ( self , segments : List [ spacy . tokens . doc . Doc ]) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[spacy.tokens.doc.Doc]): The list of segments to merge. Returns: spacy.tokens.doc.Doc: The merged doc. \"\"\" return Doc . from_docs ( segments ) split ( docs , n = 1000 , merge_threshold = 0.5 , overlap = None ) \u00a4 Split spaCy docs into chunks by a fixed number of tokens. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] A spaCy doc or list of spaCy docs. required n int The number of tokens to split on. 1000 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None Returns: Type Description List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]] List[Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]]: A list of spaCy docs (segments) for List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]] the input doc or a list of segment lists for multiple docs. Note n , merge_threshold , and overlap are referenced from the validated model in case Pydantic has coerced them into the expected data types. Source code in lexos\\cutter\\ginsu.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def split ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , ) -> List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]]: \"\"\"Split spaCy docs into chunks by a fixed number of tokens. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: List[Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]]: A list of spaCy docs (segments) for the input doc or a list of segment lists for multiple docs. Note: `n`, `merge_threshold`, and `overlap` are referenced from the validated model in case Pydantic has coerced them into the expected data types. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments split_on_milestones ( docs , milestone , preserve_milestones = True ) \u00a4 Split document on a milestone. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] The document(s) to be split. required milestone Union [ dict , str ] A variable representing the value(s) to be matched. required preserve_milestones bool If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. True Source code in lexos\\cutter\\ginsu.py 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def split_on_milestones ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], preserve_milestones : bool = True , ): \"\"\"Split document on a milestone. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): The document(s) to be split. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. \"\"\" # Validate input try : _ = SplitMilestoneModel ( docs = docs , milestone = milestone , preserve_milestones = preserve_milestones ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc_on_milestones ( docs , milestone , preserve_milestones ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc_on_milestones ( doc , milestone , preserve_milestones ) ) return all_segments splitn ( docs , n = 2 , merge_threshold = 0.5 , overlap = None ) \u00a4 Get a specific number of sequential segments from a spaCy doc or docs. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] A spaCy doc or list of spaCy docs. required n int The number of segments to create. 2 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None Returns: Name Type Description list list A list of lists with where the inner list is the resulting segments list for each doc. Note For this implementation, see https://stackoverflow.com/a/54802737 . See split() for more information on the validation model. Source code in lexos\\cutter\\ginsu.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def splitn ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Get a specific number of sequential segments from a spaCy doc or docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of segments to create. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of lists with where the inner list is the resulting segments for each doc. Note: For this implementation, see https://stackoverflow.com/a/54802737. See `split()` for more information on the validation model. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _splitn_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _splitn_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments lexos.cutter.ginsu.SplitMilestoneModel \u00a4 Bases: BaseModel Validate the input for split functions. Source code in lexos\\cutter\\ginsu.py 17 18 19 20 21 22 23 24 25 26 27 class SplitMilestoneModel ( BaseModel ): \"\"\"Validate the input for split functions.\"\"\" docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] milestone : Union [ dict , str ] preserve_milestones : Optional [ bool ] = True class Config : \"\"\"Config for SplitMilestoneModel.\"\"\" arbitrary_types_allowed = True Config \u00a4 Config for SplitMilestoneModel. Source code in lexos\\cutter\\ginsu.py 24 25 26 27 class Config : \"\"\"Config for SplitMilestoneModel.\"\"\" arbitrary_types_allowed = True lexos.cutter.ginsu.SplitModel \u00a4 Bases: BaseModel Validate the input for split functions. Source code in lexos\\cutter\\ginsu.py 30 31 32 33 34 35 36 37 38 39 40 41 class SplitModel ( BaseModel ): \"\"\"Validate the input for split functions.\"\"\" docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] n : Optional [ int ] = 1000 merge_threshold : Optional [ float ] = 0.5 overlap : Optional [ int ] = None class Config : \"\"\"Config for SplitModel.\"\"\" arbitrary_types_allowed = True Config \u00a4 Config for SplitModel. Source code in lexos\\cutter\\ginsu.py 38 39 40 41 class Config : \"\"\"Config for SplitModel.\"\"\" arbitrary_types_allowed = True","title":"Ginsu"},{"location":"api/cutter/ginsu/#ginsu","text":"The Ginsu class allows the user to cut documents pre-tokenized with spaCy. Documents can be split into a pre-determined number of segments, based on the number of tokens, or based on tokens defined as milestones.","title":"Ginsu"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.Ginsu","text":"Codename Ginsu. https://www.youtube.com/watch?v=Sv_uL1Ar0oM . Note: Does not work on wood or watermelons. To do Allow the user to set token._.is_milestone on the fly. Source code in lexos\\cutter\\ginsu.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 class Ginsu : \"\"\"Codename Ginsu. https://www.youtube.com/watch?v=Sv_uL1Ar0oM. Note: Does not work on wood or watermelons. To do: - Allow the user to set token._.is_milestone on the fly. \"\"\" def __init__ ( self , config : dict = None ): \"\"\"Initialize the class.\"\"\" self . config = config def _chunk_doc ( self , doc : list , n : int = 1000 ) -> Callable : \"\"\"Yield successive n-sized chunks from a spaCy doc by a fixed number of tokens. Args: docs (list): A list of spaCy docs. n (int): The number of tokens to split on. Returns: list: A list of spaCy docs. \"\"\" for i in range ( 0 , len ( doc ), n ): yield doc [ i : i + n ] . as_doc () def _create_overlapping_segments ( self , segments : List [ spacy . tokens . doc . Doc ], overlap : int , ) -> List [ spacy . tokens . doc . Doc ]: \"\"\"Create overlapping segments. Args: segments (List[spacy.tokens.doc.Doc]): A list of spaCy docs. overlap (int): The number of tokens to overlap. Returns: List[spacy.tokens.doc.Doc]: A list of spaCy docs. \"\"\" overlapped_segs = [] for i , seg in enumerate ( segments ): if i == 0 : # Get the first overlap tokens from the second segment overlapped_segs . append ( Doc . from_docs ([ seg , segments [ i + 1 ][: overlap ] . as_doc ()]) ) else : if i < len ( segments ) - 1 : # Get the last overlap tokens from the previous segment # and the first from the next segment overlapped_segs . append ( Doc . from_docs ( [ segments [ i - 1 ][ - overlap :] . as_doc (), seg , segments [ i + 1 ][: overlap ] . as_doc (), ] ) ) else : # Get the last overlap tokens from the previous segment overlapped_segs . append ( Doc . from_docs ([ segments [ i - 1 ][ - overlap :] . as_doc (), seg ]) ) return overlapped_segs def _get_milestone_result ( self , attr : str , token : object , value : Union [ str , tuple ] ) -> bool : \"\"\"Test a token for a match. If value is a tuple, it must have the form `(pattern, operator)`, where pattern is the string or regex pattern to match, and operator is the method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: attr (str): The spaCy token attribute to test. token (object): The token to test. value (Union[str, tuple]): The value to test. Returns: bool: Whether the token matches the query. \"\"\" if attr == \"is_milestone\" : if token . _ . is_milestone == True : return True else : return False elif isinstance ( value , str ) or isinstance ( value , bool ): if getattr ( token , attr ) == value : return True else : return False elif isinstance ( value , tuple ): pattern = value [ 0 ] operator = value [ 1 ] if operator == \"in\" : if getattr ( token , attr ) in pattern : return True else : return False elif operator == \"not_in\" : if getattr ( token , attr ) not in pattern : return True else : return False elif operator == \"starts_with\" : if getattr ( token , attr ) . startswith ( pattern ): return True else : return False elif operator == \"ends_with\" : if getattr ( token , attr ) . endswith ( pattern ): return True else : return False elif operator == \"re_match\" : if re . match ( pattern , getattr ( token , attr )): return True else : return False elif operator == \"re_search\" : if re . search ( pattern , getattr ( token , attr )): return True else : return False def _matches_milestone ( self , token : object , milestone : Union [ dict , list , str ] ) -> bool : \"\"\"Test a token for a match. Args: token (object): The token to test. milestone (Union[dict, str]): A variable representing the value(s) to be matched. Returns: bool: Whether the token matches the query. \"\"\" if isinstance ( milestone , str ): if token . text == milestone : return True else : return False elif isinstance ( milestone , list ): if token . text in milestone : return True else : return False elif isinstance ( milestone , dict ): return self . _parse_milestone_dict ( token , milestone ) def _parse_milestone_dict ( self , token , milestone_dict ): \"\"\"Parse a milestone dictionary and get results for each criterion. Key-value pairs in `milestone_dict` will be interpreted as token attributes and their values. If the value is given as a tuple, it must have the form `(pattern, operator)`, where the pattern is the string or regex pattern to match, and the operator is the matching method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: token (object): The token to test. milestone_dict (dict): A dict in the format given above. Returns: bool: Whether the token matches the query. \"\"\" # Get lists and_ = milestone_dict . get ( \"and\" , {}) or_ = milestone_dict . get ( \"or\" , {}) and_valid = True or_valid = False # Iterate through the and_ list for query_dict in and_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token fails to satisfy all criteria if self . _get_milestone_result ( attr , token , value ): and_valid = True else : and_valid = False # Iterate through the or_ list for query_dict in or_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token satisfies at least one criterion if self . _get_milestone_result ( attr , token , value ): or_valid = True # Determine if there is a match with \"and\" and \"or\" if and_valid and or_valid : is_match = True elif and_valid and not or_valid : is_match = True elif not and_valid and or_valid : is_match = True else : is_match = False # Handle keywords other than \"and\" and \"or\" for attr , value in milestone_dict . items (): if attr not in [ \"and\" , \"or\" ]: if self . _get_milestone_result ( attr , token , value ): is_match = True else : is_match = False # Return the result return is_match def _split_doc ( self , doc : spacy . tokens . doc . Doc , n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Split a spaCy doc into chunks by a fixed number of tokens. Args: doc (spacy.tokens.doc.Doc): A spaCy doc. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of spaCy docs. \"\"\" segments = list ( self . _chunk_doc ( doc , n )) # Apply the merge threshold if len ( segments [ - 1 ]) < n * merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single doc segments [ - 1 ] = Doc . from_docs ([ segments [ - 1 ], last_seg ]) if overlap : return self . _create_overlapping_segments ( segments , overlap ) else : return segments def _splitn_doc ( self , doc : spacy . tokens . doc . Doc , n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Get a specific number of sequential segments from a spaCy doc. Args: doc (spacy.tokens.doc.Doc): A spaCy doc. n (int): The number of segments to create. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of spaCy doc segments. Note: For this implementation, see https://stackoverflow.com/a/54802737. See `split()` for more information on the validation model. \"\"\" # Validate input try : model = SplitModel ( docs = doc , n = n , merge_threshold = merge_threshold , overlap = overlap ) except Exception as e : raise LexosException ( e ) # Get the number of tokens per segment (d) and the remaining tokens (r) d , r = divmod ( len ( doc ), model . n ) # Get the segments segments = [] for i in range ( model . n ): index = ( d + 1 ) * ( i if i < r else r ) + d * ( 0 if i < r else i - r ) segments . append ( doc [ index : index + ( d + 1 if i < r else d )] . as_doc ()) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single doc segments [ - 1 ] = Doc . from_docs ([ segments [ - 1 ], last_seg ]) if overlap : segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in segments ] # Convert the list of list segments to a list of spaCy doc segments segmented_doc = [] for segment in segments : if isinstance ( segment , spacy . tokens . doc . Doc ): segmented_doc . append ( segment ) else : segmented_doc . append ( segment . as_doc ()) return segmented_doc def _split_doc_on_milestones ( self , doc : spacy . tokens . doc . Doc , milestone : Union [ dict , str ], preserve_milestones : bool = True , ): \"\"\"Split document on a milestone. Args: doc (spacy.tokens.doc.Doc): The document to be split. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. \"\"\" segments = [] indices = [ i for i , x in enumerate ( doc ) if self . _matches_milestone ( x , milestone ) ] for start , end in zip ([ 0 , * indices ], [ * indices , len ( doc )]): if preserve_milestones : segments . append ( doc [ start : end ] . as_doc ()) else : segments . append ( doc [ start + 1 : end ] . as_doc ()) return segments def merge ( self , segments : List [ spacy . tokens . doc . Doc ]) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[spacy.tokens.doc.Doc]): The list of segments to merge. Returns: spacy.tokens.doc.Doc: The merged doc. \"\"\" return Doc . from_docs ( segments ) def split ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , ) -> List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]]: \"\"\"Split spaCy docs into chunks by a fixed number of tokens. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: List[Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]]: A list of spaCy docs (segments) for the input doc or a list of segment lists for multiple docs. Note: `n`, `merge_threshold`, and `overlap` are referenced from the validated model in case Pydantic has coerced them into the expected data types. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments def splitn ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Get a specific number of sequential segments from a spaCy doc or docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of segments to create. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of lists with where the inner list is the resulting segments for each doc. Note: For this implementation, see https://stackoverflow.com/a/54802737. See `split()` for more information on the validation model. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _splitn_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _splitn_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments def split_on_milestones ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], preserve_milestones : bool = True , ): \"\"\"Split document on a milestone. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): The document(s) to be split. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. \"\"\" # Validate input try : _ = SplitMilestoneModel ( docs = docs , milestone = milestone , preserve_milestones = preserve_milestones ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc_on_milestones ( docs , milestone , preserve_milestones ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc_on_milestones ( doc , milestone , preserve_milestones ) ) return all_segments","title":"Ginsu"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.Ginsu.__init__","text":"Initialize the class. Source code in lexos\\cutter\\ginsu.py 55 56 57 def __init__ ( self , config : dict = None ): \"\"\"Initialize the class.\"\"\" self . config = config","title":"__init__()"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.Ginsu.merge","text":"Merge a list of segments into a single string. Parameters: Name Type Description Default segments List [ spacy . tokens . doc . Doc ] The list of segments to merge. required Returns: Type Description str spacy.tokens.doc.Doc: The merged doc. Source code in lexos\\cutter\\ginsu.py 374 375 376 377 378 379 380 381 382 383 def merge ( self , segments : List [ spacy . tokens . doc . Doc ]) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[spacy.tokens.doc.Doc]): The list of segments to merge. Returns: spacy.tokens.doc.Doc: The merged doc. \"\"\" return Doc . from_docs ( segments )","title":"merge()"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.Ginsu.split","text":"Split spaCy docs into chunks by a fixed number of tokens. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] A spaCy doc or list of spaCy docs. required n int The number of tokens to split on. 1000 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None Returns: Type Description List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]] List[Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]]: A list of spaCy docs (segments) for List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]] the input doc or a list of segment lists for multiple docs. Note n , merge_threshold , and overlap are referenced from the validated model in case Pydantic has coerced them into the expected data types. Source code in lexos\\cutter\\ginsu.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def split ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , ) -> List [ Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]]]: \"\"\"Split spaCy docs into chunks by a fixed number of tokens. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: List[Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]]: A list of spaCy docs (segments) for the input doc or a list of segment lists for multiple docs. Note: `n`, `merge_threshold`, and `overlap` are referenced from the validated model in case Pydantic has coerced them into the expected data types. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments","title":"split()"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.Ginsu.split_on_milestones","text":"Split document on a milestone. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] The document(s) to be split. required milestone Union [ dict , str ] A variable representing the value(s) to be matched. required preserve_milestones bool If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. True Source code in lexos\\cutter\\ginsu.py 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def split_on_milestones ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], preserve_milestones : bool = True , ): \"\"\"Split document on a milestone. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): The document(s) to be split. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. \"\"\" # Validate input try : _ = SplitMilestoneModel ( docs = docs , milestone = milestone , preserve_milestones = preserve_milestones ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _split_doc_on_milestones ( docs , milestone , preserve_milestones ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _split_doc_on_milestones ( doc , milestone , preserve_milestones ) ) return all_segments","title":"split_on_milestones()"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.Ginsu.splitn","text":"Get a specific number of sequential segments from a spaCy doc or docs. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] A spaCy doc or list of spaCy docs. required n int The number of segments to create. 2 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None Returns: Name Type Description list list A list of lists with where the inner list is the resulting segments list for each doc. Note For this implementation, see https://stackoverflow.com/a/54802737 . See split() for more information on the validation model. Source code in lexos\\cutter\\ginsu.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def splitn ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , ) -> list : \"\"\"Get a specific number of sequential segments from a spaCy doc or docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or list of spaCy docs. n (int): The number of segments to create. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. Returns: list: A list of lists with where the inner list is the resulting segments for each doc. Note: For this implementation, see https://stackoverflow.com/a/54802737. See `split()` for more information on the validation model. \"\"\" # Validate input try : model = SplitModel ( docs = docs , n = n , merge_threshold = merge_threshold , overlap = overlap ) except ValidationError as e : raise LexosException ( e ) # Handle single docs if isinstance ( docs , spacy . tokens . doc . Doc ): return self . _splitn_doc ( docs , model . n , model . merge_threshold , model . overlap ) # Handle multiple docs else : all_segments = [] for doc in docs : all_segments . append ( self . _splitn_doc ( doc , model . n , model . merge_threshold , model . overlap ) ) return all_segments","title":"splitn()"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.SplitMilestoneModel","text":"Bases: BaseModel Validate the input for split functions. Source code in lexos\\cutter\\ginsu.py 17 18 19 20 21 22 23 24 25 26 27 class SplitMilestoneModel ( BaseModel ): \"\"\"Validate the input for split functions.\"\"\" docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] milestone : Union [ dict , str ] preserve_milestones : Optional [ bool ] = True class Config : \"\"\"Config for SplitMilestoneModel.\"\"\" arbitrary_types_allowed = True","title":"SplitMilestoneModel"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.SplitMilestoneModel.Config","text":"Config for SplitMilestoneModel. Source code in lexos\\cutter\\ginsu.py 24 25 26 27 class Config : \"\"\"Config for SplitMilestoneModel.\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.SplitModel","text":"Bases: BaseModel Validate the input for split functions. Source code in lexos\\cutter\\ginsu.py 30 31 32 33 34 35 36 37 38 39 40 41 class SplitModel ( BaseModel ): \"\"\"Validate the input for split functions.\"\"\" docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] n : Optional [ int ] = 1000 merge_threshold : Optional [ float ] = 0.5 overlap : Optional [ int ] = None class Config : \"\"\"Config for SplitModel.\"\"\" arbitrary_types_allowed = True","title":"SplitModel"},{"location":"api/cutter/ginsu/#lexos.cutter.ginsu.SplitModel.Config","text":"Config for SplitModel. Source code in lexos\\cutter\\ginsu.py 38 39 40 41 class Config : \"\"\"Config for SplitModel.\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"api/cutter/machete/","text":"Machete \u00a4 The Machete class allows the user to cut raw text strings. Documents can be split into a pre-determined number of segments, based on the number of tokens (using a non language-aware tokenizer), based on pre-defined token lists, or based on patterns defined as milestones. lexos.cutter.machete.Machete \u00a4 Codename Machete. Source code in lexos\\cutter\\machete.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 class Machete : \"\"\"Codename Machete.\"\"\" def __init__ ( self , tokenizer : str = \"whitespace\" ): \"\"\"Initialize the class.\"\"\" self . tokenizer = tokenizer def _chunk_tokens ( self , tokens : list , n : int = 1000 ) -> Callable : \"\"\"Yield successive n-sized chunks from a list by a fixed number of tokens. Args: tokens (list): A list of tokens. n (int): The number of tokens to split on. Returns: list: A list of token lists (segments). \"\"\" for i in range ( 0 , len ( tokens ), n ): yield tokens [ i : i + n ] def _create_overlapping_segments ( self , segments : List [ str ], overlap : int ) -> List [ str ]: \"\"\"Create overlapping segments. Args: segments (List[str]): A list of token strings. overlap (int): The number of tokens to overlap. Returns: List[str]: A list of token strings. \"\"\" overlapped_segs = [] for i , seg in enumerate ( segments ): if i == 0 : # Get the first overlap tokens from the second segment overlapped_segs . append ( seg + segments [ i + 1 ][: overlap ]) else : if i < len ( segments ) - 1 : # Get the last overlap tokens from the previous segment overlapped_segs . append ( seg + segments [ i + 1 ][: overlap ]) else : # Get the last segment overlapped_segs . append ( seg ) return overlapped_segs def _tokenize ( self , text : str , tokenizer : str = None ) -> list : \"\"\"Tokenize an input string without a language model. Loads a tokenizer function from the registry. Args: text (str): The input string. Returns: list: A list of tokens. \"\"\" if not tokenizer : tokenizer = registry . load ( self . tokenizer ) else : try : tokenizer = registry . load ( tokenizer ) except ValueError : raise LexosException ( \"The specified tokenizer could not be found in the tokenizer registry.\" ) return tokenizer ( text ) def merge ( self , segments : List [ str ], sep = \" \" ) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[str]): The list of segments to merge. sep (str): The separator to use. Returns: str: The merged string. \"\"\" return sep . join ( segments ) def split ( self , texts : Union [ List [ str ], str ], n = 1000 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts into chunks by a fixed number of tokens. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) segments = list ( self . _chunk_tokens ( tokens , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments def splitn ( self , texts : Union [ List [ str ], str ], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Get a specific number of sequential segments from a string or list of strings. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of segments to create. Calculated automatically. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: For this implementation, see https://stackoverflow.com/a/54802737. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) # Get the number of tokens per segment (d) and the remaining tokens (r) d , r = divmod ( len ( tokens ), model . n ) # Get the segments segments = [] for i in range ( model . n ): index = ( d + 1 ) * ( i if i < r else r ) + d * ( 0 if i < r else i - r ) segments . append ( tokens [ index : index + ( d + 1 if i < r else d )]) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments def split_list ( self , text : List [ str ], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , as_string : bool = False , ) -> list : \"\"\"Split a list into chunks by a fixed number of tokens. Args: text (List[str]): A list of tokens. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of token lists, one token list for each segment. \"\"\" # Validate input try : model = SplitListModel ( text = text , n = n , merge_threshold = merge_threshold , overlap = overlap , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if isinstance ( model . text [ 0 ], str ): model . text = [ model . text ] # Process the texts into segments all_segments = [] for text in model . text : segments = list ( self . _chunk_tokens ( text , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = [ segments [ - 1 ] + last_seg ] all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments def split_on_milestones ( self , texts : Union [ List [ str ], str ], milestone : Union [ dict , str ], preserve_milestones : bool = True , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts on milestones. Args: texts (Union[List[str], str]): A text string or list of text strings. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: The choice of tokenizer can lead to some unexpected results with regard to spacing around the milestone. The default behaviour is to delete the milestone and any following whitespace. If milestones are preserved, the milestone will occur at the beginning of the following segment and will be followed by a single space. If the segments are returned with `as_string=False`, each token will have a following space and it will be up to the end user to remove the space if desired. \"\"\" # Validate input try : model = SplitMilestoneModel ( texts = texts , milestone = milestone , preserve_milestones = preserve_milestones , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] milestone_pat = re . compile ( milestone ) for text in model . texts : cut_on_milestone = [] seg = [] # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) for i , token in enumerate ( tokens ): if re . match ( milestone_pat , token . strip () ): # token.strip() == milestone: cut_on_milestone . append ( seg ) j = i if preserve_milestones : seg = [ f \" { milestone } \" ] else : seg = [] else : seg . append ( token ) # Add the last segment cut_on_milestone . append ( tokens [ j + 1 :]) all_segments . append ( cut_on_milestone ) if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] # If no milestone was found, return the original texts if len ( all_segments ) == 1 and all_segments [ 0 ] == []: return [ model . texts ] else : return all_segments __init__ ( tokenizer = 'whitespace' ) \u00a4 Initialize the class. Source code in lexos\\cutter\\machete.py 48 49 50 def __init__ ( self , tokenizer : str = \"whitespace\" ): \"\"\"Initialize the class.\"\"\" self . tokenizer = tokenizer merge ( segments , sep = ' ' ) \u00a4 Merge a list of segments into a single string. Parameters: Name Type Description Default segments List [ str ] The list of segments to merge. required sep str The separator to use. ' ' Returns: Name Type Description str str The merged string. Source code in lexos\\cutter\\machete.py 113 114 115 116 117 118 119 120 121 122 123 def merge ( self , segments : List [ str ], sep = \" \" ) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[str]): The list of segments to merge. sep (str): The separator to use. Returns: str: The merged string. \"\"\" return sep . join ( segments ) split ( texts , n = 1000 , merge_threshold = 0.5 , overlap = None , tokenizer = None , as_string = True ) \u00a4 Split texts into chunks by a fixed number of tokens. Parameters: Name Type Description Default texts Union [ List [ str ], str ] A text string or list of text strings. required n int The number of tokens to split on. 1000 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None tokenizer str The name of the tokenizer function to use. None as_string bool Whether to return the segments as a list of strings. True Returns: Name Type Description list list A list of lists or strings (segments) for each text. Source code in lexos\\cutter\\machete.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def split ( self , texts : Union [ List [ str ], str ], n = 1000 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts into chunks by a fixed number of tokens. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) segments = list ( self . _chunk_tokens ( tokens , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments split_list ( text , n = 1000 , merge_threshold = 0.5 , overlap = None , as_string = False ) \u00a4 Split a list into chunks by a fixed number of tokens. Parameters: Name Type Description Default text List [ str ] A list of tokens. required n int The number of tokens to split on. 1000 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None as_string bool Whether to return the segments as a list of strings. False Returns: Name Type Description list list A list of token lists, one token list for each segment. Source code in lexos\\cutter\\machete.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def split_list ( self , text : List [ str ], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , as_string : bool = False , ) -> list : \"\"\"Split a list into chunks by a fixed number of tokens. Args: text (List[str]): A list of tokens. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of token lists, one token list for each segment. \"\"\" # Validate input try : model = SplitListModel ( text = text , n = n , merge_threshold = merge_threshold , overlap = overlap , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if isinstance ( model . text [ 0 ], str ): model . text = [ model . text ] # Process the texts into segments all_segments = [] for text in model . text : segments = list ( self . _chunk_tokens ( text , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = [ segments [ - 1 ] + last_seg ] all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments split_on_milestones ( texts , milestone , preserve_milestones = True , tokenizer = None , as_string = True ) \u00a4 Split texts on milestones. Parameters: Name Type Description Default texts Union [ List [ str ], str ] A text string or list of text strings. required milestone Union [ dict , str ] A variable representing the value(s) to be matched. required preserve_milestones bool If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. True tokenizer str The name of the tokenizer function to use. None as_string bool Whether to return the segments as a list of strings. True Returns: Name Type Description list list A list of lists or strings (segments) for each text. Note The choice of tokenizer can lead to some unexpected results with regard to spacing around the milestone. The default behaviour is to delete the milestone and any following whitespace. If milestones are preserved, the milestone will occur at the beginning of the following segment and will be followed by a single space. If the segments are returned with as_string=False , each token will have a following space and it will be up to the end user to remove the space if desired. Source code in lexos\\cutter\\machete.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def split_on_milestones ( self , texts : Union [ List [ str ], str ], milestone : Union [ dict , str ], preserve_milestones : bool = True , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts on milestones. Args: texts (Union[List[str], str]): A text string or list of text strings. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: The choice of tokenizer can lead to some unexpected results with regard to spacing around the milestone. The default behaviour is to delete the milestone and any following whitespace. If milestones are preserved, the milestone will occur at the beginning of the following segment and will be followed by a single space. If the segments are returned with `as_string=False`, each token will have a following space and it will be up to the end user to remove the space if desired. \"\"\" # Validate input try : model = SplitMilestoneModel ( texts = texts , milestone = milestone , preserve_milestones = preserve_milestones , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] milestone_pat = re . compile ( milestone ) for text in model . texts : cut_on_milestone = [] seg = [] # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) for i , token in enumerate ( tokens ): if re . match ( milestone_pat , token . strip () ): # token.strip() == milestone: cut_on_milestone . append ( seg ) j = i if preserve_milestones : seg = [ f \" { milestone } \" ] else : seg = [] else : seg . append ( token ) # Add the last segment cut_on_milestone . append ( tokens [ j + 1 :]) all_segments . append ( cut_on_milestone ) if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] # If no milestone was found, return the original texts if len ( all_segments ) == 1 and all_segments [ 0 ] == []: return [ model . texts ] else : return all_segments splitn ( texts , n = 2 , merge_threshold = 0.5 , overlap = None , tokenizer = None , as_string = True ) \u00a4 Get a specific number of sequential segments from a string or list of strings. Parameters: Name Type Description Default texts Union [ List [ str ], str ] A text string or list of text strings. required n int The number of segments to create. Calculated automatically. 2 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None tokenizer str The name of the tokenizer function to use. None as_string bool Whether to return the segments as a list of strings. True Returns: Name Type Description list list A list of lists or strings (segments) for each text. Note For this implementation, see https://stackoverflow.com/a/54802737 . Source code in lexos\\cutter\\machete.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def splitn ( self , texts : Union [ List [ str ], str ], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Get a specific number of sequential segments from a string or list of strings. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of segments to create. Calculated automatically. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: For this implementation, see https://stackoverflow.com/a/54802737. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) # Get the number of tokens per segment (d) and the remaining tokens (r) d , r = divmod ( len ( tokens ), model . n ) # Get the segments segments = [] for i in range ( model . n ): index = ( d + 1 ) * ( i if i < r else r ) + d * ( 0 if i < r else i - r ) segments . append ( tokens [ index : index + ( d + 1 if i < r else d )]) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments lexos.cutter.machete.SplitListModel \u00a4 Bases: BaseModel Validate the input for split_list function. Source code in lexos\\cutter\\machete.py 15 16 17 18 19 20 21 class SplitListModel ( BaseModel ): \"\"\"Validate the input for split_list function.\"\"\" text : List [ str ] n : Optional [ int ] = 1000 merge_threshold : Optional [ float ] = 0.5 overlap : Optional [ int ] = None lexos.cutter.machete.SplitMilestoneModel \u00a4 Bases: BaseModel Validate the input for split_on_miletone function. Source code in lexos\\cutter\\machete.py 24 25 26 27 28 29 30 31 class SplitMilestoneModel ( BaseModel ): \"\"\"Validate the input for split_on_miletone function.\"\"\" texts : Union [ List [ str ], str ] milestone : Union [ dict , str ] preserve_milestones : Optional [ bool ] = False tokenizer : Optional [ str ] = None as_string : Optional [ bool ] = True lexos.cutter.machete.SplitModel \u00a4 Bases: BaseModel Validate the input for split functions. Source code in lexos\\cutter\\machete.py 34 35 36 37 38 39 40 41 42 class SplitModel ( BaseModel ): \"\"\"Validate the input for split functions.\"\"\" texts : Union [ List [ str ], str ] n : Optional [ int ] = 1000 merge_threshold : Optional [ float ] = 0.5 overlap : Optional [ int ] = None tokenizer : Optional [ str ] = None as_string : Optional [ bool ] = True","title":"Machete"},{"location":"api/cutter/machete/#machete","text":"The Machete class allows the user to cut raw text strings. Documents can be split into a pre-determined number of segments, based on the number of tokens (using a non language-aware tokenizer), based on pre-defined token lists, or based on patterns defined as milestones.","title":"Machete"},{"location":"api/cutter/machete/#lexos.cutter.machete.Machete","text":"Codename Machete. Source code in lexos\\cutter\\machete.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 class Machete : \"\"\"Codename Machete.\"\"\" def __init__ ( self , tokenizer : str = \"whitespace\" ): \"\"\"Initialize the class.\"\"\" self . tokenizer = tokenizer def _chunk_tokens ( self , tokens : list , n : int = 1000 ) -> Callable : \"\"\"Yield successive n-sized chunks from a list by a fixed number of tokens. Args: tokens (list): A list of tokens. n (int): The number of tokens to split on. Returns: list: A list of token lists (segments). \"\"\" for i in range ( 0 , len ( tokens ), n ): yield tokens [ i : i + n ] def _create_overlapping_segments ( self , segments : List [ str ], overlap : int ) -> List [ str ]: \"\"\"Create overlapping segments. Args: segments (List[str]): A list of token strings. overlap (int): The number of tokens to overlap. Returns: List[str]: A list of token strings. \"\"\" overlapped_segs = [] for i , seg in enumerate ( segments ): if i == 0 : # Get the first overlap tokens from the second segment overlapped_segs . append ( seg + segments [ i + 1 ][: overlap ]) else : if i < len ( segments ) - 1 : # Get the last overlap tokens from the previous segment overlapped_segs . append ( seg + segments [ i + 1 ][: overlap ]) else : # Get the last segment overlapped_segs . append ( seg ) return overlapped_segs def _tokenize ( self , text : str , tokenizer : str = None ) -> list : \"\"\"Tokenize an input string without a language model. Loads a tokenizer function from the registry. Args: text (str): The input string. Returns: list: A list of tokens. \"\"\" if not tokenizer : tokenizer = registry . load ( self . tokenizer ) else : try : tokenizer = registry . load ( tokenizer ) except ValueError : raise LexosException ( \"The specified tokenizer could not be found in the tokenizer registry.\" ) return tokenizer ( text ) def merge ( self , segments : List [ str ], sep = \" \" ) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[str]): The list of segments to merge. sep (str): The separator to use. Returns: str: The merged string. \"\"\" return sep . join ( segments ) def split ( self , texts : Union [ List [ str ], str ], n = 1000 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts into chunks by a fixed number of tokens. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) segments = list ( self . _chunk_tokens ( tokens , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments def splitn ( self , texts : Union [ List [ str ], str ], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Get a specific number of sequential segments from a string or list of strings. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of segments to create. Calculated automatically. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: For this implementation, see https://stackoverflow.com/a/54802737. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) # Get the number of tokens per segment (d) and the remaining tokens (r) d , r = divmod ( len ( tokens ), model . n ) # Get the segments segments = [] for i in range ( model . n ): index = ( d + 1 ) * ( i if i < r else r ) + d * ( 0 if i < r else i - r ) segments . append ( tokens [ index : index + ( d + 1 if i < r else d )]) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments def split_list ( self , text : List [ str ], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , as_string : bool = False , ) -> list : \"\"\"Split a list into chunks by a fixed number of tokens. Args: text (List[str]): A list of tokens. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of token lists, one token list for each segment. \"\"\" # Validate input try : model = SplitListModel ( text = text , n = n , merge_threshold = merge_threshold , overlap = overlap , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if isinstance ( model . text [ 0 ], str ): model . text = [ model . text ] # Process the texts into segments all_segments = [] for text in model . text : segments = list ( self . _chunk_tokens ( text , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = [ segments [ - 1 ] + last_seg ] all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments def split_on_milestones ( self , texts : Union [ List [ str ], str ], milestone : Union [ dict , str ], preserve_milestones : bool = True , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts on milestones. Args: texts (Union[List[str], str]): A text string or list of text strings. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: The choice of tokenizer can lead to some unexpected results with regard to spacing around the milestone. The default behaviour is to delete the milestone and any following whitespace. If milestones are preserved, the milestone will occur at the beginning of the following segment and will be followed by a single space. If the segments are returned with `as_string=False`, each token will have a following space and it will be up to the end user to remove the space if desired. \"\"\" # Validate input try : model = SplitMilestoneModel ( texts = texts , milestone = milestone , preserve_milestones = preserve_milestones , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] milestone_pat = re . compile ( milestone ) for text in model . texts : cut_on_milestone = [] seg = [] # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) for i , token in enumerate ( tokens ): if re . match ( milestone_pat , token . strip () ): # token.strip() == milestone: cut_on_milestone . append ( seg ) j = i if preserve_milestones : seg = [ f \" { milestone } \" ] else : seg = [] else : seg . append ( token ) # Add the last segment cut_on_milestone . append ( tokens [ j + 1 :]) all_segments . append ( cut_on_milestone ) if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] # If no milestone was found, return the original texts if len ( all_segments ) == 1 and all_segments [ 0 ] == []: return [ model . texts ] else : return all_segments","title":"Machete"},{"location":"api/cutter/machete/#lexos.cutter.machete.Machete.__init__","text":"Initialize the class. Source code in lexos\\cutter\\machete.py 48 49 50 def __init__ ( self , tokenizer : str = \"whitespace\" ): \"\"\"Initialize the class.\"\"\" self . tokenizer = tokenizer","title":"__init__()"},{"location":"api/cutter/machete/#lexos.cutter.machete.Machete.merge","text":"Merge a list of segments into a single string. Parameters: Name Type Description Default segments List [ str ] The list of segments to merge. required sep str The separator to use. ' ' Returns: Name Type Description str str The merged string. Source code in lexos\\cutter\\machete.py 113 114 115 116 117 118 119 120 121 122 123 def merge ( self , segments : List [ str ], sep = \" \" ) -> str : \"\"\"Merge a list of segments into a single string. Args: segments (List[str]): The list of segments to merge. sep (str): The separator to use. Returns: str: The merged string. \"\"\" return sep . join ( segments )","title":"merge()"},{"location":"api/cutter/machete/#lexos.cutter.machete.Machete.split","text":"Split texts into chunks by a fixed number of tokens. Parameters: Name Type Description Default texts Union [ List [ str ], str ] A text string or list of text strings. required n int The number of tokens to split on. 1000 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None tokenizer str The name of the tokenizer function to use. None as_string bool Whether to return the segments as a list of strings. True Returns: Name Type Description list list A list of lists or strings (segments) for each text. Source code in lexos\\cutter\\machete.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def split ( self , texts : Union [ List [ str ], str ], n = 1000 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts into chunks by a fixed number of tokens. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) segments = list ( self . _chunk_tokens ( tokens , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments","title":"split()"},{"location":"api/cutter/machete/#lexos.cutter.machete.Machete.split_list","text":"Split a list into chunks by a fixed number of tokens. Parameters: Name Type Description Default text List [ str ] A list of tokens. required n int The number of tokens to split on. 1000 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None as_string bool Whether to return the segments as a list of strings. False Returns: Name Type Description list list A list of token lists, one token list for each segment. Source code in lexos\\cutter\\machete.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def split_list ( self , text : List [ str ], n : int = 1000 , merge_threshold : float = 0.5 , overlap : int = None , as_string : bool = False , ) -> list : \"\"\"Split a list into chunks by a fixed number of tokens. Args: text (List[str]): A list of tokens. n (int): The number of tokens to split on. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of token lists, one token list for each segment. \"\"\" # Validate input try : model = SplitListModel ( text = text , n = n , merge_threshold = merge_threshold , overlap = overlap , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if isinstance ( model . text [ 0 ], str ): model . text = [ model . text ] # Process the texts into segments all_segments = [] for text in model . text : segments = list ( self . _chunk_tokens ( text , model . n )) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = [ segments [ - 1 ] + last_seg ] all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments","title":"split_list()"},{"location":"api/cutter/machete/#lexos.cutter.machete.Machete.split_on_milestones","text":"Split texts on milestones. Parameters: Name Type Description Default texts Union [ List [ str ], str ] A text string or list of text strings. required milestone Union [ dict , str ] A variable representing the value(s) to be matched. required preserve_milestones bool If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. True tokenizer str The name of the tokenizer function to use. None as_string bool Whether to return the segments as a list of strings. True Returns: Name Type Description list list A list of lists or strings (segments) for each text. Note The choice of tokenizer can lead to some unexpected results with regard to spacing around the milestone. The default behaviour is to delete the milestone and any following whitespace. If milestones are preserved, the milestone will occur at the beginning of the following segment and will be followed by a single space. If the segments are returned with as_string=False , each token will have a following space and it will be up to the end user to remove the space if desired. Source code in lexos\\cutter\\machete.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def split_on_milestones ( self , texts : Union [ List [ str ], str ], milestone : Union [ dict , str ], preserve_milestones : bool = True , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Split texts on milestones. Args: texts (Union[List[str], str]): A text string or list of text strings. milestone (Union[dict, str]): A variable representing the value(s) to be matched. preserve_milestones (bool): If True, the milestone token will be preserved at the beginning of every segment. Otherwise, it will be deleted. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: The choice of tokenizer can lead to some unexpected results with regard to spacing around the milestone. The default behaviour is to delete the milestone and any following whitespace. If milestones are preserved, the milestone will occur at the beginning of the following segment and will be followed by a single space. If the segments are returned with `as_string=False`, each token will have a following space and it will be up to the end user to remove the space if desired. \"\"\" # Validate input try : model = SplitMilestoneModel ( texts = texts , milestone = milestone , preserve_milestones = preserve_milestones , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] milestone_pat = re . compile ( milestone ) for text in model . texts : cut_on_milestone = [] seg = [] # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) for i , token in enumerate ( tokens ): if re . match ( milestone_pat , token . strip () ): # token.strip() == milestone: cut_on_milestone . append ( seg ) j = i if preserve_milestones : seg = [ f \" { milestone } \" ] else : seg = [] else : seg . append ( token ) # Add the last segment cut_on_milestone . append ( tokens [ j + 1 :]) all_segments . append ( cut_on_milestone ) if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] # If no milestone was found, return the original texts if len ( all_segments ) == 1 and all_segments [ 0 ] == []: return [ model . texts ] else : return all_segments","title":"split_on_milestones()"},{"location":"api/cutter/machete/#lexos.cutter.machete.Machete.splitn","text":"Get a specific number of sequential segments from a string or list of strings. Parameters: Name Type Description Default texts Union [ List [ str ], str ] A text string or list of text strings. required n int The number of segments to create. Calculated automatically. 2 merge_threshold float The threshold to merge the last segment. 0.5 overlap int The number of tokens to overlap. None tokenizer str The name of the tokenizer function to use. None as_string bool Whether to return the segments as a list of strings. True Returns: Name Type Description list list A list of lists or strings (segments) for each text. Note For this implementation, see https://stackoverflow.com/a/54802737 . Source code in lexos\\cutter\\machete.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def splitn ( self , texts : Union [ List [ str ], str ], n : int = 2 , merge_threshold : float = 0.5 , overlap : int = None , tokenizer : str = None , as_string : bool = True , ) -> list : \"\"\"Get a specific number of sequential segments from a string or list of strings. Args: texts (Union[List[str], str]): A text string or list of text strings. n (int): The number of segments to create. Calculated automatically. merge_threshold (float): The threshold to merge the last segment. overlap (int): The number of tokens to overlap. tokenizer (str): The name of the tokenizer function to use. as_string (bool): Whether to return the segments as a list of strings. Returns: list: A list of lists or strings (segments) for each text. Note: For this implementation, see https://stackoverflow.com/a/54802737. \"\"\" # Validate input try : model = SplitModel ( texts = texts , n = n , merge_threshold = merge_threshold , overlap = overlap , tokenizer = tokenizer , as_string = as_string , ) except Exception as e : raise LexosException ( e ) # Ensure a list of texts as the starting point if not isinstance ( model . texts , list ): model . texts = [ model . texts ] # Process the texts into segments all_segments = [] for text in model . texts : # Tokenise the text tokens = self . _tokenize ( text , tokenizer = model . tokenizer ) # Get the number of tokens per segment (d) and the remaining tokens (r) d , r = divmod ( len ( tokens ), model . n ) # Get the segments segments = [] for i in range ( model . n ): index = ( d + 1 ) * ( i if i < r else r ) + d * ( 0 if i < r else i - r ) segments . append ( tokens [ index : index + ( d + 1 if i < r else d )]) # Apply the merge threshold if len ( segments [ - 1 ]) < model . n * model . merge_threshold : last_seg = segments . pop ( - 1 ) # Combine the last two segments into a single list segments [ - 1 ] = segments [ - 1 ] + last_seg all_segments . append ( segments ) if overlap : all_segments = [ self . _create_overlapping_segments ( segment , model . overlap ) for segment in all_segments ] if as_string : all_segments = [ [ \"\" . join ( segment ) for segment in text ] for text in all_segments ] return all_segments","title":"splitn()"},{"location":"api/cutter/machete/#lexos.cutter.machete.SplitListModel","text":"Bases: BaseModel Validate the input for split_list function. Source code in lexos\\cutter\\machete.py 15 16 17 18 19 20 21 class SplitListModel ( BaseModel ): \"\"\"Validate the input for split_list function.\"\"\" text : List [ str ] n : Optional [ int ] = 1000 merge_threshold : Optional [ float ] = 0.5 overlap : Optional [ int ] = None","title":"SplitListModel"},{"location":"api/cutter/machete/#lexos.cutter.machete.SplitMilestoneModel","text":"Bases: BaseModel Validate the input for split_on_miletone function. Source code in lexos\\cutter\\machete.py 24 25 26 27 28 29 30 31 class SplitMilestoneModel ( BaseModel ): \"\"\"Validate the input for split_on_miletone function.\"\"\" texts : Union [ List [ str ], str ] milestone : Union [ dict , str ] preserve_milestones : Optional [ bool ] = False tokenizer : Optional [ str ] = None as_string : Optional [ bool ] = True","title":"SplitMilestoneModel"},{"location":"api/cutter/machete/#lexos.cutter.machete.SplitModel","text":"Bases: BaseModel Validate the input for split functions. Source code in lexos\\cutter\\machete.py 34 35 36 37 38 39 40 41 42 class SplitModel ( BaseModel ): \"\"\"Validate the input for split functions.\"\"\" texts : Union [ List [ str ], str ] n : Optional [ int ] = 1000 merge_threshold : Optional [ float ] = 0.5 overlap : Optional [ int ] = None tokenizer : Optional [ str ] = None as_string : Optional [ bool ] = True","title":"SplitModel"},{"location":"api/cutter/merge/","text":"Merge \u00a4 Merge is a set of standalone functions for merging strings, spaCy Docs, and files. lexos . cutter . merge . merge ( segments , sep = None ) \u00a4 Merge a list of segments into a single string. Parameters: Name Type Description Default segments _type_ The list of segments to merge. required sep str The separator to use when merging strings. Defaults to None. None Source code in lexos\\cutter\\merge.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def merge ( segments : Union [ List [ str ], List [ spacy . tokens . doc . Doc ]], sep = None ): \"\"\"Merge a list of segments into a single string. Args: segments (_type_): The list of segments to merge. sep (str, optional): The separator to use when merging strings. Defaults to None. \"\"\" if all ( isinstance ( segment , str ) for segment in segments ): if sep is None : sep = \"\" return sep . join ( segments ) elif all ( isinstance ( segment , spacy . tokens . doc . Doc ) for segment in segments ): return Doc . from_docs ( segments ) else : raise LexosException ( \"All segments must be either strings or spacy.tokens.doc.Doc.\" ) lexos . cutter . merge . merge_files ( segment_files , output_file = 'merged_files.txt' , binary = False ) \u00a4 Merge two files into a single string. Parameters: Name Type Description Default segment_files List [ str ] List of files to be merged. required output_file str The name of the output file. 'merged_files.txt' binary bool Whether to read and write files as binary. Defaults to False. False Source code in lexos\\cutter\\merge.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def merge_files ( segment_files : List [ str ], output_file : str = \"merged_files.txt\" , binary : bool = False , ) -> None : \"\"\"Merge two files into a single string. Args: segment_files (List[str]): List of files to be merged. output_file (str, optional): The name of the output file. binary (bool, optional): Whether to read and write files as binary. Defaults to False. \"\"\" if binary : read_mode = \"rb\" write_mode = \"wb\" else : read_mode = \"r\" write_mode = \"w\" with open ( \"merged_file.txt\" , write_mode ) as out_file : for file in segment_files : try : with open ( output_file , read_mode ) as f : shutil . copyfileobj ( file , out_file , 1024 * 1024 * 10 ) except Exception as e : raise LexosException ( f \"Error merging files: { e } .\" )","title":"Merge"},{"location":"api/cutter/merge/#merge","text":"Merge is a set of standalone functions for merging strings, spaCy Docs, and files.","title":"Merge"},{"location":"api/cutter/merge/#lexos.cutter.merge.merge","text":"Merge a list of segments into a single string. Parameters: Name Type Description Default segments _type_ The list of segments to merge. required sep str The separator to use when merging strings. Defaults to None. None Source code in lexos\\cutter\\merge.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def merge ( segments : Union [ List [ str ], List [ spacy . tokens . doc . Doc ]], sep = None ): \"\"\"Merge a list of segments into a single string. Args: segments (_type_): The list of segments to merge. sep (str, optional): The separator to use when merging strings. Defaults to None. \"\"\" if all ( isinstance ( segment , str ) for segment in segments ): if sep is None : sep = \"\" return sep . join ( segments ) elif all ( isinstance ( segment , spacy . tokens . doc . Doc ) for segment in segments ): return Doc . from_docs ( segments ) else : raise LexosException ( \"All segments must be either strings or spacy.tokens.doc.Doc.\" )","title":"merge()"},{"location":"api/cutter/merge/#lexos.cutter.merge.merge_files","text":"Merge two files into a single string. Parameters: Name Type Description Default segment_files List [ str ] List of files to be merged. required output_file str The name of the output file. 'merged_files.txt' binary bool Whether to read and write files as binary. Defaults to False. False Source code in lexos\\cutter\\merge.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def merge_files ( segment_files : List [ str ], output_file : str = \"merged_files.txt\" , binary : bool = False , ) -> None : \"\"\"Merge two files into a single string. Args: segment_files (List[str]): List of files to be merged. output_file (str, optional): The name of the output file. binary (bool, optional): Whether to read and write files as binary. Defaults to False. \"\"\" if binary : read_mode = \"rb\" write_mode = \"wb\" else : read_mode = \"r\" write_mode = \"w\" with open ( \"merged_file.txt\" , write_mode ) as out_file : for file in segment_files : try : with open ( output_file , read_mode ) as f : shutil . copyfileobj ( file , out_file , 1024 * 1024 * 10 ) except Exception as e : raise LexosException ( f \"Error merging files: { e } .\" )","title":"merge_files()"},{"location":"api/cutter/milestones/","text":"Milestones \u00a4 The milestones component of Cutter allows you to pre-assign milestone tokens prior to cutting. lexos.cutter.milestones.Milestones \u00a4 Milestones class. Source code in lexos\\cutter\\milestones.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class Milestones : \"\"\"Milestones class.\"\"\" def __init__ ( self , config : dict = None ): \"\"\"Initialise a Milestones object. Args: config (dict): Arbitrary configuration. \"\"\" self . config = config def set ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], ) -> Union [ List [ object ], object ]: \"\"\"Set the milestones for a doc or a list of docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or a list of spaCy docs. milestone (Union[dict, str]): The milestone token(s) to match. Returns: Union[List[spacy.tokens.doc.Doc], spacy.tokens.doc.Doc]: A spaCy doc or list of spacy docs with `doc._.is_milestone` set. \"\"\" # Holder for processed docs result = [] # Make sure single docs are in a list if not isinstance ( docs , list ): docs = [ docs ] # Set milestones on each doc for doc in docs : result . append ( self . _set_milestones ( doc , milestone )) # Return the processed docs list or a single processed doc if len ( result ) == 1 : return result [ 0 ] else : return result def _set_milestones ( self , doc : spacy . tokens . doc . Doc , milestone : str ) -> spacy . tokens . doc . Doc : \"\"\"Set the milestones for a doc. Args: doc (spacy.tokens.doc.Doc): A spaCy doc. milestone (str): The milestone token(s) to match. Returns: object: A spaCy doc with `doc._.is_milestone` set. \"\"\" # If the doc tokens do not have `is_milestone`, set them all to False if not doc [ 0 ] . has_extension ( \"is_milestone\" ): Token . set_extension ( \"is_milestone\" , default = False , force = True ) # Go through the doc, setting `is_milestone` for each match for token in doc : if self . _matches_milestone ( token , milestone ): token . _ . is_milestone = True else : token . _ . is_milestone = False return doc def _matches_milestone ( self , token : spacy . tokens . token . Token , milestone : Union [ dict , list , str ] ) -> bool : \"\"\"Check if a token matches a milestone. Args: token (spacy.tokens.token.Token): The token to test. milestone (Union[dict, list, str]): The milestone token(s) to match. Returns: bool: Whether the token matches the milestone. \"\"\" if isinstance ( milestone , str ): if token . text == milestone : return True else : return False elif isinstance ( milestone , list ): if token . text in milestone : return True else : return False elif isinstance ( milestone , dict ): return self . _parse_milestone_dict ( token , milestone ) def _parse_milestone_dict ( self , token , milestone_dict ): \"\"\"Parse a milestone dictionary and get results for each criterion. Key-value pairs in `milestone_dict` will be interpreted as token attributes and their values. If the value is given as a tuple, it must have the form `(pattern, operator)`, where the pattern is the string or regex pattern to match, and the operator is the matching method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: token (spacy.tokens.token.Token): The token to test. milestone_dict (dict): A dict in the format given above. Returns: bool: Whether the token matches the query. \"\"\" # Get lists and_ = milestone_dict . get ( \"and\" , {}) or_ = milestone_dict . get ( \"or\" , {}) and_valid = True or_valid = False # Iterate through the and_ list for query_dict in and_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token fails to satisfy all criteria if self . _get_milestone_result ( attr , token , value ): and_valid = True else : and_valid = False # Iterate through the or_ list for query_dict in or_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token satisfies at least one criterion if self . _get_milestone_result ( attr , token , value ): or_valid = True # Determine if there is a match with \"and\" and \"or\" if and_valid and or_valid : is_match = True elif and_valid and not or_valid : is_match = True elif not and_valid and or_valid : is_match = True else : is_match = False # Handle keywords other than \"and\" and \"or\" for attr , value in milestone_dict . items (): if attr not in [ \"and\" , \"or\" ]: if self . _get_milestone_result ( attr , token , value ): is_match = True else : is_match = False # Return the result return is_match def _get_milestone_result ( self , attr : str , token : spacy . tokens . token . Token , value : Union [ str , tuple ] ) -> bool : \"\"\"Test a token for a match. If value is a tuple, it must have the form `(pattern, operator)`, where pattern is the string or regex pattern to match, and operator is the method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: attr (str): The attribute to test. token (spacy.tokens.token.Token): The token to test. value (Union[str, tuple]): The value to test. Returns: bool: Whether the token matches the query. \"\"\" if isinstance ( value , str ): if getattr ( token , attr ) == value : return True else : return False elif isinstance ( value , tuple ): pattern = value [ 0 ] operator = value [ 1 ] if operator == \"in\" : if getattr ( token , attr ) in pattern : return True else : return False elif operator == \"not_in\" : if getattr ( token , attr ) not in pattern : return True else : return False elif operator == \"starts_with\" : if getattr ( token , attr ) . startswith ( pattern ): return True else : return False elif operator == \"ends_with\" : if getattr ( token , attr ) . endswith ( pattern ): return True else : return False elif operator == \"re_match\" : if re . match ( pattern , getattr ( token , attr )): return True else : return False elif operator == \"re_search\" : if re . search ( pattern , getattr ( token , attr )): return True else : return False __init__ ( config = None ) \u00a4 Initialise a Milestones object. Parameters: Name Type Description Default config dict Arbitrary configuration. None Source code in lexos\\cutter\\milestones.py 24 25 26 27 28 29 30 def __init__ ( self , config : dict = None ): \"\"\"Initialise a Milestones object. Args: config (dict): Arbitrary configuration. \"\"\" self . config = config set ( docs , milestone ) \u00a4 Set the milestones for a doc or a list of docs. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] A spaCy doc or a list of spaCy docs. required milestone Union [ dict , str ] The milestone token(s) to match. required Returns: Type Description Union [ List [ object ], object ] Union[List[spacy.tokens.doc.Doc], spacy.tokens.doc.Doc]: A spaCy doc or list of spacy docs with doc._.is_milestone set. Source code in lexos\\cutter\\milestones.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def set ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], ) -> Union [ List [ object ], object ]: \"\"\"Set the milestones for a doc or a list of docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or a list of spaCy docs. milestone (Union[dict, str]): The milestone token(s) to match. Returns: Union[List[spacy.tokens.doc.Doc], spacy.tokens.doc.Doc]: A spaCy doc or list of spacy docs with `doc._.is_milestone` set. \"\"\" # Holder for processed docs result = [] # Make sure single docs are in a list if not isinstance ( docs , list ): docs = [ docs ] # Set milestones on each doc for doc in docs : result . append ( self . _set_milestones ( doc , milestone )) # Return the processed docs list or a single processed doc if len ( result ) == 1 : return result [ 0 ] else : return result","title":"Milestones"},{"location":"api/cutter/milestones/#milestones","text":"The milestones component of Cutter allows you to pre-assign milestone tokens prior to cutting.","title":"Milestones"},{"location":"api/cutter/milestones/#lexos.cutter.milestones.Milestones","text":"Milestones class. Source code in lexos\\cutter\\milestones.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class Milestones : \"\"\"Milestones class.\"\"\" def __init__ ( self , config : dict = None ): \"\"\"Initialise a Milestones object. Args: config (dict): Arbitrary configuration. \"\"\" self . config = config def set ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], ) -> Union [ List [ object ], object ]: \"\"\"Set the milestones for a doc or a list of docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or a list of spaCy docs. milestone (Union[dict, str]): The milestone token(s) to match. Returns: Union[List[spacy.tokens.doc.Doc], spacy.tokens.doc.Doc]: A spaCy doc or list of spacy docs with `doc._.is_milestone` set. \"\"\" # Holder for processed docs result = [] # Make sure single docs are in a list if not isinstance ( docs , list ): docs = [ docs ] # Set milestones on each doc for doc in docs : result . append ( self . _set_milestones ( doc , milestone )) # Return the processed docs list or a single processed doc if len ( result ) == 1 : return result [ 0 ] else : return result def _set_milestones ( self , doc : spacy . tokens . doc . Doc , milestone : str ) -> spacy . tokens . doc . Doc : \"\"\"Set the milestones for a doc. Args: doc (spacy.tokens.doc.Doc): A spaCy doc. milestone (str): The milestone token(s) to match. Returns: object: A spaCy doc with `doc._.is_milestone` set. \"\"\" # If the doc tokens do not have `is_milestone`, set them all to False if not doc [ 0 ] . has_extension ( \"is_milestone\" ): Token . set_extension ( \"is_milestone\" , default = False , force = True ) # Go through the doc, setting `is_milestone` for each match for token in doc : if self . _matches_milestone ( token , milestone ): token . _ . is_milestone = True else : token . _ . is_milestone = False return doc def _matches_milestone ( self , token : spacy . tokens . token . Token , milestone : Union [ dict , list , str ] ) -> bool : \"\"\"Check if a token matches a milestone. Args: token (spacy.tokens.token.Token): The token to test. milestone (Union[dict, list, str]): The milestone token(s) to match. Returns: bool: Whether the token matches the milestone. \"\"\" if isinstance ( milestone , str ): if token . text == milestone : return True else : return False elif isinstance ( milestone , list ): if token . text in milestone : return True else : return False elif isinstance ( milestone , dict ): return self . _parse_milestone_dict ( token , milestone ) def _parse_milestone_dict ( self , token , milestone_dict ): \"\"\"Parse a milestone dictionary and get results for each criterion. Key-value pairs in `milestone_dict` will be interpreted as token attributes and their values. If the value is given as a tuple, it must have the form `(pattern, operator)`, where the pattern is the string or regex pattern to match, and the operator is the matching method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: token (spacy.tokens.token.Token): The token to test. milestone_dict (dict): A dict in the format given above. Returns: bool: Whether the token matches the query. \"\"\" # Get lists and_ = milestone_dict . get ( \"and\" , {}) or_ = milestone_dict . get ( \"or\" , {}) and_valid = True or_valid = False # Iterate through the and_ list for query_dict in and_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token fails to satisfy all criteria if self . _get_milestone_result ( attr , token , value ): and_valid = True else : and_valid = False # Iterate through the or_ list for query_dict in or_ : # Get the attribute and value attr , value = list ( query_dict . items ())[ 0 ] # The token satisfies at least one criterion if self . _get_milestone_result ( attr , token , value ): or_valid = True # Determine if there is a match with \"and\" and \"or\" if and_valid and or_valid : is_match = True elif and_valid and not or_valid : is_match = True elif not and_valid and or_valid : is_match = True else : is_match = False # Handle keywords other than \"and\" and \"or\" for attr , value in milestone_dict . items (): if attr not in [ \"and\" , \"or\" ]: if self . _get_milestone_result ( attr , token , value ): is_match = True else : is_match = False # Return the result return is_match def _get_milestone_result ( self , attr : str , token : spacy . tokens . token . Token , value : Union [ str , tuple ] ) -> bool : \"\"\"Test a token for a match. If value is a tuple, it must have the form `(pattern, operator)`, where pattern is the string or regex pattern to match, and operator is the method to use. Valid operators are \"in\", \"not_in\", \"starts_with\", \"ends_with\", \"re_match\", and \"re_search\". The prefix \"re_\" implies that the pattern is a regex, and either `re.match` or `re.search` will be used. Args: attr (str): The attribute to test. token (spacy.tokens.token.Token): The token to test. value (Union[str, tuple]): The value to test. Returns: bool: Whether the token matches the query. \"\"\" if isinstance ( value , str ): if getattr ( token , attr ) == value : return True else : return False elif isinstance ( value , tuple ): pattern = value [ 0 ] operator = value [ 1 ] if operator == \"in\" : if getattr ( token , attr ) in pattern : return True else : return False elif operator == \"not_in\" : if getattr ( token , attr ) not in pattern : return True else : return False elif operator == \"starts_with\" : if getattr ( token , attr ) . startswith ( pattern ): return True else : return False elif operator == \"ends_with\" : if getattr ( token , attr ) . endswith ( pattern ): return True else : return False elif operator == \"re_match\" : if re . match ( pattern , getattr ( token , attr )): return True else : return False elif operator == \"re_search\" : if re . search ( pattern , getattr ( token , attr )): return True else : return False","title":"Milestones"},{"location":"api/cutter/milestones/#lexos.cutter.milestones.Milestones.__init__","text":"Initialise a Milestones object. Parameters: Name Type Description Default config dict Arbitrary configuration. None Source code in lexos\\cutter\\milestones.py 24 25 26 27 28 29 30 def __init__ ( self , config : dict = None ): \"\"\"Initialise a Milestones object. Args: config (dict): Arbitrary configuration. \"\"\" self . config = config","title":"__init__()"},{"location":"api/cutter/milestones/#lexos.cutter.milestones.Milestones.set","text":"Set the milestones for a doc or a list of docs. Parameters: Name Type Description Default docs Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]] A spaCy doc or a list of spaCy docs. required milestone Union [ dict , str ] The milestone token(s) to match. required Returns: Type Description Union [ List [ object ], object ] Union[List[spacy.tokens.doc.Doc], spacy.tokens.doc.Doc]: A spaCy doc or list of spacy docs with doc._.is_milestone set. Source code in lexos\\cutter\\milestones.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def set ( self , docs : Union [ spacy . tokens . doc . Doc , List [ spacy . tokens . doc . Doc ]], milestone : Union [ dict , str ], ) -> Union [ List [ object ], object ]: \"\"\"Set the milestones for a doc or a list of docs. Args: docs (Union[spacy.tokens.doc.Doc, List[spacy.tokens.doc.Doc]]): A spaCy doc or a list of spaCy docs. milestone (Union[dict, str]): The milestone token(s) to match. Returns: Union[List[spacy.tokens.doc.Doc], spacy.tokens.doc.Doc]: A spaCy doc or list of spacy docs with `doc._.is_milestone` set. \"\"\" # Holder for processed docs result = [] # Make sure single docs are in a list if not isinstance ( docs , list ): docs = [ docs ] # Set milestones on each doc for doc in docs : result . append ( self . _set_milestones ( doc , milestone )) # Return the processed docs list or a single processed doc if len ( result ) == 1 : return result [ 0 ] else : return result","title":"set()"},{"location":"api/cutter/registry/","text":"Cutter Registry \u00a4 The Ginsu class allows the user to cut documents pre-tokenized with spaCy. Documents can be split into a pre-determined number of segments, based on the number of tokens, or based on tokens defined as milestones. lexos . cutter . registry . character_tokenizer ( text ) \u00a4 Tokenize by single characters, keeping whitespace. Parameters: Name Type Description Default text str The text to tokenize. required Returns: Name Type Description list list A list of character tokens. Source code in lexos\\cutter\\registry.py 24 25 26 27 28 29 30 31 32 33 def character_tokenizer ( text : str ) -> list : \"\"\"Tokenize by single characters, keeping whitespace. Args: text: The text to tokenize. Returns: list: A list of character tokens. \"\"\" return [ char for char in text ] lexos . cutter . registry . linebreak_tokenizer ( text ) \u00a4 Tokenize by linebreaks, keeping whitespace. Parameters: Name Type Description Default text str The text to tokenize. required Returns: Name Type Description list list A list of line tokens. Source code in lexos\\cutter\\registry.py 37 38 39 40 41 42 43 44 45 46 def linebreak_tokenizer ( text : str ) -> list : \"\"\"Tokenize by linebreaks, keeping whitespace. Args: text: The text to tokenize. Returns: list: A list of line tokens. \"\"\" return text . splitlines ( keepends = True ) lexos . cutter . registry . whitespace_tokenizer ( text ) \u00a4 Tokenize on whitespace, keeping whitespace. Parameters: Name Type Description Default text str The text to tokenize. required Returns: Name Type Description list list A list of pseudo-word tokens. Source code in lexos\\cutter\\registry.py 11 12 13 14 15 16 17 18 19 20 def whitespace_tokenizer ( text : str ) -> list : \"\"\"Tokenize on whitespace, keeping whitespace. Args: text: The text to tokenize. Returns: list: A list of pseudo-word tokens. \"\"\" return re . findall ( r \"\\S+\\s*\" , text ) lexos . cutter . registry . load ( s ) \u00a4 Load a single tokenizer from a string. Parameters: Name Type Description Default s str The name of the function. required Returns: Name Type Description list Callable A tokenizer function. Source code in lexos\\cutter\\registry.py 55 56 57 58 59 60 61 62 63 64 def load ( s : str ) -> Callable : \"\"\"Load a single tokenizer from a string. Args: s: The name of the function. Returns: list: A tokenizer function. \"\"\" return tokenizers . get ( s )","title":"Registry"},{"location":"api/cutter/registry/#cutter-registry","text":"The Ginsu class allows the user to cut documents pre-tokenized with spaCy. Documents can be split into a pre-determined number of segments, based on the number of tokens, or based on tokens defined as milestones.","title":"Cutter Registry"},{"location":"api/cutter/registry/#lexos.cutter.registry.character_tokenizer","text":"Tokenize by single characters, keeping whitespace. Parameters: Name Type Description Default text str The text to tokenize. required Returns: Name Type Description list list A list of character tokens. Source code in lexos\\cutter\\registry.py 24 25 26 27 28 29 30 31 32 33 def character_tokenizer ( text : str ) -> list : \"\"\"Tokenize by single characters, keeping whitespace. Args: text: The text to tokenize. Returns: list: A list of character tokens. \"\"\" return [ char for char in text ]","title":"character_tokenizer()"},{"location":"api/cutter/registry/#lexos.cutter.registry.linebreak_tokenizer","text":"Tokenize by linebreaks, keeping whitespace. Parameters: Name Type Description Default text str The text to tokenize. required Returns: Name Type Description list list A list of line tokens. Source code in lexos\\cutter\\registry.py 37 38 39 40 41 42 43 44 45 46 def linebreak_tokenizer ( text : str ) -> list : \"\"\"Tokenize by linebreaks, keeping whitespace. Args: text: The text to tokenize. Returns: list: A list of line tokens. \"\"\" return text . splitlines ( keepends = True )","title":"linebreak_tokenizer()"},{"location":"api/cutter/registry/#lexos.cutter.registry.whitespace_tokenizer","text":"Tokenize on whitespace, keeping whitespace. Parameters: Name Type Description Default text str The text to tokenize. required Returns: Name Type Description list list A list of pseudo-word tokens. Source code in lexos\\cutter\\registry.py 11 12 13 14 15 16 17 18 19 20 def whitespace_tokenizer ( text : str ) -> list : \"\"\"Tokenize on whitespace, keeping whitespace. Args: text: The text to tokenize. Returns: list: A list of pseudo-word tokens. \"\"\" return re . findall ( r \"\\S+\\s*\" , text )","title":"whitespace_tokenizer()"},{"location":"api/cutter/registry/#lexos.cutter.registry.load","text":"Load a single tokenizer from a string. Parameters: Name Type Description Default s str The name of the function. required Returns: Name Type Description list Callable A tokenizer function. Source code in lexos\\cutter\\registry.py 55 56 57 58 59 60 61 62 63 64 def load ( s : str ) -> Callable : \"\"\"Load a single tokenizer from a string. Args: s: The name of the function. Returns: list: A tokenizer function. \"\"\" return tokenizers . get ( s )","title":"load()"},{"location":"api/dtm/","text":"DTM \u00a4 The DTM module contains a basic DTM class. lexos.dtm.DTM \u00a4 Class for a document-term matrix. Source code in lexos\\dtm\\__init__.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 class DTM : \"\"\"Class for a document-term matrix.\"\"\" def __init__ ( self , docs = List [ Union [ List [ str ], spacy . tokens . doc . Doc ]], labels : List [ str ] = None , ) -> None : \"\"\"Initialise the DTM. Args: docs (List[Union[List[str], spacy.tokens.doc.Doc]]): A list of spaCy docs or a list of token lists. labels (List[str]): A list of labels for the documents. Returns: None \"\"\" self . docs = docs self . table = None if not labels : self . labels = [ \"doc\" + str ( i ) for i in range ( len ( docs ))] else : self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build () def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_token_lists = DtmData ( docs = self . docs ) . docs self . matrix = self . vectorizer . fit_transform ( doc_token_lists ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index () def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 , numeric_only = True ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 , numeric_only = True ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 , numeric_only = True ) tmp [ \"median\" ] = median . round ( rounding ) return tmp def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM , ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" if alg != SORTING_ALGORITHM : self . _validate_sorting_algorithm ( alg ) df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )] def least_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) df = df [ start :] return df . tail ( max_n_terms ) def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ] def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False , ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms , ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer def _validate_sorting_algorithm ( self , alg : Any ) -> bool : \"\"\"Ensure that the specified sorting algorithm is a valid natsort locale. Args: alg: The sorting algorithm to validate. Returns: bool: Whether the sorting algorithm is valid. \"\"\" if alg not in [ e for e in ns ]: locales = \", \" . join ([ f \"ns. { e . name } \" for e in ns ]) err = ( f \"Invalid sorting algorithm: { alg } .\" , f \"Valid algorithms for `alg` are: { locales } .\" , \"See https://natsort.readthedocs.io/en/stable/api.html#natsort.ns.\" , ) raise LexosException ( \" \" . join ( err )) return True __init__ ( docs = List [ Union [ List [ str ], spacy . tokens . doc . Doc ]], labels = None ) \u00a4 Initialise the DTM. Parameters: Name Type Description Default docs List [ Union [ List [ str ], spacy . tokens . doc . Doc ]] A list of spaCy docs or a list of token lists. List[Union[List[str], spacy.tokens.doc.Doc]] labels List [ str ] A list of labels for the documents. None Returns: Type Description None None Source code in lexos\\dtm\\__init__.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , docs = List [ Union [ List [ str ], spacy . tokens . doc . Doc ]], labels : List [ str ] = None , ) -> None : \"\"\"Initialise the DTM. Args: docs (List[Union[List[str], spacy.tokens.doc.Doc]]): A list of spaCy docs or a list of token lists. labels (List[str]): A list of labels for the documents. Returns: None \"\"\" self . docs = docs self . table = None if not labels : self . labels = [ \"doc\" + str ( i ) for i in range ( len ( docs ))] else : self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build () build () \u00a4 Build a new DTM matrix based on the current vectorizer. Source code in lexos\\dtm\\__init__.py 73 74 75 76 77 78 79 def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_token_lists = DtmData ( docs = self . docs ) . docs self . matrix = self . vectorizer . fit_transform ( doc_token_lists ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None get_freq_table ( rounding = 3 , as_percent = False ) \u00a4 Get a table with the relative frequencies of terms in each document. Parameters: Name Type Description Default rounding int The number of digits to round floats. 3 as_percent bool Whether to return the frequencies as percentages. False Returns: Type Description pd . DataFrame pd.DataFrame: A dataframe with the relative frequencies. Source code in lexos\\dtm\\__init__.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index () get_stats_table ( stats = 'sum' , rounding = 3 ) \u00a4 Get a table with the sum, mean, and/or median calculated for each row. Parameters: Name Type Description Default stats Union [ List [ str ], str ] One or more of \"sum\", \"mean\", and/or \"median\". 'sum' rounding int The number of digits to round floats. 3 Returns: Type Description pd . DataFrame pd.DataFrame: A dataframe with the calculated statistics. Source code in lexos\\dtm\\__init__.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 , numeric_only = True ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 , numeric_only = True ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 , numeric_only = True ) tmp [ \"median\" ] = median . round ( rounding ) return tmp get_table ( transpose = False ) \u00a4 Get a Textacy document-term matrix as a pandas dataframe. Parameters: Name Type Description Default transpose bool If True, terms are columns and docs are rows. False Returns: Type Description pd . DataFrame pd.Dataframe Source code in lexos\\dtm\\__init__.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df get_term_counts ( sort_by = [ 'terms' , 'sum' ], ascending = True , alg = SORTING_ALGORITHM ) \u00a4 Get a list of term counts with optional sorting. Parameters: Name Type Description Default sort_by Union [ list , List [ str ]] The column(s) to sort by in order of preference. ['terms', 'sum'] ascending Union [ bool , List [ bool ]] Whether to sort values in ascending or descending order. True Returns: Name Type Description List tuple A list of tuples containing terms and counts. Source code in lexos\\dtm\\__init__.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM , ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" if alg != SORTING_ALGORITHM : self . _validate_sorting_algorithm ( alg ) df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )] get_terms () \u00a4 Get an alphabetical list of terms. Source code in lexos\\dtm\\__init__.py 153 154 155 def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms least_frequent ( max_n_terms = 100 , start = 0 ) \u00a4 Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. 0 Returns: Type Description pd . DataFrame pd.DataFrame: The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def least_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) df = df [ start :] return df . tail ( max_n_terms ) most_frequent ( max_n_terms = 100 , start = 0 ) \u00a4 Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. 0 Returns: Type Description pd . DataFrame pd.DataFrame: The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ] set_vectorizer ( tf_type = 'linear' , idf_type = None , dl_type = None , norm = None , min_df = 1 , max_df = 1.0 , max_n_terms = None , vocabulary_terms = None , new = False ) \u00a4 Set the vectorizer. By default, returns a vectorizer that gets raw counts. Source code in lexos\\dtm\\__init__.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False , ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms , ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer lexos.dtm.DtmData \u00a4 Bases: BaseModel DtmData class. This model validates the input data for the DTM and, if necessary, coerces it to a list of token lists. Source code in lexos\\dtm\\__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DtmData ( BaseModel ): \"\"\"DtmData class. This model validates the input data for the DTM and, if necessary, coerces it to a list of token lists. \"\"\" docs : List [ Union [ List [ str ], spacy . tokens . doc . Doc ]] class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True @validator ( \"docs\" , pre = True , always = True ) def ensure_token_lists ( cls , v ): \"\"\"Coerces input to a list of token lists where each token is a string.\"\"\" tokens = [] for doc in v : if isinstance ( doc , spacy . tokens . doc . Doc ): tokens . append ([ token . text for token in doc ]) elif isinstance ( doc , list ): if all ( isinstance ( sub , str ) for sub in doc ): tokens . append ( doc ) else : raise LexosException ( \"Each list item must be a string.\" ) else : raise LexosException ( \"Could not parse the document list.\" ) return tokens Config \u00a4 Config class. Source code in lexos\\dtm\\__init__.py 24 25 26 27 class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True ensure_token_lists ( v ) \u00a4 Coerces input to a list of token lists where each token is a string. Source code in lexos\\dtm\\__init__.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @validator ( \"docs\" , pre = True , always = True ) def ensure_token_lists ( cls , v ): \"\"\"Coerces input to a list of token lists where each token is a string.\"\"\" tokens = [] for doc in v : if isinstance ( doc , spacy . tokens . doc . Doc ): tokens . append ([ token . text for token in doc ]) elif isinstance ( doc , list ): if all ( isinstance ( sub , str ) for sub in doc ): tokens . append ( doc ) else : raise LexosException ( \"Each list item must be a string.\" ) else : raise LexosException ( \"Could not parse the document list.\" ) return tokens","title":"DTM"},{"location":"api/dtm/#dtm","text":"The DTM module contains a basic DTM class.","title":"DTM"},{"location":"api/dtm/#lexos.dtm.DTM","text":"Class for a document-term matrix. Source code in lexos\\dtm\\__init__.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 class DTM : \"\"\"Class for a document-term matrix.\"\"\" def __init__ ( self , docs = List [ Union [ List [ str ], spacy . tokens . doc . Doc ]], labels : List [ str ] = None , ) -> None : \"\"\"Initialise the DTM. Args: docs (List[Union[List[str], spacy.tokens.doc.Doc]]): A list of spaCy docs or a list of token lists. labels (List[str]): A list of labels for the documents. Returns: None \"\"\" self . docs = docs self . table = None if not labels : self . labels = [ \"doc\" + str ( i ) for i in range ( len ( docs ))] else : self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build () def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_token_lists = DtmData ( docs = self . docs ) . docs self . matrix = self . vectorizer . fit_transform ( doc_token_lists ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index () def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 , numeric_only = True ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 , numeric_only = True ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 , numeric_only = True ) tmp [ \"median\" ] = median . round ( rounding ) return tmp def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM , ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" if alg != SORTING_ALGORITHM : self . _validate_sorting_algorithm ( alg ) df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )] def least_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) df = df [ start :] return df . tail ( max_n_terms ) def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ] def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False , ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms , ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer def _validate_sorting_algorithm ( self , alg : Any ) -> bool : \"\"\"Ensure that the specified sorting algorithm is a valid natsort locale. Args: alg: The sorting algorithm to validate. Returns: bool: Whether the sorting algorithm is valid. \"\"\" if alg not in [ e for e in ns ]: locales = \", \" . join ([ f \"ns. { e . name } \" for e in ns ]) err = ( f \"Invalid sorting algorithm: { alg } .\" , f \"Valid algorithms for `alg` are: { locales } .\" , \"See https://natsort.readthedocs.io/en/stable/api.html#natsort.ns.\" , ) raise LexosException ( \" \" . join ( err )) return True","title":"DTM"},{"location":"api/dtm/#lexos.dtm.DTM.__init__","text":"Initialise the DTM. Parameters: Name Type Description Default docs List [ Union [ List [ str ], spacy . tokens . doc . Doc ]] A list of spaCy docs or a list of token lists. List[Union[List[str], spacy.tokens.doc.Doc]] labels List [ str ] A list of labels for the documents. None Returns: Type Description None None Source code in lexos\\dtm\\__init__.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , docs = List [ Union [ List [ str ], spacy . tokens . doc . Doc ]], labels : List [ str ] = None , ) -> None : \"\"\"Initialise the DTM. Args: docs (List[Union[List[str], spacy.tokens.doc.Doc]]): A list of spaCy docs or a list of token lists. labels (List[str]): A list of labels for the documents. Returns: None \"\"\" self . docs = docs self . table = None if not labels : self . labels = [ \"doc\" + str ( i ) for i in range ( len ( docs ))] else : self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build ()","title":"__init__()"},{"location":"api/dtm/#lexos.dtm.DTM.build","text":"Build a new DTM matrix based on the current vectorizer. Source code in lexos\\dtm\\__init__.py 73 74 75 76 77 78 79 def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_token_lists = DtmData ( docs = self . docs ) . docs self . matrix = self . vectorizer . fit_transform ( doc_token_lists ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None","title":"build()"},{"location":"api/dtm/#lexos.dtm.DTM.get_freq_table","text":"Get a table with the relative frequencies of terms in each document. Parameters: Name Type Description Default rounding int The number of digits to round floats. 3 as_percent bool Whether to return the frequencies as percentages. False Returns: Type Description pd . DataFrame pd.DataFrame: A dataframe with the relative frequencies. Source code in lexos\\dtm\\__init__.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index ()","title":"get_freq_table()"},{"location":"api/dtm/#lexos.dtm.DTM.get_stats_table","text":"Get a table with the sum, mean, and/or median calculated for each row. Parameters: Name Type Description Default stats Union [ List [ str ], str ] One or more of \"sum\", \"mean\", and/or \"median\". 'sum' rounding int The number of digits to round floats. 3 Returns: Type Description pd . DataFrame pd.DataFrame: A dataframe with the calculated statistics. Source code in lexos\\dtm\\__init__.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 , numeric_only = True ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 , numeric_only = True ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 , numeric_only = True ) tmp [ \"median\" ] = median . round ( rounding ) return tmp","title":"get_stats_table()"},{"location":"api/dtm/#lexos.dtm.DTM.get_table","text":"Get a Textacy document-term matrix as a pandas dataframe. Parameters: Name Type Description Default transpose bool If True, terms are columns and docs are rows. False Returns: Type Description pd . DataFrame pd.Dataframe Source code in lexos\\dtm\\__init__.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df","title":"get_table()"},{"location":"api/dtm/#lexos.dtm.DTM.get_term_counts","text":"Get a list of term counts with optional sorting. Parameters: Name Type Description Default sort_by Union [ list , List [ str ]] The column(s) to sort by in order of preference. ['terms', 'sum'] ascending Union [ bool , List [ bool ]] Whether to sort values in ascending or descending order. True Returns: Name Type Description List tuple A list of tuples containing terms and counts. Source code in lexos\\dtm\\__init__.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM , ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" if alg != SORTING_ALGORITHM : self . _validate_sorting_algorithm ( alg ) df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )]","title":"get_term_counts()"},{"location":"api/dtm/#lexos.dtm.DTM.get_terms","text":"Get an alphabetical list of terms. Source code in lexos\\dtm\\__init__.py 153 154 155 def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms","title":"get_terms()"},{"location":"api/dtm/#lexos.dtm.DTM.least_frequent","text":"Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. 0 Returns: Type Description pd . DataFrame pd.DataFrame: The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def least_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) df = df [ start :] return df . tail ( max_n_terms )","title":"least_frequent()"},{"location":"api/dtm/#lexos.dtm.DTM.most_frequent","text":"Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. 0 Returns: Type Description pd . DataFrame pd.DataFrame: The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ]","title":"most_frequent()"},{"location":"api/dtm/#lexos.dtm.DTM.set_vectorizer","text":"Set the vectorizer. By default, returns a vectorizer that gets raw counts. Source code in lexos\\dtm\\__init__.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False , ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms , ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer","title":"set_vectorizer()"},{"location":"api/dtm/#lexos.dtm.DtmData","text":"Bases: BaseModel DtmData class. This model validates the input data for the DTM and, if necessary, coerces it to a list of token lists. Source code in lexos\\dtm\\__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DtmData ( BaseModel ): \"\"\"DtmData class. This model validates the input data for the DTM and, if necessary, coerces it to a list of token lists. \"\"\" docs : List [ Union [ List [ str ], spacy . tokens . doc . Doc ]] class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True @validator ( \"docs\" , pre = True , always = True ) def ensure_token_lists ( cls , v ): \"\"\"Coerces input to a list of token lists where each token is a string.\"\"\" tokens = [] for doc in v : if isinstance ( doc , spacy . tokens . doc . Doc ): tokens . append ([ token . text for token in doc ]) elif isinstance ( doc , list ): if all ( isinstance ( sub , str ) for sub in doc ): tokens . append ( doc ) else : raise LexosException ( \"Each list item must be a string.\" ) else : raise LexosException ( \"Could not parse the document list.\" ) return tokens","title":"DtmData"},{"location":"api/dtm/#lexos.dtm.DtmData.Config","text":"Config class. Source code in lexos\\dtm\\__init__.py 24 25 26 27 class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"api/dtm/#lexos.dtm.DtmData.ensure_token_lists","text":"Coerces input to a list of token lists where each token is a string. Source code in lexos\\dtm\\__init__.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @validator ( \"docs\" , pre = True , always = True ) def ensure_token_lists ( cls , v ): \"\"\"Coerces input to a list of token lists where each token is a string.\"\"\" tokens = [] for doc in v : if isinstance ( doc , spacy . tokens . doc . Doc ): tokens . append ([ token . text for token in doc ]) elif isinstance ( doc , list ): if all ( isinstance ( sub , str ) for sub in doc ): tokens . append ( doc ) else : raise LexosException ( \"Each list item must be a string.\" ) else : raise LexosException ( \"Could not parse the document list.\" ) return tokens","title":"ensure_token_lists()"},{"location":"api/io/","text":"IO \u00a4 The IO module manages input and output functions. It contains two loader modules: smart and dataset . The smart provides an interface for loading texts in a variety of formats, whether from local files or from urls. The dataset module provides method for loading or downloading large numbers of texts that are generally stored in a single file.","title":"IO"},{"location":"api/io/#io","text":"The IO module manages input and output functions. It contains two loader modules: smart and dataset . The smart provides an interface for loading texts in a variety of formats, whether from local files or from urls. The dataset module provides method for loading or downloading large numbers of texts that are generally stored in a single file.","title":"IO"},{"location":"api/io/dataset/","text":"Dataset \u00a4 The DatasetLoader class is used to load multiple texts stored in a single file. lexos.io.dataset.Dataset \u00a4 Bases: BaseModel Dataset class. Source code in lexos\\io\\dataset.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 class Dataset ( BaseModel ): \"\"\"Dataset class.\"\"\" data : Optional [ List [ Dict [ str , str ]]] = None class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ] @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The texts of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ] def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ] def df ( self ) -> pd . DataFrame : \"\"\"Return the dataframe of the object data. Returns: pd.DataFrame: The dataframe of the object data. \"\"\" return pd . DataFrame ( self . data ) @classmethod def parse_csv ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse CSV/TSV texts into the Dataset object. Args: source (str): The string or path to file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_csv ( source , ** kwargs ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"CSV and TSV files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( \"\" . join ( err )) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_dict ( cls : Type [ \"Model\" ], source : dict ,) -> \"Model\" : \"\"\"Alias for cls.parse_obj(). Args: source (dict): The dict to parse. Returns: Model: A dataset object. \"\"\" return cls . parse_obj ({ \"data\" : source }) @classmethod def parse_excel ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse Excel files into the Dataset object. Args: source (str): The path to the Excel file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : df = pd . read_excel ( source , ** kwargs ) except Exception as e : raise LexosException ( f \"Could not read { source } : { e } \" ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"Excel files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_json ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse JSON files or strings. Args: source (str): The json string to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : with open ( source ) as f : df = pd . read_json ( f , ** kwargs ) except Exception : df = pd . read_json ( io . StringIO ( source ), ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_jsonl ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string or path to file containing the lines to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_json ( source , lines = True , ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON and JSONL files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_string ( cls : Type [ \"Model\" ], source : str , labels : Optional [ List [ str ]] = None , locations : Optional [ List [ str ]] = None , ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string containing the lines to parse. labels (Optional[List[str]]): The names of the texts. locations (Optional[List[str]]): The locations of the texts. Returns: Model: A dataset object. \"\"\" if not labels : raise LexosException ( \"Please use the `labels` argument to provide a list of labels for each row in your data.\" ) # Handle files try : with open ( source , \"r\" , encoding = \"utf-8\" ) as f : source = f . readlines () # Handle strings except Exception : source = source . split ( \" \\n \" ) if len ( labels ) != len ( source ): raise LexosException ( f \"The number of labels ( { len ( labels ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) else : data = [{ \"title\" : labels [ i ], \"text\" : line } for i , line in enumerate ( source )] if locations : if len ( locations ) == len ( source ): for i , _ in enumerate ( data ): data [ i ][ \"locations\" ] = locations [ i ] else : raise LexosException ( f \"The number of locations ( { len ( locations ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) return cls . parse_obj ({ \"data\" : data }) @staticmethod def _get_file_like ( source : str ) -> IO [ AnyStr ]: \"\"\"Read the source into a buffer. Args: source: str: A path or string containing the source. Returns: IO[AnyStr]: A file-like object containing the source. \"\"\" if utils . is_file ( source ) or utils . is_github_dir ( source ) == False : try : with open ( source , \"r\" , encoding = \"utf-8\" ) as f : source = f . read () except : pass return io . StringIO ( source ) else : raise LexosException ( f \" { source } is not a valid file path or input string.\" ) Config \u00a4 Config class. Source code in lexos\\io\\dataset.py 46 47 48 49 class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True __getitem__ ( item ) \u00a4 Get an item from dataset. Parameters: Name Type Description Default item int The index of the item to get. required Returns: Type Description Dict [ str , str ] Dict[str, str]: The item at the given index. Source code in lexos\\io\\dataset.py 90 91 92 93 94 95 96 97 98 99 def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ] __iter__ () \u00a4 Iterate over the dataset. Returns: Name Type Description Iterable Iterable The dataset. Source code in lexos\\io\\dataset.py 81 82 83 84 85 86 87 88 def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item df () \u00a4 Return the dataframe of the object data. Returns: Type Description pd . DataFrame pd.DataFrame: The dataframe of the object data. Source code in lexos\\io\\dataset.py 101 102 103 104 105 106 107 def df ( self ) -> pd . DataFrame : \"\"\"Return the dataframe of the object data. Returns: pd.DataFrame: The dataframe of the object data. \"\"\" return pd . DataFrame ( self . data ) locations () property \u00a4 Return the locations of the object data. Returns: Type Description List [ str ] List[str]: The locations of the object data. Source code in lexos\\io\\dataset.py 51 52 53 54 55 56 57 58 59 60 61 @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None names () property \u00a4 Return the names of the object data. Returns: Type Description List [ str ] List[str]: The names of the object data. Source code in lexos\\io\\dataset.py 63 64 65 66 67 68 69 70 @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ] parse_csv ( source , title_col = None , text_col = None , ** kwargs ) classmethod \u00a4 Parse CSV/TSV texts into the Dataset object. Parameters: Name Type Description Default source str The string or path to file containing the texts to parse. required title_col Optional [ str ] The column name to convert to \"title\". None text_col Optional [ str ] The column name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 @classmethod def parse_csv ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse CSV/TSV texts into the Dataset object. Args: source (str): The string or path to file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_csv ( source , ** kwargs ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"CSV and TSV files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( \"\" . join ( err )) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) parse_dict ( source ) classmethod \u00a4 Alias for cls.parse_obj(). Parameters: Name Type Description Default source dict The dict to parse. required Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 142 143 144 145 146 147 148 149 150 151 152 @classmethod def parse_dict ( cls : Type [ \"Model\" ], source : dict ,) -> \"Model\" : \"\"\"Alias for cls.parse_obj(). Args: source (dict): The dict to parse. Returns: Model: A dataset object. \"\"\" return cls . parse_obj ({ \"data\" : source }) parse_excel ( source , title_col = None , text_col = None , ** kwargs ) classmethod \u00a4 Parse Excel files into the Dataset object. Parameters: Name Type Description Default source str The path to the Excel file containing the texts to parse. required title_col Optional [ str ] The column name to convert to \"title\". None text_col Optional [ str ] The column name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @classmethod def parse_excel ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse Excel files into the Dataset object. Args: source (str): The path to the Excel file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : df = pd . read_excel ( source , ** kwargs ) except Exception as e : raise LexosException ( f \"Could not read { source } : { e } \" ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"Excel files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) parse_json ( source , title_field = None , text_field = None , ** kwargs ) classmethod \u00a4 Parse JSON files or strings. Parameters: Name Type Description Default source str The json string to parse. required title_field Optional [ str ] The field name to convert to \"title\". None text_field Optional [ str ] The field name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @classmethod def parse_json ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse JSON files or strings. Args: source (str): The json string to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : with open ( source ) as f : df = pd . read_json ( f , ** kwargs ) except Exception : df = pd . read_json ( io . StringIO ( source ), ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) parse_jsonl ( source , title_field = None , text_field = None , ** kwargs ) classmethod \u00a4 Parse lineated texts into the Dataset object. Parameters: Name Type Description Default source str The string or path to file containing the lines to parse. required title_field Optional [ str ] The field name to convert to \"title\". None text_field Optional [ str ] The field name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 @classmethod def parse_jsonl ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string or path to file containing the lines to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_json ( source , lines = True , ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON and JSONL files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) parse_string ( source , labels = None , locations = None ) classmethod \u00a4 Parse lineated texts into the Dataset object. Parameters: Name Type Description Default source str The string containing the lines to parse. required labels Optional [ List [ str ]] The names of the texts. None locations Optional [ List [ str ]] The locations of the texts. None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @classmethod def parse_string ( cls : Type [ \"Model\" ], source : str , labels : Optional [ List [ str ]] = None , locations : Optional [ List [ str ]] = None , ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string containing the lines to parse. labels (Optional[List[str]]): The names of the texts. locations (Optional[List[str]]): The locations of the texts. Returns: Model: A dataset object. \"\"\" if not labels : raise LexosException ( \"Please use the `labels` argument to provide a list of labels for each row in your data.\" ) # Handle files try : with open ( source , \"r\" , encoding = \"utf-8\" ) as f : source = f . readlines () # Handle strings except Exception : source = source . split ( \" \\n \" ) if len ( labels ) != len ( source ): raise LexosException ( f \"The number of labels ( { len ( labels ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) else : data = [{ \"title\" : labels [ i ], \"text\" : line } for i , line in enumerate ( source )] if locations : if len ( locations ) == len ( source ): for i , _ in enumerate ( data ): data [ i ][ \"locations\" ] = locations [ i ] else : raise LexosException ( f \"The number of locations ( { len ( locations ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) return cls . parse_obj ({ \"data\" : data }) texts () property \u00a4 Return the texts of the object data. Returns: Type Description List [ str ] List[str]: The texts of the object data. Source code in lexos\\io\\dataset.py 72 73 74 75 76 77 78 79 @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The texts of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ] lexos.io.dataset.DatasetLoader \u00a4 Loads a dataset. Usage loader = DatasetLoader(source) dataset = loader.dataset Notes Different types of data may require different keyword parameters. Error messages provide some help in identifying what keywords are required. The class will handle lists of sources, but errors may occur if the sources are of different formats or require different arguments or argument values. Source code in lexos\\io\\dataset.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 class DatasetLoader : \"\"\"Loads a dataset. Usage: loader = DatasetLoader(source) dataset = loader.dataset Notes: - Different types of data may require different keyword parameters. Error messages provide some help in identifying what keywords are required. - The class will handle lists of sources, but errors may occur if the sources are of different formats or require different arguments or argument values. \"\"\" def __init__ ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Union [ Dataset , List [ Dataset ]]: \"\"\"Initialise the loader. Args: source (Any): The source type to detect. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Dataset or list of Dataset object. \"\"\" if isinstance ( source , list ): new_data = [ self . load ( item , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) . data for item in source ] # Data a Dataset with the flattened list of dicts self . data = Dataset ( data = list ( itertools . chain ( * new_data ))) else : self . data = self . load ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ] @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ] def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ] def load ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Dataset : \"\"\"Load the given file. Args: source (Any): The source the data to load. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Data object. \"\"\" if not utils . is_dir ( source ) and not utils . is_github_dir ( source ): # and not utils.is_url(source): ext = Path ( source ) . suffix if ext == \"\" or ext == \".txt\" : return Dataset . parse_string ( source , labels , locations ) elif ext == \".csv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".tsv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".xlsx\" : return Dataset . parse_excel ( source , title_col , text_col , ** kwargs ) elif ext == \".json\" : return Dataset . parse_json ( source , title_field , text_field , ** kwargs ) elif ext == \".jsonl\" : return Dataset . parse_jsonl ( source , title_field , text_field , ** kwargs ) elif ext == \".zip\" : return self . _load_zip ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , * kwargs , ) elif utils . is_dir ( source ) or utils . is_github_dir ( source ): new_data = [] if utils . is_github_dir ( source ): paths = utils . get_github_raw_paths ( source ) else : paths = utils . get_paths ( source ) for path in paths : new_data . append ( self . load ( path , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) ) # Return a Dataset with the flattened list of dicts return Dataset ( data = list ( itertools . chain ( * new_data ))) else : raise LexosException ( f \" { source } is an unknown source type or requires different arguments than the other sources in the directory.\" ) def _load_zip ( self , file_path : str , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Dataset : \"\"\" Load a zip file. Args: file_path (str): The path to the file to load. source (Any): The source the data to load. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Data object. \"\"\" new_data = [] with open ( file_path , \"rb\" ) as f : with zipfile . ZipFile ( f ) as zip : with tempfile . TemporaryDirectory () as tempdir : zip . extractall ( tempdir ) for tmp_path in Path ( tempdir ) . glob ( \"**/*\" ): if ( tmp_path . is_file () and not tmp_path . suffix == \"\" and not str ( tmp_path ) . startswith ( \"__MACOSX\" ) and not str ( tmp_path ) . startswith ( \".ds_store\" ) ): new_data . append ( self . load ( tmp_path , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) . data ) # Return a Dataset with the flattened list of dicts return Dataset ( data = list ( itertools . chain ( * new_data ))) __getitem__ ( item ) \u00a4 Get an item from dataset. Parameters: Name Type Description Default item int The index of the item to get. required Returns: Type Description Dict [ str , str ] Dict[str, str]: The item at the given index. Source code in lexos\\io\\dataset.py 437 438 439 440 441 442 443 444 445 446 def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ] __init__ ( source , labels = None , locations = None , title_col = None , text_col = None , title_field = None , text_field = None , location_col = None , location_field = None , ** kwargs ) \u00a4 Initialise the loader. Parameters: Name Type Description Default source Any The source type to detect. required labels List [ str ] The labels to use. None locations Optional [ List [ str ]] The locations of the texts. None title_col str The name of the column containing the titles. None text_col str The name of the column containing the texts. None title_field str The name of the field containing the titles. None text_field str The name of the field containing the texts. None location_col str The name of the column containing the locations. None location_field str The name of the field containing the locations. None Returns: Name Type Description Dataset Union [ Dataset , List [ Dataset ]] A Dataset or list of Dataset object. Source code in lexos\\io\\dataset.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def __init__ ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Union [ Dataset , List [ Dataset ]]: \"\"\"Initialise the loader. Args: source (Any): The source type to detect. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Dataset or list of Dataset object. \"\"\" if isinstance ( source , list ): new_data = [ self . load ( item , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) . data for item in source ] # Data a Dataset with the flattened list of dicts self . data = Dataset ( data = list ( itertools . chain ( * new_data ))) else : self . data = self . load ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) __iter__ () \u00a4 Iterate over the dataset. Returns: Name Type Description Iterable Iterable The dataset. Source code in lexos\\io\\dataset.py 428 429 430 431 432 433 434 435 def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item load ( source , labels = None , locations = None , title_col = None , text_col = None , title_field = None , text_field = None , location_col = None , location_field = None , ** kwargs ) \u00a4 Load the given file. Parameters: Name Type Description Default source Any The source the data to load. required labels List [ str ] The labels to use. None locations Optional [ List [ str ]] The locations of the texts. None title_col str The name of the column containing the titles. None text_col str The name of the column containing the texts. None title_field str The name of the field containing the titles. None text_field str The name of the field containing the texts. None location_col str The name of the column containing the locations. None location_field str The name of the field containing the locations. None Returns: Name Type Description Dataset Dataset A Data object. Source code in lexos\\io\\dataset.py 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 def load ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Dataset : \"\"\"Load the given file. Args: source (Any): The source the data to load. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Data object. \"\"\" if not utils . is_dir ( source ) and not utils . is_github_dir ( source ): # and not utils.is_url(source): ext = Path ( source ) . suffix if ext == \"\" or ext == \".txt\" : return Dataset . parse_string ( source , labels , locations ) elif ext == \".csv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".tsv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".xlsx\" : return Dataset . parse_excel ( source , title_col , text_col , ** kwargs ) elif ext == \".json\" : return Dataset . parse_json ( source , title_field , text_field , ** kwargs ) elif ext == \".jsonl\" : return Dataset . parse_jsonl ( source , title_field , text_field , ** kwargs ) elif ext == \".zip\" : return self . _load_zip ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , * kwargs , ) elif utils . is_dir ( source ) or utils . is_github_dir ( source ): new_data = [] if utils . is_github_dir ( source ): paths = utils . get_github_raw_paths ( source ) else : paths = utils . get_paths ( source ) for path in paths : new_data . append ( self . load ( path , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) ) # Return a Dataset with the flattened list of dicts return Dataset ( data = list ( itertools . chain ( * new_data ))) else : raise LexosException ( f \" { source } is an unknown source type or requires different arguments than the other sources in the directory.\" ) locations () property \u00a4 Return the locations of the object data. Returns: Type Description List [ str ] List[str]: The locations of the object data. Source code in lexos\\io\\dataset.py 398 399 400 401 402 403 404 405 406 407 408 @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None names () property \u00a4 Return the names of the object data. Returns: Type Description List [ str ] List[str]: The names of the object data. Source code in lexos\\io\\dataset.py 410 411 412 413 414 415 416 417 @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ] texts () property \u00a4 Return the texts of the object data. Returns: Type Description List [ str ] List[str]: The names of the object data. Source code in lexos\\io\\dataset.py 419 420 421 422 423 424 425 426 @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ]","title":"Dataset"},{"location":"api/io/dataset/#dataset","text":"The DatasetLoader class is used to load multiple texts stored in a single file.","title":"Dataset"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset","text":"Bases: BaseModel Dataset class. Source code in lexos\\io\\dataset.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 class Dataset ( BaseModel ): \"\"\"Dataset class.\"\"\" data : Optional [ List [ Dict [ str , str ]]] = None class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ] @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The texts of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ] def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ] def df ( self ) -> pd . DataFrame : \"\"\"Return the dataframe of the object data. Returns: pd.DataFrame: The dataframe of the object data. \"\"\" return pd . DataFrame ( self . data ) @classmethod def parse_csv ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse CSV/TSV texts into the Dataset object. Args: source (str): The string or path to file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_csv ( source , ** kwargs ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"CSV and TSV files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( \"\" . join ( err )) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_dict ( cls : Type [ \"Model\" ], source : dict ,) -> \"Model\" : \"\"\"Alias for cls.parse_obj(). Args: source (dict): The dict to parse. Returns: Model: A dataset object. \"\"\" return cls . parse_obj ({ \"data\" : source }) @classmethod def parse_excel ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse Excel files into the Dataset object. Args: source (str): The path to the Excel file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : df = pd . read_excel ( source , ** kwargs ) except Exception as e : raise LexosException ( f \"Could not read { source } : { e } \" ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"Excel files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_json ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse JSON files or strings. Args: source (str): The json string to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : with open ( source ) as f : df = pd . read_json ( f , ** kwargs ) except Exception : df = pd . read_json ( io . StringIO ( source ), ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_jsonl ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string or path to file containing the lines to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_json ( source , lines = True , ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON and JSONL files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )}) @classmethod def parse_string ( cls : Type [ \"Model\" ], source : str , labels : Optional [ List [ str ]] = None , locations : Optional [ List [ str ]] = None , ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string containing the lines to parse. labels (Optional[List[str]]): The names of the texts. locations (Optional[List[str]]): The locations of the texts. Returns: Model: A dataset object. \"\"\" if not labels : raise LexosException ( \"Please use the `labels` argument to provide a list of labels for each row in your data.\" ) # Handle files try : with open ( source , \"r\" , encoding = \"utf-8\" ) as f : source = f . readlines () # Handle strings except Exception : source = source . split ( \" \\n \" ) if len ( labels ) != len ( source ): raise LexosException ( f \"The number of labels ( { len ( labels ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) else : data = [{ \"title\" : labels [ i ], \"text\" : line } for i , line in enumerate ( source )] if locations : if len ( locations ) == len ( source ): for i , _ in enumerate ( data ): data [ i ][ \"locations\" ] = locations [ i ] else : raise LexosException ( f \"The number of locations ( { len ( locations ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) return cls . parse_obj ({ \"data\" : data }) @staticmethod def _get_file_like ( source : str ) -> IO [ AnyStr ]: \"\"\"Read the source into a buffer. Args: source: str: A path or string containing the source. Returns: IO[AnyStr]: A file-like object containing the source. \"\"\" if utils . is_file ( source ) or utils . is_github_dir ( source ) == False : try : with open ( source , \"r\" , encoding = \"utf-8\" ) as f : source = f . read () except : pass return io . StringIO ( source ) else : raise LexosException ( f \" { source } is not a valid file path or input string.\" )","title":"Dataset"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.Config","text":"Config class. Source code in lexos\\io\\dataset.py 46 47 48 49 class Config : \"\"\"Config class.\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.__getitem__","text":"Get an item from dataset. Parameters: Name Type Description Default item int The index of the item to get. required Returns: Type Description Dict [ str , str ] Dict[str, str]: The item at the given index. Source code in lexos\\io\\dataset.py 90 91 92 93 94 95 96 97 98 99 def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ]","title":"__getitem__()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.__iter__","text":"Iterate over the dataset. Returns: Name Type Description Iterable Iterable The dataset. Source code in lexos\\io\\dataset.py 81 82 83 84 85 86 87 88 def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item","title":"__iter__()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.df","text":"Return the dataframe of the object data. Returns: Type Description pd . DataFrame pd.DataFrame: The dataframe of the object data. Source code in lexos\\io\\dataset.py 101 102 103 104 105 106 107 def df ( self ) -> pd . DataFrame : \"\"\"Return the dataframe of the object data. Returns: pd.DataFrame: The dataframe of the object data. \"\"\" return pd . DataFrame ( self . data )","title":"df()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.locations","text":"Return the locations of the object data. Returns: Type Description List [ str ] List[str]: The locations of the object data. Source code in lexos\\io\\dataset.py 51 52 53 54 55 56 57 58 59 60 61 @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None","title":"locations()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.names","text":"Return the names of the object data. Returns: Type Description List [ str ] List[str]: The names of the object data. Source code in lexos\\io\\dataset.py 63 64 65 66 67 68 69 70 @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ]","title":"names()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.parse_csv","text":"Parse CSV/TSV texts into the Dataset object. Parameters: Name Type Description Default source str The string or path to file containing the texts to parse. required title_col Optional [ str ] The column name to convert to \"title\". None text_col Optional [ str ] The column name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 @classmethod def parse_csv ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse CSV/TSV texts into the Dataset object. Args: source (str): The string or path to file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_csv ( source , ** kwargs ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"CSV and TSV files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( \"\" . join ( err )) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )})","title":"parse_csv()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.parse_dict","text":"Alias for cls.parse_obj(). Parameters: Name Type Description Default source dict The dict to parse. required Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 142 143 144 145 146 147 148 149 150 151 152 @classmethod def parse_dict ( cls : Type [ \"Model\" ], source : dict ,) -> \"Model\" : \"\"\"Alias for cls.parse_obj(). Args: source (dict): The dict to parse. Returns: Model: A dataset object. \"\"\" return cls . parse_obj ({ \"data\" : source })","title":"parse_dict()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.parse_excel","text":"Parse Excel files into the Dataset object. Parameters: Name Type Description Default source str The path to the Excel file containing the texts to parse. required title_col Optional [ str ] The column name to convert to \"title\". None text_col Optional [ str ] The column name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 @classmethod def parse_excel ( cls : Type [ \"Model\" ], source : str , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse Excel files into the Dataset object. Args: source (str): The path to the Excel file containing the texts to parse. title_col (Optional[str]): The column name to convert to \"title\". text_col (Optional[str]): The column name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : df = pd . read_excel ( source , ** kwargs ) except Exception as e : raise LexosException ( f \"Could not read { source } : { e } \" ) if title_col : df = df . rename ( columns = { title_col : \"title\" }) if text_col : df = df . rename ( columns = { text_col : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"Excel files must contain headers named `title` and `text`. \" , \"You can convert the names of existing headers to these with the \" , \"`title_col` and `text_col` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )})","title":"parse_excel()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.parse_json","text":"Parse JSON files or strings. Parameters: Name Type Description Default source str The json string to parse. required title_field Optional [ str ] The field name to convert to \"title\". None text_field Optional [ str ] The field name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @classmethod def parse_json ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse JSON files or strings. Args: source (str): The json string to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" try : with open ( source ) as f : df = pd . read_json ( f , ** kwargs ) except Exception : df = pd . read_json ( io . StringIO ( source ), ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )})","title":"parse_json()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.parse_jsonl","text":"Parse lineated texts into the Dataset object. Parameters: Name Type Description Default source str The string or path to file containing the lines to parse. required title_field Optional [ str ] The field name to convert to \"title\". None text_field Optional [ str ] The field name to convert to \"text\". None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 @classmethod def parse_jsonl ( cls : Type [ \"Model\" ], source : str , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string or path to file containing the lines to parse. title_field (Optional[str]): The field name to convert to \"title\". text_field (Optional[str]): The field name to convert to \"text\". Returns: Model: A dataset object. \"\"\" source = cls . _get_file_like ( source ) df = pd . read_json ( source , lines = True , ** kwargs ) if title_field : df = df . rename ( columns = { title_field : \"title\" }) if text_field : df = df . rename ( columns = { text_field : \"text\" }) if \"title\" not in df . columns or \"text\" not in df . columns : err = ( \"JSON and JSONL files must contain fields named `title` and `text`. \" , \"You can convert the names of existing fields to these with the \" , \"`title_field` and `text_field` parameters.\" , ) raise LexosException ( err ) return cls . parse_obj ({ \"data\" : df . to_dict ( orient = \"records\" )})","title":"parse_jsonl()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.parse_string","text":"Parse lineated texts into the Dataset object. Parameters: Name Type Description Default source str The string containing the lines to parse. required labels Optional [ List [ str ]] The names of the texts. None locations Optional [ List [ str ]] The locations of the texts. None Returns: Name Type Description Model 'Model' A dataset object. Source code in lexos\\io\\dataset.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @classmethod def parse_string ( cls : Type [ \"Model\" ], source : str , labels : Optional [ List [ str ]] = None , locations : Optional [ List [ str ]] = None , ) -> \"Model\" : \"\"\"Parse lineated texts into the Dataset object. Args: source (str): The string containing the lines to parse. labels (Optional[List[str]]): The names of the texts. locations (Optional[List[str]]): The locations of the texts. Returns: Model: A dataset object. \"\"\" if not labels : raise LexosException ( \"Please use the `labels` argument to provide a list of labels for each row in your data.\" ) # Handle files try : with open ( source , \"r\" , encoding = \"utf-8\" ) as f : source = f . readlines () # Handle strings except Exception : source = source . split ( \" \\n \" ) if len ( labels ) != len ( source ): raise LexosException ( f \"The number of labels ( { len ( labels ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) else : data = [{ \"title\" : labels [ i ], \"text\" : line } for i , line in enumerate ( source )] if locations : if len ( locations ) == len ( source ): for i , _ in enumerate ( data ): data [ i ][ \"locations\" ] = locations [ i ] else : raise LexosException ( f \"The number of locations ( { len ( locations ) } ) does not match the number of lines ( { len ( source ) } ) in your data.\" ) return cls . parse_obj ({ \"data\" : data })","title":"parse_string()"},{"location":"api/io/dataset/#lexos.io.dataset.Dataset.texts","text":"Return the texts of the object data. Returns: Type Description List [ str ] List[str]: The texts of the object data. Source code in lexos\\io\\dataset.py 72 73 74 75 76 77 78 79 @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The texts of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ]","title":"texts()"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader","text":"Loads a dataset. Usage loader = DatasetLoader(source) dataset = loader.dataset Notes Different types of data may require different keyword parameters. Error messages provide some help in identifying what keywords are required. The class will handle lists of sources, but errors may occur if the sources are of different formats or require different arguments or argument values. Source code in lexos\\io\\dataset.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 class DatasetLoader : \"\"\"Loads a dataset. Usage: loader = DatasetLoader(source) dataset = loader.dataset Notes: - Different types of data may require different keyword parameters. Error messages provide some help in identifying what keywords are required. - The class will handle lists of sources, but errors may occur if the sources are of different formats or require different arguments or argument values. \"\"\" def __init__ ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Union [ Dataset , List [ Dataset ]]: \"\"\"Initialise the loader. Args: source (Any): The source type to detect. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Dataset or list of Dataset object. \"\"\" if isinstance ( source , list ): new_data = [ self . load ( item , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) . data for item in source ] # Data a Dataset with the flattened list of dicts self . data = Dataset ( data = list ( itertools . chain ( * new_data ))) else : self . data = self . load ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ] @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ] def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ] def load ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Dataset : \"\"\"Load the given file. Args: source (Any): The source the data to load. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Data object. \"\"\" if not utils . is_dir ( source ) and not utils . is_github_dir ( source ): # and not utils.is_url(source): ext = Path ( source ) . suffix if ext == \"\" or ext == \".txt\" : return Dataset . parse_string ( source , labels , locations ) elif ext == \".csv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".tsv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".xlsx\" : return Dataset . parse_excel ( source , title_col , text_col , ** kwargs ) elif ext == \".json\" : return Dataset . parse_json ( source , title_field , text_field , ** kwargs ) elif ext == \".jsonl\" : return Dataset . parse_jsonl ( source , title_field , text_field , ** kwargs ) elif ext == \".zip\" : return self . _load_zip ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , * kwargs , ) elif utils . is_dir ( source ) or utils . is_github_dir ( source ): new_data = [] if utils . is_github_dir ( source ): paths = utils . get_github_raw_paths ( source ) else : paths = utils . get_paths ( source ) for path in paths : new_data . append ( self . load ( path , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) ) # Return a Dataset with the flattened list of dicts return Dataset ( data = list ( itertools . chain ( * new_data ))) else : raise LexosException ( f \" { source } is an unknown source type or requires different arguments than the other sources in the directory.\" ) def _load_zip ( self , file_path : str , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Dataset : \"\"\" Load a zip file. Args: file_path (str): The path to the file to load. source (Any): The source the data to load. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Data object. \"\"\" new_data = [] with open ( file_path , \"rb\" ) as f : with zipfile . ZipFile ( f ) as zip : with tempfile . TemporaryDirectory () as tempdir : zip . extractall ( tempdir ) for tmp_path in Path ( tempdir ) . glob ( \"**/*\" ): if ( tmp_path . is_file () and not tmp_path . suffix == \"\" and not str ( tmp_path ) . startswith ( \"__MACOSX\" ) and not str ( tmp_path ) . startswith ( \".ds_store\" ) ): new_data . append ( self . load ( tmp_path , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) . data ) # Return a Dataset with the flattened list of dicts return Dataset ( data = list ( itertools . chain ( * new_data )))","title":"DatasetLoader"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader.__getitem__","text":"Get an item from dataset. Parameters: Name Type Description Default item int The index of the item to get. required Returns: Type Description Dict [ str , str ] Dict[str, str]: The item at the given index. Source code in lexos\\io\\dataset.py 437 438 439 440 441 442 443 444 445 446 def __getitem__ ( self , item : int ) -> Dict [ str , str ]: \"\"\"Get an item from dataset. Args: item: The index of the item to get. Returns: Dict[str, str]: The item at the given index. \"\"\" return self . data [ item ]","title":"__getitem__()"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader.__init__","text":"Initialise the loader. Parameters: Name Type Description Default source Any The source type to detect. required labels List [ str ] The labels to use. None locations Optional [ List [ str ]] The locations of the texts. None title_col str The name of the column containing the titles. None text_col str The name of the column containing the texts. None title_field str The name of the field containing the titles. None text_field str The name of the field containing the texts. None location_col str The name of the column containing the locations. None location_field str The name of the field containing the locations. None Returns: Name Type Description Dataset Union [ Dataset , List [ Dataset ]] A Dataset or list of Dataset object. Source code in lexos\\io\\dataset.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def __init__ ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Union [ Dataset , List [ Dataset ]]: \"\"\"Initialise the loader. Args: source (Any): The source type to detect. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Dataset or list of Dataset object. \"\"\" if isinstance ( source , list ): new_data = [ self . load ( item , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) . data for item in source ] # Data a Dataset with the flattened list of dicts self . data = Dataset ( data = list ( itertools . chain ( * new_data ))) else : self . data = self . load ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , )","title":"__init__()"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader.__iter__","text":"Iterate over the dataset. Returns: Name Type Description Iterable Iterable The dataset. Source code in lexos\\io\\dataset.py 428 429 430 431 432 433 434 435 def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the dataset. Returns: Iterable: The dataset. \"\"\" for item in iter ( self . data ): yield item","title":"__iter__()"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader.load","text":"Load the given file. Parameters: Name Type Description Default source Any The source the data to load. required labels List [ str ] The labels to use. None locations Optional [ List [ str ]] The locations of the texts. None title_col str The name of the column containing the titles. None text_col str The name of the column containing the texts. None title_field str The name of the field containing the titles. None text_field str The name of the field containing the texts. None location_col str The name of the column containing the locations. None location_field str The name of the field containing the locations. None Returns: Name Type Description Dataset Dataset A Data object. Source code in lexos\\io\\dataset.py 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 def load ( self , source : Any , labels : List [ str ] = None , locations : Optional [ List [ str ]] = None , title_col : Optional [ str ] = None , text_col : Optional [ str ] = None , title_field : Optional [ str ] = None , text_field : Optional [ str ] = None , location_col : Optional [ str ] = None , location_field : Optional [ str ] = None , ** kwargs : Dict [ str , str ], ) -> Dataset : \"\"\"Load the given file. Args: source (Any): The source the data to load. labels (List[str]): The labels to use. locations (Optional[List[str]]): The locations of the texts. title_col (str): The name of the column containing the titles. text_col (str): The name of the column containing the texts. title_field (str): The name of the field containing the titles. text_field (str): The name of the field containing the texts. location_col (str): The name of the column containing the locations. location_field (str): The name of the field containing the locations. Returns: Dataset: A Data object. \"\"\" if not utils . is_dir ( source ) and not utils . is_github_dir ( source ): # and not utils.is_url(source): ext = Path ( source ) . suffix if ext == \"\" or ext == \".txt\" : return Dataset . parse_string ( source , labels , locations ) elif ext == \".csv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".tsv\" : return Dataset . parse_csv ( source , title_col , text_col , ** kwargs ) elif ext == \".xlsx\" : return Dataset . parse_excel ( source , title_col , text_col , ** kwargs ) elif ext == \".json\" : return Dataset . parse_json ( source , title_field , text_field , ** kwargs ) elif ext == \".jsonl\" : return Dataset . parse_jsonl ( source , title_field , text_field , ** kwargs ) elif ext == \".zip\" : return self . _load_zip ( source , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , * kwargs , ) elif utils . is_dir ( source ) or utils . is_github_dir ( source ): new_data = [] if utils . is_github_dir ( source ): paths = utils . get_github_raw_paths ( source ) else : paths = utils . get_paths ( source ) for path in paths : new_data . append ( self . load ( path , labels , locations , title_col , text_col , title_field , text_field , location_col , location_field , ** kwargs , ) ) # Return a Dataset with the flattened list of dicts return Dataset ( data = list ( itertools . chain ( * new_data ))) else : raise LexosException ( f \" { source } is an unknown source type or requires different arguments than the other sources in the directory.\" )","title":"load()"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader.locations","text":"Return the locations of the object data. Returns: Type Description List [ str ] List[str]: The locations of the object data. Source code in lexos\\io\\dataset.py 398 399 400 401 402 403 404 405 406 407 408 @property def locations ( self ) -> List [ str ]: \"\"\"Return the locations of the object data. Returns: List[str]: The locations of the object data. \"\"\" if any ( \"location\" in item for item in self . data ): return [ item [ \"locations\" ] for item in self . data ] else : return None","title":"locations()"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader.names","text":"Return the names of the object data. Returns: Type Description List [ str ] List[str]: The names of the object data. Source code in lexos\\io\\dataset.py 410 411 412 413 414 415 416 417 @property def names ( self ) -> List [ str ]: \"\"\"Return the names of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"title\" ] for item in self . data ]","title":"names()"},{"location":"api/io/dataset/#lexos.io.dataset.DatasetLoader.texts","text":"Return the texts of the object data. Returns: Type Description List [ str ] List[str]: The names of the object data. Source code in lexos\\io\\dataset.py 419 420 421 422 423 424 425 426 @property def texts ( self ) -> List [ str ]: \"\"\"Return the texts of the object data. Returns: List[str]: The names of the object data. \"\"\" return [ item [ \"text\" ] for item in self . data ]","title":"texts()"},{"location":"api/io/smart/","text":"Smart \u00a4 The smart Loader class is the primary component of the IO module, superseding the previous basic and advanced Loader . lexos.io.smart.Loader \u00a4 Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\smart.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Loader : \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . names = [] self . locations = [] self . texts = [] self . errors = [] self . decode = True def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the loader. Returns: Iterable: The loader. \"\"\" for i , _ in enumerate ( iter ( self . names )): yield Text ( self . source , self . names [ i ], self . locations [ i ], self . texts [ i ]) def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text. Args: text (Union[bytes, str]): The text to decode. Returns: str: The decoded text. \"\"\" return utils . _decode_bytes ( text ) def _add_text ( self , path : str , text : Union [ bytes , str ]) -> None : \"\"\"Decode and add a text. Args: path (str): The path to the text file. text (str): The text string. \"\"\" try : if self . decode : self . texts . append ( self . _decode ( text )) else : self . texts . append ( text ) self . names . append ( utils . ensure_path ( path ) . stem ) self . locations . append ( path ) except LexosException : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"encoding_error\" ]}) def _ensure_source ( self , source : Union [ List [ Union [ Path , str ]], Path , str ]) -> None : \"\"\"Ensure that either the object or the method supplies a source value. Args: source (Union[List[Union[Path, str]], Path, str]): The source. Returns: None: None. Raises: LexosException: If no source is provided. \"\"\" if source : self . source = source else : try : assert self . source is not None except AssertionError : raise LexosException ( LANG [ \"no_source\" ]) def _handle_source ( self , path : Union [ Path , str ]) -> None : \"\"\"Add a text based on source type. Args: path (str): The path to the text file. \"\"\" ext = utils . ensure_path ( path ) . suffix path = str ( path ) if ext == \".zip\" : self . _handle_zip ( path ) else : if ext in [ \".docx\" , \".pdf\" , \".zip\" ]: with open ( path , \"rb\" ) as f : bytes = io . BytesIO ( f . read ()) if ext == \".docx\" : self . _add_text ( path , docx2txt . process ( bytes )) elif ext == \".pdf\" : self . _add_text ( path , extract_text ( bytes )) elif ext == \".zip\" : self . _handle_zip ( path ) else : with open ( path , \"rb\" ) as f : self . _add_text ( path , f . read ()) def _handle_zip ( self , path : str ) -> None : \"\"\"Extract a zip file and add each text inside. Args: path (str): The path to the zip file. \"\"\" with open ( path , \"rb\" ) as f : with zipfile . ZipFile ( f ) as zip : namelist = [ n for n in zip . namelist () if Path ( n ) . suffix != \"\" ] for info in namelist : if not str ( info ) . startswith ( \"__MACOSX\" ) and not str ( info ) . startswith ( \".ds_store\" ): self . _add_text ( path , zip . read ( info )) def _validate_source ( self , source : Any , is_valid : bool = True ) -> bool : \"\"\"Validate that the source is a string or Path. Args: source (Any): A source. Returns: bool: Whether the source is valid. \"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): is_valid = False return is_valid def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ] = None , decode : bool = True , ) -> None : \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. \"\"\" self . _ensure_source ( source ) if decode : self . decode = decode if not isinstance ( self . source , list ): self . source = [ self . source ] for path in self . source : if self . _validate_source ( path ): if \"github.com\" in str ( path ): filepaths = utils . get_github_raw_paths ( path ) for filepath in filepaths : self . _handle_source ( filepath ) elif utils . is_file ( path ) or utils . is_url ( path ): self . _handle_source ( path ) elif utils . is_dir ( path ): for filepath in utils . ensure_path ( path ) . rglob ( \"*\" ): self . _handle_source ( filepath ) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"io_error\" ]}) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"format_error\" ]}) __init__ () \u00a4 init method. Source code in lexos\\io\\smart.py 40 41 42 43 44 45 46 47 def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . names = [] self . locations = [] self . texts = [] self . errors = [] self . decode = True __iter__ () \u00a4 Iterate over the loader. Returns: Name Type Description Iterable Iterable The loader. Source code in lexos\\io\\smart.py 49 50 51 52 53 54 55 56 def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the loader. Returns: Iterable: The loader. \"\"\" for i , _ in enumerate ( iter ( self . names )): yield Text ( self . source , self . names [ i ], self . locations [ i ], self . texts [ i ]) load ( source = None , decode = True ) \u00a4 Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union [ List [ Path , str ], Path , str ] A source or list of sources. None decode bool Whether to decode the source. True Source code in lexos\\io\\smart.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ] = None , decode : bool = True , ) -> None : \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. \"\"\" self . _ensure_source ( source ) if decode : self . decode = decode if not isinstance ( self . source , list ): self . source = [ self . source ] for path in self . source : if self . _validate_source ( path ): if \"github.com\" in str ( path ): filepaths = utils . get_github_raw_paths ( path ) for filepath in filepaths : self . _handle_source ( filepath ) elif utils . is_file ( path ) or utils . is_url ( path ): self . _handle_source ( path ) elif utils . is_dir ( path ): for filepath in utils . ensure_path ( path ) . rglob ( \"*\" ): self . _handle_source ( filepath ) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"io_error\" ]}) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"format_error\" ]}) lexos.io.smart.Text \u00a4 Class for accessing a text from an iterator. Source code in lexos\\io\\smart.py 194 195 196 197 198 199 200 201 202 203 204 class Text : \"\"\"Class for accessing a text from an iterator.\"\"\" def __init__ ( self , source : str = None , name : str = \"\" , location : str = \"\" , text : str = \"\" ) -> None : \"\"\"__init__ method.\"\"\" self . source = source self . name = name self . location = location self . text = text __init__ ( source = None , name = '' , location = '' , text = '' ) \u00a4 init method. Source code in lexos\\io\\smart.py 197 198 199 200 201 202 203 204 def __init__ ( self , source : str = None , name : str = \"\" , location : str = \"\" , text : str = \"\" ) -> None : \"\"\"__init__ method.\"\"\" self . source = source self . name = name self . location = location self . text = text","title":"Smart"},{"location":"api/io/smart/#smart","text":"The smart Loader class is the primary component of the IO module, superseding the previous basic and advanced Loader .","title":"Smart"},{"location":"api/io/smart/#lexos.io.smart.Loader","text":"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\smart.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Loader : \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . names = [] self . locations = [] self . texts = [] self . errors = [] self . decode = True def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the loader. Returns: Iterable: The loader. \"\"\" for i , _ in enumerate ( iter ( self . names )): yield Text ( self . source , self . names [ i ], self . locations [ i ], self . texts [ i ]) def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text. Args: text (Union[bytes, str]): The text to decode. Returns: str: The decoded text. \"\"\" return utils . _decode_bytes ( text ) def _add_text ( self , path : str , text : Union [ bytes , str ]) -> None : \"\"\"Decode and add a text. Args: path (str): The path to the text file. text (str): The text string. \"\"\" try : if self . decode : self . texts . append ( self . _decode ( text )) else : self . texts . append ( text ) self . names . append ( utils . ensure_path ( path ) . stem ) self . locations . append ( path ) except LexosException : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"encoding_error\" ]}) def _ensure_source ( self , source : Union [ List [ Union [ Path , str ]], Path , str ]) -> None : \"\"\"Ensure that either the object or the method supplies a source value. Args: source (Union[List[Union[Path, str]], Path, str]): The source. Returns: None: None. Raises: LexosException: If no source is provided. \"\"\" if source : self . source = source else : try : assert self . source is not None except AssertionError : raise LexosException ( LANG [ \"no_source\" ]) def _handle_source ( self , path : Union [ Path , str ]) -> None : \"\"\"Add a text based on source type. Args: path (str): The path to the text file. \"\"\" ext = utils . ensure_path ( path ) . suffix path = str ( path ) if ext == \".zip\" : self . _handle_zip ( path ) else : if ext in [ \".docx\" , \".pdf\" , \".zip\" ]: with open ( path , \"rb\" ) as f : bytes = io . BytesIO ( f . read ()) if ext == \".docx\" : self . _add_text ( path , docx2txt . process ( bytes )) elif ext == \".pdf\" : self . _add_text ( path , extract_text ( bytes )) elif ext == \".zip\" : self . _handle_zip ( path ) else : with open ( path , \"rb\" ) as f : self . _add_text ( path , f . read ()) def _handle_zip ( self , path : str ) -> None : \"\"\"Extract a zip file and add each text inside. Args: path (str): The path to the zip file. \"\"\" with open ( path , \"rb\" ) as f : with zipfile . ZipFile ( f ) as zip : namelist = [ n for n in zip . namelist () if Path ( n ) . suffix != \"\" ] for info in namelist : if not str ( info ) . startswith ( \"__MACOSX\" ) and not str ( info ) . startswith ( \".ds_store\" ): self . _add_text ( path , zip . read ( info )) def _validate_source ( self , source : Any , is_valid : bool = True ) -> bool : \"\"\"Validate that the source is a string or Path. Args: source (Any): A source. Returns: bool: Whether the source is valid. \"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): is_valid = False return is_valid def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ] = None , decode : bool = True , ) -> None : \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. \"\"\" self . _ensure_source ( source ) if decode : self . decode = decode if not isinstance ( self . source , list ): self . source = [ self . source ] for path in self . source : if self . _validate_source ( path ): if \"github.com\" in str ( path ): filepaths = utils . get_github_raw_paths ( path ) for filepath in filepaths : self . _handle_source ( filepath ) elif utils . is_file ( path ) or utils . is_url ( path ): self . _handle_source ( path ) elif utils . is_dir ( path ): for filepath in utils . ensure_path ( path ) . rglob ( \"*\" ): self . _handle_source ( filepath ) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"io_error\" ]}) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"format_error\" ]})","title":"Loader"},{"location":"api/io/smart/#lexos.io.smart.Loader.__init__","text":"init method. Source code in lexos\\io\\smart.py 40 41 42 43 44 45 46 47 def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . names = [] self . locations = [] self . texts = [] self . errors = [] self . decode = True","title":"__init__()"},{"location":"api/io/smart/#lexos.io.smart.Loader.__iter__","text":"Iterate over the loader. Returns: Name Type Description Iterable Iterable The loader. Source code in lexos\\io\\smart.py 49 50 51 52 53 54 55 56 def __iter__ ( self ) -> Iterable : \"\"\"Iterate over the loader. Returns: Iterable: The loader. \"\"\" for i , _ in enumerate ( iter ( self . names )): yield Text ( self . source , self . names [ i ], self . locations [ i ], self . texts [ i ])","title":"__iter__()"},{"location":"api/io/smart/#lexos.io.smart.Loader.load","text":"Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union [ List [ Path , str ], Path , str ] A source or list of sources. None decode bool Whether to decode the source. True Source code in lexos\\io\\smart.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ] = None , decode : bool = True , ) -> None : \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. \"\"\" self . _ensure_source ( source ) if decode : self . decode = decode if not isinstance ( self . source , list ): self . source = [ self . source ] for path in self . source : if self . _validate_source ( path ): if \"github.com\" in str ( path ): filepaths = utils . get_github_raw_paths ( path ) for filepath in filepaths : self . _handle_source ( filepath ) elif utils . is_file ( path ) or utils . is_url ( path ): self . _handle_source ( path ) elif utils . is_dir ( path ): for filepath in utils . ensure_path ( path ) . rglob ( \"*\" ): self . _handle_source ( filepath ) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"io_error\" ]}) else : self . errors . append ({ \"path\" : path , \"message\" : LANG [ \"format_error\" ]})","title":"load()"},{"location":"api/io/smart/#lexos.io.smart.Text","text":"Class for accessing a text from an iterator. Source code in lexos\\io\\smart.py 194 195 196 197 198 199 200 201 202 203 204 class Text : \"\"\"Class for accessing a text from an iterator.\"\"\" def __init__ ( self , source : str = None , name : str = \"\" , location : str = \"\" , text : str = \"\" ) -> None : \"\"\"__init__ method.\"\"\" self . source = source self . name = name self . location = location self . text = text","title":"Text"},{"location":"api/io/smart/#lexos.io.smart.Text.__init__","text":"init method. Source code in lexos\\io\\smart.py 197 198 199 200 201 202 203 204 def __init__ ( self , source : str = None , name : str = \"\" , location : str = \"\" , text : str = \"\" ) -> None : \"\"\"__init__ method.\"\"\" self . source = source self . name = name self . location = location self . text = text","title":"__init__()"},{"location":"api/language_model/","text":"Language Model \u00a4 The language_model module is used to train, evaluate, and package language models. It consists of a LanguageModel class, a Timer class, and three debugging functions. lexos.language_model.LanguageModel \u00a4 Create a LanguageModel object. Source code in lexos\\language_model\\__init__.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 class LanguageModel : \"\"\"Create a LanguageModel object.\"\"\" def __init__ ( self , model_dir : str = \"language_model\" , config_file : str = \"config.cfg\" , training_file : str = \"train.conllu\" , dev_file : str = \"dev.conllu\" , test_file : str = \"test.conllu\" , gpu : int = - 1 , lang : str = \"xx\" , package_name : str = \"model_sm\" , package_version : str = \"1.0.0\" , components : List [ str ] = [ \"tagger\" ], optimize : str = \"efficiency\" , exclusive_classes : bool = True , force : bool = False , recipe : str = None , ): \"\"\"Initialise the LanguageModel object.\"\"\" self . model_dir = model_dir self . config = None self . config_file = config_file self . config_filepath = f \" { self . model_dir } / { self . config_file } \" self . training_file = training_file self . dev_file = dev_file self . test_file = test_file self . gpu = gpu self . lang = lang self . package_name = package_name self . package_version = package_version self . components = components self . optimize = optimize self . exclusive_classes = exclusive_classes msg = Printer () # Issue a warning if the user is working in a Jupyter notebook if ( is_in_jupyter and Path . cwd () . name == \"lexos\" and Path ( self . model_dir ) . resolve () . parts [ - 2 ] == \"lexos\" ): msg . warn ( \"It looks like you're calling `LanguageModel()` from within a \" \"Jupyter notebook or a similar environment. If you set the system path \" \"to the Lexos API folder to import locally, you should configure \" \"your model directory with an absolute path or a path relative. \" \"to the lexos directory.\" ) # Otherwise, we're safe to proceed else : # Create the model directory if it doesn't exist Path ( self . model_dir ) . mkdir ( parents = True , exist_ok = True ) # Create model folders self . assets_dir = f \" { self . model_dir } /assets/ { self . lang } \" self . corpus_dir = f \" { self . model_dir } /corpus/ { self . lang } \" self . metrics_dir = f \" { self . model_dir } /metrics/ { self . lang } \" self . training_dir = f \" { self . model_dir } /training/ { self . lang } \" for dir in [ self . assets_dir , self . corpus_dir , self . metrics_dir , self . training_dir , ]: Path ( dir ) . mkdir ( parents = True , exist_ok = True ) msg . good ( f \"Created assets directory: { self . assets_dir } .\" ) msg . good ( f \"Created corpus directory: { self . corpus_dir } .\" ) msg . good ( f \"Created metrics directory: { self . metrics_dir } .\" ) msg . good ( f \"Created training directory: { self . training_dir } .\" ) # Create the base config file if it doesn't exist if not Path ( self . config_filepath ) . is_file () or force == True : # Load a recipe if one was provided if recipe : try : self . load_config ( recipe ) except FileNotFoundError : msg . fail ( f \"Recipe file { recipe } not found.\" ) else : if self . gpu == - 1 : gpu = False else : gpu = True self . config = init_config ( lang = self . lang , pipeline = self . components , optimize = self . optimize , gpu = gpu , ) # Add the file paths to the config files = { \"train\" : self . training_file , \"dev\" : self . dev_file , \"test\" : self . test_file , } for k , file in files . items (): if file is not None : self . config [ \"paths\" ][ k ] = f ' { self . corpus_dir } / { file . replace ( \".conllu\" , \".spacy\" ) } ' self . config . to_disk ( self . config_filepath ) self . _fix_config () msg . good ( f \"Saved config file to { self . config_filepath } .\" ) msg . text ( \"You can now add your data and train your pipeline.\" ) else : msg . warn ( f \" { self . config_filepath } already exists. \" \"You can now add your data and train your pipeline.\" ) def _fix_config ( self ) -> None : \"\"\"Fix the config file. This is a hack because init_config() does not generate useable max_length and score_weights values. \"\"\" self . config [ \"corpora\" ][ \"train\" ][ \"max_length\" ] = 2000 self . config [ \"training\" ][ \"score_weights\" ] = { \"morph_per_feat\" : None , \"dep_las_per_type\" : None , \"sents_p\" : None , \"sents_r\" : None , \"tag_acc\" : 0.33 , \"pos_acc\" : 0.17 , \"morph_acc\" : 0.17 , \"dep_uas\" : 0.17 , \"dep_las\" : 0.17 , \"sents_f\" : 0.0 , } self . config . to_disk ( self . config_filepath ) def convert_assets ( self , n_sents = 10 , merge_subtokens = True ): \"\"\"Convert CONLLU assets to spaCy DocBins. Args: n_sents (int): The number of sentences per doc (0 to disable). merge_subtokens (bool): Whether to merge CoNLL-U subtokens. \"\"\" msg = Printer () success = True files = [ self . training_file , self . dev_file , self . test_file ] for file in files : filepath = Path ( f \" { self . assets_dir } / { file } \" ) if filepath . exists (): try : convert ( input_path = filepath , output_dir = self . corpus_dir , file_type = \"spacy\" , converter = \"conllu\" , n_sents = n_sents , merge_subtokens = merge_subtokens , ) except Exception : success = False msg . fail ( f \"Error converting { file } . Check that the CONLLU file formatting is valid.\" ) if success : msg . good ( f \"Assets converted and saved to { self . corpus_dir } .\" ) msg . text ( \"You can now train your pipeline.\" ) else : msg . fail ( \"Failed to convert one or more assets.\" ) def copy_assets ( self , training_file : str = None , dev_file : str = None , test_file : str = None ): \"\"\"Copy assets to the assets folder. Args: training_file (str): The path to the training file to copy. dev_file (str): The path to the dev file to copy. test_file (str): The path to the test file to copy. \"\"\" for asset in [ training_file , dev_file , test_file ]: if asset is not None : try : with open ( asset , \"rb\" ) as f : content = f . read () except IOError : raise IOError ( f \" { asset } not found.\" ) save_path = f \" { self . assets_dir } / { Path ( asset ) . name } \" with open ( save_path , \"wb\" ) as f : f . write ( content ) msg = Printer () msg . good ( f \"Copied assets to { self . assets_dir } .\" ) def evaluate ( self , model : str = None , testfile : Union [ Path , str ] = None , output : Path = None , use_gpu : int = - 1 , gold_preproc : bool = False , displacy_path : Optional [ Path ] = None , displacy_limit : int = 25 , silent : bool = False , ) -> None : \"\"\"Evaluate a spaCy model. Args: model (str): The path to the model to evaluate. testfile (Union[Path, str]): The path to the test file to evaluate. output (Path): The path to the output file. use_gpu (int): The GPU to use. gold_preproc (bool): Whether to use gold preprocessing. displacy_path (Optional[Path]): The path to the displacy package. displacy_limit (int): The number of tokens to display. silent (bool): Whether to suppress output. Returns: Dict[str, Any]: The evaluation results. \"\"\" if isinstance ( testfile , str ): testfile = Path ( testfile ) output = Path ( f \" { self . metrics_dir } / { self . lang } \" ) . with_suffix ( \".json\" ) if not use_gpu : use_gpu = self . gpu evaluate ( model = model , data_path = testfile , output = output , use_gpu = use_gpu , gold_preproc = gold_preproc , displacy_path = displacy_path , displacy_limit = displacy_limit , silent = silent , ) def fill_config ( self , path : Union [ str , Path ], output_file : Union [ str , Path ] = None , pretraining : bool = False , diff : bool = False , code_path : Union [ str , Path ] = None , ) -> None : \"\"\" Fill partial config file with default values. Adds all missing settings from the current config and creates all objects, checks the registered functions for their default values, and updates the config file. Although the `LanguageModel` class automatically generates a full config file, this method may be useful for debugging. DOCS: https://spacy.io/api/cli#init-fill-config Args: path (Union[str, Path]): Path to the config file to fill. output_file (Union[str, Path]): Path to output .cfg file. pretraining (bool): Include config for pretraining (with \"spacy pretrain\"). diff (bool): Print a visual diff highlighting the changes. code_path (Union[str, Path]): Path to Python file with additional code (registered functions) to be imported. \"\"\" if not output_path : output_path = self . config_filepath if isinstance ( path , str ): path = Path ( path ) if isinstance ( output_file , str ): output_file = Path ( output_file ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) fill_config ( output_file , path , pretraining = pretraining , diff = diff ) self . config = Config () . from_disk ( self . config_filepath ) def load_config ( self , filepath : Path = None ): \"\"\"Load the config from the config file. Use this method if you wish to load the config from a different file. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to the config file. \"\"\" if not filepath : filepath = self . config_filepath self . config = load_config ( filepath ) self . config . to_disk ( self . config_filepath ) def package ( self , input_dir : str = None , output_dir : str = None , meta_path : Optional [ str ] = None , code_paths : Optional [ List [ str ]] = [], name : str = None , version : str = None , create_meta : bool = False , create_sdist : bool = True , create_wheel : bool = False , force : bool = False , silent : bool = False , ) -> None : \"\"\"Package the model so that it can be installed.\"\"\" input_dir = Path ( input_dir ) output_dir = Path ( output_dir ) if meta_path : meta_path = Path ( meta_path ) if code_paths : code_paths = [ Path ( p ) for p in code_paths ] self . packages_dir = output_dir output_dir . mkdir ( parents = True , exist_ok = True ) package ( input_dir , output_dir , meta_path = meta_path , code_paths = code_paths , name = name , version = version , create_meta = create_meta , create_sdist = create_sdist , create_wheel = create_wheel , force = force , silent = silent , ) # Print the paths to the package tarfile = f \" { self . lang } _ { name } - { version } .tar.gz\" print ( f \"Model Directory for spacy.load(): { output_dir } / { self . lang } - { version } \" ) print ( f \"Binary file (for pip install): { output_dir } / { self . lang } - { version } /dist/ { tarfile } \" ) def save_config ( self , filepath : Path = None ): \"\"\"Save the config from the config file. Use this method to save a config file after making modifications. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to a config file (for saving a copy). \"\"\" if not filepath : filepath = self . config_filepath self . config . to_disk ( self . config_filepath ) def train ( self ): \"\"\"Train the corpus.\"\"\" timer = Timer () config = load_config ( self . config_filepath ) nlp = init_nlp ( config , use_gpu = self . gpu ) train ( nlp = nlp , output_path = Path ( self . training_dir ), use_gpu = self . gpu , stdout = sys . stdout , stderr = sys . stderr , ) msg = Printer () msg . text ( f \"Time elapsed: { timer . get_time_elapsed () } \" ) __init__ ( model_dir = 'language_model' , config_file = 'config.cfg' , training_file = 'train.conllu' , dev_file = 'dev.conllu' , test_file = 'test.conllu' , gpu =- 1 , lang = 'xx' , package_name = 'model_sm' , package_version = '1.0.0' , components = [ 'tagger' ], optimize = 'efficiency' , exclusive_classes = True , force = False , recipe = None ) \u00a4 Initialise the LanguageModel object. Source code in lexos\\language_model\\__init__.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __init__ ( self , model_dir : str = \"language_model\" , config_file : str = \"config.cfg\" , training_file : str = \"train.conllu\" , dev_file : str = \"dev.conllu\" , test_file : str = \"test.conllu\" , gpu : int = - 1 , lang : str = \"xx\" , package_name : str = \"model_sm\" , package_version : str = \"1.0.0\" , components : List [ str ] = [ \"tagger\" ], optimize : str = \"efficiency\" , exclusive_classes : bool = True , force : bool = False , recipe : str = None , ): \"\"\"Initialise the LanguageModel object.\"\"\" self . model_dir = model_dir self . config = None self . config_file = config_file self . config_filepath = f \" { self . model_dir } / { self . config_file } \" self . training_file = training_file self . dev_file = dev_file self . test_file = test_file self . gpu = gpu self . lang = lang self . package_name = package_name self . package_version = package_version self . components = components self . optimize = optimize self . exclusive_classes = exclusive_classes msg = Printer () # Issue a warning if the user is working in a Jupyter notebook if ( is_in_jupyter and Path . cwd () . name == \"lexos\" and Path ( self . model_dir ) . resolve () . parts [ - 2 ] == \"lexos\" ): msg . warn ( \"It looks like you're calling `LanguageModel()` from within a \" \"Jupyter notebook or a similar environment. If you set the system path \" \"to the Lexos API folder to import locally, you should configure \" \"your model directory with an absolute path or a path relative. \" \"to the lexos directory.\" ) # Otherwise, we're safe to proceed else : # Create the model directory if it doesn't exist Path ( self . model_dir ) . mkdir ( parents = True , exist_ok = True ) # Create model folders self . assets_dir = f \" { self . model_dir } /assets/ { self . lang } \" self . corpus_dir = f \" { self . model_dir } /corpus/ { self . lang } \" self . metrics_dir = f \" { self . model_dir } /metrics/ { self . lang } \" self . training_dir = f \" { self . model_dir } /training/ { self . lang } \" for dir in [ self . assets_dir , self . corpus_dir , self . metrics_dir , self . training_dir , ]: Path ( dir ) . mkdir ( parents = True , exist_ok = True ) msg . good ( f \"Created assets directory: { self . assets_dir } .\" ) msg . good ( f \"Created corpus directory: { self . corpus_dir } .\" ) msg . good ( f \"Created metrics directory: { self . metrics_dir } .\" ) msg . good ( f \"Created training directory: { self . training_dir } .\" ) # Create the base config file if it doesn't exist if not Path ( self . config_filepath ) . is_file () or force == True : # Load a recipe if one was provided if recipe : try : self . load_config ( recipe ) except FileNotFoundError : msg . fail ( f \"Recipe file { recipe } not found.\" ) else : if self . gpu == - 1 : gpu = False else : gpu = True self . config = init_config ( lang = self . lang , pipeline = self . components , optimize = self . optimize , gpu = gpu , ) # Add the file paths to the config files = { \"train\" : self . training_file , \"dev\" : self . dev_file , \"test\" : self . test_file , } for k , file in files . items (): if file is not None : self . config [ \"paths\" ][ k ] = f ' { self . corpus_dir } / { file . replace ( \".conllu\" , \".spacy\" ) } ' self . config . to_disk ( self . config_filepath ) self . _fix_config () msg . good ( f \"Saved config file to { self . config_filepath } .\" ) msg . text ( \"You can now add your data and train your pipeline.\" ) else : msg . warn ( f \" { self . config_filepath } already exists. \" \"You can now add your data and train your pipeline.\" ) convert_assets ( n_sents = 10 , merge_subtokens = True ) \u00a4 Convert CONLLU assets to spaCy DocBins. Parameters: Name Type Description Default n_sents int The number of sentences per doc (0 to disable). 10 merge_subtokens bool Whether to merge CoNLL-U subtokens. True Source code in lexos\\language_model\\__init__.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def convert_assets ( self , n_sents = 10 , merge_subtokens = True ): \"\"\"Convert CONLLU assets to spaCy DocBins. Args: n_sents (int): The number of sentences per doc (0 to disable). merge_subtokens (bool): Whether to merge CoNLL-U subtokens. \"\"\" msg = Printer () success = True files = [ self . training_file , self . dev_file , self . test_file ] for file in files : filepath = Path ( f \" { self . assets_dir } / { file } \" ) if filepath . exists (): try : convert ( input_path = filepath , output_dir = self . corpus_dir , file_type = \"spacy\" , converter = \"conllu\" , n_sents = n_sents , merge_subtokens = merge_subtokens , ) except Exception : success = False msg . fail ( f \"Error converting { file } . Check that the CONLLU file formatting is valid.\" ) if success : msg . good ( f \"Assets converted and saved to { self . corpus_dir } .\" ) msg . text ( \"You can now train your pipeline.\" ) else : msg . fail ( \"Failed to convert one or more assets.\" ) copy_assets ( training_file = None , dev_file = None , test_file = None ) \u00a4 Copy assets to the assets folder. Parameters: Name Type Description Default training_file str The path to the training file to copy. None dev_file str The path to the dev file to copy. None test_file str The path to the test file to copy. None Source code in lexos\\language_model\\__init__.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def copy_assets ( self , training_file : str = None , dev_file : str = None , test_file : str = None ): \"\"\"Copy assets to the assets folder. Args: training_file (str): The path to the training file to copy. dev_file (str): The path to the dev file to copy. test_file (str): The path to the test file to copy. \"\"\" for asset in [ training_file , dev_file , test_file ]: if asset is not None : try : with open ( asset , \"rb\" ) as f : content = f . read () except IOError : raise IOError ( f \" { asset } not found.\" ) save_path = f \" { self . assets_dir } / { Path ( asset ) . name } \" with open ( save_path , \"wb\" ) as f : f . write ( content ) msg = Printer () msg . good ( f \"Copied assets to { self . assets_dir } .\" ) evaluate ( model = None , testfile = None , output = None , use_gpu =- 1 , gold_preproc = False , displacy_path = None , displacy_limit = 25 , silent = False ) \u00a4 Evaluate a spaCy model. Parameters: Name Type Description Default model str The path to the model to evaluate. None testfile Union [ Path , str ] The path to the test file to evaluate. None output Path The path to the output file. None use_gpu int The GPU to use. -1 gold_preproc bool Whether to use gold preprocessing. False displacy_path Optional [ Path ] The path to the displacy package. None displacy_limit int The number of tokens to display. 25 silent bool Whether to suppress output. False Returns: Type Description None Dict[str, Any]: The evaluation results. Source code in lexos\\language_model\\__init__.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def evaluate ( self , model : str = None , testfile : Union [ Path , str ] = None , output : Path = None , use_gpu : int = - 1 , gold_preproc : bool = False , displacy_path : Optional [ Path ] = None , displacy_limit : int = 25 , silent : bool = False , ) -> None : \"\"\"Evaluate a spaCy model. Args: model (str): The path to the model to evaluate. testfile (Union[Path, str]): The path to the test file to evaluate. output (Path): The path to the output file. use_gpu (int): The GPU to use. gold_preproc (bool): Whether to use gold preprocessing. displacy_path (Optional[Path]): The path to the displacy package. displacy_limit (int): The number of tokens to display. silent (bool): Whether to suppress output. Returns: Dict[str, Any]: The evaluation results. \"\"\" if isinstance ( testfile , str ): testfile = Path ( testfile ) output = Path ( f \" { self . metrics_dir } / { self . lang } \" ) . with_suffix ( \".json\" ) if not use_gpu : use_gpu = self . gpu evaluate ( model = model , data_path = testfile , output = output , use_gpu = use_gpu , gold_preproc = gold_preproc , displacy_path = displacy_path , displacy_limit = displacy_limit , silent = silent , ) fill_config ( path , output_file = None , pretraining = False , diff = False , code_path = None ) \u00a4 Fill partial config file with default values. Adds all missing settings from the current config and creates all objects, checks the registered functions for their default values, and updates the config file. Although the LanguageModel class automatically generates a full config file, this method may be useful for debugging. DOCS: https://spacy.io/api/cli#init-fill-config Parameters: Name Type Description Default path Union [ str , Path ] Path to the config file to fill. required output_file Union [ str , Path ] Path to output .cfg file. None pretraining bool Include config for pretraining (with \"spacy pretrain\"). False diff bool Print a visual diff highlighting the changes. False code_path Union [ str , Path ] Path to Python file with additional code (registered functions) to be imported. None Source code in lexos\\language_model\\__init__.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def fill_config ( self , path : Union [ str , Path ], output_file : Union [ str , Path ] = None , pretraining : bool = False , diff : bool = False , code_path : Union [ str , Path ] = None , ) -> None : \"\"\" Fill partial config file with default values. Adds all missing settings from the current config and creates all objects, checks the registered functions for their default values, and updates the config file. Although the `LanguageModel` class automatically generates a full config file, this method may be useful for debugging. DOCS: https://spacy.io/api/cli#init-fill-config Args: path (Union[str, Path]): Path to the config file to fill. output_file (Union[str, Path]): Path to output .cfg file. pretraining (bool): Include config for pretraining (with \"spacy pretrain\"). diff (bool): Print a visual diff highlighting the changes. code_path (Union[str, Path]): Path to Python file with additional code (registered functions) to be imported. \"\"\" if not output_path : output_path = self . config_filepath if isinstance ( path , str ): path = Path ( path ) if isinstance ( output_file , str ): output_file = Path ( output_file ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) fill_config ( output_file , path , pretraining = pretraining , diff = diff ) self . config = Config () . from_disk ( self . config_filepath ) load_config ( filepath = None ) \u00a4 Load the config from the config file. Use this method if you wish to load the config from a different file. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the LanguageModel . Parameters: Name Type Description Default filepath Path The path to the config file. None Source code in lexos\\language_model\\__init__.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def load_config ( self , filepath : Path = None ): \"\"\"Load the config from the config file. Use this method if you wish to load the config from a different file. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to the config file. \"\"\" if not filepath : filepath = self . config_filepath self . config = load_config ( filepath ) self . config . to_disk ( self . config_filepath ) package ( input_dir = None , output_dir = None , meta_path = None , code_paths = [], name = None , version = None , create_meta = False , create_sdist = True , create_wheel = False , force = False , silent = False ) \u00a4 Package the model so that it can be installed. Source code in lexos\\language_model\\__init__.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def package ( self , input_dir : str = None , output_dir : str = None , meta_path : Optional [ str ] = None , code_paths : Optional [ List [ str ]] = [], name : str = None , version : str = None , create_meta : bool = False , create_sdist : bool = True , create_wheel : bool = False , force : bool = False , silent : bool = False , ) -> None : \"\"\"Package the model so that it can be installed.\"\"\" input_dir = Path ( input_dir ) output_dir = Path ( output_dir ) if meta_path : meta_path = Path ( meta_path ) if code_paths : code_paths = [ Path ( p ) for p in code_paths ] self . packages_dir = output_dir output_dir . mkdir ( parents = True , exist_ok = True ) package ( input_dir , output_dir , meta_path = meta_path , code_paths = code_paths , name = name , version = version , create_meta = create_meta , create_sdist = create_sdist , create_wheel = create_wheel , force = force , silent = silent , ) # Print the paths to the package tarfile = f \" { self . lang } _ { name } - { version } .tar.gz\" print ( f \"Model Directory for spacy.load(): { output_dir } / { self . lang } - { version } \" ) print ( f \"Binary file (for pip install): { output_dir } / { self . lang } - { version } /dist/ { tarfile } \" ) save_config ( filepath = None ) \u00a4 Save the config from the config file. Use this method to save a config file after making modifications. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the LanguageModel . Parameters: Name Type Description Default filepath Path The path to a config file (for saving a copy). None Source code in lexos\\language_model\\__init__.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def save_config ( self , filepath : Path = None ): \"\"\"Save the config from the config file. Use this method to save a config file after making modifications. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to a config file (for saving a copy). \"\"\" if not filepath : filepath = self . config_filepath self . config . to_disk ( self . config_filepath ) train () \u00a4 Train the corpus. Source code in lexos\\language_model\\__init__.py 388 389 390 391 392 393 394 395 396 397 398 399 400 401 def train ( self ): \"\"\"Train the corpus.\"\"\" timer = Timer () config = load_config ( self . config_filepath ) nlp = init_nlp ( config , use_gpu = self . gpu ) train ( nlp = nlp , output_path = Path ( self . training_dir ), use_gpu = self . gpu , stdout = sys . stdout , stderr = sys . stderr , ) msg = Printer () msg . text ( f \"Time elapsed: { timer . get_time_elapsed () } \" ) lexos.language_model.Timer \u00a4 Create a timer object. Source code in lexos\\language_model\\__init__.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Timer : \"\"\"Create a timer object.\"\"\" def __init__ ( self ): \"\"\"Initialise the timer object.\"\"\" self . start = time () def get_time_elapsed ( self ): \"\"\"Get the elapsed time and format it as hours, minutes, and seconds.\"\"\" end = time () m , s = divmod ( end - self . start , 60 ) h , m = divmod ( m , 60 ) time_str = \" %02d : %02d : %02d \" % ( h , m , s ) return time_str __init__ () \u00a4 Initialise the timer object. Source code in lexos\\language_model\\__init__.py 31 32 33 def __init__ ( self ): \"\"\"Initialise the timer object.\"\"\" self . start = time () get_time_elapsed () \u00a4 Get the elapsed time and format it as hours, minutes, and seconds. Source code in lexos\\language_model\\__init__.py 35 36 37 38 39 40 41 def get_time_elapsed ( self ): \"\"\"Get the elapsed time and format it as hours, minutes, and seconds.\"\"\" end = time () m , s = divmod ( end - self . start , 60 ) h , m = divmod ( m , 60 ) time_str = \" %02d : %02d : %02d \" % ( h , m , s ) return time_str lexos . language_model . debug_config ( config_path , overrides = {}, code_path = None , show_funcs = False , show_vars = False ) \u00a4 Debug a config file and show validation errors. The function will create all objects in the tree and validate them. Note that some config validation errors are blocking and will prevent the rest of the config from being resolved. This means that you may not see all validation errors at once and some issues are only shown once previous errors have been fixed. As with the 'train' command, you can override settings from the config by passing arguments in the overrides dict. DOCS: https://spacy.io/api/cli#debug-config Parameters: Name Type Description Default config_path Union [ str , Path ] Path to the configuration file. required overrides dict A dictionary of config overrides. {} code_path Union [ str , Path ] Path to Python file with additional code (registered functions) to be imported. None show_funcs bool Show an overview of all registered functions used in the config and where they come from (modules, files etc.). False show_vars bool Show an overview of all variables referenced in the config and their values. This will also reflect variables overwritten in the function call. False Source code in lexos\\language_model\\__init__.py 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def debug_config ( config_path : Union [ str , Path ], overrides : dict = {}, code_path : Union [ str , Path ] = None , show_funcs : bool = False , show_vars : bool = False , ): \"\"\"Debug a config file and show validation errors. The function will create all objects in the tree and validate them. Note that some config validation errors are blocking and will prevent the rest of the config from being resolved. This means that you may not see all validation errors at once and some issues are only shown once previous errors have been fixed. As with the 'train' command, you can override settings from the config by passing arguments in the `overrides` dict. DOCS: https://spacy.io/api/cli#debug-config Args: config_path (Union[str, Path]): Path to the configuration file. overrides (dict): A dictionary of config overrides. code_path (Union[str, Path]): Path to Python file with additional code (registered functions) to be imported. show_funcs (bool): Show an overview of all registered functions used in the config and where they come from (modules, files etc.). show_vars (bool): Show an overview of all variables referenced in the config and their values. This will also reflect variables overwritten in the function call. \"\"\" if isinstance ( config_path , str ): config_path = Path ( config_path ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) spacy_debug_config ( config_path , overrides = overrides , show_funcs = show_funcs , show_vars = show_vars ) lexos . language_model . debug_data ( config_path , overrides = {}, code_path = None , ignore_warnings = False , verbose = False , no_format = False ) \u00a4 Analyze, debug and validate your training and development data. Outputs useful stats, and can help you find problems like invalid entity annotations, cyclic dependencies, low data labels and more. DOCS: https://spacy.io/api/cli#debug-data Parameters: Name Type Description Default config_path Union [ str , Path ] Path to the configuration file. required overrides dict A dictionary of config overrides. {} code_path Union [ str , Path ] Path to Python file with additional code (registered functions) to be imported. None ignore_warnings bool Ignore warnings, only show stats and errors. False verbose bool Print additional information and explanations. False no_format bool Don't pretty-print the results. False Note The only way to avoid the SystemExit: 1 error is to make a copy of the module and remove the sys.exit() call at the end. Source code in lexos\\language_model\\__init__.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 def debug_data ( config_path : Union [ str , Path ], overrides : dict = {}, code_path : Union [ str , Path ] = None , ignore_warnings : bool = False , verbose : bool = False , no_format : bool = False , ): \"\"\"Analyze, debug and validate your training and development data. Outputs useful stats, and can help you find problems like invalid entity annotations, cyclic dependencies, low data labels and more. DOCS: https://spacy.io/api/cli#debug-data Args: config_path: Path to the configuration file. overrides: A dictionary of config overrides. code_path: Path to Python file with additional code (registered functions) to be imported. ignore_warnings: Ignore warnings, only show stats and errors. verbose: Print additional information and explanations. no_format: Don't pretty-print the results. Note: The only way to avoid the `SystemExit: 1` error is to make a copy of the module and remove the `sys.exit()` call at the end. \"\"\" msg = Printer () msg . info ( \"Note: If at least one error is found at the end of the analysis, \" \"the script will terminate with a `SystemExit: 1` error code.\" ) if isinstance ( config_path , str ): config_path = Path ( config_path ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) spacy_debug_data ( config_path , config_overrides = overrides , ignore_warnings = ignore_warnings , verbose = verbose , no_format = no_format , silent = False , ) lexos . language_model . debug_model ( config_path , config_overrides = {}, component = 'tagger' , layers = [], dimensions = False , parameters = False , gradients = False , attributes = False , P0 = False , P1 = False , P2 = False , P3 = False , use_gpu =- 1 ) \u00a4 Debug a trained model. Parameters: Name Type Description Default config_path Union [ str , Path ] Path to the config file. required config_overrides dict A dictionary of config overrides. {} component str Name of the pipeline component of which the model should be analysed 'tagger' layers str List of layer IDs to print. [] dimensions bool Whether to show dimensions. False parameters bool Whether to show parameters. False gradients bool Whether to show gradients. False attributes bool Whether to show attributes. False P0 bool Whether to print the model before training. False P1 bool Whether to print the model after initialization. False P2 bool Whether to print the model after training. False P3 bool Whether to print final predictions. False use_gpu int GPU ID or -1 for CPU -1 Source code in lexos\\language_model\\__init__.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 def debug_model ( config_path : Union [ str , Path ], config_overrides : dict = {}, component : str = \"tagger\" , layers : List [ int ] = [], dimensions : bool = False , parameters : bool = False , gradients : bool = False , attributes : bool = False , P0 : bool = False , P1 : bool = False , P2 : bool = False , P3 : bool = False , use_gpu : int = - 1 , ): \"\"\"Debug a trained model. Args: config_path (Union[str, Path]): Path to the config file. config_overrides (dict): A dictionary of config overrides. component (str): Name of the pipeline component of which the model should be analysed layers (str): List of layer IDs to print. dimensions (bool): Whether to show dimensions. parameters (bool): Whether to show parameters. gradients (bool): Whether to show gradients. attributes (bool): Whether to show attributes. P0 (bool): Whether to print the model before training. P1 (bool): Whether to print the model after initialization. P2 (bool): Whether to print the model after training. P3 (bool): Whether to print final predictions. use_gpu (int): GPU ID or -1 for CPU \"\"\" if isinstance ( config_path , str ): config_path = Path ( config_path ) setup_gpu ( use_gpu ) layers = [ int ( x ) for x in layers ] print_settings = { \"dimensions\" : dimensions , \"parameters\" : parameters , \"gradients\" : gradients , \"attributes\" : attributes , \"layers\" : layers , \"print_before_training\" : P0 , \"print_after_init\" : P1 , \"print_after_training\" : P2 , \"print_prediction\" : P3 , } with show_validation_error ( config_path ): raw_config = load_config ( config_path , overrides = config_overrides , interpolate = False ) config = raw_config . interpolate () allocator = config [ \"training\" ][ \"gpu_allocator\" ] if use_gpu >= 0 and allocator : set_gpu_allocator ( allocator ) with show_validation_error ( config_path ): nlp = load_model_from_config ( raw_config ) config = nlp . config . interpolate () T = registry . resolve ( config [ \"training\" ], schema = ConfigSchemaTraining ) seed = T [ \"seed\" ] msg = Printer () if seed is not None : msg . info ( f \"Fixing random seed: { seed } \" ) fix_random_seed ( seed ) pipe = nlp . get_pipe ( component ) spacy_debug_model ( config , T , nlp , pipe , print_settings = print_settings )","title":"Language_Model"},{"location":"api/language_model/#language-model","text":"The language_model module is used to train, evaluate, and package language models. It consists of a LanguageModel class, a Timer class, and three debugging functions.","title":"Language Model"},{"location":"api/language_model/#lexos.language_model.LanguageModel","text":"Create a LanguageModel object. Source code in lexos\\language_model\\__init__.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 class LanguageModel : \"\"\"Create a LanguageModel object.\"\"\" def __init__ ( self , model_dir : str = \"language_model\" , config_file : str = \"config.cfg\" , training_file : str = \"train.conllu\" , dev_file : str = \"dev.conllu\" , test_file : str = \"test.conllu\" , gpu : int = - 1 , lang : str = \"xx\" , package_name : str = \"model_sm\" , package_version : str = \"1.0.0\" , components : List [ str ] = [ \"tagger\" ], optimize : str = \"efficiency\" , exclusive_classes : bool = True , force : bool = False , recipe : str = None , ): \"\"\"Initialise the LanguageModel object.\"\"\" self . model_dir = model_dir self . config = None self . config_file = config_file self . config_filepath = f \" { self . model_dir } / { self . config_file } \" self . training_file = training_file self . dev_file = dev_file self . test_file = test_file self . gpu = gpu self . lang = lang self . package_name = package_name self . package_version = package_version self . components = components self . optimize = optimize self . exclusive_classes = exclusive_classes msg = Printer () # Issue a warning if the user is working in a Jupyter notebook if ( is_in_jupyter and Path . cwd () . name == \"lexos\" and Path ( self . model_dir ) . resolve () . parts [ - 2 ] == \"lexos\" ): msg . warn ( \"It looks like you're calling `LanguageModel()` from within a \" \"Jupyter notebook or a similar environment. If you set the system path \" \"to the Lexos API folder to import locally, you should configure \" \"your model directory with an absolute path or a path relative. \" \"to the lexos directory.\" ) # Otherwise, we're safe to proceed else : # Create the model directory if it doesn't exist Path ( self . model_dir ) . mkdir ( parents = True , exist_ok = True ) # Create model folders self . assets_dir = f \" { self . model_dir } /assets/ { self . lang } \" self . corpus_dir = f \" { self . model_dir } /corpus/ { self . lang } \" self . metrics_dir = f \" { self . model_dir } /metrics/ { self . lang } \" self . training_dir = f \" { self . model_dir } /training/ { self . lang } \" for dir in [ self . assets_dir , self . corpus_dir , self . metrics_dir , self . training_dir , ]: Path ( dir ) . mkdir ( parents = True , exist_ok = True ) msg . good ( f \"Created assets directory: { self . assets_dir } .\" ) msg . good ( f \"Created corpus directory: { self . corpus_dir } .\" ) msg . good ( f \"Created metrics directory: { self . metrics_dir } .\" ) msg . good ( f \"Created training directory: { self . training_dir } .\" ) # Create the base config file if it doesn't exist if not Path ( self . config_filepath ) . is_file () or force == True : # Load a recipe if one was provided if recipe : try : self . load_config ( recipe ) except FileNotFoundError : msg . fail ( f \"Recipe file { recipe } not found.\" ) else : if self . gpu == - 1 : gpu = False else : gpu = True self . config = init_config ( lang = self . lang , pipeline = self . components , optimize = self . optimize , gpu = gpu , ) # Add the file paths to the config files = { \"train\" : self . training_file , \"dev\" : self . dev_file , \"test\" : self . test_file , } for k , file in files . items (): if file is not None : self . config [ \"paths\" ][ k ] = f ' { self . corpus_dir } / { file . replace ( \".conllu\" , \".spacy\" ) } ' self . config . to_disk ( self . config_filepath ) self . _fix_config () msg . good ( f \"Saved config file to { self . config_filepath } .\" ) msg . text ( \"You can now add your data and train your pipeline.\" ) else : msg . warn ( f \" { self . config_filepath } already exists. \" \"You can now add your data and train your pipeline.\" ) def _fix_config ( self ) -> None : \"\"\"Fix the config file. This is a hack because init_config() does not generate useable max_length and score_weights values. \"\"\" self . config [ \"corpora\" ][ \"train\" ][ \"max_length\" ] = 2000 self . config [ \"training\" ][ \"score_weights\" ] = { \"morph_per_feat\" : None , \"dep_las_per_type\" : None , \"sents_p\" : None , \"sents_r\" : None , \"tag_acc\" : 0.33 , \"pos_acc\" : 0.17 , \"morph_acc\" : 0.17 , \"dep_uas\" : 0.17 , \"dep_las\" : 0.17 , \"sents_f\" : 0.0 , } self . config . to_disk ( self . config_filepath ) def convert_assets ( self , n_sents = 10 , merge_subtokens = True ): \"\"\"Convert CONLLU assets to spaCy DocBins. Args: n_sents (int): The number of sentences per doc (0 to disable). merge_subtokens (bool): Whether to merge CoNLL-U subtokens. \"\"\" msg = Printer () success = True files = [ self . training_file , self . dev_file , self . test_file ] for file in files : filepath = Path ( f \" { self . assets_dir } / { file } \" ) if filepath . exists (): try : convert ( input_path = filepath , output_dir = self . corpus_dir , file_type = \"spacy\" , converter = \"conllu\" , n_sents = n_sents , merge_subtokens = merge_subtokens , ) except Exception : success = False msg . fail ( f \"Error converting { file } . Check that the CONLLU file formatting is valid.\" ) if success : msg . good ( f \"Assets converted and saved to { self . corpus_dir } .\" ) msg . text ( \"You can now train your pipeline.\" ) else : msg . fail ( \"Failed to convert one or more assets.\" ) def copy_assets ( self , training_file : str = None , dev_file : str = None , test_file : str = None ): \"\"\"Copy assets to the assets folder. Args: training_file (str): The path to the training file to copy. dev_file (str): The path to the dev file to copy. test_file (str): The path to the test file to copy. \"\"\" for asset in [ training_file , dev_file , test_file ]: if asset is not None : try : with open ( asset , \"rb\" ) as f : content = f . read () except IOError : raise IOError ( f \" { asset } not found.\" ) save_path = f \" { self . assets_dir } / { Path ( asset ) . name } \" with open ( save_path , \"wb\" ) as f : f . write ( content ) msg = Printer () msg . good ( f \"Copied assets to { self . assets_dir } .\" ) def evaluate ( self , model : str = None , testfile : Union [ Path , str ] = None , output : Path = None , use_gpu : int = - 1 , gold_preproc : bool = False , displacy_path : Optional [ Path ] = None , displacy_limit : int = 25 , silent : bool = False , ) -> None : \"\"\"Evaluate a spaCy model. Args: model (str): The path to the model to evaluate. testfile (Union[Path, str]): The path to the test file to evaluate. output (Path): The path to the output file. use_gpu (int): The GPU to use. gold_preproc (bool): Whether to use gold preprocessing. displacy_path (Optional[Path]): The path to the displacy package. displacy_limit (int): The number of tokens to display. silent (bool): Whether to suppress output. Returns: Dict[str, Any]: The evaluation results. \"\"\" if isinstance ( testfile , str ): testfile = Path ( testfile ) output = Path ( f \" { self . metrics_dir } / { self . lang } \" ) . with_suffix ( \".json\" ) if not use_gpu : use_gpu = self . gpu evaluate ( model = model , data_path = testfile , output = output , use_gpu = use_gpu , gold_preproc = gold_preproc , displacy_path = displacy_path , displacy_limit = displacy_limit , silent = silent , ) def fill_config ( self , path : Union [ str , Path ], output_file : Union [ str , Path ] = None , pretraining : bool = False , diff : bool = False , code_path : Union [ str , Path ] = None , ) -> None : \"\"\" Fill partial config file with default values. Adds all missing settings from the current config and creates all objects, checks the registered functions for their default values, and updates the config file. Although the `LanguageModel` class automatically generates a full config file, this method may be useful for debugging. DOCS: https://spacy.io/api/cli#init-fill-config Args: path (Union[str, Path]): Path to the config file to fill. output_file (Union[str, Path]): Path to output .cfg file. pretraining (bool): Include config for pretraining (with \"spacy pretrain\"). diff (bool): Print a visual diff highlighting the changes. code_path (Union[str, Path]): Path to Python file with additional code (registered functions) to be imported. \"\"\" if not output_path : output_path = self . config_filepath if isinstance ( path , str ): path = Path ( path ) if isinstance ( output_file , str ): output_file = Path ( output_file ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) fill_config ( output_file , path , pretraining = pretraining , diff = diff ) self . config = Config () . from_disk ( self . config_filepath ) def load_config ( self , filepath : Path = None ): \"\"\"Load the config from the config file. Use this method if you wish to load the config from a different file. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to the config file. \"\"\" if not filepath : filepath = self . config_filepath self . config = load_config ( filepath ) self . config . to_disk ( self . config_filepath ) def package ( self , input_dir : str = None , output_dir : str = None , meta_path : Optional [ str ] = None , code_paths : Optional [ List [ str ]] = [], name : str = None , version : str = None , create_meta : bool = False , create_sdist : bool = True , create_wheel : bool = False , force : bool = False , silent : bool = False , ) -> None : \"\"\"Package the model so that it can be installed.\"\"\" input_dir = Path ( input_dir ) output_dir = Path ( output_dir ) if meta_path : meta_path = Path ( meta_path ) if code_paths : code_paths = [ Path ( p ) for p in code_paths ] self . packages_dir = output_dir output_dir . mkdir ( parents = True , exist_ok = True ) package ( input_dir , output_dir , meta_path = meta_path , code_paths = code_paths , name = name , version = version , create_meta = create_meta , create_sdist = create_sdist , create_wheel = create_wheel , force = force , silent = silent , ) # Print the paths to the package tarfile = f \" { self . lang } _ { name } - { version } .tar.gz\" print ( f \"Model Directory for spacy.load(): { output_dir } / { self . lang } - { version } \" ) print ( f \"Binary file (for pip install): { output_dir } / { self . lang } - { version } /dist/ { tarfile } \" ) def save_config ( self , filepath : Path = None ): \"\"\"Save the config from the config file. Use this method to save a config file after making modifications. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to a config file (for saving a copy). \"\"\" if not filepath : filepath = self . config_filepath self . config . to_disk ( self . config_filepath ) def train ( self ): \"\"\"Train the corpus.\"\"\" timer = Timer () config = load_config ( self . config_filepath ) nlp = init_nlp ( config , use_gpu = self . gpu ) train ( nlp = nlp , output_path = Path ( self . training_dir ), use_gpu = self . gpu , stdout = sys . stdout , stderr = sys . stderr , ) msg = Printer () msg . text ( f \"Time elapsed: { timer . get_time_elapsed () } \" )","title":"LanguageModel"},{"location":"api/language_model/#lexos.language_model.LanguageModel.__init__","text":"Initialise the LanguageModel object. Source code in lexos\\language_model\\__init__.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __init__ ( self , model_dir : str = \"language_model\" , config_file : str = \"config.cfg\" , training_file : str = \"train.conllu\" , dev_file : str = \"dev.conllu\" , test_file : str = \"test.conllu\" , gpu : int = - 1 , lang : str = \"xx\" , package_name : str = \"model_sm\" , package_version : str = \"1.0.0\" , components : List [ str ] = [ \"tagger\" ], optimize : str = \"efficiency\" , exclusive_classes : bool = True , force : bool = False , recipe : str = None , ): \"\"\"Initialise the LanguageModel object.\"\"\" self . model_dir = model_dir self . config = None self . config_file = config_file self . config_filepath = f \" { self . model_dir } / { self . config_file } \" self . training_file = training_file self . dev_file = dev_file self . test_file = test_file self . gpu = gpu self . lang = lang self . package_name = package_name self . package_version = package_version self . components = components self . optimize = optimize self . exclusive_classes = exclusive_classes msg = Printer () # Issue a warning if the user is working in a Jupyter notebook if ( is_in_jupyter and Path . cwd () . name == \"lexos\" and Path ( self . model_dir ) . resolve () . parts [ - 2 ] == \"lexos\" ): msg . warn ( \"It looks like you're calling `LanguageModel()` from within a \" \"Jupyter notebook or a similar environment. If you set the system path \" \"to the Lexos API folder to import locally, you should configure \" \"your model directory with an absolute path or a path relative. \" \"to the lexos directory.\" ) # Otherwise, we're safe to proceed else : # Create the model directory if it doesn't exist Path ( self . model_dir ) . mkdir ( parents = True , exist_ok = True ) # Create model folders self . assets_dir = f \" { self . model_dir } /assets/ { self . lang } \" self . corpus_dir = f \" { self . model_dir } /corpus/ { self . lang } \" self . metrics_dir = f \" { self . model_dir } /metrics/ { self . lang } \" self . training_dir = f \" { self . model_dir } /training/ { self . lang } \" for dir in [ self . assets_dir , self . corpus_dir , self . metrics_dir , self . training_dir , ]: Path ( dir ) . mkdir ( parents = True , exist_ok = True ) msg . good ( f \"Created assets directory: { self . assets_dir } .\" ) msg . good ( f \"Created corpus directory: { self . corpus_dir } .\" ) msg . good ( f \"Created metrics directory: { self . metrics_dir } .\" ) msg . good ( f \"Created training directory: { self . training_dir } .\" ) # Create the base config file if it doesn't exist if not Path ( self . config_filepath ) . is_file () or force == True : # Load a recipe if one was provided if recipe : try : self . load_config ( recipe ) except FileNotFoundError : msg . fail ( f \"Recipe file { recipe } not found.\" ) else : if self . gpu == - 1 : gpu = False else : gpu = True self . config = init_config ( lang = self . lang , pipeline = self . components , optimize = self . optimize , gpu = gpu , ) # Add the file paths to the config files = { \"train\" : self . training_file , \"dev\" : self . dev_file , \"test\" : self . test_file , } for k , file in files . items (): if file is not None : self . config [ \"paths\" ][ k ] = f ' { self . corpus_dir } / { file . replace ( \".conllu\" , \".spacy\" ) } ' self . config . to_disk ( self . config_filepath ) self . _fix_config () msg . good ( f \"Saved config file to { self . config_filepath } .\" ) msg . text ( \"You can now add your data and train your pipeline.\" ) else : msg . warn ( f \" { self . config_filepath } already exists. \" \"You can now add your data and train your pipeline.\" )","title":"__init__()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.convert_assets","text":"Convert CONLLU assets to spaCy DocBins. Parameters: Name Type Description Default n_sents int The number of sentences per doc (0 to disable). 10 merge_subtokens bool Whether to merge CoNLL-U subtokens. True Source code in lexos\\language_model\\__init__.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def convert_assets ( self , n_sents = 10 , merge_subtokens = True ): \"\"\"Convert CONLLU assets to spaCy DocBins. Args: n_sents (int): The number of sentences per doc (0 to disable). merge_subtokens (bool): Whether to merge CoNLL-U subtokens. \"\"\" msg = Printer () success = True files = [ self . training_file , self . dev_file , self . test_file ] for file in files : filepath = Path ( f \" { self . assets_dir } / { file } \" ) if filepath . exists (): try : convert ( input_path = filepath , output_dir = self . corpus_dir , file_type = \"spacy\" , converter = \"conllu\" , n_sents = n_sents , merge_subtokens = merge_subtokens , ) except Exception : success = False msg . fail ( f \"Error converting { file } . Check that the CONLLU file formatting is valid.\" ) if success : msg . good ( f \"Assets converted and saved to { self . corpus_dir } .\" ) msg . text ( \"You can now train your pipeline.\" ) else : msg . fail ( \"Failed to convert one or more assets.\" )","title":"convert_assets()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.copy_assets","text":"Copy assets to the assets folder. Parameters: Name Type Description Default training_file str The path to the training file to copy. None dev_file str The path to the dev file to copy. None test_file str The path to the test file to copy. None Source code in lexos\\language_model\\__init__.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def copy_assets ( self , training_file : str = None , dev_file : str = None , test_file : str = None ): \"\"\"Copy assets to the assets folder. Args: training_file (str): The path to the training file to copy. dev_file (str): The path to the dev file to copy. test_file (str): The path to the test file to copy. \"\"\" for asset in [ training_file , dev_file , test_file ]: if asset is not None : try : with open ( asset , \"rb\" ) as f : content = f . read () except IOError : raise IOError ( f \" { asset } not found.\" ) save_path = f \" { self . assets_dir } / { Path ( asset ) . name } \" with open ( save_path , \"wb\" ) as f : f . write ( content ) msg = Printer () msg . good ( f \"Copied assets to { self . assets_dir } .\" )","title":"copy_assets()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.evaluate","text":"Evaluate a spaCy model. Parameters: Name Type Description Default model str The path to the model to evaluate. None testfile Union [ Path , str ] The path to the test file to evaluate. None output Path The path to the output file. None use_gpu int The GPU to use. -1 gold_preproc bool Whether to use gold preprocessing. False displacy_path Optional [ Path ] The path to the displacy package. None displacy_limit int The number of tokens to display. 25 silent bool Whether to suppress output. False Returns: Type Description None Dict[str, Any]: The evaluation results. Source code in lexos\\language_model\\__init__.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def evaluate ( self , model : str = None , testfile : Union [ Path , str ] = None , output : Path = None , use_gpu : int = - 1 , gold_preproc : bool = False , displacy_path : Optional [ Path ] = None , displacy_limit : int = 25 , silent : bool = False , ) -> None : \"\"\"Evaluate a spaCy model. Args: model (str): The path to the model to evaluate. testfile (Union[Path, str]): The path to the test file to evaluate. output (Path): The path to the output file. use_gpu (int): The GPU to use. gold_preproc (bool): Whether to use gold preprocessing. displacy_path (Optional[Path]): The path to the displacy package. displacy_limit (int): The number of tokens to display. silent (bool): Whether to suppress output. Returns: Dict[str, Any]: The evaluation results. \"\"\" if isinstance ( testfile , str ): testfile = Path ( testfile ) output = Path ( f \" { self . metrics_dir } / { self . lang } \" ) . with_suffix ( \".json\" ) if not use_gpu : use_gpu = self . gpu evaluate ( model = model , data_path = testfile , output = output , use_gpu = use_gpu , gold_preproc = gold_preproc , displacy_path = displacy_path , displacy_limit = displacy_limit , silent = silent , )","title":"evaluate()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.fill_config","text":"Fill partial config file with default values. Adds all missing settings from the current config and creates all objects, checks the registered functions for their default values, and updates the config file. Although the LanguageModel class automatically generates a full config file, this method may be useful for debugging. DOCS: https://spacy.io/api/cli#init-fill-config Parameters: Name Type Description Default path Union [ str , Path ] Path to the config file to fill. required output_file Union [ str , Path ] Path to output .cfg file. None pretraining bool Include config for pretraining (with \"spacy pretrain\"). False diff bool Print a visual diff highlighting the changes. False code_path Union [ str , Path ] Path to Python file with additional code (registered functions) to be imported. None Source code in lexos\\language_model\\__init__.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def fill_config ( self , path : Union [ str , Path ], output_file : Union [ str , Path ] = None , pretraining : bool = False , diff : bool = False , code_path : Union [ str , Path ] = None , ) -> None : \"\"\" Fill partial config file with default values. Adds all missing settings from the current config and creates all objects, checks the registered functions for their default values, and updates the config file. Although the `LanguageModel` class automatically generates a full config file, this method may be useful for debugging. DOCS: https://spacy.io/api/cli#init-fill-config Args: path (Union[str, Path]): Path to the config file to fill. output_file (Union[str, Path]): Path to output .cfg file. pretraining (bool): Include config for pretraining (with \"spacy pretrain\"). diff (bool): Print a visual diff highlighting the changes. code_path (Union[str, Path]): Path to Python file with additional code (registered functions) to be imported. \"\"\" if not output_path : output_path = self . config_filepath if isinstance ( path , str ): path = Path ( path ) if isinstance ( output_file , str ): output_file = Path ( output_file ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) fill_config ( output_file , path , pretraining = pretraining , diff = diff ) self . config = Config () . from_disk ( self . config_filepath )","title":"fill_config()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.load_config","text":"Load the config from the config file. Use this method if you wish to load the config from a different file. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the LanguageModel . Parameters: Name Type Description Default filepath Path The path to the config file. None Source code in lexos\\language_model\\__init__.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def load_config ( self , filepath : Path = None ): \"\"\"Load the config from the config file. Use this method if you wish to load the config from a different file. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to the config file. \"\"\" if not filepath : filepath = self . config_filepath self . config = load_config ( filepath ) self . config . to_disk ( self . config_filepath )","title":"load_config()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.package","text":"Package the model so that it can be installed. Source code in lexos\\language_model\\__init__.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def package ( self , input_dir : str = None , output_dir : str = None , meta_path : Optional [ str ] = None , code_paths : Optional [ List [ str ]] = [], name : str = None , version : str = None , create_meta : bool = False , create_sdist : bool = True , create_wheel : bool = False , force : bool = False , silent : bool = False , ) -> None : \"\"\"Package the model so that it can be installed.\"\"\" input_dir = Path ( input_dir ) output_dir = Path ( output_dir ) if meta_path : meta_path = Path ( meta_path ) if code_paths : code_paths = [ Path ( p ) for p in code_paths ] self . packages_dir = output_dir output_dir . mkdir ( parents = True , exist_ok = True ) package ( input_dir , output_dir , meta_path = meta_path , code_paths = code_paths , name = name , version = version , create_meta = create_meta , create_sdist = create_sdist , create_wheel = create_wheel , force = force , silent = silent , ) # Print the paths to the package tarfile = f \" { self . lang } _ { name } - { version } .tar.gz\" print ( f \"Model Directory for spacy.load(): { output_dir } / { self . lang } - { version } \" ) print ( f \"Binary file (for pip install): { output_dir } / { self . lang } - { version } /dist/ { tarfile } \" )","title":"package()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.save_config","text":"Save the config from the config file. Use this method to save a config file after making modifications. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the LanguageModel . Parameters: Name Type Description Default filepath Path The path to a config file (for saving a copy). None Source code in lexos\\language_model\\__init__.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def save_config ( self , filepath : Path = None ): \"\"\"Save the config from the config file. Use this method to save a config file after making modifications. But use with caution, as the old config file will be overwritten, and the new config file may have different paths or a different pipeline from those you used to instantiate the `LanguageModel`. Args: filepath (Path): The path to a config file (for saving a copy). \"\"\" if not filepath : filepath = self . config_filepath self . config . to_disk ( self . config_filepath )","title":"save_config()"},{"location":"api/language_model/#lexos.language_model.LanguageModel.train","text":"Train the corpus. Source code in lexos\\language_model\\__init__.py 388 389 390 391 392 393 394 395 396 397 398 399 400 401 def train ( self ): \"\"\"Train the corpus.\"\"\" timer = Timer () config = load_config ( self . config_filepath ) nlp = init_nlp ( config , use_gpu = self . gpu ) train ( nlp = nlp , output_path = Path ( self . training_dir ), use_gpu = self . gpu , stdout = sys . stdout , stderr = sys . stderr , ) msg = Printer () msg . text ( f \"Time elapsed: { timer . get_time_elapsed () } \" )","title":"train()"},{"location":"api/language_model/#lexos.language_model.Timer","text":"Create a timer object. Source code in lexos\\language_model\\__init__.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Timer : \"\"\"Create a timer object.\"\"\" def __init__ ( self ): \"\"\"Initialise the timer object.\"\"\" self . start = time () def get_time_elapsed ( self ): \"\"\"Get the elapsed time and format it as hours, minutes, and seconds.\"\"\" end = time () m , s = divmod ( end - self . start , 60 ) h , m = divmod ( m , 60 ) time_str = \" %02d : %02d : %02d \" % ( h , m , s ) return time_str","title":"Timer"},{"location":"api/language_model/#lexos.language_model.Timer.__init__","text":"Initialise the timer object. Source code in lexos\\language_model\\__init__.py 31 32 33 def __init__ ( self ): \"\"\"Initialise the timer object.\"\"\" self . start = time ()","title":"__init__()"},{"location":"api/language_model/#lexos.language_model.Timer.get_time_elapsed","text":"Get the elapsed time and format it as hours, minutes, and seconds. Source code in lexos\\language_model\\__init__.py 35 36 37 38 39 40 41 def get_time_elapsed ( self ): \"\"\"Get the elapsed time and format it as hours, minutes, and seconds.\"\"\" end = time () m , s = divmod ( end - self . start , 60 ) h , m = divmod ( m , 60 ) time_str = \" %02d : %02d : %02d \" % ( h , m , s ) return time_str","title":"get_time_elapsed()"},{"location":"api/language_model/#lexos.language_model.debug_config","text":"Debug a config file and show validation errors. The function will create all objects in the tree and validate them. Note that some config validation errors are blocking and will prevent the rest of the config from being resolved. This means that you may not see all validation errors at once and some issues are only shown once previous errors have been fixed. As with the 'train' command, you can override settings from the config by passing arguments in the overrides dict. DOCS: https://spacy.io/api/cli#debug-config Parameters: Name Type Description Default config_path Union [ str , Path ] Path to the configuration file. required overrides dict A dictionary of config overrides. {} code_path Union [ str , Path ] Path to Python file with additional code (registered functions) to be imported. None show_funcs bool Show an overview of all registered functions used in the config and where they come from (modules, files etc.). False show_vars bool Show an overview of all variables referenced in the config and their values. This will also reflect variables overwritten in the function call. False Source code in lexos\\language_model\\__init__.py 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def debug_config ( config_path : Union [ str , Path ], overrides : dict = {}, code_path : Union [ str , Path ] = None , show_funcs : bool = False , show_vars : bool = False , ): \"\"\"Debug a config file and show validation errors. The function will create all objects in the tree and validate them. Note that some config validation errors are blocking and will prevent the rest of the config from being resolved. This means that you may not see all validation errors at once and some issues are only shown once previous errors have been fixed. As with the 'train' command, you can override settings from the config by passing arguments in the `overrides` dict. DOCS: https://spacy.io/api/cli#debug-config Args: config_path (Union[str, Path]): Path to the configuration file. overrides (dict): A dictionary of config overrides. code_path (Union[str, Path]): Path to Python file with additional code (registered functions) to be imported. show_funcs (bool): Show an overview of all registered functions used in the config and where they come from (modules, files etc.). show_vars (bool): Show an overview of all variables referenced in the config and their values. This will also reflect variables overwritten in the function call. \"\"\" if isinstance ( config_path , str ): config_path = Path ( config_path ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) spacy_debug_config ( config_path , overrides = overrides , show_funcs = show_funcs , show_vars = show_vars )","title":"debug_config()"},{"location":"api/language_model/#lexos.language_model.debug_data","text":"Analyze, debug and validate your training and development data. Outputs useful stats, and can help you find problems like invalid entity annotations, cyclic dependencies, low data labels and more. DOCS: https://spacy.io/api/cli#debug-data Parameters: Name Type Description Default config_path Union [ str , Path ] Path to the configuration file. required overrides dict A dictionary of config overrides. {} code_path Union [ str , Path ] Path to Python file with additional code (registered functions) to be imported. None ignore_warnings bool Ignore warnings, only show stats and errors. False verbose bool Print additional information and explanations. False no_format bool Don't pretty-print the results. False Note The only way to avoid the SystemExit: 1 error is to make a copy of the module and remove the sys.exit() call at the end. Source code in lexos\\language_model\\__init__.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 def debug_data ( config_path : Union [ str , Path ], overrides : dict = {}, code_path : Union [ str , Path ] = None , ignore_warnings : bool = False , verbose : bool = False , no_format : bool = False , ): \"\"\"Analyze, debug and validate your training and development data. Outputs useful stats, and can help you find problems like invalid entity annotations, cyclic dependencies, low data labels and more. DOCS: https://spacy.io/api/cli#debug-data Args: config_path: Path to the configuration file. overrides: A dictionary of config overrides. code_path: Path to Python file with additional code (registered functions) to be imported. ignore_warnings: Ignore warnings, only show stats and errors. verbose: Print additional information and explanations. no_format: Don't pretty-print the results. Note: The only way to avoid the `SystemExit: 1` error is to make a copy of the module and remove the `sys.exit()` call at the end. \"\"\" msg = Printer () msg . info ( \"Note: If at least one error is found at the end of the analysis, \" \"the script will terminate with a `SystemExit: 1` error code.\" ) if isinstance ( config_path , str ): config_path = Path ( config_path ) if isinstance ( code_path , str ): code_path = Path ( code_path ) import_code ( code_path ) spacy_debug_data ( config_path , config_overrides = overrides , ignore_warnings = ignore_warnings , verbose = verbose , no_format = no_format , silent = False , )","title":"debug_data()"},{"location":"api/language_model/#lexos.language_model.debug_model","text":"Debug a trained model. Parameters: Name Type Description Default config_path Union [ str , Path ] Path to the config file. required config_overrides dict A dictionary of config overrides. {} component str Name of the pipeline component of which the model should be analysed 'tagger' layers str List of layer IDs to print. [] dimensions bool Whether to show dimensions. False parameters bool Whether to show parameters. False gradients bool Whether to show gradients. False attributes bool Whether to show attributes. False P0 bool Whether to print the model before training. False P1 bool Whether to print the model after initialization. False P2 bool Whether to print the model after training. False P3 bool Whether to print final predictions. False use_gpu int GPU ID or -1 for CPU -1 Source code in lexos\\language_model\\__init__.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 def debug_model ( config_path : Union [ str , Path ], config_overrides : dict = {}, component : str = \"tagger\" , layers : List [ int ] = [], dimensions : bool = False , parameters : bool = False , gradients : bool = False , attributes : bool = False , P0 : bool = False , P1 : bool = False , P2 : bool = False , P3 : bool = False , use_gpu : int = - 1 , ): \"\"\"Debug a trained model. Args: config_path (Union[str, Path]): Path to the config file. config_overrides (dict): A dictionary of config overrides. component (str): Name of the pipeline component of which the model should be analysed layers (str): List of layer IDs to print. dimensions (bool): Whether to show dimensions. parameters (bool): Whether to show parameters. gradients (bool): Whether to show gradients. attributes (bool): Whether to show attributes. P0 (bool): Whether to print the model before training. P1 (bool): Whether to print the model after initialization. P2 (bool): Whether to print the model after training. P3 (bool): Whether to print final predictions. use_gpu (int): GPU ID or -1 for CPU \"\"\" if isinstance ( config_path , str ): config_path = Path ( config_path ) setup_gpu ( use_gpu ) layers = [ int ( x ) for x in layers ] print_settings = { \"dimensions\" : dimensions , \"parameters\" : parameters , \"gradients\" : gradients , \"attributes\" : attributes , \"layers\" : layers , \"print_before_training\" : P0 , \"print_after_init\" : P1 , \"print_after_training\" : P2 , \"print_prediction\" : P3 , } with show_validation_error ( config_path ): raw_config = load_config ( config_path , overrides = config_overrides , interpolate = False ) config = raw_config . interpolate () allocator = config [ \"training\" ][ \"gpu_allocator\" ] if use_gpu >= 0 and allocator : set_gpu_allocator ( allocator ) with show_validation_error ( config_path ): nlp = load_model_from_config ( raw_config ) config = nlp . config . interpolate () T = registry . resolve ( config [ \"training\" ], schema = ConfigSchemaTraining ) seed = T [ \"seed\" ] msg = Printer () if seed is not None : msg . info ( f \"Fixing random seed: { seed } \" ) fix_random_seed ( seed ) pipe = nlp . get_pipe ( component ) spacy_debug_model ( config , T , nlp , pipe , print_settings = print_settings )","title":"debug_model()"},{"location":"api/scrubber/","text":"Scrubber \u00a4 Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into eight submodules: normalize A set of functions for massaging text into standardized forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Contains the lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Scrubber"},{"location":"api/scrubber/#scrubber","text":"Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into eight submodules: normalize A set of functions for massaging text into standardized forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Contains the lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Scrubber"},{"location":"api/scrubber/normalize/","text":"Normalize \u00a4 The normalize component of Scrubber contains functions to perform a variety of text manipulations. The functions are frequently applied at the beginning of a scrubbing pipeline. lexos . scrubber . normalize . bullet_points ( text ) \u00a4 Normalize bullet points. Normalises all \"fancy\" bullet point symbols in text to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's utils.normalize_bullets . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def bullet_points ( text : str ) -> str : \"\"\"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in `text` to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's `utils.normalize_bullets`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_BULLET_POINTS . sub ( r \"\\1-\" , text ) lexos . scrubber . normalize . hyphenated_words ( text ) \u00a4 Normalize hyphenated words. Normalize words in text that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's utils.normalize_hyphens . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def hyphenated_words ( text : str ) -> str : \"\"\"Normalize hyphenated words. Normalize words in `text` that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's `utils.normalize_hyphens`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_HYPHENATED_WORD . sub ( r \"\\1\\2\" , text ) lexos . scrubber . normalize . lower_case ( text ) \u00a4 Convert text to lower case. Parameters: Name Type Description Default text str The text to convert to lower case. required Returns: Type Description str The converted text. Source code in lexos\\scrubber\\normalize.py 41 42 43 44 45 46 47 48 49 50 def lower_case ( text : str ) -> str : \"\"\"Convert `text` to lower case. Args: text (str): The text to convert to lower case. Returns: The converted text. \"\"\" return text . lower () lexos . scrubber . normalize . quotation_marks ( text ) \u00a4 Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in text to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's utils.normalize_quotation_marks . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def quotation_marks ( text : str ) -> str : \"\"\"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in `text` to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's `utils.normalize_quotation_marks`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return text . translate ( resources . QUOTE_TRANSLATION_TABLE ) lexos . scrubber . normalize . repeating_chars ( text , * , chars , maxn = 1 ) \u00a4 Normalize repeating characters in text . Truncating their number of consecutive repetitions to maxn . Duplicates Textacy's utils.normalize_repeating_chars . Parameters: Name Type Description Default text str The text to normalize. required chars str One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". required maxn int Maximum number of consecutive repetitions of chars to which longer repetitions will be truncated. 1 Returns: Type Description str str Source code in lexos\\scrubber\\normalize.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def repeating_chars ( text : str , * , chars : str , maxn : int = 1 ) -> str : \"\"\"Normalize repeating characters in `text`. Truncating their number of consecutive repetitions to `maxn`. Duplicates Textacy's `utils.normalize_repeating_chars`. Args: text (str): The text to normalize. chars: One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". maxn: Maximum number of consecutive repetitions of `chars` to which longer repetitions will be truncated. Returns: str \"\"\" return re . sub ( r \"( {} ){{ {} ,}}\" . format ( re . escape ( chars ), maxn + 1 ), chars * maxn , text ) lexos . scrubber . normalize . unicode ( text , * , form = 'NFC' ) \u00a4 Normalize unicode characters in text into canonical forms. Duplicates Textacy's utils.normalize_unicode . Parameters: Name Type Description Default text str The text to normalize. required form Literal ['NFC', 'NFD', 'NFKC', 'NFKD'] Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. 'NFC' See Also Source code in lexos\\scrubber\\normalize.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def unicode ( text : str , * , form : Literal [ \"NFC\" , \"NFD\" , \"NFKC\" , \"NFKD\" ] = \"NFC\" ) -> str : \"\"\"Normalize unicode characters in `text` into canonical forms. Duplicates Textacy's `utils.normalize_unicode`. Args: text (str): The text to normalize. form: Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"\"\" return unicodedata . normalize ( form , text ) lexos . scrubber . normalize . whitespace ( text ) \u00a4 Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def whitespace ( text : str ) -> str : \"\"\"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" text = resources . RE_ZWSP . sub ( \"\" , text ) text = resources . RE_LINEBREAK . sub ( r \"\\n\" , text ) text = resources . RE_NONBREAKING_SPACE . sub ( \" \" , text ) return text . strip ()","title":"Normalize"},{"location":"api/scrubber/normalize/#normalize","text":"The normalize component of Scrubber contains functions to perform a variety of text manipulations. The functions are frequently applied at the beginning of a scrubbing pipeline.","title":"Normalize"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.bullet_points","text":"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in text to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's utils.normalize_bullets . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def bullet_points ( text : str ) -> str : \"\"\"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in `text` to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's `utils.normalize_bullets`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_BULLET_POINTS . sub ( r \"\\1-\" , text )","title":"bullet_points()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.hyphenated_words","text":"Normalize hyphenated words. Normalize words in text that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's utils.normalize_hyphens . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def hyphenated_words ( text : str ) -> str : \"\"\"Normalize hyphenated words. Normalize words in `text` that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's `utils.normalize_hyphens`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_HYPHENATED_WORD . sub ( r \"\\1\\2\" , text )","title":"hyphenated_words()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.lower_case","text":"Convert text to lower case. Parameters: Name Type Description Default text str The text to convert to lower case. required Returns: Type Description str The converted text. Source code in lexos\\scrubber\\normalize.py 41 42 43 44 45 46 47 48 49 50 def lower_case ( text : str ) -> str : \"\"\"Convert `text` to lower case. Args: text (str): The text to convert to lower case. Returns: The converted text. \"\"\" return text . lower ()","title":"lower_case()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.quotation_marks","text":"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in text to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's utils.normalize_quotation_marks . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def quotation_marks ( text : str ) -> str : \"\"\"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in `text` to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's `utils.normalize_quotation_marks`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return text . translate ( resources . QUOTE_TRANSLATION_TABLE )","title":"quotation_marks()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.repeating_chars","text":"Normalize repeating characters in text . Truncating their number of consecutive repetitions to maxn . Duplicates Textacy's utils.normalize_repeating_chars . Parameters: Name Type Description Default text str The text to normalize. required chars str One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". required maxn int Maximum number of consecutive repetitions of chars to which longer repetitions will be truncated. 1 Returns: Type Description str str Source code in lexos\\scrubber\\normalize.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def repeating_chars ( text : str , * , chars : str , maxn : int = 1 ) -> str : \"\"\"Normalize repeating characters in `text`. Truncating their number of consecutive repetitions to `maxn`. Duplicates Textacy's `utils.normalize_repeating_chars`. Args: text (str): The text to normalize. chars: One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". maxn: Maximum number of consecutive repetitions of `chars` to which longer repetitions will be truncated. Returns: str \"\"\" return re . sub ( r \"( {} ){{ {} ,}}\" . format ( re . escape ( chars ), maxn + 1 ), chars * maxn , text )","title":"repeating_chars()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.unicode","text":"Normalize unicode characters in text into canonical forms. Duplicates Textacy's utils.normalize_unicode . Parameters: Name Type Description Default text str The text to normalize. required form Literal ['NFC', 'NFD', 'NFKC', 'NFKD'] Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. 'NFC' See Also Source code in lexos\\scrubber\\normalize.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def unicode ( text : str , * , form : Literal [ \"NFC\" , \"NFD\" , \"NFKC\" , \"NFKD\" ] = \"NFC\" ) -> str : \"\"\"Normalize unicode characters in `text` into canonical forms. Duplicates Textacy's `utils.normalize_unicode`. Args: text (str): The text to normalize. form: Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"\"\" return unicodedata . normalize ( form , text )","title":"unicode()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.whitespace","text":"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def whitespace ( text : str ) -> str : \"\"\"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" text = resources . RE_ZWSP . sub ( \"\" , text ) text = resources . RE_LINEBREAK . sub ( r \"\\n\" , text ) text = resources . RE_NONBREAKING_SPACE . sub ( \" \" , text ) return text . strip ()","title":"whitespace()"},{"location":"api/scrubber/pipeline/","text":"Pipeline \u00a4 The pipeline component of Scrubber is used to manage an ordered application of Scrubber component functions to text. lexos . scrubber . pipeline . make_pipeline ( * funcs ) \u00a4 Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. python -m pip install cytoolz is required for this function to work. Use pipe (an alias for functools.partial ) to pass arguments to preprocessors. from lexos import scrubber scrubber = Scrubber . pipeline . make_pipeline ( scrubber . replace . hashtags , scrubber . replace . emojis , pipe ( scrubber . remove . punctuation , only = [ \".\" , \"?\" , \"!\" ]) ) scrubber ( \"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\" ) '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' Parameters: Name Type Description Default *funcs dict A series of functions to be applied to the text. () Returns: Type Description Callable [[ str ], str ] Pipeline composed of *funcs that applies each in sequential order. Source code in lexos\\scrubber\\pipeline.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def make_pipeline ( * funcs : Callable [[ str ], str ]) -> Callable [[ str ], str ]: \"\"\"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. `python -m pip install cytoolz` is required for this function to work. Use `pipe` (an alias for `functools.partial`) to pass arguments to preprocessors. ```python from lexos import scrubber scrubber = Scrubber.pipeline.make_pipeline( scrubber.replace.hashtags, scrubber.replace.emojis, pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"]) ) scrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\") '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' ``` Args: *funcs (dict): A series of functions to be applied to the text. Returns: Pipeline composed of ``*funcs`` that applies each in sequential order. \"\"\" return functoolz . compose_left ( * funcs ) lexos . scrubber . pipeline . make_pipeline_from_tuple ( funcs ) \u00a4 Return a pipeline from a tuple. Parameters: Name Type Description Default funcs tuple A tuple containing callables or string names of functions. required Returns a tuple of functions. Source code in lexos\\scrubber\\pipeline.py 62 63 64 65 66 67 68 69 70 def make_pipeline_from_tuple ( funcs : tuple ) -> tuple : \"\"\"Return a pipeline from a tuple. Args: funcs (tuple): A tuple containing callables or string names of functions. Returns a tuple of functions. \"\"\" return make_pipeline ( * [ eval ( x ) if isinstance ( x , str ) else x for x in funcs ]) Note lexos.scrubber.pipeline.make_pipeline_from_tuple is deprecated. It should not be necessary if you are using lexos.scrubber.registry . lexos . scrubber . pipeline . pipe ( func , * args , ** kwargs ) \u00a4 Apply functool.partial and add __name__ to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Parameters: Name Type Description Default func Callable A callable. required Returns: Type Description Callable A partial function with __name__ set to the name of the function. Source code in lexos\\scrubber\\pipeline.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def pipe ( func : Callable , * args , ** kwargs ) -> Callable : \"\"\"Apply functool.partial and add `__name__` to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Args: func (Callable): A callable. Returns: A partial function with `__name__` set to the name of the function. \"\"\" if not args and not kwargs : return func else : partial_func = partial ( func , * args , ** kwargs ) update_wrapper ( partial_func , func ) return partial_func","title":"Pipeline"},{"location":"api/scrubber/pipeline/#pipeline","text":"The pipeline component of Scrubber is used to manage an ordered application of Scrubber component functions to text.","title":"Pipeline"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline","text":"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. python -m pip install cytoolz is required for this function to work. Use pipe (an alias for functools.partial ) to pass arguments to preprocessors. from lexos import scrubber scrubber = Scrubber . pipeline . make_pipeline ( scrubber . replace . hashtags , scrubber . replace . emojis , pipe ( scrubber . remove . punctuation , only = [ \".\" , \"?\" , \"!\" ]) ) scrubber ( \"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\" ) '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' Parameters: Name Type Description Default *funcs dict A series of functions to be applied to the text. () Returns: Type Description Callable [[ str ], str ] Pipeline composed of *funcs that applies each in sequential order. Source code in lexos\\scrubber\\pipeline.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def make_pipeline ( * funcs : Callable [[ str ], str ]) -> Callable [[ str ], str ]: \"\"\"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. `python -m pip install cytoolz` is required for this function to work. Use `pipe` (an alias for `functools.partial`) to pass arguments to preprocessors. ```python from lexos import scrubber scrubber = Scrubber.pipeline.make_pipeline( scrubber.replace.hashtags, scrubber.replace.emojis, pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"]) ) scrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\") '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' ``` Args: *funcs (dict): A series of functions to be applied to the text. Returns: Pipeline composed of ``*funcs`` that applies each in sequential order. \"\"\" return functoolz . compose_left ( * funcs )","title":"make_pipeline()"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline_from_tuple","text":"Return a pipeline from a tuple. Parameters: Name Type Description Default funcs tuple A tuple containing callables or string names of functions. required Returns a tuple of functions. Source code in lexos\\scrubber\\pipeline.py 62 63 64 65 66 67 68 69 70 def make_pipeline_from_tuple ( funcs : tuple ) -> tuple : \"\"\"Return a pipeline from a tuple. Args: funcs (tuple): A tuple containing callables or string names of functions. Returns a tuple of functions. \"\"\" return make_pipeline ( * [ eval ( x ) if isinstance ( x , str ) else x for x in funcs ]) Note lexos.scrubber.pipeline.make_pipeline_from_tuple is deprecated. It should not be necessary if you are using lexos.scrubber.registry .","title":"make_pipeline_from_tuple()"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.pipe","text":"Apply functool.partial and add __name__ to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Parameters: Name Type Description Default func Callable A callable. required Returns: Type Description Callable A partial function with __name__ set to the name of the function. Source code in lexos\\scrubber\\pipeline.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def pipe ( func : Callable , * args , ** kwargs ) -> Callable : \"\"\"Apply functool.partial and add `__name__` to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Args: func (Callable): A callable. Returns: A partial function with `__name__` set to the name of the function. \"\"\" if not args and not kwargs : return func else : partial_func = partial ( func , * args , ** kwargs ) update_wrapper ( partial_func , func ) return partial_func","title":"pipe()"},{"location":"api/scrubber/registry/","text":"Registry \u00a4 The registry component of Scrubber maintains a catalogue of registered functions that can be imported individually as needed. The registry enables the functions to be referenced by name using string values. The code registry is created and accessed using the catalogue library by Explosion. Registered functions can be retrieved individually using lower_case = scrubber_components.get(\"lower_case\") . Multiple functions can be loaded using the load_components function: lexos . scrubber . registry . load_component ( s ) \u00a4 Load a single component from a string. Parameters: Name Type Description Default s str The name of the function. required Source code in lexos\\scrubber\\registry.py 46 47 48 49 50 51 52 def load_component ( s : str ): \"\"\"Load a single component from a string. Args: s: The name of the function. \"\"\" return scrubber_components . get ( s ) lexos . scrubber . registry . load_components ( t ) \u00a4 Load components from a tuple. Parameters: Name Type Description Default t tuple A tuple containing string names of functions. required Source code in lexos\\scrubber\\registry.py 55 56 57 58 59 60 61 62 def load_components ( t : tuple ): \"\"\"Load components from a tuple. Args: t: A tuple containing string names of functions. \"\"\" for item in t : yield scrubber_components . get ( item ) Note Custom functions can be registered by first creating the function and then adding it to the registry. An example is given below: from lexos.scrubber.registry import scrubber_components def title_case ( text ): \"\"\"Convert text to title case using `title()`\"\"\" return text . title () scrubber_components . register ( \"title_case\" , func = title_case )","title":"Registry"},{"location":"api/scrubber/registry/#registry","text":"The registry component of Scrubber maintains a catalogue of registered functions that can be imported individually as needed. The registry enables the functions to be referenced by name using string values. The code registry is created and accessed using the catalogue library by Explosion. Registered functions can be retrieved individually using lower_case = scrubber_components.get(\"lower_case\") . Multiple functions can be loaded using the load_components function:","title":"Registry"},{"location":"api/scrubber/registry/#lexos.scrubber.registry.load_component","text":"Load a single component from a string. Parameters: Name Type Description Default s str The name of the function. required Source code in lexos\\scrubber\\registry.py 46 47 48 49 50 51 52 def load_component ( s : str ): \"\"\"Load a single component from a string. Args: s: The name of the function. \"\"\" return scrubber_components . get ( s )","title":"load_component()"},{"location":"api/scrubber/registry/#lexos.scrubber.registry.load_components","text":"Load components from a tuple. Parameters: Name Type Description Default t tuple A tuple containing string names of functions. required Source code in lexos\\scrubber\\registry.py 55 56 57 58 59 60 61 62 def load_components ( t : tuple ): \"\"\"Load components from a tuple. Args: t: A tuple containing string names of functions. \"\"\" for item in t : yield scrubber_components . get ( item ) Note Custom functions can be registered by first creating the function and then adding it to the registry. An example is given below: from lexos.scrubber.registry import scrubber_components def title_case ( text ): \"\"\"Convert text to title case using `title()`\"\"\" return text . title () scrubber_components . register ( \"title_case\" , func = title_case )","title":"load_components()"},{"location":"api/scrubber/remove/","text":"Remove \u00a4 The remove component of Scrubber contains a set of functions for removing strings and patterns from text. lexos . scrubber . remove . accents ( text , * , fast = False , accents = None ) \u00a4 Remove accents from any accented unicode characters in text , either by replacing them with ASCII equivalents or removing them entirely. Parameters: Name Type Description Default text str The text from which accents will be removed. required fast bool If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. False accents Union [ str , tuple ] An optional string or tuple of strings indicating the names of diacritics to be stripped. None Returns: Type Description str str fast=True can be significantly faster than fast=False , but its transformation of text is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table For a more powerful (but slower) alternative, check out unidecode : https://github.com/avian2/unidecode Source code in lexos\\scrubber\\remove.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def accents ( text : str , * , fast : bool = False , accents : Union [ str , tuple ] = None ) -> str : \"\"\"Remove accents from any accented unicode characters in `text`, either by replacing them with ASCII equivalents or removing them entirely. Args: text (str): The text from which accents will be removed. fast: If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. accents: An optional string or tuple of strings indicating the names of diacritics to be stripped. Returns: str Note: `fast=True` can be significantly faster than `fast=False`, but its transformation of `text` is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out `unidecode`: https://github.com/avian2/unidecode \"\"\" if fast is False : if accents : if isinstance ( accents , str ): accents = set ( unicodedata . lookup ( accents )) elif len ( accents ) == 1 : accents = set ( unicodedata . lookup ( accents [ 0 ])) else : accents = set ( map ( unicodedata . lookup , accents )) return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if char not in accents ) else : return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if not unicodedata . combining ( char ) ) else : return ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ascii\" , errors = \"ignore\" ) . decode ( \"ascii\" ) ) lexos . scrubber . remove . brackets ( text , * , only = None ) \u00a4 Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Parameters: Name Type Description Default text str The text from which brackets will be removed. required only Optional [ str | Collection [ str ]] Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, \"square\" removes only those contents found between square brackets, while [\"round\", \"square\"] removes those contents found between square or round brackets, but not curly. None Returns: Type Description str str Note This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. Source code in lexos\\scrubber\\remove.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def brackets ( text : str , * , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Args: text (str): The text from which brackets will be removed. only: Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, `\"square\"` removes only those contents found between square brackets, while `[\"round\", \"square\"]` removes those contents found between square or round brackets, but not curly. Returns: str Note: This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. \"\"\" only = utils . to_collection ( only , val_type = str , col_type = set ) if only is None or \"curly\" in only : text = resources . RE_BRACKETS_CURLY . sub ( \"\" , text ) if only is None or \"square\" in only : text = resources . RE_BRACKETS_SQUARE . sub ( \"\" , text ) if only is None or \"round\" in only : text = resources . RE_BRACKETS_ROUND . sub ( \"\" , text ) return text lexos . scrubber . remove . digits ( text , * , only = None ) \u00a4 Remove digits. Remove digits from text by replacing all instances of digits (or a subset thereof specified by only ) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Parameters: Name Type Description Default text str The text from which digits will be removed. required only Optional [ str | Collection [ str ]] Remove only those digits specified here. For example, \"9\" removes only 9, while [\"1\", \"2\", \"3\"] removes 1, 2, 3; if None, all unicode digits marks are removed. None Returns: Type Description str str Source code in lexos\\scrubber\\remove.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def digits ( text : str , * , only : Optional [ str | Collection [ str ]] = None ) -> str : \"\"\"Remove digits. Remove digits from `text` by replacing all instances of digits (or a subset thereof specified by `only`) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Args: text (str): The text from which digits will be removed. only: Remove only those digits specified here. For example, `\"9\"` removes only 9, while `[\"1\", \"2\", \"3\"]` removes 1, 2, 3; if None, all unicode digits marks are removed. Returns: str \"\"\" if only : if isinstance ( only , list ): pattern = re . compile ( f '[ { \"\" . join ( only ) } ]' ) else : pattern = re . compile ( only ) else : # Using \".\" to represent any unicode character used to indicate # a decimal number, and \"***\" to represent any sequence of # unicode digits, this pattern will match: # 1) *** # 2) ***.*** unicode_digits = \"\" for i in range ( sys . maxunicode ): if unicodedata . category ( chr ( i )) . startswith ( \"N\" ): unicode_digits = unicode_digits + chr ( i ) pattern = re . compile ( r \"([+-]?[\" + re . escape ( unicode_digits ) + r \"])|((?<=\" + re . escape ( unicode_digits ) + r \")[\\u0027|\\u002C|\\u002E|\\u00B7|\" r \"\\u02D9|\\u066B|\\u066C|\\u2396][\" + re . escape ( unicode_digits ) + r \"]+)\" , re . UNICODE , ) return str ( re . sub ( pattern , r \"\" , text )) lexos . scrubber . remove . new_lines ( text ) \u00a4 Remove new lines. Remove all line-breaking spaces. Parameters: Name Type Description Default text str The text from which new lines will be removed. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\remove.py 236 237 238 239 240 241 242 243 244 245 246 247 def new_lines ( text : str ) -> str : \"\"\"Remove new lines. Remove all line-breaking spaces. Args: text (str): The text from which new lines will be removed. Returns: The normalized text. \"\"\" return resources . RE_LINEBREAK . sub ( \"\" , text ) . strip () lexos . scrubber . remove . pattern ( text , * , pattern ) \u00a4 Remove strings from text using a regex pattern. Parameters: Name Type Description Default text str The text from which patterns will be removed. required pattern Union [ str , Collection [ str ]] The pattern to match. required Returns: Type Description str str Source code in lexos\\scrubber\\remove.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def pattern ( text : str , * , pattern : Union [ str , Collection [ str ]]) -> str : \"\"\"Remove strings from `text` using a regex pattern. Args: text (str): The text from which patterns will be removed. pattern: The pattern to match. Returns: str \"\"\" if isinstance ( pattern , list ): pattern = \"|\" . join ( pattern ) pat = re . compile ( pattern ) return re . sub ( pat , \"\" , text ) lexos . scrubber . remove . project_gutenberg_headers ( text ) \u00a4 Remove Project Gutenberg headers and footers. Parameters: Name Type Description Default text str The text from which headers and footers will be removed. required Returns: Type Description str str Notes This function is reproduced from Gutenberg package's strip_headers() function ( https://github.com/c-w/gutenberg ), itself a port ofthe C++ utility by Johannes Krugel. Source code in lexos\\scrubber\\remove.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def project_gutenberg_headers ( text : str ) -> str : \"\"\"Remove Project Gutenberg headers and footers. Args: text (str): The text from which headers and footers will be removed. Returns: str Notes: This function is reproduced from Gutenberg package's `strip_headers()` function (https://github.com/c-w/gutenberg), itself a port ofthe C++ utility by Johannes Krugel. \"\"\" lines = text . splitlines () sep = str ( os . linesep ) out = [] i = 0 footer_found = False ignore_section = False for line in lines : reset = False if i <= 600 : # Check if the header ends here if any ( line . startswith ( token ) for token in resources . TEXT_START_MARKERS ): reset = True # If it's the end of the header, delete the output produced so far. # May be done several times, if multiple lines occur indicating the # end of the header if reset : out = [] continue if i >= 100 : # Check if the footer begins here if any ( line . startswith ( token ) for token in resources . TEXT_END_MARKERS ): footer_found = True # If it's the beginning of the footer, stop output if footer_found : break if any ( line . startswith ( token ) for token in resources . LEGALESE_START_MARKERS ): ignore_section = True continue elif any ( line . startswith ( token ) for token in resources . LEGALESE_END_MARKERS ): ignore_section = False continue if not ignore_section : out . append ( line . rstrip ( sep )) i += 1 return sep . join ( out ) . strip () lexos . scrubber . remove . punctuation ( text , * , exclude = None , only = None ) \u00a4 Remove punctuation from text . Removes all instances of punctuation (or a subset thereof specified by only ). Parameters: Name Type Description Default text str The text from which punctuation will be removed. required exclude Optional [ str | Collection [ str ]] Remove all punctuation except designated characters. None only Optional [ str | Collection [ str ]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\remove.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove punctuation from `text`. Removes all instances of punctuation (or a subset thereof specified by `only`). Args: text (str): The text from which punctuation will be removed. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \"\" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) else : exclude = [] # Note: We can't use the cached translation table because it replaces # the punctuation with whitespace, so we have to build a new one. translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \"\" , ) return text . translate ( translation_table ) lexos . scrubber . remove . tabs ( text ) \u00a4 Remove tabs. If you want to replace tabs with a single space, use normalize.whitespace() instead. Parameters: Name Type Description Default text str The text from which tabs will be removed. required Returns: Type Description str The stripped text. Source code in lexos\\scrubber\\remove.py 313 314 315 316 317 318 319 320 321 322 323 324 325 def tabs ( text : str ) -> str : \"\"\"Remove tabs. If you want to replace tabs with a single space, use `normalize.whitespace()` instead. Args: text (str): The text from which tabs will be removed. Returns: The stripped text. \"\"\" return resources . RE_TAB . sub ( \"\" , text ) lexos . scrubber . remove . tags ( text , sep = ' ' , remove_whitespace = True ) \u00a4 Remove tags from text . Parameters: Name Type Description Default text str The text from which tags will be removed. required sep str A string to insert between tags and text found between them. ' ' remove_whitespace bool If True, remove extra whitespace between text after tags are removed. True Returns: Type Description str A string containing just the text found between tags and other str non-data elements. Note If you want to perfom selective removal of tags, use replace.tag_map instead. This function relies on the stdlib html.parser.HTMLParser . It appears to work for stripping tags from both html and xml. Using lxml or BeautifulSoup might be faster, but this is untested. This function preserves text in comments, as well as tags Source code in lexos\\scrubber\\remove.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def tags ( text : str , sep : str = \" \" , remove_whitespace : bool = True ) -> str : \"\"\"Remove tags from `text`. Args: text (str): The text from which tags will be removed. sep: A string to insert between tags and text found between them. remove_whitespace: If True, remove extra whitespace between text after tags are removed. Returns: A string containing just the text found between tags and other non-data elements. Note: - If you want to perfom selective removal of tags, use `replace.tag_map` instead. - This function relies on the stdlib `html.parser.HTMLParser`. It appears to work for stripping tags from both html and xml. Using `lxml` or BeautifulSoup might be faster, but this is untested. - This function preserves text in comments, as well as tags \"\"\" parser = resources . HTMLTextExtractor () parser . feed ( text ) text = parser . get_text ( sep = sep ) if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , sep , text , re . UNICODE ) return text Note Tag handling has been ported over from the Lexos web app, which uses BeautifulSoup and lxml to parse the tree. It will be good to watch the development of selectolax , which claims to be more efficient, at least for HTML. An implementation with spaCy is available in the spacy-html-tokenizer , though it may not be right for integration into Lexos since the output is a doc in which tokens are sentences.","title":"Remove"},{"location":"api/scrubber/remove/#remove","text":"The remove component of Scrubber contains a set of functions for removing strings and patterns from text.","title":"Remove"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.accents","text":"Remove accents from any accented unicode characters in text , either by replacing them with ASCII equivalents or removing them entirely. Parameters: Name Type Description Default text str The text from which accents will be removed. required fast bool If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. False accents Union [ str , tuple ] An optional string or tuple of strings indicating the names of diacritics to be stripped. None Returns: Type Description str str fast=True can be significantly faster than fast=False , but its transformation of text is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table For a more powerful (but slower) alternative, check out unidecode : https://github.com/avian2/unidecode Source code in lexos\\scrubber\\remove.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def accents ( text : str , * , fast : bool = False , accents : Union [ str , tuple ] = None ) -> str : \"\"\"Remove accents from any accented unicode characters in `text`, either by replacing them with ASCII equivalents or removing them entirely. Args: text (str): The text from which accents will be removed. fast: If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. accents: An optional string or tuple of strings indicating the names of diacritics to be stripped. Returns: str Note: `fast=True` can be significantly faster than `fast=False`, but its transformation of `text` is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out `unidecode`: https://github.com/avian2/unidecode \"\"\" if fast is False : if accents : if isinstance ( accents , str ): accents = set ( unicodedata . lookup ( accents )) elif len ( accents ) == 1 : accents = set ( unicodedata . lookup ( accents [ 0 ])) else : accents = set ( map ( unicodedata . lookup , accents )) return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if char not in accents ) else : return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if not unicodedata . combining ( char ) ) else : return ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ascii\" , errors = \"ignore\" ) . decode ( \"ascii\" ) )","title":"accents()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.brackets","text":"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Parameters: Name Type Description Default text str The text from which brackets will be removed. required only Optional [ str | Collection [ str ]] Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, \"square\" removes only those contents found between square brackets, while [\"round\", \"square\"] removes those contents found between square or round brackets, but not curly. None Returns: Type Description str str Note This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. Source code in lexos\\scrubber\\remove.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def brackets ( text : str , * , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Args: text (str): The text from which brackets will be removed. only: Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, `\"square\"` removes only those contents found between square brackets, while `[\"round\", \"square\"]` removes those contents found between square or round brackets, but not curly. Returns: str Note: This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. \"\"\" only = utils . to_collection ( only , val_type = str , col_type = set ) if only is None or \"curly\" in only : text = resources . RE_BRACKETS_CURLY . sub ( \"\" , text ) if only is None or \"square\" in only : text = resources . RE_BRACKETS_SQUARE . sub ( \"\" , text ) if only is None or \"round\" in only : text = resources . RE_BRACKETS_ROUND . sub ( \"\" , text ) return text","title":"brackets()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.digits","text":"Remove digits. Remove digits from text by replacing all instances of digits (or a subset thereof specified by only ) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Parameters: Name Type Description Default text str The text from which digits will be removed. required only Optional [ str | Collection [ str ]] Remove only those digits specified here. For example, \"9\" removes only 9, while [\"1\", \"2\", \"3\"] removes 1, 2, 3; if None, all unicode digits marks are removed. None Returns: Type Description str str Source code in lexos\\scrubber\\remove.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def digits ( text : str , * , only : Optional [ str | Collection [ str ]] = None ) -> str : \"\"\"Remove digits. Remove digits from `text` by replacing all instances of digits (or a subset thereof specified by `only`) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Args: text (str): The text from which digits will be removed. only: Remove only those digits specified here. For example, `\"9\"` removes only 9, while `[\"1\", \"2\", \"3\"]` removes 1, 2, 3; if None, all unicode digits marks are removed. Returns: str \"\"\" if only : if isinstance ( only , list ): pattern = re . compile ( f '[ { \"\" . join ( only ) } ]' ) else : pattern = re . compile ( only ) else : # Using \".\" to represent any unicode character used to indicate # a decimal number, and \"***\" to represent any sequence of # unicode digits, this pattern will match: # 1) *** # 2) ***.*** unicode_digits = \"\" for i in range ( sys . maxunicode ): if unicodedata . category ( chr ( i )) . startswith ( \"N\" ): unicode_digits = unicode_digits + chr ( i ) pattern = re . compile ( r \"([+-]?[\" + re . escape ( unicode_digits ) + r \"])|((?<=\" + re . escape ( unicode_digits ) + r \")[\\u0027|\\u002C|\\u002E|\\u00B7|\" r \"\\u02D9|\\u066B|\\u066C|\\u2396][\" + re . escape ( unicode_digits ) + r \"]+)\" , re . UNICODE , ) return str ( re . sub ( pattern , r \"\" , text ))","title":"digits()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.new_lines","text":"Remove new lines. Remove all line-breaking spaces. Parameters: Name Type Description Default text str The text from which new lines will be removed. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\remove.py 236 237 238 239 240 241 242 243 244 245 246 247 def new_lines ( text : str ) -> str : \"\"\"Remove new lines. Remove all line-breaking spaces. Args: text (str): The text from which new lines will be removed. Returns: The normalized text. \"\"\" return resources . RE_LINEBREAK . sub ( \"\" , text ) . strip ()","title":"new_lines()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.pattern","text":"Remove strings from text using a regex pattern. Parameters: Name Type Description Default text str The text from which patterns will be removed. required pattern Union [ str , Collection [ str ]] The pattern to match. required Returns: Type Description str str Source code in lexos\\scrubber\\remove.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def pattern ( text : str , * , pattern : Union [ str , Collection [ str ]]) -> str : \"\"\"Remove strings from `text` using a regex pattern. Args: text (str): The text from which patterns will be removed. pattern: The pattern to match. Returns: str \"\"\" if isinstance ( pattern , list ): pattern = \"|\" . join ( pattern ) pat = re . compile ( pattern ) return re . sub ( pat , \"\" , text )","title":"pattern()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.project_gutenberg_headers","text":"Remove Project Gutenberg headers and footers. Parameters: Name Type Description Default text str The text from which headers and footers will be removed. required Returns: Type Description str str Notes This function is reproduced from Gutenberg package's strip_headers() function ( https://github.com/c-w/gutenberg ), itself a port ofthe C++ utility by Johannes Krugel. Source code in lexos\\scrubber\\remove.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def project_gutenberg_headers ( text : str ) -> str : \"\"\"Remove Project Gutenberg headers and footers. Args: text (str): The text from which headers and footers will be removed. Returns: str Notes: This function is reproduced from Gutenberg package's `strip_headers()` function (https://github.com/c-w/gutenberg), itself a port ofthe C++ utility by Johannes Krugel. \"\"\" lines = text . splitlines () sep = str ( os . linesep ) out = [] i = 0 footer_found = False ignore_section = False for line in lines : reset = False if i <= 600 : # Check if the header ends here if any ( line . startswith ( token ) for token in resources . TEXT_START_MARKERS ): reset = True # If it's the end of the header, delete the output produced so far. # May be done several times, if multiple lines occur indicating the # end of the header if reset : out = [] continue if i >= 100 : # Check if the footer begins here if any ( line . startswith ( token ) for token in resources . TEXT_END_MARKERS ): footer_found = True # If it's the beginning of the footer, stop output if footer_found : break if any ( line . startswith ( token ) for token in resources . LEGALESE_START_MARKERS ): ignore_section = True continue elif any ( line . startswith ( token ) for token in resources . LEGALESE_END_MARKERS ): ignore_section = False continue if not ignore_section : out . append ( line . rstrip ( sep )) i += 1 return sep . join ( out ) . strip ()","title":"project_gutenberg_headers()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.punctuation","text":"Remove punctuation from text . Removes all instances of punctuation (or a subset thereof specified by only ). Parameters: Name Type Description Default text str The text from which punctuation will be removed. required exclude Optional [ str | Collection [ str ]] Remove all punctuation except designated characters. None only Optional [ str | Collection [ str ]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\remove.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove punctuation from `text`. Removes all instances of punctuation (or a subset thereof specified by `only`). Args: text (str): The text from which punctuation will be removed. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \"\" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) else : exclude = [] # Note: We can't use the cached translation table because it replaces # the punctuation with whitespace, so we have to build a new one. translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \"\" , ) return text . translate ( translation_table )","title":"punctuation()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tabs","text":"Remove tabs. If you want to replace tabs with a single space, use normalize.whitespace() instead. Parameters: Name Type Description Default text str The text from which tabs will be removed. required Returns: Type Description str The stripped text. Source code in lexos\\scrubber\\remove.py 313 314 315 316 317 318 319 320 321 322 323 324 325 def tabs ( text : str ) -> str : \"\"\"Remove tabs. If you want to replace tabs with a single space, use `normalize.whitespace()` instead. Args: text (str): The text from which tabs will be removed. Returns: The stripped text. \"\"\" return resources . RE_TAB . sub ( \"\" , text )","title":"tabs()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tags","text":"Remove tags from text . Parameters: Name Type Description Default text str The text from which tags will be removed. required sep str A string to insert between tags and text found between them. ' ' remove_whitespace bool If True, remove extra whitespace between text after tags are removed. True Returns: Type Description str A string containing just the text found between tags and other str non-data elements. Note If you want to perfom selective removal of tags, use replace.tag_map instead. This function relies on the stdlib html.parser.HTMLParser . It appears to work for stripping tags from both html and xml. Using lxml or BeautifulSoup might be faster, but this is untested. This function preserves text in comments, as well as tags Source code in lexos\\scrubber\\remove.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def tags ( text : str , sep : str = \" \" , remove_whitespace : bool = True ) -> str : \"\"\"Remove tags from `text`. Args: text (str): The text from which tags will be removed. sep: A string to insert between tags and text found between them. remove_whitespace: If True, remove extra whitespace between text after tags are removed. Returns: A string containing just the text found between tags and other non-data elements. Note: - If you want to perfom selective removal of tags, use `replace.tag_map` instead. - This function relies on the stdlib `html.parser.HTMLParser`. It appears to work for stripping tags from both html and xml. Using `lxml` or BeautifulSoup might be faster, but this is untested. - This function preserves text in comments, as well as tags \"\"\" parser = resources . HTMLTextExtractor () parser . feed ( text ) text = parser . get_text ( sep = sep ) if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , sep , text , re . UNICODE ) return text Note Tag handling has been ported over from the Lexos web app, which uses BeautifulSoup and lxml to parse the tree. It will be good to watch the development of selectolax , which claims to be more efficient, at least for HTML. An implementation with spaCy is available in the spacy-html-tokenizer , though it may not be right for integration into Lexos since the output is a doc in which tokens are sentences.","title":"tags()"},{"location":"api/scrubber/replace/","text":"Replace \u00a4 The replace component of Scrubber contains a set of functions for replacing strings and patterns in text. Important Some functions have the same names as functions in the remove component. To distinguish them in the registry, replace functions with the same names are prefixed with re_ . When loaded into a script, they can be given any name the user desires. lexos . scrubber . replace . currency_symbols ( text , repl = '_CUR_' ) \u00a4 Replace all currency symbols in text with repl . Parameters: Name Type Description Default text str The text in which currency symbols will be replaced. required repl str The replacement value for currency symbols. '_CUR_' Returns: Name Type Description str str The text with currency symbols replaced. Source code in lexos\\scrubber\\replace.py 15 16 17 18 19 20 21 22 23 24 25 def currency_symbols ( text : str , repl : str = \"_CUR_\" ) -> str : \"\"\"Replace all currency symbols in `text` with `repl`. Args: text (str): The text in which currency symbols will be replaced. repl (str): The replacement value for currency symbols. Returns: str: The text with currency symbols replaced. \"\"\" return resources . RE_CURRENCY_SYMBOL . sub ( repl , text ) lexos . scrubber . replace . digits ( text , repl = '_DIGIT_' ) \u00a4 Replace all digits in text with repl . Parameters: Name Type Description Default text str The text in which digits will be replaced. required repl str The replacement value for digits. '_DIGIT_' Returns: Name Type Description str str The text with digits replaced. Source code in lexos\\scrubber\\replace.py 28 29 30 31 32 33 34 35 36 37 38 def digits ( text : str , repl : str = \"_DIGIT_\" ) -> str : \"\"\"Replace all digits in `text` with `repl`. Args: text (str): The text in which digits will be replaced. repl (str): The replacement value for digits. Returns: str: The text with digits replaced. \"\"\" return resources . RE_NUMBER . sub ( repl , text ) lexos . scrubber . replace . emails ( text , repl = '_EMAIL_' ) \u00a4 Replace all email addresses in text with repl . Parameters: Name Type Description Default text str The text in which emails will be replaced. required repl str The replacement value for emails. '_EMAIL_' Returns: Name Type Description str str The text with emails replaced. Source code in lexos\\scrubber\\replace.py 41 42 43 44 45 46 47 48 49 50 51 def emails ( text : str , repl : str = \"_EMAIL_\" ) -> str : \"\"\"Replace all email addresses in `text` with `repl`. Args: text (str): The text in which emails will be replaced. repl (str): The replacement value for emails. Returns: str: The text with emails replaced. \"\"\" return resources . RE_EMAIL . sub ( repl , text ) lexos . scrubber . replace . emojis ( text , repl = '_EMOJI_' ) \u00a4 Replace all emoji and pictographs in text with repl . Parameters: Name Type Description Default text str The text in which emojis will be replaced. required repl str The replacement value for emojis. '_EMOJI_' Returns: Name Type Description str str The text with emojis replaced. Note If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! Source code in lexos\\scrubber\\replace.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def emojis ( text : str , repl : str = \"_EMOJI_\" ) -> str : \"\"\" Replace all emoji and pictographs in `text` with `repl`. Args: text (str): The text in which emojis will be replaced. repl (str): The replacement value for emojis. Returns: str: The text with emojis replaced. Note: If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! \"\"\" return resources . RE_EMOJI . sub ( repl , text ) lexos . scrubber . replace . hashtags ( text , repl = '_HASHTAG_' ) \u00a4 Replace all hashtags in text with repl . Parameters: Name Type Description Default text str The text in which hashtags will be replaced. required repl str The replacement value for hashtags. '_HASHTAG_' Returns: Name Type Description str str The text with currency hashtags replaced. Source code in lexos\\scrubber\\replace.py 73 74 75 76 77 78 79 80 81 82 83 def hashtags ( text : str , repl : str = \"_HASHTAG_\" ) -> str : \"\"\"Replace all hashtags in `text` with `repl`. Args: text (str): The text in which hashtags will be replaced. repl (str): The replacement value for hashtags. Returns: str: The text with currency hashtags replaced. \"\"\" return resources . RE_HASHTAG . sub ( repl , text ) lexos . scrubber . replace . pattern ( text , * , pattern ) \u00a4 Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which a pattern or pattern will be replaced. required pattern Union [ dict , Collection [ dict ]] (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). required Returns: Name Type Description str str The text with pattern(s) replaced. Source code in lexos\\scrubber\\replace.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def pattern ( text : str , * , pattern : Union [ dict , Collection [ dict ]]) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which a pattern or pattern will be replaced. pattern: (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). Returns: str: The text with pattern(s) replaced. \"\"\" pattern = utils . ensure_list ( pattern ) for pat in pattern : k = str ( * pat ) match = re . compile ( k ) text = re . sub ( match , pat [ k ], text ) return text lexos . scrubber . replace . phone_numbers ( text , repl = '_PHONE_' ) \u00a4 Replace all phone numbers in text with repl . Parameters: Name Type Description Default text str The text in which phone numbers will be replaced. required repl str The replacement value for phone numbers. '_PHONE_' Returns: Name Type Description str str The text with phone numbers replaced. Source code in lexos\\scrubber\\replace.py 105 106 107 108 109 110 111 112 113 114 115 def phone_numbers ( text : str , repl : str = \"_PHONE_\" ) -> str : \"\"\"Replace all phone numbers in `text` with `repl`. Args: text (str): The text in which phone numbers will be replaced. repl (str): The replacement value for phone numbers. Returns: str: The text with phone numbers replaced. \"\"\" return resources . RE_PHONE_NUMBER . sub ( repl , text ) lexos . scrubber . replace . process_tag_replace_options ( orig_text , tag , action , attribute ) \u00a4 Replace html-style tags in text files according to user options. Parameters: Name Type Description Default orig_text str The user's text containing the original tag. required tag str The particular tag to be processed. required action str A string specifying the action to be performed on the tag. required attribute str Replacement value for tag when \"replace_with_attribute\" is specified. required Returns: Name Type Description str str The text after the specified tag is processed. Notes Action options are: \"remove_tag\": Remove the tag \"remove_element\": Remove the element and contents \"replace_element\": Replace the tag with the specified attribute The replacement of a tag with the value of an attribute may not be supported. This needs a second look. Source code in lexos\\scrubber\\replace.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def process_tag_replace_options ( orig_text : str , tag : str , action : str , attribute : str ) -> str : \"\"\"Replace html-style tags in text files according to user options. Args: orig_text: The user's text containing the original tag. tag: The particular tag to be processed. action: A string specifying the action to be performed on the tag. attribute: Replacement value for tag when \"replace_with_attribute\" is specified. Returns: str: The text after the specified tag is processed. Notes: - Action options are: - \"remove_tag\": Remove the tag - \"remove_element\": Remove the element and contents - \"replace_element\": Replace the tag with the specified attribute - The replacement of a tag with the value of an attribute may not be supported. This needs a second look. \"\"\" if action == \"remove_tag\" : # searching for variants this specific tag: <tag> ... pattern = re . compile ( r \"<(?:\" + tag + r '(?=\\s)(?!(?:[^>\" \\' ]|\"[^\"]*\"| \\' [^ \\' ]* \\' )*?(?<=\\s)' r '\\s*=)(?!\\s*/?>)\\s+(?:\".*?\"| \\' .*? \\' |[^>]*?)+|/?' + tag + r \"\\s*/?)>\" , re . MULTILINE | re . DOTALL | re . UNICODE , ) # substitute all matching patterns with one space processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"remove_element\" : # <[whitespaces] TAG [SPACE attributes]> contents </[whitespaces]TAG> # as applied across newlines, (re.MULTILINE), on re.UNICODE, # and .* includes newlines (re.DOTALL) pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \"( .+?>|>).+?</\\s*\" + re . escape ( tag ) + \">\" , re . MULTILINE | re . DOTALL | re . UNICODE , ) processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"replace_element\" : pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \".*?>.+?</\\s*\" + re . escape ( tag ) + \".*?>\" , re . MULTILINE | re . DOTALL | re . UNICODE , ) processed_text = re . sub ( pattern , attribute , orig_text ) else : processed_text = orig_text # Leave Tag Alone return processed_text lexos . scrubber . replace . punctuation ( text , * , exclude = None , only = None ) \u00a4 Replace punctuation from text . Replaces all instances of punctuation (or a subset thereof specified by only ) with whitespace. Parameters: Name Type Description Default text str The text in which punctuation will be replaced. required exclude Optional [ str | Collection [ str ]] Remove all punctuation except designated characters. None only Optional [ str | Collection [ str ]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\replace.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Replace punctuation from `text`. Replaces all instances of punctuation (or a subset thereof specified by `only`) with whitespace. Args: text (str): The text in which punctuation will be replaced. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \" \" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \" \" , ) else : translation_table = resources . PUNCT_TRANSLATION_TABLE return text . translate ( translation_table ) lexos . scrubber . replace . special_characters ( text , * , is_html = False , ruleset = None ) \u00a4 Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which special characters will be replaced. required is_html bool Whether to replace HTML entities. False ruleset dict A dict containing the special characters to match and their replacements. None Returns: Type Description str str Source code in lexos\\scrubber\\replace.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def special_characters ( text : str , * , is_html : bool = False , ruleset : dict = None , ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which special characters will be replaced. is_html (bool): Whether to replace HTML entities. ruleset (dict): A dict containing the special characters to match and their replacements. Returns: str \"\"\" if is_html : text = html . unescape ( text ) else : for k , v in ruleset . items (): match = re . compile ( k ) text = re . sub ( match , v , text ) return text lexos . scrubber . replace . tag_map ( text , map , remove_comments = True , remove_doctype = True , remove_whitespace = False ) \u00a4 Handle tags that are found in the text. Parameters: Name Type Description Default text str The text in which tags will be replaced. required remove_comments bool Whether to remove comments. True remove_doctype bool Whether to remove the doctype or xml declaration. True remove_whitespace bool Whether to remove whitespace. False Returns: Name Type Description str str The text after tags have been replaced. Source code in lexos\\scrubber\\replace.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def tag_map ( text : str , # xmlhandlingoptions: List[dict], map : Dict [ str ], remove_comments : bool = True , remove_doctype : bool = True , remove_whitespace : bool = False , ) -> str : \"\"\"Handle tags that are found in the text. Args: text (str): The text in which tags will be replaced. remove_comments (bool): Whether to remove comments. remove_doctype (bool): Whether to remove the doctype or xml declaration. remove_whitespace (bool): Whether to remove whitespace. Returns: str: The text after tags have been replaced. \"\"\" if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) # Remove extra white space if remove_doctype : doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations text = re . sub ( r \"(<\\?.*?>)\" , \"\" , text ) # Remove xml declarations if remove_comments : text = re . sub ( r \"(<!--.*?-->)\" , \"\" , text ) # Remove comments # This matches the DOCTYPE and all internal entity declarations doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations # Visit each tag: for tag , opts in map . items (): action = opts [ \"action\" ] attribute = opts [ \"attribute\" ] text = process_tag_replace_options ( text , tag , action , attribute ) # One last catch-all removes extra whitespace from all the removed tags if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) return text lexos . scrubber . replace . urls ( text , repl = '_URL_' ) \u00a4 Replace all URLs in text with repl . Parameters: Name Type Description Default text str The text in which urls will be replaced. required repl str The replacement value for urls. '_URL_' Returns: Name Type Description str str The text with urls replaced. Source code in lexos\\scrubber\\replace.py 293 294 295 296 297 298 299 300 301 302 303 def urls ( text : str , repl : str = \"_URL_\" ) -> str : \"\"\"Replace all URLs in `text` with `repl`. Args: text (str): The text in which urls will be replaced. repl (str): The replacement value for urls. Returns: str: The text with urls replaced. \"\"\" return resources . RE_SHORT_URL . sub ( repl , resources . RE_URL . sub ( repl , text )) lexos . scrubber . replace . user_handles ( text , repl = '_USER_' ) \u00a4 Replace all (Twitter-style) user handles in text with repl . Parameters: Name Type Description Default text str The text in which user handles will be replaced. required repl str The replacement value for user handles. '_USER_' Returns: Name Type Description str str The text with user handles replaced. Source code in lexos\\scrubber\\replace.py 306 307 308 309 310 311 312 313 314 315 316 def user_handles ( text : str , repl : str = \"_USER_\" ) -> str : \"\"\"Replace all (Twitter-style) user handles in `text` with `repl`. Args: text (str): The text in which user handles will be replaced. repl (str): The replacement value for user handles. Returns: str: The text with user handles replaced. \"\"\" return resources . RE_USER_HANDLE . sub ( repl , text )","title":"Replace"},{"location":"api/scrubber/replace/#replace","text":"The replace component of Scrubber contains a set of functions for replacing strings and patterns in text. Important Some functions have the same names as functions in the remove component. To distinguish them in the registry, replace functions with the same names are prefixed with re_ . When loaded into a script, they can be given any name the user desires.","title":"Replace"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.currency_symbols","text":"Replace all currency symbols in text with repl . Parameters: Name Type Description Default text str The text in which currency symbols will be replaced. required repl str The replacement value for currency symbols. '_CUR_' Returns: Name Type Description str str The text with currency symbols replaced. Source code in lexos\\scrubber\\replace.py 15 16 17 18 19 20 21 22 23 24 25 def currency_symbols ( text : str , repl : str = \"_CUR_\" ) -> str : \"\"\"Replace all currency symbols in `text` with `repl`. Args: text (str): The text in which currency symbols will be replaced. repl (str): The replacement value for currency symbols. Returns: str: The text with currency symbols replaced. \"\"\" return resources . RE_CURRENCY_SYMBOL . sub ( repl , text )","title":"currency_symbols()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.digits","text":"Replace all digits in text with repl . Parameters: Name Type Description Default text str The text in which digits will be replaced. required repl str The replacement value for digits. '_DIGIT_' Returns: Name Type Description str str The text with digits replaced. Source code in lexos\\scrubber\\replace.py 28 29 30 31 32 33 34 35 36 37 38 def digits ( text : str , repl : str = \"_DIGIT_\" ) -> str : \"\"\"Replace all digits in `text` with `repl`. Args: text (str): The text in which digits will be replaced. repl (str): The replacement value for digits. Returns: str: The text with digits replaced. \"\"\" return resources . RE_NUMBER . sub ( repl , text )","title":"digits()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emails","text":"Replace all email addresses in text with repl . Parameters: Name Type Description Default text str The text in which emails will be replaced. required repl str The replacement value for emails. '_EMAIL_' Returns: Name Type Description str str The text with emails replaced. Source code in lexos\\scrubber\\replace.py 41 42 43 44 45 46 47 48 49 50 51 def emails ( text : str , repl : str = \"_EMAIL_\" ) -> str : \"\"\"Replace all email addresses in `text` with `repl`. Args: text (str): The text in which emails will be replaced. repl (str): The replacement value for emails. Returns: str: The text with emails replaced. \"\"\" return resources . RE_EMAIL . sub ( repl , text )","title":"emails()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emojis","text":"Replace all emoji and pictographs in text with repl . Parameters: Name Type Description Default text str The text in which emojis will be replaced. required repl str The replacement value for emojis. '_EMOJI_' Returns: Name Type Description str str The text with emojis replaced. Note If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! Source code in lexos\\scrubber\\replace.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def emojis ( text : str , repl : str = \"_EMOJI_\" ) -> str : \"\"\" Replace all emoji and pictographs in `text` with `repl`. Args: text (str): The text in which emojis will be replaced. repl (str): The replacement value for emojis. Returns: str: The text with emojis replaced. Note: If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! \"\"\" return resources . RE_EMOJI . sub ( repl , text )","title":"emojis()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.hashtags","text":"Replace all hashtags in text with repl . Parameters: Name Type Description Default text str The text in which hashtags will be replaced. required repl str The replacement value for hashtags. '_HASHTAG_' Returns: Name Type Description str str The text with currency hashtags replaced. Source code in lexos\\scrubber\\replace.py 73 74 75 76 77 78 79 80 81 82 83 def hashtags ( text : str , repl : str = \"_HASHTAG_\" ) -> str : \"\"\"Replace all hashtags in `text` with `repl`. Args: text (str): The text in which hashtags will be replaced. repl (str): The replacement value for hashtags. Returns: str: The text with currency hashtags replaced. \"\"\" return resources . RE_HASHTAG . sub ( repl , text )","title":"hashtags()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.pattern","text":"Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which a pattern or pattern will be replaced. required pattern Union [ dict , Collection [ dict ]] (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). required Returns: Name Type Description str str The text with pattern(s) replaced. Source code in lexos\\scrubber\\replace.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def pattern ( text : str , * , pattern : Union [ dict , Collection [ dict ]]) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which a pattern or pattern will be replaced. pattern: (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). Returns: str: The text with pattern(s) replaced. \"\"\" pattern = utils . ensure_list ( pattern ) for pat in pattern : k = str ( * pat ) match = re . compile ( k ) text = re . sub ( match , pat [ k ], text ) return text","title":"pattern()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.phone_numbers","text":"Replace all phone numbers in text with repl . Parameters: Name Type Description Default text str The text in which phone numbers will be replaced. required repl str The replacement value for phone numbers. '_PHONE_' Returns: Name Type Description str str The text with phone numbers replaced. Source code in lexos\\scrubber\\replace.py 105 106 107 108 109 110 111 112 113 114 115 def phone_numbers ( text : str , repl : str = \"_PHONE_\" ) -> str : \"\"\"Replace all phone numbers in `text` with `repl`. Args: text (str): The text in which phone numbers will be replaced. repl (str): The replacement value for phone numbers. Returns: str: The text with phone numbers replaced. \"\"\" return resources . RE_PHONE_NUMBER . sub ( repl , text )","title":"phone_numbers()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.process_tag_replace_options","text":"Replace html-style tags in text files according to user options. Parameters: Name Type Description Default orig_text str The user's text containing the original tag. required tag str The particular tag to be processed. required action str A string specifying the action to be performed on the tag. required attribute str Replacement value for tag when \"replace_with_attribute\" is specified. required Returns: Name Type Description str str The text after the specified tag is processed. Notes Action options are: \"remove_tag\": Remove the tag \"remove_element\": Remove the element and contents \"replace_element\": Replace the tag with the specified attribute The replacement of a tag with the value of an attribute may not be supported. This needs a second look. Source code in lexos\\scrubber\\replace.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def process_tag_replace_options ( orig_text : str , tag : str , action : str , attribute : str ) -> str : \"\"\"Replace html-style tags in text files according to user options. Args: orig_text: The user's text containing the original tag. tag: The particular tag to be processed. action: A string specifying the action to be performed on the tag. attribute: Replacement value for tag when \"replace_with_attribute\" is specified. Returns: str: The text after the specified tag is processed. Notes: - Action options are: - \"remove_tag\": Remove the tag - \"remove_element\": Remove the element and contents - \"replace_element\": Replace the tag with the specified attribute - The replacement of a tag with the value of an attribute may not be supported. This needs a second look. \"\"\" if action == \"remove_tag\" : # searching for variants this specific tag: <tag> ... pattern = re . compile ( r \"<(?:\" + tag + r '(?=\\s)(?!(?:[^>\" \\' ]|\"[^\"]*\"| \\' [^ \\' ]* \\' )*?(?<=\\s)' r '\\s*=)(?!\\s*/?>)\\s+(?:\".*?\"| \\' .*? \\' |[^>]*?)+|/?' + tag + r \"\\s*/?)>\" , re . MULTILINE | re . DOTALL | re . UNICODE , ) # substitute all matching patterns with one space processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"remove_element\" : # <[whitespaces] TAG [SPACE attributes]> contents </[whitespaces]TAG> # as applied across newlines, (re.MULTILINE), on re.UNICODE, # and .* includes newlines (re.DOTALL) pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \"( .+?>|>).+?</\\s*\" + re . escape ( tag ) + \">\" , re . MULTILINE | re . DOTALL | re . UNICODE , ) processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"replace_element\" : pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \".*?>.+?</\\s*\" + re . escape ( tag ) + \".*?>\" , re . MULTILINE | re . DOTALL | re . UNICODE , ) processed_text = re . sub ( pattern , attribute , orig_text ) else : processed_text = orig_text # Leave Tag Alone return processed_text","title":"process_tag_replace_options()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.punctuation","text":"Replace punctuation from text . Replaces all instances of punctuation (or a subset thereof specified by only ) with whitespace. Parameters: Name Type Description Default text str The text in which punctuation will be replaced. required exclude Optional [ str | Collection [ str ]] Remove all punctuation except designated characters. None only Optional [ str | Collection [ str ]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\replace.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Replace punctuation from `text`. Replaces all instances of punctuation (or a subset thereof specified by `only`) with whitespace. Args: text (str): The text in which punctuation will be replaced. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \" \" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \" \" , ) else : translation_table = resources . PUNCT_TRANSLATION_TABLE return text . translate ( translation_table )","title":"punctuation()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.special_characters","text":"Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which special characters will be replaced. required is_html bool Whether to replace HTML entities. False ruleset dict A dict containing the special characters to match and their replacements. None Returns: Type Description str str Source code in lexos\\scrubber\\replace.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def special_characters ( text : str , * , is_html : bool = False , ruleset : dict = None , ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which special characters will be replaced. is_html (bool): Whether to replace HTML entities. ruleset (dict): A dict containing the special characters to match and their replacements. Returns: str \"\"\" if is_html : text = html . unescape ( text ) else : for k , v in ruleset . items (): match = re . compile ( k ) text = re . sub ( match , v , text ) return text","title":"special_characters()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.tag_map","text":"Handle tags that are found in the text. Parameters: Name Type Description Default text str The text in which tags will be replaced. required remove_comments bool Whether to remove comments. True remove_doctype bool Whether to remove the doctype or xml declaration. True remove_whitespace bool Whether to remove whitespace. False Returns: Name Type Description str str The text after tags have been replaced. Source code in lexos\\scrubber\\replace.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def tag_map ( text : str , # xmlhandlingoptions: List[dict], map : Dict [ str ], remove_comments : bool = True , remove_doctype : bool = True , remove_whitespace : bool = False , ) -> str : \"\"\"Handle tags that are found in the text. Args: text (str): The text in which tags will be replaced. remove_comments (bool): Whether to remove comments. remove_doctype (bool): Whether to remove the doctype or xml declaration. remove_whitespace (bool): Whether to remove whitespace. Returns: str: The text after tags have been replaced. \"\"\" if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) # Remove extra white space if remove_doctype : doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations text = re . sub ( r \"(<\\?.*?>)\" , \"\" , text ) # Remove xml declarations if remove_comments : text = re . sub ( r \"(<!--.*?-->)\" , \"\" , text ) # Remove comments # This matches the DOCTYPE and all internal entity declarations doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations # Visit each tag: for tag , opts in map . items (): action = opts [ \"action\" ] attribute = opts [ \"attribute\" ] text = process_tag_replace_options ( text , tag , action , attribute ) # One last catch-all removes extra whitespace from all the removed tags if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) return text","title":"tag_map()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.urls","text":"Replace all URLs in text with repl . Parameters: Name Type Description Default text str The text in which urls will be replaced. required repl str The replacement value for urls. '_URL_' Returns: Name Type Description str str The text with urls replaced. Source code in lexos\\scrubber\\replace.py 293 294 295 296 297 298 299 300 301 302 303 def urls ( text : str , repl : str = \"_URL_\" ) -> str : \"\"\"Replace all URLs in `text` with `repl`. Args: text (str): The text in which urls will be replaced. repl (str): The replacement value for urls. Returns: str: The text with urls replaced. \"\"\" return resources . RE_SHORT_URL . sub ( repl , resources . RE_URL . sub ( repl , text ))","title":"urls()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.user_handles","text":"Replace all (Twitter-style) user handles in text with repl . Parameters: Name Type Description Default text str The text in which user handles will be replaced. required repl str The replacement value for user handles. '_USER_' Returns: Name Type Description str str The text with user handles replaced. Source code in lexos\\scrubber\\replace.py 306 307 308 309 310 311 312 313 314 315 316 def user_handles ( text : str , repl : str = \"_USER_\" ) -> str : \"\"\"Replace all (Twitter-style) user handles in `text` with `repl`. Args: text (str): The text in which user handles will be replaced. repl (str): The replacement value for user handles. Returns: str: The text with user handles replaced. \"\"\" return resources . RE_USER_HANDLE . sub ( repl , text )","title":"user_handles()"},{"location":"api/scrubber/resources/","text":"Resources \u00a4 The resources component of Scrubber contains a set of functions for replacing strings and patterns in text. lexos.scrubber.resources.HTMLTextExtractor \u00a4 Bases: html . parser . HTMLParser Simple subclass of :class: html.parser.HTMLParser . Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via HTMLTextExtractor.get_text() . Note Users probably shouldn't deal with this class directly; instead, use :func: remove.remove_html_tags()`. Source code in lexos\\scrubber\\resources.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class HTMLTextExtractor ( html . parser . HTMLParser ): \"\"\"Simple subclass of :class:`html.parser.HTMLParser`. Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via `HTMLTextExtractor.get_text()`. Note: Users probably shouldn't deal with this class directly; instead, use `:func:`remove.remove_html_tags()`. \"\"\" def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = [] def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data ) def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip () __init__ () \u00a4 Initialize the parser. Source code in lexos\\scrubber\\resources.py 23 24 25 26 def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = [] get_text ( sep = '' ) \u00a4 Return the collected text. Source code in lexos\\scrubber\\resources.py 32 33 34 def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip () handle_data ( data ) \u00a4 Handle data elements. Source code in lexos\\scrubber\\resources.py 28 29 30 def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data ) lexos . scrubber . resources . _get_punct_translation_table () cached \u00a4 Get the punctuation translation table. Source code in lexos\\scrubber\\resources.py 170 171 172 173 174 175 176 177 178 179 180 @functools . lru_cache ( maxsize = None ) def _get_punct_translation_table (): \"\"\"Get the punctuation translation table.\"\"\" return dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) ), \" \" , ) lexos . scrubber . resources . __getattr__ ( name ) \u00a4 Call an attribute lookup from a table. Source code in lexos\\scrubber\\resources.py 184 185 186 187 188 189 def __getattr__ ( name : str ) -> Any : \"\"\"Call an attribute lookup from a table.\"\"\" if name == \"PUNCT_TRANSLATION_TABLE\" : return _get_punct_translation_table () else : raise AttributeError ( f \"module { __name__ !r} has no attribute { name !r} \" ) Constants \u00a4 There are also a number of constants: QUOTE_TRANSLATION_TABLE RE_BRACKETS_CURLY RE_BRACKETS_ROUND RE_BRACKETS_SQUARE RE_BULLET_POINTS RE_CURRENCY_SYMBOL RE_EMAIL RE_EMOJI RE_HASHTAG RE_HYPHENATED_WORD RE_LINEBREAK RE_NONBREAKING_SPACE RE_NUMBER RE_PHONE_NUMBER RE_SHORT_URL RE_TAB RE_URL RE_USER_HANDLE RE_ZWSP","title":"Resources"},{"location":"api/scrubber/resources/#resources","text":"The resources component of Scrubber contains a set of functions for replacing strings and patterns in text.","title":"Resources"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor","text":"Bases: html . parser . HTMLParser Simple subclass of :class: html.parser.HTMLParser . Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via HTMLTextExtractor.get_text() . Note Users probably shouldn't deal with this class directly; instead, use :func: remove.remove_html_tags()`. Source code in lexos\\scrubber\\resources.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class HTMLTextExtractor ( html . parser . HTMLParser ): \"\"\"Simple subclass of :class:`html.parser.HTMLParser`. Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via `HTMLTextExtractor.get_text()`. Note: Users probably shouldn't deal with this class directly; instead, use `:func:`remove.remove_html_tags()`. \"\"\" def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = [] def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data ) def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip ()","title":"HTMLTextExtractor"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.__init__","text":"Initialize the parser. Source code in lexos\\scrubber\\resources.py 23 24 25 26 def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = []","title":"__init__()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.get_text","text":"Return the collected text. Source code in lexos\\scrubber\\resources.py 32 33 34 def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip ()","title":"get_text()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.handle_data","text":"Handle data elements. Source code in lexos\\scrubber\\resources.py 28 29 30 def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data )","title":"handle_data()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources._get_punct_translation_table","text":"Get the punctuation translation table. Source code in lexos\\scrubber\\resources.py 170 171 172 173 174 175 176 177 178 179 180 @functools . lru_cache ( maxsize = None ) def _get_punct_translation_table (): \"\"\"Get the punctuation translation table.\"\"\" return dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) ), \" \" , )","title":"_get_punct_translation_table()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.__getattr__","text":"Call an attribute lookup from a table. Source code in lexos\\scrubber\\resources.py 184 185 186 187 188 189 def __getattr__ ( name : str ) -> Any : \"\"\"Call an attribute lookup from a table.\"\"\" if name == \"PUNCT_TRANSLATION_TABLE\" : return _get_punct_translation_table () else : raise AttributeError ( f \"module { __name__ !r} has no attribute { name !r} \" )","title":"__getattr__()"},{"location":"api/scrubber/resources/#constants","text":"There are also a number of constants: QUOTE_TRANSLATION_TABLE RE_BRACKETS_CURLY RE_BRACKETS_ROUND RE_BRACKETS_SQUARE RE_BULLET_POINTS RE_CURRENCY_SYMBOL RE_EMAIL RE_EMOJI RE_HASHTAG RE_HYPHENATED_WORD RE_LINEBREAK RE_NONBREAKING_SPACE RE_NUMBER RE_PHONE_NUMBER RE_SHORT_URL RE_TAB RE_URL RE_USER_HANDLE RE_ZWSP","title":"Constants"},{"location":"api/scrubber/scrubber/","text":"Scrubber \u00a4 The scrubber component of Scrubber contains a class for managing scrubbing pipelines. lexos.scrubber.scrubber.Scrubber \u00a4 Scrubber class. Sample usage scrubber = Scrubber() scrubber.to_lower(doc) Source code in lexos\\scrubber\\scrubber.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class Scrubber : \"\"\"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) \"\"\" def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline ) def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts __init__ () \u00a4 Initialize the Scrubber class. Source code in lexos\\scrubber\\scrubber.py 22 23 24 25 def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None add_pipeline ( * funcs ) \u00a4 Add a pipeline. Parameters: Name Type Description Default *funcs Callable [[ str ], str ] The functions to add to the pipeline. () Source code in lexos\\scrubber\\scrubber.py 27 28 29 30 31 32 33 def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) get_pipeline () \u00a4 Return a tuple representation of the pipeline. Source code in lexos\\scrubber\\scrubber.py 35 36 37 38 39 40 41 42 43 def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) scrub ( data ) \u00a4 Scrub a text or list of texts. Parameters: Name Type Description Default data Union [ List [ str ], str ] The text or list of texts to scrub. required Returns: Name Type Description list List [ str ] A list of scrubbed texts. Source code in lexos\\scrubber\\scrubber.py 65 66 67 68 69 70 71 72 73 74 75 76 def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts set_pipeline ( pipeline ) \u00a4 Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of get_pipeline() , rather than passing callables. Parameters: Name Type Description Default pipeline tuple A tuple of functions. required Source code in lexos\\scrubber\\scrubber.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline )","title":"Scrubber"},{"location":"api/scrubber/scrubber/#scrubber","text":"The scrubber component of Scrubber contains a class for managing scrubbing pipelines.","title":"Scrubber"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber","text":"Scrubber class. Sample usage scrubber = Scrubber() scrubber.to_lower(doc) Source code in lexos\\scrubber\\scrubber.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class Scrubber : \"\"\"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) \"\"\" def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline ) def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts","title":"Scrubber"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.__init__","text":"Initialize the Scrubber class. Source code in lexos\\scrubber\\scrubber.py 22 23 24 25 def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None","title":"__init__()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.add_pipeline","text":"Add a pipeline. Parameters: Name Type Description Default *funcs Callable [[ str ], str ] The functions to add to the pipeline. () Source code in lexos\\scrubber\\scrubber.py 27 28 29 30 31 32 33 def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs )","title":"add_pipeline()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.get_pipeline","text":"Return a tuple representation of the pipeline. Source code in lexos\\scrubber\\scrubber.py 35 36 37 38 39 40 41 42 43 def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline )","title":"get_pipeline()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.scrub","text":"Scrub a text or list of texts. Parameters: Name Type Description Default data Union [ List [ str ], str ] The text or list of texts to scrub. required Returns: Name Type Description list List [ str ] A list of scrubbed texts. Source code in lexos\\scrubber\\scrubber.py 65 66 67 68 69 70 71 72 73 74 75 76 def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts","title":"scrub()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.set_pipeline","text":"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of get_pipeline() , rather than passing callables. Parameters: Name Type Description Default pipeline tuple A tuple of functions. required Source code in lexos\\scrubber\\scrubber.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline )","title":"set_pipeline()"},{"location":"api/scrubber/utils/","text":"Utils \u00a4 The utils component of Scrubber contains helper functions shared by the other components. lexos . scrubber . utils . get_tags ( text ) \u00a4 Get information about the tags in a text. Parameters: Name Type Description Default text str The text to be analyzed. required Returns: Name Type Description dict dict A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names dict in the data and \"attributes\" is a list of dicts containing the attributes and values dict for those tags that have attributes. Note The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. Source code in lexos\\scrubber\\utils.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def get_tags ( text : str ) -> dict : \"\"\"Get information about the tags in a text. Args: text (str): The text to be analyzed. Returns: dict: A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. Note: The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. \"\"\" import json import re from xml.etree import ElementTree from natsort import humansorted tags = [] attributes = [] try : root = ElementTree . fromstring ( text ) for element in root . iter (): if re . sub ( \"{.+}\" , \"\" , element . tag ) not in tags : tags . append ( re . sub ( \"{.+}\" , \"\" , element . tag )) if element . attrib != {}: attributes . append ({ re . sub ( \"{.+}\" , \"\" , element . tag ): element . attrib }) tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) except ElementTree . ParseError : import bs4 from bs4 import BeautifulSoup soup = BeautifulSoup ( text , \"xml\" ) for e in soup : if isinstance ( e , bs4 . element . ProcessingInstruction ): e . extract () [ tags . append ( tag . name ) for tag in soup . find_all ()] [ attributes . append ({ tag . name : tag . attrs }) for tag in soup . find_all ()] tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) return { \"tags\" : tags , \"attributes\" : attributes }","title":"Utils"},{"location":"api/scrubber/utils/#utils","text":"The utils component of Scrubber contains helper functions shared by the other components.","title":"Utils"},{"location":"api/scrubber/utils/#lexos.scrubber.utils.get_tags","text":"Get information about the tags in a text. Parameters: Name Type Description Default text str The text to be analyzed. required Returns: Name Type Description dict dict A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names dict in the data and \"attributes\" is a list of dicts containing the attributes and values dict for those tags that have attributes. Note The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. Source code in lexos\\scrubber\\utils.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def get_tags ( text : str ) -> dict : \"\"\"Get information about the tags in a text. Args: text (str): The text to be analyzed. Returns: dict: A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. Note: The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. \"\"\" import json import re from xml.etree import ElementTree from natsort import humansorted tags = [] attributes = [] try : root = ElementTree . fromstring ( text ) for element in root . iter (): if re . sub ( \"{.+}\" , \"\" , element . tag ) not in tags : tags . append ( re . sub ( \"{.+}\" , \"\" , element . tag )) if element . attrib != {}: attributes . append ({ re . sub ( \"{.+}\" , \"\" , element . tag ): element . attrib }) tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) except ElementTree . ParseError : import bs4 from bs4 import BeautifulSoup soup = BeautifulSoup ( text , \"xml\" ) for e in soup : if isinstance ( e , bs4 . element . ProcessingInstruction ): e . extract () [ tags . append ( tag . name ) for tag in soup . find_all ()] [ attributes . append ({ tag . name : tag . attrs }) for tag in soup . find_all ()] tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) return { \"tags\" : tags , \"attributes\" : attributes }","title":"get_tags()"},{"location":"api/tokenizer/","text":"Tokenizer \u00a4 The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used. lexos . tokenizer . _add_remove_stopwords ( nlp , add_stopwords , remove_stopwords ) \u00a4 Add and remove stopwords from the model. Parameters: Name Type Description Default nlp spacy . Vocab The model to add stopwords to. required add_stopwords Union [ List [ str ], str ] A list of stopwords to add to the model. required remove_stopwords Union [ bool , List [ str ], str ] A list of stopwords to remove from the model, or True to remove all stopwords. required Returns: Type Description spacy . Vocab spacy.Vocab: The model with stopwords added and removed. Source code in lexos\\tokenizer\\__init__.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def _add_remove_stopwords ( nlp : spacy . Vocab , add_stopwords : Union [ List [ str ], str ], remove_stopwords : Union [ bool , List [ str ], str ], ) -> spacy . Vocab : \"\"\"Add and remove stopwords from the model. Args: nlp (spacy.Vocab): The model to add stopwords to. add_stopwords (Union[List[str], str]): A list of stopwords to add to the model. remove_stopwords (Union[bool, List[str], str]): A list of stopwords to remove from the model, or `True` to remove all stopwords. Returns: spacy.Vocab: The model with stopwords added and removed. \"\"\" if add_stopwords : if not isinstance ( add_stopwords , list ): add_stopwords = [ add_stopwords ] for term in add_stopwords : nlp . vocab [ term ] . is_stop = True if remove_stopwords : if remove_stopwords == True : for term in nlp . vocab : if term . is_stop : term . is_stop = False else : if not isinstance ( remove_stopwords , list ): remove_stopwords = [ remove_stopwords ] for term in remove_stopwords : nlp . vocab [ term ] . is_stop = False return nlp lexos . tokenizer . _get_disabled_components ( disable = None , pipeline_components = None ) \u00a4 Get a list of components to disable in the pipeline. Source code in lexos\\tokenizer\\__init__.py 72 73 74 75 76 77 78 79 80 81 82 83 def _get_disabled_components ( disable : List [ str ] = None , pipeline_components : dict = None ) -> List [ str ]: \"\"\"Get a list of components to disable in the pipeline.\"\"\" if disable is None : disable = [] custom_disable = [] if \"disable\" in pipeline_components : for component in pipeline_components [ \"disable\" ]: custom_disable . append ( component ) disable . extend ( custom_disable ) return list ( set ( disable )) lexos . tokenizer . _get_excluded_components ( exclude = None , pipeline_components = None ) \u00a4 Get a list of components to exclude from the pipeline. Source code in lexos\\tokenizer\\__init__.py 58 59 60 61 62 63 64 65 66 67 68 69 def _get_excluded_components ( exclude : List [ str ] = None , pipeline_components : dict = None ) -> List [ str ]: \"\"\"Get a list of components to exclude from the pipeline.\"\"\" if exclude is None : exclude = [] custom_exclude = [] if \"exclude\" in pipeline_components : for component in pipeline_components [ \"exclude\" ]: custom_exclude . append ( component ) exclude . extend ( custom_exclude ) return list ( set ( exclude )) lexos . tokenizer . _load_model ( model , disable = None , exclude = None ) \u00a4 Load a model from a file. Parameters: Name Type Description Default model str The path to the model. required disable List [ str ] A list of spaCy pipeline components to disable. None exclude List [ str ] A list of spaCy pipeline components to exclude. None Returns: Name Type Description object object The loaded model. Note Attempts to disable or exclude components not found in the pipeline are ignored without raising an error. Source code in lexos\\tokenizer\\__init__.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def _load_model ( model : str , disable : List [ str ] = None , exclude : List [ str ] = None ) -> object : \"\"\"Load a model from a file. Args: model (str): The path to the model. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. Returns: object: The loaded model. Note: Attempts to disable or exclude components not found in the pipeline are ignored without raising an error. \"\"\" try : return spacy . load ( model , disable = disable , exclude = exclude ) except Exception : raise LexosException ( f \"Error loading model { model } . Please check the name and try again. You may need to install the model on your system.\" ) lexos . tokenizer . _validate_input ( input ) \u00a4 Ensure that input is a string, Doc, or bytes. Parameters: Name Type Description Default input Any The input to be tested. required Returns: Type Description None None Raises: Type Description LexosException ( Exception ) Raise an error if the input is not valid. Source code in lexos\\tokenizer\\__init__.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def _validate_input ( input : Any ) -> None : \"\"\"Ensure that input is a string, Doc, or bytes. Args: input (Any): The input to be tested. Returns: None Raises: LexosException (Exception): Raise an error if the input is not valid. \"\"\" if not isinstance ( input , list ): input = [ input ] for item in input : if isinstance ( item , ( str , spacy . tokens . doc . Doc , bytes )): return True else : message = f \"Error reading { item } . { LANG [ 'format_error' ] } { str ( type ( item )) } \" raise LexosException ( message ) lexos . tokenizer . doc_from_ngrams ( ngrams , model = 'xx_sent_ud_sm' , strict = False , disable = [], exclude = []) \u00a4 Generate spaCy doc from a list of ngrams. Parameters: Name Type Description Default ngrams list A list of ngrams. required model object The language model to use for tokenisation. 'xx_sent_ud_sm' strict bool Whether to preserve token divisions, include whitespace in the source. False disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] Returns: Name Type Description object object A spaCy doc Notes The strict=False setting will allow spaCy's language model to remove whitespace from ngrams and split punctuation into separate tokens. strict=True will preserve the sequences in the source list. Source code in lexos\\tokenizer\\__init__.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def doc_from_ngrams ( ngrams : list , model = \"xx_sent_ud_sm\" , strict : bool = False , disable : List [ str ] = [], exclude : List [ str ] = [], ) -> object : \"\"\"Generate spaCy doc from a list of ngrams. Args: ngrams (list): A list of ngrams. model (object): The language model to use for tokenisation. strict (bool): Whether to preserve token divisions, include whitespace in the source. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. Returns: object: A spaCy doc Notes: The `strict=False` setting will allow spaCy's language model to remove whitespace from ngrams and split punctuation into separate tokens. `strict=True` will preserve the sequences in the source list. \"\"\" nlp = _load_model ( model , disable = disable , exclude = exclude ) if strict : spaces = [ False for token in ngrams if token != \"\" ] doc = spacy . tokens . doc . Doc ( nlp . vocab , words = ngrams , spaces = spaces ) # Run the standard pipeline against the doc for _ , proc in nlp . pipeline : doc = proc ( doc ) return doc else : text = \" \" . join ([ x . replace ( \" \" , \"\" ) for x in ngrams ]) return nlp ( text ) lexos . tokenizer . docs_from_ngrams ( ngrams , model = 'xx_sent_ud_sm' , strict = False , disable = [], exclude = []) \u00a4 Generate spaCy doc from a list of ngram lists. Parameters: Name Type Description Default ngrams List [ list ] A list of ngram lists. required model object The language model to use for tokenisation. 'xx_sent_ud_sm' strict bool Whether to preserve token divisions, include whitespace in the source. False disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] Returns: Type Description List [ object ] List[object]: A list of spaCy docs Source code in lexos\\tokenizer\\__init__.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def docs_from_ngrams ( ngrams : List [ list ], model = \"xx_sent_ud_sm\" , strict = False , disable : List [ str ] = [], exclude : List [ str ] = [], ) -> List [ object ]: \"\"\"Generate spaCy doc from a list of ngram lists. Args: ngrams (List[list]): A list of ngram lists. model (object): The language model to use for tokenisation. strict (bool): Whether to preserve token divisions, include whitespace in the source. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. Returns: List[object]: A list of spaCy docs \"\"\" docs = [] for ngram_list in ngrams : doc = doc_from_ngrams ( ngram_list , model , strict , disable = disable , exclude = exclude ) docs . append ( doc ) return docs lexos . tokenizer . generate_character_ngrams ( text , size = 1 , drop_whitespace = True ) \u00a4 Generate character n-grams from raw text. Parameters: Name Type Description Default text str The source text. required size int The size of the ngram. 1 drop_whitespace bool Whether to preserve whitespace in the ngram list. True Returns: Type Description List [ str ] List[str]: A list of ngrams Source code in lexos\\tokenizer\\__init__.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def generate_character_ngrams ( text : str , size : int = 1 , drop_whitespace : bool = True ) -> List [ str ]: \"\"\"Generate character n-grams from raw text. Args: text (str): The source text. size (int): The size of the ngram. drop_whitespace (bool): Whether to preserve whitespace in the ngram list. Returns: List[str]: A list of ngrams \"\"\" from textwrap import wrap return wrap ( text , size , drop_whitespace = drop_whitespace ) lexos . tokenizer . make_doc ( text , model = 'xx_sent_ud_sm' , max_length = 2000000 , disable = [], exclude = [], add_stopwords = [], remove_stopwords = [], pipeline_components = []) \u00a4 Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' max_length int The maximum length of the doc. 2000000 disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] add_stopwords Union [ List [ str ], str ] A list of stop words to add to the model. [] remove_stopwords Union [ bool , List [ str ], str ] A list of stop words to remove from the model. If True is specified, all stop words will be removed. [] pipeline_components List [ dict ] A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. [] Returns: Name Type Description object object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , max_length : int = 2000000 , disable : List [ str ] = [], exclude : List [ str ] = [], add_stopwords : Union [ List [ str ], str ] = [], remove_stopwords : Union [ bool , List [ str ], str ] = [], pipeline_components : List [ dict ] = [], ) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. max_length (int): The maximum length of the doc. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. add_stopwords (Union[List[str], str]): A list of stop words to add to the model. remove_stopwords (Union[bool, List[str], str]): A list of stop words to remove from the model. If `True` is specified, all stop words will be removed. pipeline_components (List[dict]): A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. Returns: object: A spaCy doc object. \"\"\" _validate_input ( text ) disable = _get_disabled_components ( disable , pipeline_components ) exclude = _get_excluded_components ( exclude , pipeline_components ) nlp = _load_model ( model , disable = disable , exclude = exclude ) nlp . max_length = max_length _add_remove_stopwords ( nlp , add_stopwords , remove_stopwords ) if pipeline_components and \"custom\" in pipeline_components : for component in pipeline_components [ \"custom\" ]: nlp . add_pipe ( ** component ) return nlp ( text ) lexos . tokenizer . make_docs ( texts , model = 'xx_sent_ud_sm' , max_length = 2000000 , disable = [], exclude = [], add_stopwords = [], remove_stopwords = [], pipeline_components = []) \u00a4 Return a list of docs from a text or list of texts. Parameters: Name Type Description Default texts Union [ List [ str ], str ] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' max_length int The maximum length of the doc. 2000000 disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] add_stopwords Union [ List [ str ], str ] A list of stop words to add to the model. [] remove_stopwords Union [ bool , List [ str ], str ] A list of stop words to remove from the model. If True is specified, all stop words will be removed. [] pipeline_components List [ dict ] A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. [] Returns: Type Description List [ object ] List[object]: A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , max_length : int = 2000000 , disable : List [ str ] = [], exclude : List [ str ] = [], add_stopwords : Union [ List [ str ], str ] = [], remove_stopwords : Union [ bool , List [ str ], str ] = [], pipeline_components : List [ dict ] = [], ) -> List [ object ]: \"\"\"Return a list of docs from a text or list of texts. Args: texts (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. max_length (int): The maximum length of the doc. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. add_stopwords (Union[List[str], str]): A list of stop words to add to the model. remove_stopwords (Union[bool, List[str], str]): A list of stop words to remove from the model. If `True` is specified, all stop words will be removed. pipeline_components (List[dict]): A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. Returns: List[object]: A list of spaCy doc objects. \"\"\" if _validate_input ( texts ): disable = _get_disabled_components ( disable , pipeline_components ) exclude = _get_excluded_components ( exclude , pipeline_components ) nlp = _load_model ( model , disable = disable , exclude = exclude ) nlp . max_length = max_length _add_remove_stopwords ( nlp , add_stopwords , remove_stopwords ) if pipeline_components and \"custom\" in pipeline_components : for component in pipeline_components [ \"custom\" ]: nlp . add_pipe ( ** component ) return list ( nlp . pipe ( utils . ensure_list ( texts ))) lexos . tokenizer . ngrams_from_doc ( doc , size = 2 ) \u00a4 Generate a list of ngrams from a spaCy doc. A wrapper for textacy.extract.basics.ngrams . With basic functionality. Further functionality can be accessed by calling textacy directly. Parameters: Name Type Description Default doc object A spaCy doc required size int The size of the ngrams. 2 Returns: Type Description List [ str ] List[str]: A list of ngrams. Source code in lexos\\tokenizer\\__init__.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def ngrams_from_doc ( doc : object , size : int = 2 ) -> List [ str ]: \"\"\"Generate a list of ngrams from a spaCy doc. A wrapper for `textacy.extract.basics.ngrams`. With basic functionality. Further functionality can be accessed by calling `textacy` directly. Args: doc (object): A spaCy doc size (int): The size of the ngrams. Returns: List[str]: A list of ngrams. \"\"\" from textacy.extract.basics import ngrams as textacy_ngrams if size < 1 : raise LexosException ( \"The ngram size must be greater than 0.\" ) ngrams = list ( textacy_ngrams ( doc , size , min_freq = 1 )) # Ensure quoted strings are returned return [ token . text for token in ngrams ]","title":"Tokenizer"},{"location":"api/tokenizer/#tokenizer","text":"The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used.","title":"Tokenizer"},{"location":"api/tokenizer/#lexos.tokenizer._add_remove_stopwords","text":"Add and remove stopwords from the model. Parameters: Name Type Description Default nlp spacy . Vocab The model to add stopwords to. required add_stopwords Union [ List [ str ], str ] A list of stopwords to add to the model. required remove_stopwords Union [ bool , List [ str ], str ] A list of stopwords to remove from the model, or True to remove all stopwords. required Returns: Type Description spacy . Vocab spacy.Vocab: The model with stopwords added and removed. Source code in lexos\\tokenizer\\__init__.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def _add_remove_stopwords ( nlp : spacy . Vocab , add_stopwords : Union [ List [ str ], str ], remove_stopwords : Union [ bool , List [ str ], str ], ) -> spacy . Vocab : \"\"\"Add and remove stopwords from the model. Args: nlp (spacy.Vocab): The model to add stopwords to. add_stopwords (Union[List[str], str]): A list of stopwords to add to the model. remove_stopwords (Union[bool, List[str], str]): A list of stopwords to remove from the model, or `True` to remove all stopwords. Returns: spacy.Vocab: The model with stopwords added and removed. \"\"\" if add_stopwords : if not isinstance ( add_stopwords , list ): add_stopwords = [ add_stopwords ] for term in add_stopwords : nlp . vocab [ term ] . is_stop = True if remove_stopwords : if remove_stopwords == True : for term in nlp . vocab : if term . is_stop : term . is_stop = False else : if not isinstance ( remove_stopwords , list ): remove_stopwords = [ remove_stopwords ] for term in remove_stopwords : nlp . vocab [ term ] . is_stop = False return nlp","title":"_add_remove_stopwords()"},{"location":"api/tokenizer/#lexos.tokenizer._get_disabled_components","text":"Get a list of components to disable in the pipeline. Source code in lexos\\tokenizer\\__init__.py 72 73 74 75 76 77 78 79 80 81 82 83 def _get_disabled_components ( disable : List [ str ] = None , pipeline_components : dict = None ) -> List [ str ]: \"\"\"Get a list of components to disable in the pipeline.\"\"\" if disable is None : disable = [] custom_disable = [] if \"disable\" in pipeline_components : for component in pipeline_components [ \"disable\" ]: custom_disable . append ( component ) disable . extend ( custom_disable ) return list ( set ( disable ))","title":"_get_disabled_components()"},{"location":"api/tokenizer/#lexos.tokenizer._get_excluded_components","text":"Get a list of components to exclude from the pipeline. Source code in lexos\\tokenizer\\__init__.py 58 59 60 61 62 63 64 65 66 67 68 69 def _get_excluded_components ( exclude : List [ str ] = None , pipeline_components : dict = None ) -> List [ str ]: \"\"\"Get a list of components to exclude from the pipeline.\"\"\" if exclude is None : exclude = [] custom_exclude = [] if \"exclude\" in pipeline_components : for component in pipeline_components [ \"exclude\" ]: custom_exclude . append ( component ) exclude . extend ( custom_exclude ) return list ( set ( exclude ))","title":"_get_excluded_components()"},{"location":"api/tokenizer/#lexos.tokenizer._load_model","text":"Load a model from a file. Parameters: Name Type Description Default model str The path to the model. required disable List [ str ] A list of spaCy pipeline components to disable. None exclude List [ str ] A list of spaCy pipeline components to exclude. None Returns: Name Type Description object object The loaded model. Note Attempts to disable or exclude components not found in the pipeline are ignored without raising an error. Source code in lexos\\tokenizer\\__init__.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def _load_model ( model : str , disable : List [ str ] = None , exclude : List [ str ] = None ) -> object : \"\"\"Load a model from a file. Args: model (str): The path to the model. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. Returns: object: The loaded model. Note: Attempts to disable or exclude components not found in the pipeline are ignored without raising an error. \"\"\" try : return spacy . load ( model , disable = disable , exclude = exclude ) except Exception : raise LexosException ( f \"Error loading model { model } . Please check the name and try again. You may need to install the model on your system.\" )","title":"_load_model()"},{"location":"api/tokenizer/#lexos.tokenizer._validate_input","text":"Ensure that input is a string, Doc, or bytes. Parameters: Name Type Description Default input Any The input to be tested. required Returns: Type Description None None Raises: Type Description LexosException ( Exception ) Raise an error if the input is not valid. Source code in lexos\\tokenizer\\__init__.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def _validate_input ( input : Any ) -> None : \"\"\"Ensure that input is a string, Doc, or bytes. Args: input (Any): The input to be tested. Returns: None Raises: LexosException (Exception): Raise an error if the input is not valid. \"\"\" if not isinstance ( input , list ): input = [ input ] for item in input : if isinstance ( item , ( str , spacy . tokens . doc . Doc , bytes )): return True else : message = f \"Error reading { item } . { LANG [ 'format_error' ] } { str ( type ( item )) } \" raise LexosException ( message )","title":"_validate_input()"},{"location":"api/tokenizer/#lexos.tokenizer.doc_from_ngrams","text":"Generate spaCy doc from a list of ngrams. Parameters: Name Type Description Default ngrams list A list of ngrams. required model object The language model to use for tokenisation. 'xx_sent_ud_sm' strict bool Whether to preserve token divisions, include whitespace in the source. False disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] Returns: Name Type Description object object A spaCy doc Notes The strict=False setting will allow spaCy's language model to remove whitespace from ngrams and split punctuation into separate tokens. strict=True will preserve the sequences in the source list. Source code in lexos\\tokenizer\\__init__.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def doc_from_ngrams ( ngrams : list , model = \"xx_sent_ud_sm\" , strict : bool = False , disable : List [ str ] = [], exclude : List [ str ] = [], ) -> object : \"\"\"Generate spaCy doc from a list of ngrams. Args: ngrams (list): A list of ngrams. model (object): The language model to use for tokenisation. strict (bool): Whether to preserve token divisions, include whitespace in the source. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. Returns: object: A spaCy doc Notes: The `strict=False` setting will allow spaCy's language model to remove whitespace from ngrams and split punctuation into separate tokens. `strict=True` will preserve the sequences in the source list. \"\"\" nlp = _load_model ( model , disable = disable , exclude = exclude ) if strict : spaces = [ False for token in ngrams if token != \"\" ] doc = spacy . tokens . doc . Doc ( nlp . vocab , words = ngrams , spaces = spaces ) # Run the standard pipeline against the doc for _ , proc in nlp . pipeline : doc = proc ( doc ) return doc else : text = \" \" . join ([ x . replace ( \" \" , \"\" ) for x in ngrams ]) return nlp ( text )","title":"doc_from_ngrams()"},{"location":"api/tokenizer/#lexos.tokenizer.docs_from_ngrams","text":"Generate spaCy doc from a list of ngram lists. Parameters: Name Type Description Default ngrams List [ list ] A list of ngram lists. required model object The language model to use for tokenisation. 'xx_sent_ud_sm' strict bool Whether to preserve token divisions, include whitespace in the source. False disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] Returns: Type Description List [ object ] List[object]: A list of spaCy docs Source code in lexos\\tokenizer\\__init__.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def docs_from_ngrams ( ngrams : List [ list ], model = \"xx_sent_ud_sm\" , strict = False , disable : List [ str ] = [], exclude : List [ str ] = [], ) -> List [ object ]: \"\"\"Generate spaCy doc from a list of ngram lists. Args: ngrams (List[list]): A list of ngram lists. model (object): The language model to use for tokenisation. strict (bool): Whether to preserve token divisions, include whitespace in the source. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. Returns: List[object]: A list of spaCy docs \"\"\" docs = [] for ngram_list in ngrams : doc = doc_from_ngrams ( ngram_list , model , strict , disable = disable , exclude = exclude ) docs . append ( doc ) return docs","title":"docs_from_ngrams()"},{"location":"api/tokenizer/#lexos.tokenizer.generate_character_ngrams","text":"Generate character n-grams from raw text. Parameters: Name Type Description Default text str The source text. required size int The size of the ngram. 1 drop_whitespace bool Whether to preserve whitespace in the ngram list. True Returns: Type Description List [ str ] List[str]: A list of ngrams Source code in lexos\\tokenizer\\__init__.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def generate_character_ngrams ( text : str , size : int = 1 , drop_whitespace : bool = True ) -> List [ str ]: \"\"\"Generate character n-grams from raw text. Args: text (str): The source text. size (int): The size of the ngram. drop_whitespace (bool): Whether to preserve whitespace in the ngram list. Returns: List[str]: A list of ngrams \"\"\" from textwrap import wrap return wrap ( text , size , drop_whitespace = drop_whitespace )","title":"generate_character_ngrams()"},{"location":"api/tokenizer/#lexos.tokenizer.make_doc","text":"Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' max_length int The maximum length of the doc. 2000000 disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] add_stopwords Union [ List [ str ], str ] A list of stop words to add to the model. [] remove_stopwords Union [ bool , List [ str ], str ] A list of stop words to remove from the model. If True is specified, all stop words will be removed. [] pipeline_components List [ dict ] A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. [] Returns: Name Type Description object object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , max_length : int = 2000000 , disable : List [ str ] = [], exclude : List [ str ] = [], add_stopwords : Union [ List [ str ], str ] = [], remove_stopwords : Union [ bool , List [ str ], str ] = [], pipeline_components : List [ dict ] = [], ) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. max_length (int): The maximum length of the doc. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. add_stopwords (Union[List[str], str]): A list of stop words to add to the model. remove_stopwords (Union[bool, List[str], str]): A list of stop words to remove from the model. If `True` is specified, all stop words will be removed. pipeline_components (List[dict]): A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. Returns: object: A spaCy doc object. \"\"\" _validate_input ( text ) disable = _get_disabled_components ( disable , pipeline_components ) exclude = _get_excluded_components ( exclude , pipeline_components ) nlp = _load_model ( model , disable = disable , exclude = exclude ) nlp . max_length = max_length _add_remove_stopwords ( nlp , add_stopwords , remove_stopwords ) if pipeline_components and \"custom\" in pipeline_components : for component in pipeline_components [ \"custom\" ]: nlp . add_pipe ( ** component ) return nlp ( text )","title":"make_doc()"},{"location":"api/tokenizer/#lexos.tokenizer.make_docs","text":"Return a list of docs from a text or list of texts. Parameters: Name Type Description Default texts Union [ List [ str ], str ] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' max_length int The maximum length of the doc. 2000000 disable List [ str ] A list of spaCy pipeline components to disable. [] exclude List [ str ] A list of spaCy pipeline components to exclude. [] add_stopwords Union [ List [ str ], str ] A list of stop words to add to the model. [] remove_stopwords Union [ bool , List [ str ], str ] A list of stop words to remove from the model. If True is specified, all stop words will be removed. [] pipeline_components List [ dict ] A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. [] Returns: Type Description List [ object ] List[object]: A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , max_length : int = 2000000 , disable : List [ str ] = [], exclude : List [ str ] = [], add_stopwords : Union [ List [ str ], str ] = [], remove_stopwords : Union [ bool , List [ str ], str ] = [], pipeline_components : List [ dict ] = [], ) -> List [ object ]: \"\"\"Return a list of docs from a text or list of texts. Args: texts (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. max_length (int): The maximum length of the doc. disable (List[str]): A list of spaCy pipeline components to disable. exclude (List[str]): A list of spaCy pipeline components to exclude. add_stopwords (Union[List[str], str]): A list of stop words to add to the model. remove_stopwords (Union[bool, List[str], str]): A list of stop words to remove from the model. If `True` is specified, all stop words will be removed. pipeline_components (List[dict]): A list custom component dicts to add to the pipeline. See https://spacy.io/api/language/#add_pipe for more information. Returns: List[object]: A list of spaCy doc objects. \"\"\" if _validate_input ( texts ): disable = _get_disabled_components ( disable , pipeline_components ) exclude = _get_excluded_components ( exclude , pipeline_components ) nlp = _load_model ( model , disable = disable , exclude = exclude ) nlp . max_length = max_length _add_remove_stopwords ( nlp , add_stopwords , remove_stopwords ) if pipeline_components and \"custom\" in pipeline_components : for component in pipeline_components [ \"custom\" ]: nlp . add_pipe ( ** component ) return list ( nlp . pipe ( utils . ensure_list ( texts )))","title":"make_docs()"},{"location":"api/tokenizer/#lexos.tokenizer.ngrams_from_doc","text":"Generate a list of ngrams from a spaCy doc. A wrapper for textacy.extract.basics.ngrams . With basic functionality. Further functionality can be accessed by calling textacy directly. Parameters: Name Type Description Default doc object A spaCy doc required size int The size of the ngrams. 2 Returns: Type Description List [ str ] List[str]: A list of ngrams. Source code in lexos\\tokenizer\\__init__.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def ngrams_from_doc ( doc : object , size : int = 2 ) -> List [ str ]: \"\"\"Generate a list of ngrams from a spaCy doc. A wrapper for `textacy.extract.basics.ngrams`. With basic functionality. Further functionality can be accessed by calling `textacy` directly. Args: doc (object): A spaCy doc size (int): The size of the ngrams. Returns: List[str]: A list of ngrams. \"\"\" from textacy.extract.basics import ngrams as textacy_ngrams if size < 1 : raise LexosException ( \"The ngram size must be greater than 0.\" ) ngrams = list ( textacy_ngrams ( doc , size , min_freq = 1 )) # Ensure quoted strings are returned return [ token . text for token in ngrams ]","title":"ngrams_from_doc()"},{"location":"api/tokenizer/extensions/","text":"Extensions \u00a4 This is a set of extensions to spaCy docs allowing custom attributes and methods. Typically, they woudld be accessed with an underscore prefix like doc._.is_fruit or doc._.get(\"is_fruit\") . Extensions are set with code like fruits = [ \"apple\" , \"pear\" , \"banana\" , \"orange\" , \"strawberry\" ] is_fruit_getter = lambda token : token . text in fruits Token . set_extension ( \"is_fruit\" , getter = is_fruit_getter ) See the spaCy custom attributes documentation for full details. lexos . tokenizer . extensions . is_fruit_getter = lambda token : token . text in fruits module-attribute \u00a4 Note This is really a proof of concept function. A better example can be added in the future.","title":"Extensions"},{"location":"api/tokenizer/extensions/#extensions","text":"This is a set of extensions to spaCy docs allowing custom attributes and methods. Typically, they woudld be accessed with an underscore prefix like doc._.is_fruit or doc._.get(\"is_fruit\") . Extensions are set with code like fruits = [ \"apple\" , \"pear\" , \"banana\" , \"orange\" , \"strawberry\" ] is_fruit_getter = lambda token : token . text in fruits Token . set_extension ( \"is_fruit\" , getter = is_fruit_getter ) See the spaCy custom attributes documentation for full details.","title":"Extensions"},{"location":"api/tokenizer/extensions/#lexos.tokenizer.extensions.is_fruit_getter","text":"Note This is really a proof of concept function. A better example can be added in the future.","title":"is_fruit_getter"},{"location":"api/tokenizer/lexosdoc/","text":"LexosDoc \u00a4 A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. lexos.tokenizer.lexosdoc.LexosDoc \u00a4 A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. Source code in lexos\\tokenizer\\lexosdoc.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class LexosDoc : \"\"\"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called `tokenizer.py` and import them. If certain functions get used commonly, they can be turned into Doc extensions. \"\"\" def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" if isinstance ( doc , spacy . tokens . doc . Doc ): self . doc = doc else : raise LexosException ( \"The input must be a spaCy doc.\" ) def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False , ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self (object): A spaCy doc. limit (int): The maximum number of tokens to count. start (Any): The index of the first token to count. end (Any): The index of the last token to count after limit is applied. filters (List[Union[Dict[str, str], str]]): A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df (bool): Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : self . _validate_filters ( filters ) for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return self . _dataframe ( term_counts , columns ) else : return term_counts def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ( [ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]] ) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return self . _dataframe ( rows , cols ) def _bool_filter ( self , token : object , filters : List [ str ]) -> bool : \"\"\"Filter a token based on a list of boolean filters. Args: token (object): A spaCy token. filters (str): A list of boolean filters (the names of spaCy token attributes). Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != []: for filter in filters : if getattr ( token , filter ): return False else : return True else : return True def _dataframe ( self , rows : List [ dict ], columns : List [ str ]) -> pd . DataFrame : \"\"\"Return a pandas dataframe of the doc attributes. Args: rows (List[dict]): A list of dicts with the doc attributes. columns (List[str]): A list of column names. Returns: pd.DataFrame: A pandas dataframe of the doc attributes. Raises: LexosException: If a pandas exception occurs. \"\"\" try : return pd . DataFrame ( rows , columns = columns ) except Exception as e : raise LexosException ( e ) def _dict_filter ( self , token : object , filters : List [ Dict [ str , str ]], regex : bool = False ) -> bool : \"\"\"Filter a token based on a list of dictionary filters. Args: token (object): A spaCy token. filters (List[Dict[str, str]]): A list of filter dictionaries with keys as spaCy token attributes. regex (bool): Whether to match the dictionary value using regex. Returns: bool: Whether the token passes the filters. \"\"\" if not isinstance ( token , spacy . tokens . Token ): raise LexosException ( \"The input must be a spaCy token.\" ) if not isinstance ( regex , bool ): raise LexosException ( \"The regex flag must be a boolean.\" ) if filters and filters != {}: for filter , value in filters . items (): if ( regex and re . search ( re . compile ( value ), getattr ( token , filter )) is not None ): return False elif getattr ( token , filter ) == value : return False else : return True else : return True def _validate_filters ( self , filters : List [ str ]) -> None : \"\"\"Ensure that filters are in the correct format. Args: filters (Union[List[Dict[str, str]], List[str])): A list of filter dictionaries with keys or a list of boolean filters (the names of spaCy token attributes). Returns: None Raises: LexosException: If the format for the filter is not correct. \"\"\" message = \"The filter must be a list of filter dictionaries with keys or a list of boolean filters (the names of spaCy token attributes)\" if not isinstance ( filters , list ) or any ( not isinstance ( x , ( dict , str )) for x in filters ): raise LexosException ( message ) __init__ ( doc ) \u00a4 Initialize a LexosDoc object. Source code in lexos\\tokenizer\\lexosdoc.py 25 26 27 28 29 30 def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" if isinstance ( doc , spacy . tokens . doc . Doc ): self . doc = doc else : raise LexosException ( \"The input must be a spaCy doc.\" ) get_term_counts ( limit = None , start = 0 , end = None , filters = None , regex = False , normalize = False , normalize_with_filters = False , as_df = False ) \u00a4 Get a list of word counts for each token in the doc. Parameters: Name Type Description Default self object A spaCy doc. required limit int The maximum number of tokens to count. None start Any The index of the first token to count. 0 end Any The index of the last token to count after limit is applied. None filters List [ Union [ Dict [ str , str ], str ]] A list of Doc attributes to ignore. None regex bool Whether to match the dictionary value using regex. False normalize bool Whether to return raw counts or relative frequencies. False normalize_with_filters bool Whether to normalize based on the number of tokens after filters are applied. False as_df bool Whether to return a pandas dataframe. False Returns: Type Description Union [ List , pd . DataFrame ] Union[List, pd.DataFrame]: A list of word count tuples for Union [ List , pd . DataFrame ] each token in the doc. Alternatively, a pandas dataframe. Source code in lexos\\tokenizer\\lexosdoc.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False , ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self (object): A spaCy doc. limit (int): The maximum number of tokens to count. start (Any): The index of the first token to count. end (Any): The index of the last token to count after limit is applied. filters (List[Union[Dict[str, str], str]]): A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df (bool): Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : self . _validate_filters ( filters ) for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return self . _dataframe ( term_counts , columns ) else : return term_counts get_token_attrs () \u00a4 Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. Source code in lexos\\tokenizer\\lexosdoc.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ( [ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]] ) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } get_tokens () \u00a4 Return a list of tokens in the doc. Source code in lexos\\tokenizer\\lexosdoc.py 92 93 94 def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] to_dataframe ( cols = [ 'text' ], show_ranges = True ) \u00a4 Get a pandas dataframe of the doc attributes. Parameters: Name Type Description Default cols List [ str ] A list of columns to include in the dataframe. ['text'] show_ranges bool Whether to include the token start and end positions in the dataframe. True Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call LexosDoc.get_token_attrs() first to check which attributes are available for the doc. Source code in lexos\\tokenizer\\lexosdoc.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return self . _dataframe ( rows , cols )","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexosdoc","text":"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions.","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc","text":"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. Source code in lexos\\tokenizer\\lexosdoc.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 class LexosDoc : \"\"\"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called `tokenizer.py` and import them. If certain functions get used commonly, they can be turned into Doc extensions. \"\"\" def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" if isinstance ( doc , spacy . tokens . doc . Doc ): self . doc = doc else : raise LexosException ( \"The input must be a spaCy doc.\" ) def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False , ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self (object): A spaCy doc. limit (int): The maximum number of tokens to count. start (Any): The index of the first token to count. end (Any): The index of the last token to count after limit is applied. filters (List[Union[Dict[str, str], str]]): A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df (bool): Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : self . _validate_filters ( filters ) for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return self . _dataframe ( term_counts , columns ) else : return term_counts def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ( [ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]] ) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return self . _dataframe ( rows , cols ) def _bool_filter ( self , token : object , filters : List [ str ]) -> bool : \"\"\"Filter a token based on a list of boolean filters. Args: token (object): A spaCy token. filters (str): A list of boolean filters (the names of spaCy token attributes). Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != []: for filter in filters : if getattr ( token , filter ): return False else : return True else : return True def _dataframe ( self , rows : List [ dict ], columns : List [ str ]) -> pd . DataFrame : \"\"\"Return a pandas dataframe of the doc attributes. Args: rows (List[dict]): A list of dicts with the doc attributes. columns (List[str]): A list of column names. Returns: pd.DataFrame: A pandas dataframe of the doc attributes. Raises: LexosException: If a pandas exception occurs. \"\"\" try : return pd . DataFrame ( rows , columns = columns ) except Exception as e : raise LexosException ( e ) def _dict_filter ( self , token : object , filters : List [ Dict [ str , str ]], regex : bool = False ) -> bool : \"\"\"Filter a token based on a list of dictionary filters. Args: token (object): A spaCy token. filters (List[Dict[str, str]]): A list of filter dictionaries with keys as spaCy token attributes. regex (bool): Whether to match the dictionary value using regex. Returns: bool: Whether the token passes the filters. \"\"\" if not isinstance ( token , spacy . tokens . Token ): raise LexosException ( \"The input must be a spaCy token.\" ) if not isinstance ( regex , bool ): raise LexosException ( \"The regex flag must be a boolean.\" ) if filters and filters != {}: for filter , value in filters . items (): if ( regex and re . search ( re . compile ( value ), getattr ( token , filter )) is not None ): return False elif getattr ( token , filter ) == value : return False else : return True else : return True def _validate_filters ( self , filters : List [ str ]) -> None : \"\"\"Ensure that filters are in the correct format. Args: filters (Union[List[Dict[str, str]], List[str])): A list of filter dictionaries with keys or a list of boolean filters (the names of spaCy token attributes). Returns: None Raises: LexosException: If the format for the filter is not correct. \"\"\" message = \"The filter must be a list of filter dictionaries with keys or a list of boolean filters (the names of spaCy token attributes)\" if not isinstance ( filters , list ) or any ( not isinstance ( x , ( dict , str )) for x in filters ): raise LexosException ( message )","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.__init__","text":"Initialize a LexosDoc object. Source code in lexos\\tokenizer\\lexosdoc.py 25 26 27 28 29 30 def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" if isinstance ( doc , spacy . tokens . doc . Doc ): self . doc = doc else : raise LexosException ( \"The input must be a spaCy doc.\" )","title":"__init__()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_term_counts","text":"Get a list of word counts for each token in the doc. Parameters: Name Type Description Default self object A spaCy doc. required limit int The maximum number of tokens to count. None start Any The index of the first token to count. 0 end Any The index of the last token to count after limit is applied. None filters List [ Union [ Dict [ str , str ], str ]] A list of Doc attributes to ignore. None regex bool Whether to match the dictionary value using regex. False normalize bool Whether to return raw counts or relative frequencies. False normalize_with_filters bool Whether to normalize based on the number of tokens after filters are applied. False as_df bool Whether to return a pandas dataframe. False Returns: Type Description Union [ List , pd . DataFrame ] Union[List, pd.DataFrame]: A list of word count tuples for Union [ List , pd . DataFrame ] each token in the doc. Alternatively, a pandas dataframe. Source code in lexos\\tokenizer\\lexosdoc.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False , ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self (object): A spaCy doc. limit (int): The maximum number of tokens to count. start (Any): The index of the first token to count. end (Any): The index of the last token to count after limit is applied. filters (List[Union[Dict[str, str], str]]): A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df (bool): Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : self . _validate_filters ( filters ) for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return self . _dataframe ( term_counts , columns ) else : return term_counts","title":"get_term_counts()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_token_attrs","text":"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. Source code in lexos\\tokenizer\\lexosdoc.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ( [ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]] ) return { \"spacy_attributes\" : attrs , \"extensions\" : exts }","title":"get_token_attrs()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_tokens","text":"Return a list of tokens in the doc. Source code in lexos\\tokenizer\\lexosdoc.py 92 93 94 def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ]","title":"get_tokens()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.to_dataframe","text":"Get a pandas dataframe of the doc attributes. Parameters: Name Type Description Default cols List [ str ] A list of columns to include in the dataframe. ['text'] show_ranges bool Whether to include the token start and end positions in the dataframe. True Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call LexosDoc.get_token_attrs() first to check which attributes are available for the doc. Source code in lexos\\tokenizer\\lexosdoc.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return self . _dataframe ( rows , cols )","title":"to_dataframe()"},{"location":"api/topic_model/","text":"Topic Model \u00a4 The topic_model module is used to train and visualize topic models. Currently, it works MALLET , which must be installed separately, to train models and generates visualizations with dfr-browser . lexos.topic_model.mallet.Mallet \u00a4 A wrapper for the MALLET command line tool. Source code in lexos\\topic_model\\mallet\\__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class Mallet : \"\"\"A wrapper for the MALLET command line tool.\"\"\" def __init__ ( self , model_dir : str , mallet_path : str = \"mallet\" ): \"\"\"Initialize the MALLET object. Args: model_dir (str): The directory to store the model. mallet_path (str): The path to the MALLET binary. \"\"\" self . model_dir = model_dir self . mallet_path = mallet_path def import_data ( self , docs : List [ object ], allowed : List [ str ] = None , remove_stops : bool = True , remove_punct : bool = True , use_lemmas : bool = False , ** kwargs ): \"\"\"Import data into MALLET. Args: docs (List[object]): A list of spaCy documents. allowed (List[str]): A list of POS tags that are allowed. remove_stops (bool): Whether to remove stop words. remove_punct (bool): Whether to remove punctuation. use_lemmas (bool): Whether to replace tokens with lemmas. Notes: Creates a file containing one doc per line with each doc consisting of space-separated terms repeated however many times they occurred in the source doc. This file is then over-written by the MALLET import-file command, potentially using any MALLET command flags that are passed in (although most of the work is done by the first step in the process). \"\"\" msg = Printer () if not Path ( f \" { self . model_dir } /data_skip.txt\" ) . is_file (): msg . text ( \"Bagifying data...\" ) # Set the allowable tokens if allowed : is_allowed_getter = lambda token : token . pos_ in allowed Token . set_extension ( \"is_allowed\" , getter = is_allowed_getter , force = True ) else : Token . set_extension ( \"is_allowed\" , default = True , force = True ) bags = [] # Get the token text for each doc for doc in docs : if use_lemmas : tokens = [ token . lemma_ for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] else : tokens = [ token . text for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] # Get the token counts counts = dict ( Counter ( tokens )) # Create a bag with copies of each token occurring multiple times bag = [] for k , v in counts . items (): repeated = f \" { k } \" * v bag . append ( repeated . strip ()) bags . append ( \" \" . join ( bag )) # Write the data file with a bag for each document self . data_file = f \" { self . model_dir } /data.txt\" with open ( self . data_file , \"w\" , encoding = \"utf-8\" ) as f : f . write ( \" \\n \" . join ( bags )) else : self . data_file = f \" { self . model_dir } /data.txt\" self . mallet_file = f \" { self . model_dir } /import.mallet\" # Build the MALLET import command opts = { \"keep-sequence\" : True , \"preserve-case\" : True , \"remove-stopwords\" : False , \"extra-stopwords\" : False , \"token-regex\" : '\"\\S+\"' , \"stoplist-file\" : None , } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) mallet_cmd = f \" { self . mallet_path } /mallet import-file --input { self . data_file } --output { self . mallet_file } \" mallet_cmd += \" \" . join ( cmd_opts ) msg . text ( f \"Running { mallet_cmd } \" ) mallet_cmd = shlex . split ( mallet_cmd ) # Perform the import try : # shell=True required to handle backslashes in token-regex output = check_output ( mallet_cmd , stderr = STDOUT , shell = True , universal_newlines = True ) msg . good ( \"Import complete.\" ) except CalledProcessError as e : output = e . output #.decode() msg . fail ( output ) def train ( self , mallet_file : str = None , num_topics : int = 20 , num_iterations : int = 1000 , optimize_interval : int = 10 , random_seed : int = None , ** kwargs ): \"\"\"Train a model. Args: num_topics (int): The number of topics to train. num_iterations (int): The number of iterations to train. optimize_interval (int): The number of iterations between optimization. random_seed (int): The random seed to use. \"\"\" msg = Printer () # Set the options try : if not mallet_file : mallet_file = self . mallet_file except AttributeError : msg . fail ( \"Please supply an `input` argument with the path to your MALLET import file.\" ) opts = { \"input\" : mallet_file , \"num-topics\" : str ( num_topics ), \"num-iterations\" : str ( num_iterations ), \"optimize-interval\" : str ( optimize_interval ), \"random-seed\" : random_seed , \"output-state\" : f \" { self . model_dir } /state.gz\" , \"output-topic-keys\" : f \" { self . model_dir } /keys.txt\" , \"output-doc-topics\" : f \" { self . model_dir } /composition.txt\" , \"word-topic-counts-file\" : f \" { self . model_dir } /counts.txt\" , \"output-topic-docs\" : f \" { self . model_dir } /topic-docs.txt\" , \"diagnostics-file\" : f \" { self . model_dir } /diagnostics.xml\" } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if k == \"random-seed\" : v = str ( v ) if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) cmd_opts = \" \" . join ( cmd_opts ) mallet_cmd = f \" { self . mallet_path } /mallet train-topics { cmd_opts } \" msg . text ( f \"Running { mallet_cmd } \\n \" ) p = Popen ( mallet_cmd , stdout = PIPE , stderr = STDOUT , shell = True ) ll = [] prog = re . compile ( u '\\<([^\\)]+)\\>' ) while p . poll () is None : l = p . stdout . readline () . decode () print ( l , end = '' ) # Keep track of LL/topic. try : this_ll = float ( re . findall ( '([-+]\\d+\\.\\d+)' , l )[ 0 ]) ll . append ( this_ll ) except IndexError : # Not every line will match. pass # Keep track of modeling progress try : this_iter = float ( prog . match ( l ) . groups ()[ 0 ]) progress = int ( 100. * this_iter / num_iterations ) if progress % 10 == 0 : print ( 'Modeling progress: {0} %. \\r ' . format ( progress )), except AttributeError : # Not every line will match. pass def scale ( self , model_state_file : str = None , output_file : str = None ): \"\"\"Scale a model. Args: model_state_file (str): The path to a state_file. output_file (str): The path to an output file. \"\"\" msg = Printer () msg . text ( \"Processing...\" ) if not model_state_file : model_state_file = f \" { self . model_dir } /state.gz\" if not output_file : output_file = f \" { self . model_dir } /topic_scaled.csv\" # try: # Convert the mallet output_state file to a pyLDAvis data object converted_data = scale_model . convert_mallet_data ( model_state_file ) # Get the topic coordinates in a dataframe topic_coordinates = scale_model . get_topic_coordinates ( ** converted_data ) # Save the topic coordinates to a CSV file topic_coordinates . to_csv ( output_file , index = False , header = False ) msg . good ( \"Done!\" ) __init__ ( model_dir , mallet_path = 'mallet' ) \u00a4 Initialize the MALLET object. Parameters: Name Type Description Default model_dir str The directory to store the model. required mallet_path str The path to the MALLET binary. 'mallet' Source code in lexos\\topic_model\\mallet\\__init__.py 19 20 21 22 23 24 25 26 27 def __init__ ( self , model_dir : str , mallet_path : str = \"mallet\" ): \"\"\"Initialize the MALLET object. Args: model_dir (str): The directory to store the model. mallet_path (str): The path to the MALLET binary. \"\"\" self . model_dir = model_dir self . mallet_path = mallet_path import_data ( docs , allowed = None , remove_stops = True , remove_punct = True , use_lemmas = False , ** kwargs ) \u00a4 Import data into MALLET. Parameters: Name Type Description Default docs List [ object ] A list of spaCy documents. required allowed List [ str ] A list of POS tags that are allowed. None remove_stops bool Whether to remove stop words. True remove_punct bool Whether to remove punctuation. True use_lemmas bool Whether to replace tokens with lemmas. False Notes Creates a file containing one doc per line with each doc consisting of space-separated terms repeated however many times they occurred in the source doc. This file is then over-written by the MALLET import-file command, potentially using any MALLET command flags that are passed in (although most of the work is done by the first step in the process). Source code in lexos\\topic_model\\mallet\\__init__.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def import_data ( self , docs : List [ object ], allowed : List [ str ] = None , remove_stops : bool = True , remove_punct : bool = True , use_lemmas : bool = False , ** kwargs ): \"\"\"Import data into MALLET. Args: docs (List[object]): A list of spaCy documents. allowed (List[str]): A list of POS tags that are allowed. remove_stops (bool): Whether to remove stop words. remove_punct (bool): Whether to remove punctuation. use_lemmas (bool): Whether to replace tokens with lemmas. Notes: Creates a file containing one doc per line with each doc consisting of space-separated terms repeated however many times they occurred in the source doc. This file is then over-written by the MALLET import-file command, potentially using any MALLET command flags that are passed in (although most of the work is done by the first step in the process). \"\"\" msg = Printer () if not Path ( f \" { self . model_dir } /data_skip.txt\" ) . is_file (): msg . text ( \"Bagifying data...\" ) # Set the allowable tokens if allowed : is_allowed_getter = lambda token : token . pos_ in allowed Token . set_extension ( \"is_allowed\" , getter = is_allowed_getter , force = True ) else : Token . set_extension ( \"is_allowed\" , default = True , force = True ) bags = [] # Get the token text for each doc for doc in docs : if use_lemmas : tokens = [ token . lemma_ for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] else : tokens = [ token . text for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] # Get the token counts counts = dict ( Counter ( tokens )) # Create a bag with copies of each token occurring multiple times bag = [] for k , v in counts . items (): repeated = f \" { k } \" * v bag . append ( repeated . strip ()) bags . append ( \" \" . join ( bag )) # Write the data file with a bag for each document self . data_file = f \" { self . model_dir } /data.txt\" with open ( self . data_file , \"w\" , encoding = \"utf-8\" ) as f : f . write ( \" \\n \" . join ( bags )) else : self . data_file = f \" { self . model_dir } /data.txt\" self . mallet_file = f \" { self . model_dir } /import.mallet\" # Build the MALLET import command opts = { \"keep-sequence\" : True , \"preserve-case\" : True , \"remove-stopwords\" : False , \"extra-stopwords\" : False , \"token-regex\" : '\"\\S+\"' , \"stoplist-file\" : None , } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) mallet_cmd = f \" { self . mallet_path } /mallet import-file --input { self . data_file } --output { self . mallet_file } \" mallet_cmd += \" \" . join ( cmd_opts ) msg . text ( f \"Running { mallet_cmd } \" ) mallet_cmd = shlex . split ( mallet_cmd ) # Perform the import try : # shell=True required to handle backslashes in token-regex output = check_output ( mallet_cmd , stderr = STDOUT , shell = True , universal_newlines = True ) msg . good ( \"Import complete.\" ) except CalledProcessError as e : output = e . output #.decode() msg . fail ( output ) scale ( model_state_file = None , output_file = None ) \u00a4 Scale a model. Parameters: Name Type Description Default model_state_file str The path to a state_file. None output_file str The path to an output file. None Source code in lexos\\topic_model\\mallet\\__init__.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def scale ( self , model_state_file : str = None , output_file : str = None ): \"\"\"Scale a model. Args: model_state_file (str): The path to a state_file. output_file (str): The path to an output file. \"\"\" msg = Printer () msg . text ( \"Processing...\" ) if not model_state_file : model_state_file = f \" { self . model_dir } /state.gz\" if not output_file : output_file = f \" { self . model_dir } /topic_scaled.csv\" # try: # Convert the mallet output_state file to a pyLDAvis data object converted_data = scale_model . convert_mallet_data ( model_state_file ) # Get the topic coordinates in a dataframe topic_coordinates = scale_model . get_topic_coordinates ( ** converted_data ) # Save the topic coordinates to a CSV file topic_coordinates . to_csv ( output_file , index = False , header = False ) msg . good ( \"Done!\" ) train ( mallet_file = None , num_topics = 20 , num_iterations = 1000 , optimize_interval = 10 , random_seed = None , ** kwargs ) \u00a4 Train a model. Parameters: Name Type Description Default num_topics int The number of topics to train. 20 num_iterations int The number of iterations to train. 1000 optimize_interval int The number of iterations between optimization. 10 random_seed int The random seed to use. None Source code in lexos\\topic_model\\mallet\\__init__.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def train ( self , mallet_file : str = None , num_topics : int = 20 , num_iterations : int = 1000 , optimize_interval : int = 10 , random_seed : int = None , ** kwargs ): \"\"\"Train a model. Args: num_topics (int): The number of topics to train. num_iterations (int): The number of iterations to train. optimize_interval (int): The number of iterations between optimization. random_seed (int): The random seed to use. \"\"\" msg = Printer () # Set the options try : if not mallet_file : mallet_file = self . mallet_file except AttributeError : msg . fail ( \"Please supply an `input` argument with the path to your MALLET import file.\" ) opts = { \"input\" : mallet_file , \"num-topics\" : str ( num_topics ), \"num-iterations\" : str ( num_iterations ), \"optimize-interval\" : str ( optimize_interval ), \"random-seed\" : random_seed , \"output-state\" : f \" { self . model_dir } /state.gz\" , \"output-topic-keys\" : f \" { self . model_dir } /keys.txt\" , \"output-doc-topics\" : f \" { self . model_dir } /composition.txt\" , \"word-topic-counts-file\" : f \" { self . model_dir } /counts.txt\" , \"output-topic-docs\" : f \" { self . model_dir } /topic-docs.txt\" , \"diagnostics-file\" : f \" { self . model_dir } /diagnostics.xml\" } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if k == \"random-seed\" : v = str ( v ) if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) cmd_opts = \" \" . join ( cmd_opts ) mallet_cmd = f \" { self . mallet_path } /mallet train-topics { cmd_opts } \" msg . text ( f \"Running { mallet_cmd } \\n \" ) p = Popen ( mallet_cmd , stdout = PIPE , stderr = STDOUT , shell = True ) ll = [] prog = re . compile ( u '\\<([^\\)]+)\\>' ) while p . poll () is None : l = p . stdout . readline () . decode () print ( l , end = '' ) # Keep track of LL/topic. try : this_ll = float ( re . findall ( '([-+]\\d+\\.\\d+)' , l )[ 0 ]) ll . append ( this_ll ) except IndexError : # Not every line will match. pass # Keep track of modeling progress try : this_iter = float ( prog . match ( l ) . groups ()[ 0 ]) progress = int ( 100. * this_iter / num_iterations ) if progress % 10 == 0 : print ( 'Modeling progress: {0} %. \\r ' . format ( progress )), except AttributeError : # Not every line will match. pass lexos . topic_model . mallet . scale_model . __num_dist_rows__ ( array , ndigits = 2 ) \u00a4 Check that all rows in a matrix sum to 1. Source code in lexos\\topic_model\\mallet\\scale_model.py 22 23 24 def __num_dist_rows__ ( array , ndigits : int = 2 ): \"\"\"Check that all rows in a matrix sum to 1.\"\"\" return array . shape [ 0 ] - int (( pd . DataFrame ( array ) . sum ( axis = 1 ) < 0.999 ) . sum ()) lexos.topic_model.mallet.scale_model.ValidationError \u00a4 Bases: ValueError Handle validation errors. Source code in lexos\\topic_model\\mallet\\scale_model.py 27 28 29 30 class ValidationError ( ValueError ): \"\"\"Handle validation errors.\"\"\" pass lexos . topic_model . mallet . scale_model . _input_check ( topic_term_dists , doc_topic_dists , doc_lengths , vocab , term_frequency ) \u00a4 Check input for scale_model. Parameters: Name Type Description Default topic_term_dists pd . DataFrame Matrix of topic-term probabilities. required doc_topic_dists pd . DataFrame Matrix of document-topic probabilities. required doc_lengths list List of document lengths. required vocab list List of vocabulary. required term_frequency int Minimum number of times a term must appear in a document. required Returns: Name Type Description list list List of errors. Source code in lexos\\topic_model\\mallet\\scale_model.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def _input_check ( topic_term_dists : pd . DataFrame , doc_topic_dists : pd . DataFrame , doc_lengths : list , vocab : list , term_frequency : int , ) -> list : \"\"\"Check input for scale_model. Args: topic_term_dists (pd.DataFrame): Matrix of topic-term probabilities. doc_topic_dists (pd.DataFrame): Matrix of document-topic probabilities. doc_lengths (list): List of document lengths. vocab (list): List of vocabulary. term_frequency (int): Minimum number of times a term must appear in a document. Returns: list: List of errors. \"\"\" ttds = topic_term_dists . shape dtds = doc_topic_dists . shape errors = [] def err ( msg ): \"\"\"Append error message.\"\"\" errors . append ( msg ) if dtds [ 1 ] != ttds [ 0 ]: err ( \"Number of rows of topic_term_dists does not match number of columns of doc_topic_dists; both should be equal to the number of topics in the model.\" ) if len ( doc_lengths ) != dtds [ 0 ]: err ( \"Length of doc_lengths not equal to the number of rows in doc_topic_dists; both should be equal to the number of documents in the data.\" ) W = len ( vocab ) if ttds [ 1 ] != W : err ( \"Number of terms in vocabulary does not match the number of columns of topic_term_dists (where each row of topic_term_dists is a probability distribution of terms for a given topic).\" ) if len ( term_frequency ) != W : err ( \"Length of term_frequency not equal to the number of terms in the vocabulary (len of vocab).\" ) if __num_dist_rows__ ( topic_term_dists ) != ttds [ 0 ]: err ( \"Not all rows (distributions) in topic_term_dists sum to 1.\" ) if __num_dist_rows__ ( doc_topic_dists ) != dtds [ 0 ]: err ( \"Not all rows (distributions) in doc_topic_dists sum to 1.\" ) if len ( errors ) > 0 : return errors lexos . topic_model . mallet . scale_model . _input_validate ( * args ) \u00a4 Check input for scale_model. Source code in lexos\\topic_model\\mallet\\scale_model.py 90 91 92 93 94 def _input_validate ( * args ) -> None : \"\"\"Check input for scale_model.\"\"\" res = _input_check ( * args ) if res : raise ValidationError ( \" \\n \" + \" \\n \" . join ([ \" * \" + s for s in res ])) lexos . topic_model . mallet . scale_model . _jensen_shannon ( _P , _Q ) \u00a4 Calculate Jensen-Shannon Divergence. Parameters: Name Type Description Default _P np . array Probability distribution. required _Q np . array Probability distribution. required Returns: Name Type Description float float Jensen-Shannon Divergence. Source code in lexos\\topic_model\\mallet\\scale_model.py 97 98 99 100 101 102 103 104 105 106 107 108 def _jensen_shannon ( _P : np . array , _Q : np . array ) -> float : \"\"\"Calculate Jensen-Shannon Divergence. Args: _P (np.array): Probability distribution. _Q (np.array): Probability distribution. Returns: float: Jensen-Shannon Divergence. \"\"\" _M = 0.5 * ( _P + _Q ) return 0.5 * ( entropy ( _P , _M ) + entropy ( _Q , _M )) lexos . topic_model . mallet . scale_model . _pcoa ( pair_dists , n_components = 2 ) \u00a4 Perform Principal Coordinate Analysis. AKA Classical Multidimensional Scaling Code referenced from skbio.stats.ordination.pcoa Parameters: Name Type Description Default pair_dists np . array Pairwise distances. required n_components int Number of dimensions to reduce to. 2 Returns: Type Description np . array np.array: PCoA matrix. Source code in lexos\\topic_model\\mallet\\scale_model.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def _pcoa ( pair_dists : np . array , n_components : int = 2 ) -> np . array : \"\"\"Perform Principal Coordinate Analysis. AKA Classical Multidimensional Scaling Code referenced from [skbio.stats.ordination.pcoa](https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py) Args: pair_dists (np.array): Pairwise distances. n_components (int): Number of dimensions to reduce to. Returns: np.array: PCoA matrix. \"\"\" # pairwise distance matrix is assumed symmetric pair_dists = np . asarray ( pair_dists , np . float64 ) # perform SVD on double centred distance matrix n = pair_dists . shape [ 0 ] H = np . eye ( n ) - np . ones (( n , n )) / n B = - H . dot ( pair_dists ** 2 ) . dot ( H ) / 2 eigvals , eigvecs = np . linalg . eig ( B ) # Take first n_components of eigenvalues and eigenvectors # sorted in decreasing order ix = eigvals . argsort ()[:: - 1 ][: n_components ] eigvals = eigvals [ ix ] eigvecs = eigvecs [:, ix ] # replace any remaining negative eigenvalues and associated eigenvectors with zeroes # at least 1 eigenvalue must be zero eigvals [ np . isclose ( eigvals , 0 )] = 0 if np . any ( eigvals < 0 ): ix_neg = eigvals < 0 eigvals [ ix_neg ] = np . zeros ( eigvals [ ix_neg ] . shape ) eigvecs [:, ix_neg ] = np . zeros ( eigvecs [:, ix_neg ] . shape ) return np . sqrt ( eigvals ) * eigvecs lexos . topic_model . mallet . scale_model . js_PCoA ( distributions ) \u00a4 Perform dimension reduction. Works via Jensen-Shannon Divergence & Principal Coordinate Analysis (aka Classical Multidimensional Scaling) Parameters: Name Type Description Default distributions np . array (array-like, shape ( n_dists , k )): Matrix of distributions probabilities. required Returns: Name Type Description pcoa np . array (array, shape ( n_dists , 2)) Source code in lexos\\topic_model\\mallet\\scale_model.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def js_PCoA ( distributions : np . array ) -> np . array : \"\"\"Perform dimension reduction. Works via Jensen-Shannon Divergence & Principal Coordinate Analysis (aka Classical Multidimensional Scaling) Args: distributions: (array-like, shape (`n_dists`, `k`)): Matrix of distributions probabilities. Returns: pcoa (np.array): (array, shape (`n_dists`, 2)) \"\"\" dist_matrix = squareform ( pdist ( distributions , metric = _jensen_shannon )) return _pcoa ( dist_matrix ) lexos . topic_model . mallet . scale_model . js_MMDS ( distributions , ** kwargs ) \u00a4 Perform dimension reduction. Works via Jensen-Shannon Divergence & Metric Multidimensional Scaling Parameters: Name Type Description Default distributions np . array Matrix of distributions probabilities (array-like, shape ( n_dists , k )). required **kwargs dict Keyword argument to be passed to sklearn.manifold.MDS() {} Returns: Name Type Description mmds np . array (array, shape ( n_dists , 2)) Source code in lexos\\topic_model\\mallet\\scale_model.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def js_MMDS ( distributions : np . array , ** kwargs ) -> np . array : \"\"\"Perform dimension reduction. Works via Jensen-Shannon Divergence & Metric Multidimensional Scaling Args: distributions (np.array): Matrix of distributions probabilities (array-like, shape (`n_dists`, `k`)). **kwargs (dict): Keyword argument to be passed to `sklearn.manifold.MDS()` Returns: mmds (np.array): (array, shape (`n_dists`, 2)) \"\"\" dist_matrix = squareform ( pdist ( distributions , metric = _jensen_shannon )) model = MDS ( n_components = 2 , random_state = 0 , dissimilarity = \"precomputed\" , ** kwargs ) return model . fit_transform ( dist_matrix ) lexos . topic_model . mallet . scale_model . js_TSNE ( distributions , ** kwargs ) \u00a4 Perform dimension reduction. Works via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding Parameters: Name Type Description Default distributions np . array Matrix of distributions probabilities (array-like, shape ( n_dists , k )). required **kwargs dict Keyword argument to be passed to sklearn.manifold.MDS() {} Returns: Name Type Description tsne np . array (array, shape ( n_dists , 2)) Source code in lexos\\topic_model\\mallet\\scale_model.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def js_TSNE ( distributions , ** kwargs ) -> np . array : \"\"\"Perform dimension reduction. Works via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding Args: distributions (np.array): Matrix of distributions probabilities (array-like, shape (`n_dists`, `k`)). **kwargs (dict): Keyword argument to be passed to `sklearn.manifold.MDS()` Returns: tsne (np.array): (array, shape (`n_dists`, 2)) \"\"\" dist_matrix = squareform ( pdist ( distributions , metric = _jensen_shannon )) model = TSNE ( n_components = 2 , random_state = 0 , metric = \"precomputed\" , ** kwargs ) return model . fit_transform ( dist_matrix ) lexos . topic_model . mallet . scale_model . _df_with_names ( data , index_name , columns_name ) \u00a4 Get a dataframe with names. Parameters: Name Type Description Default data pd . DataFrame Dataframe. required index_name str Name of index. required columns_name str Name of columns. required Returns: Type Description pd . DataFrame pd.DataFrame: Dataframe with names. Source code in lexos\\topic_model\\mallet\\scale_model.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def _df_with_names ( data , index_name : str , columns_name : str ) -> pd . DataFrame : \"\"\"Get a dataframe with names. Args: data (pd.DataFrame): Dataframe. index_name (str): Name of index. columns_name (str): Name of columns. Returns: pd.DataFrame: Dataframe with names. \"\"\" if isinstance ( data , pd . DataFrame ): # we want our index to be numbered df = pd . DataFrame ( data . values ) else : df = pd . DataFrame ( data ) df . index . name = index_name df . columns . name = columns_name return df lexos . topic_model . mallet . scale_model . _series_with_name ( data , name ) \u00a4 Get a series with name. Parameters: Name Type Description Default data pd . Series Series. required name str Name of series. required Returns: Type Description pd . Series pd.Series: Series with name. Source code in lexos\\topic_model\\mallet\\scale_model.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def _series_with_name ( data , name ) -> pd . Series : \"\"\"Get a series with name. Args: data (pd.Series): Series. name (str): Name of series. Returns: pd.Series: Series with name. \"\"\" if isinstance ( data , pd . Series ): data . name = name # ensures a numeric index return data . reset_index ()[ name ] else : return pd . Series ( data , name = name ) lexos . topic_model . mallet . scale_model . _topic_coordinates ( mds , topic_term_dists , topic_proportion ) \u00a4 Get coordinates for topics. Parameters: Name Type Description Default mds array, shape (`n_dists`, 2 MDS coordinates. required topic_term_dists array, shape (`n_topics`, `n_terms` Topic-term distributions. required topic_proportion array, shape (`n_topics` Topic proportions. required Returns: Type Description pd . DataFrame pd.DataFrame: Topic coordinates. Source code in lexos\\topic_model\\mallet\\scale_model.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def _topic_coordinates ( mds : np . array , topic_term_dists : np . array , topic_proportion : np . array ) -> pd . DataFrame : \"\"\"Get coordinates for topics. Args: mds (array, shape (`n_dists`, 2)): MDS coordinates. topic_term_dists (array, shape (`n_topics`, `n_terms`)): Topic-term distributions. topic_proportion (array, shape (`n_topics`)): Topic proportions. Returns: pd.DataFrame: Topic coordinates. \"\"\" K = topic_term_dists . shape [ 0 ] mds_res = mds ( topic_term_dists ) assert mds_res . shape == ( K , 2 ) mds_df = pd . DataFrame ( { \"x\" : mds_res [:, 0 ], \"y\" : mds_res [:, 1 ], \"topics\" : range ( 1 , K + 1 ), \"cluster\" : 1 , \"Freq\" : topic_proportion * 100 , } ) # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26 return mds_df lexos . topic_model . mallet . scale_model . get_topic_coordinates ( topic_term_dists , doc_topic_dists , doc_lengths , vocab , term_frequency , mds = js_PCoA , sort_topics = True ) \u00a4 Transform the topic model distributions and related corpus. Creates the data structures needed for topic bubbles. Parameters: Name Type Description Default topic_term_dists array-like, shape (`n_topics`, `n_terms` Matrix of topic-term probabilities where n_terms is len(vocab) . required doc_topic_dists array-like, shape (`n_docs`, `n_topics` Matrix of document-topic probabilities. required doc_lengths (array-like, shape n_docs ): The length of each document, i.e. the number of words in each document. The order of the numbers should be consistent with the ordering of the docs in doc_topic_dists . required vocab array-like, shape `n_terms` List of all the words in the corpus used to train the model. required term_frequency array-like, shape `n_terms` The count of each particular term over the entire corpus. The ordering of these counts should correspond with vocab and topic_term_dists . required mds Callable A function that takes topic_term_dists as an input and outputs a n_topics by 2 distance matrix. The output approximates the distance between topics. See js_PCoA() for details on the default function. A string representation currently accepts pcoa (or upper case variant), mmds (or upper case variant) and tsne (or upper case variant), if sklearn package is installed for the latter two. js_PCoA sort_topics bool Whether to sort topics by topic proportion (percentage of tokens covered). Set to False to to keep original topic order. True Returns: Name Type Description scaled_coordinates pd . DataFrame A pandas dataframe containing scaled x and y coordinates. Source code in lexos\\topic_model\\mallet\\scale_model.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def get_topic_coordinates ( topic_term_dists : np . array , doc_topic_dists : np . array , doc_lengths : list , vocab : list , term_frequency : list , mds : Callable = js_PCoA , sort_topics : bool = True , ) -> pd . DataFrame : \"\"\"Transform the topic model distributions and related corpus. Creates the data structures needed for topic bubbles. Args: topic_term_dists (array-like, shape (`n_topics`, `n_terms`)): Matrix of topic-term probabilities where `n_terms` is `len(vocab)`. doc_topic_dists (array-like, shape (`n_docs`, `n_topics`)): Matrix of document-topic probabilities. doc_lengths : (array-like, shape `n_docs`): The length of each document, i.e. the number of words in each document. The order of the numbers should be consistent with the ordering of the docs in `doc_topic_dists`. vocab (array-like, shape `n_terms`): List of all the words in the corpus used to train the model. term_frequency (array-like, shape `n_terms`): The count of each particular term over the entire corpus. The ordering of these counts should correspond with `vocab` and `topic_term_dists`. mds (Callable): A function that takes `topic_term_dists` as an input and outputs a `n_topics` by `2` distance matrix. The output approximates the distance between topics. See `js_PCoA()` for details on the default function. A string representation currently accepts `pcoa` (or upper case variant), `mmds` (or upper case variant) and `tsne` (or upper case variant), if `sklearn` package is installed for the latter two. sort_topics (bool): Whether to sort topics by topic proportion (percentage of tokens covered). Set to `False` to to keep original topic order. Returns: scaled_coordinates (pd.DataFrame): A pandas dataframe containing scaled x and y coordinates. \"\"\" # parse mds # if isinstance(mds, basestring): if isinstance ( mds , ( str , bytes )): mds = mds . lower () if mds == \"pcoa\" : mds = js_PCoA elif mds in ( \"mmds\" , \"tsne\" ): if sklearn_present : mds_opts = { \"mmds\" : js_MMDS , \"tsne\" : js_TSNE } mds = mds_opts [ mds ] else : logging . warning ( \"sklearn not present, switch to PCoA\" ) mds = js_PCoA else : logging . warning ( \"Unknown mds ` %s `, switch to PCoA\" % mds ) mds = js_PCoA topic_term_dists = _df_with_names ( topic_term_dists , \"topic\" , \"term\" ) doc_topic_dists = _df_with_names ( doc_topic_dists , \"doc\" , \"topic\" ) term_frequency = _series_with_name ( term_frequency , \"term_frequency\" ) doc_lengths = _series_with_name ( doc_lengths , \"doc_length\" ) vocab = _series_with_name ( vocab , \"vocab\" ) _input_validate ( topic_term_dists , doc_topic_dists , doc_lengths , vocab , term_frequency ) topic_freq = ( doc_topic_dists . T * doc_lengths ) . T . sum () if sort_topics : topic_proportion = ( topic_freq / topic_freq . sum ()) . sort_values ( ascending = False ) else : topic_proportion = topic_freq / topic_freq . sum () topic_order = topic_proportion . index topic_term_dists = topic_term_dists . iloc [ topic_order ] scaled_coordinates = _topic_coordinates ( mds , topic_term_dists , topic_proportion ) return scaled_coordinates lexos . topic_model . mallet . scale_model . extract_params ( statefile ) \u00a4 Extract the alpha and beta values from the statefile. Parameters: Name Type Description Default statefile str Path to statefile produced by MALLET. required Returns: Name Type Description tuple tuple A tuple of (alpha (list), beta) Source code in lexos\\topic_model\\mallet\\scale_model.py 344 345 346 347 348 349 350 351 352 353 354 355 def extract_params ( statefile : str ) -> tuple : \"\"\"Extract the alpha and beta values from the statefile. Args: statefile (str): Path to statefile produced by MALLET. Returns: tuple: A tuple of (alpha (list), beta) \"\"\" with gzip . open ( statefile , \"r\" ) as state : params = [ x . decode ( \"utf8\" ) . strip () for x in state . readlines ()[ 1 : 3 ]] return ( list ( params [ 0 ] . split ( \":\" )[ 1 ] . split ( \" \" )), float ( params [ 1 ] . split ( \":\" )[ 1 ])) lexos . topic_model . mallet . scale_model . state_to_df ( statefile ) \u00a4 Transform state file into pandas dataframe. The MALLET statefile is tab-separated, and the first two rows contain the alpha and beta hypterparamters. Parameters: Name Type Description Default statefile str Path to statefile produced by MALLET. required Returns: Type Description pd . DataFrame pd.DataFrame: The topic assignment for each token in each document of the model. Source code in lexos\\topic_model\\mallet\\scale_model.py 358 359 360 361 362 363 364 365 366 367 368 369 def state_to_df ( statefile : str ) -> pd . DataFrame : \"\"\"Transform state file into pandas dataframe. The MALLET statefile is tab-separated, and the first two rows contain the alpha and beta hypterparamters. Args: statefile (str): Path to statefile produced by MALLET. Returns: pd.DataFrame: The topic assignment for each token in each document of the model. \"\"\" return pd . read_csv ( statefile , compression = \"gzip\" , sep = \" \" , skiprows = [ 1 , 2 ]) lexos . topic_model . mallet . scale_model . pivot_and_smooth ( df , smooth_value , rows_variable , cols_variable , values_variable ) \u00a4 Turn the pandas dataframe into a data matrix. Parameters: Name Type Description Default df pd . DataFrame The aggregated dataframe. required smooth_value float Value to add to the matrix to account for the priors. required rows_variable str The name of the dataframe column to use as the rows in the matrix. required cols_variable str The name of the dataframe column to use as the columns in the matrix. required values_variable str The name of the dataframe column to use as the values in the matrix. required Returns: Type Description pd . DataFrame pd.DataFrame: A pandas matrix that has been normalized on the rows. Source code in lexos\\topic_model\\mallet\\scale_model.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def pivot_and_smooth ( df : pd . DataFrame , smooth_value : float , rows_variable : str , cols_variable : str , values_variable : str , ) -> pd . DataFrame : \"\"\"Turn the pandas dataframe into a data matrix. Args: df (pd.DataFrame): The aggregated dataframe. smooth_value (float): Value to add to the matrix to account for the priors. rows_variable (str): The name of the dataframe column to use as the rows in the matrix. cols_variable (str): The name of the dataframe column to use as the columns in the matrix. values_variable (str): The name of the dataframe column to use as the values in the matrix. Returns: pd.DataFrame: A pandas matrix that has been normalized on the rows. \"\"\" matrix = df . pivot ( index = rows_variable , columns = cols_variable , values = values_variable ) . fillna ( value = 0 ) matrix = matrix . values + smooth_value normed = sklearn . preprocessing . normalize ( matrix , norm = \"l1\" , axis = 1 ) return pd . DataFrame ( normed ) lexos . topic_model . mallet . scale_model . convert_mallet_data ( state_file ) \u00a4 Convert Mallet data to a structure compatible with pyLDAvis. Parameters: Name Type Description Default state_file string Mallet state file required Returns: Name Type Description data dict A dict containing pandas dataframes for the pyLDAvis prepare method. Source code in lexos\\topic_model\\mallet\\scale_model.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def convert_mallet_data ( state_file : str ) -> dict : \"\"\"Convert Mallet data to a structure compatible with pyLDAvis. Args: state_file (string): Mallet state file Returns: data (dict): A dict containing pandas dataframes for the pyLDAvis prepare method. \"\"\" params = extract_params ( state_file ) alpha = [ float ( x ) for x in params [ 0 ][ 1 :]] beta = params [ 1 ] df = state_to_df ( state_file ) # Ensure that NaN is a string df [ \"type\" ] = df . type . astype ( str ) # Get document lengths from statefile docs = df . groupby ( \"#doc\" )[ \"type\" ] . count () . reset_index ( name = \"doc_length\" ) # Get vocab and term frequencies from statefile vocab = df [ \"type\" ] . value_counts () . reset_index () vocab . columns = [ \"type\" , \"term_freq\" ] vocab = vocab . sort_values ( by = \"type\" , ascending = True ) phi_df = ( df . groupby ([ \"topic\" , \"type\" ])[ \"type\" ] . count () . reset_index ( name = \"token_count\" ) ) phi_df = phi_df . sort_values ( by = \"type\" , ascending = True ) phi = pivot_and_smooth ( phi_df , beta , \"topic\" , \"type\" , \"token_count\" ) theta_df = ( df . groupby ([ \"#doc\" , \"topic\" ])[ \"topic\" ] . count () . reset_index ( name = \"topic_count\" ) ) theta = pivot_and_smooth ( theta_df , alpha , \"#doc\" , \"topic\" , \"topic_count\" ) data = { \"topic_term_dists\" : phi , \"doc_topic_dists\" : theta , \"doc_lengths\" : list ( docs [ \"doc_length\" ]), \"vocab\" : list ( vocab [ \"type\" ]), \"term_frequency\" : list ( vocab [ \"term_freq\" ]), } return data lexos.topic_model.dfr_browser.DfrBrowser \u00a4 DfrBrowser class. Source code in lexos\\topic_model\\dfr_browser\\__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class DfrBrowser : \"\"\"DfrBrowser class.\"\"\" def __init__ ( self , model_dir : str = \".\" , model_state_file : str = \"state.gz\" , model_scaled_file : str = \"topic_scaled.csv\" , template_dir : str = TEMPLATE_DIR , ) -> None : \"\"\"Initialize DfrBrowser object.\"\"\" self . template_dir = template_dir self . model_dir = model_dir self . model_state_file = f \" { model_dir } / { model_state_file } \" self . model_scaled_file = f \" { model_dir } / { model_scaled_file } \" self . browser_dir = f \" { model_dir } /dfr_browser\" self . data_dir = f \" { self . browser_dir } /data\" self . num_topics = None # How to get this? # Make a browser directory and copy the template into it if not Path ( self . browser_dir ) . exists (): self . _copy_template () # Create dfr-browser files using python script self . _prepare_data () # Copy scaled file into data dir shutil . copy ( self . model_scaled_file , self . data_dir ) # Move meta.csv to data_dir, zip up, and rename, delete meta.csv copy self . _move_metadata () # Update assets self . _update_assets () def _copy_template ( self ): \"\"\"Copy the template directory to the browser directory.\"\"\" try : shutil . copytree ( Path ( self . template_dir ), Path ( self . browser_dir )) except FileNotFoundError as e : raise LexosException ( f \"Could not find dfr-browser template: { e } \" ) def _prepare_data ( self ): \"\"\"Prepare the data for the dfr-browser visualization.\"\"\" Path ( f \" { self . data_dir } \" ) . mkdir ( parents = True , exist_ok = True ) prepare_data_script = f \"python { self . browser_dir } /bin/prepare-data\" cmd = \" \" . join ( [ prepare_data_script , \"convert-state\" , self . model_state_file , \"--tw\" , f \" { self . data_dir } /tw.json\" , \"--dt\" , f \" { self . data_dir } /dt.json.zip\" , ] ) cmd = shlex . split ( cmd ) try : output = check_output ( cmd , stderr = STDOUT , shell = True , universal_newlines = True ) print ( output ) except CalledProcessError as e : raise LexosException ( e . output ) cmd = \" \" . join ( [ prepare_data_script , \"info-stub\" , \"-o\" , f \" { self . data_dir } /info.json\" ] ) cmd = shlex . split ( cmd ) try : output = check_output ( cmd , stderr = STDOUT , shell = True , universal_newlines = True ) print ( output ) except CalledProcessError as e : raise LexosException ( e . output ) def _move_metadata ( self ): \"\"\"Move meta.csv to data_dir, zip up, rename, and delete meta.csv copy.\"\"\" meta_zip = f \" { self . data_dir } /meta.csv.zip\" if Path ( meta_zip ) . exists (): Path ( meta_zip ) . unlink () browser_meta_file = f \" { self . model_dir } /meta.csv\" shutil . copy ( browser_meta_file , self . data_dir ) try : shutil . make_archive ( f \" { self . data_dir } /meta.csv\" , \"zip\" , self . data_dir , \"meta.csv\" ) except OSError as err : raise LexosException ( f \"Error writing meta.csv.zip: { err } \" ) def _update_assets ( self ): \"\"\"Update browser assets.\"\"\" # Tweak default index.html to link to JSON, not JSTOR with open ( f \" { self . browser_dir } /index.html\" , \"r\" ) as f : filedata = f . read () . replace ( \"on JSTOR\" , \"JSON\" ) with open ( f \" { self . browser_dir } /index.html\" , \"w\" ) as f : f . write ( filedata ) # Tweak js file to link to the domain with open ( f \" { self . browser_dir } /js/dfb.min.js.custom\" , \"r\" , encoding = \"utf-8\" ) as f : filedata = f . read () pat = r \"t\\.select\\( \\\" #doc_remark a\\.url \\\" \\).attr\\( \\\" href \\\" , .+?\\);\" new_pat = r 'var doc_url = document.URL.split(\"modules\")[0] + \"project_data\"; t.select(\"#doc_remark a.url\")' new_pat += r '.attr(\"href\", doc_url + \"/\" + e.url);' filedata = re . sub ( pat , new_pat , filedata ) with open ( f \" { self . browser_dir } /js/dfb.min.js\" , \"w\" , encoding = \"utf-8\" ) as f : f . write ( filedata ) def run ( self , port : int = 8080 ) -> None : \"\"\"Run the dfr-browser. This might work on the Jupyter port, but it might not. \"\"\" # run_server = f\"python {self.browser_dir}/bin/server\" import os import sys import threading import time import webbrowser as w from http.server import HTTPServer , SimpleHTTPRequestHandler # set up the HTTP server and start it in a separate daemon thread httpd = HTTPServer (( \"localhost\" , port ), SimpleHTTPRequestHandler ) thread = threading . Thread ( target = httpd . serve_forever ) thread . daemon = True # if startup time is too long we might want to be able to quit the program current_dir = os . getcwd () try : os . chdir ( self . browser_dir ) thread . start () except KeyboardInterrupt : httpd . shutdown () os . chdir ( current_dir ) sys . exit ( 0 ) # wait until the webserver finished starting up (maybe wait longer or shorter...) time . sleep ( 3 ) # start sending requests w . open ( f \"http://127.0.0.1: { port } /\" ) __init__ ( model_dir = '.' , model_state_file = 'state.gz' , model_scaled_file = 'topic_scaled.csv' , template_dir = TEMPLATE_DIR ) \u00a4 Initialize DfrBrowser object. Source code in lexos\\topic_model\\dfr_browser\\__init__.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , model_dir : str = \".\" , model_state_file : str = \"state.gz\" , model_scaled_file : str = \"topic_scaled.csv\" , template_dir : str = TEMPLATE_DIR , ) -> None : \"\"\"Initialize DfrBrowser object.\"\"\" self . template_dir = template_dir self . model_dir = model_dir self . model_state_file = f \" { model_dir } / { model_state_file } \" self . model_scaled_file = f \" { model_dir } / { model_scaled_file } \" self . browser_dir = f \" { model_dir } /dfr_browser\" self . data_dir = f \" { self . browser_dir } /data\" self . num_topics = None # How to get this? # Make a browser directory and copy the template into it if not Path ( self . browser_dir ) . exists (): self . _copy_template () # Create dfr-browser files using python script self . _prepare_data () # Copy scaled file into data dir shutil . copy ( self . model_scaled_file , self . data_dir ) # Move meta.csv to data_dir, zip up, and rename, delete meta.csv copy self . _move_metadata () # Update assets self . _update_assets () run ( port = 8080 ) \u00a4 Run the dfr-browser. This might work on the Jupyter port, but it might not. Source code in lexos\\topic_model\\dfr_browser\\__init__.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def run ( self , port : int = 8080 ) -> None : \"\"\"Run the dfr-browser. This might work on the Jupyter port, but it might not. \"\"\" # run_server = f\"python {self.browser_dir}/bin/server\" import os import sys import threading import time import webbrowser as w from http.server import HTTPServer , SimpleHTTPRequestHandler # set up the HTTP server and start it in a separate daemon thread httpd = HTTPServer (( \"localhost\" , port ), SimpleHTTPRequestHandler ) thread = threading . Thread ( target = httpd . serve_forever ) thread . daemon = True # if startup time is too long we might want to be able to quit the program current_dir = os . getcwd () try : os . chdir ( self . browser_dir ) thread . start () except KeyboardInterrupt : httpd . shutdown () os . chdir ( current_dir ) sys . exit ( 0 ) # wait until the webserver finished starting up (maybe wait longer or shorter...) time . sleep ( 3 ) # start sending requests w . open ( f \"http://127.0.0.1: { port } /\" )","title":"Topic_Model"},{"location":"api/topic_model/#topic-model","text":"The topic_model module is used to train and visualize topic models. Currently, it works MALLET , which must be installed separately, to train models and generates visualizations with dfr-browser .","title":"Topic Model"},{"location":"api/topic_model/#lexos.topic_model.mallet.Mallet","text":"A wrapper for the MALLET command line tool. Source code in lexos\\topic_model\\mallet\\__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class Mallet : \"\"\"A wrapper for the MALLET command line tool.\"\"\" def __init__ ( self , model_dir : str , mallet_path : str = \"mallet\" ): \"\"\"Initialize the MALLET object. Args: model_dir (str): The directory to store the model. mallet_path (str): The path to the MALLET binary. \"\"\" self . model_dir = model_dir self . mallet_path = mallet_path def import_data ( self , docs : List [ object ], allowed : List [ str ] = None , remove_stops : bool = True , remove_punct : bool = True , use_lemmas : bool = False , ** kwargs ): \"\"\"Import data into MALLET. Args: docs (List[object]): A list of spaCy documents. allowed (List[str]): A list of POS tags that are allowed. remove_stops (bool): Whether to remove stop words. remove_punct (bool): Whether to remove punctuation. use_lemmas (bool): Whether to replace tokens with lemmas. Notes: Creates a file containing one doc per line with each doc consisting of space-separated terms repeated however many times they occurred in the source doc. This file is then over-written by the MALLET import-file command, potentially using any MALLET command flags that are passed in (although most of the work is done by the first step in the process). \"\"\" msg = Printer () if not Path ( f \" { self . model_dir } /data_skip.txt\" ) . is_file (): msg . text ( \"Bagifying data...\" ) # Set the allowable tokens if allowed : is_allowed_getter = lambda token : token . pos_ in allowed Token . set_extension ( \"is_allowed\" , getter = is_allowed_getter , force = True ) else : Token . set_extension ( \"is_allowed\" , default = True , force = True ) bags = [] # Get the token text for each doc for doc in docs : if use_lemmas : tokens = [ token . lemma_ for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] else : tokens = [ token . text for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] # Get the token counts counts = dict ( Counter ( tokens )) # Create a bag with copies of each token occurring multiple times bag = [] for k , v in counts . items (): repeated = f \" { k } \" * v bag . append ( repeated . strip ()) bags . append ( \" \" . join ( bag )) # Write the data file with a bag for each document self . data_file = f \" { self . model_dir } /data.txt\" with open ( self . data_file , \"w\" , encoding = \"utf-8\" ) as f : f . write ( \" \\n \" . join ( bags )) else : self . data_file = f \" { self . model_dir } /data.txt\" self . mallet_file = f \" { self . model_dir } /import.mallet\" # Build the MALLET import command opts = { \"keep-sequence\" : True , \"preserve-case\" : True , \"remove-stopwords\" : False , \"extra-stopwords\" : False , \"token-regex\" : '\"\\S+\"' , \"stoplist-file\" : None , } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) mallet_cmd = f \" { self . mallet_path } /mallet import-file --input { self . data_file } --output { self . mallet_file } \" mallet_cmd += \" \" . join ( cmd_opts ) msg . text ( f \"Running { mallet_cmd } \" ) mallet_cmd = shlex . split ( mallet_cmd ) # Perform the import try : # shell=True required to handle backslashes in token-regex output = check_output ( mallet_cmd , stderr = STDOUT , shell = True , universal_newlines = True ) msg . good ( \"Import complete.\" ) except CalledProcessError as e : output = e . output #.decode() msg . fail ( output ) def train ( self , mallet_file : str = None , num_topics : int = 20 , num_iterations : int = 1000 , optimize_interval : int = 10 , random_seed : int = None , ** kwargs ): \"\"\"Train a model. Args: num_topics (int): The number of topics to train. num_iterations (int): The number of iterations to train. optimize_interval (int): The number of iterations between optimization. random_seed (int): The random seed to use. \"\"\" msg = Printer () # Set the options try : if not mallet_file : mallet_file = self . mallet_file except AttributeError : msg . fail ( \"Please supply an `input` argument with the path to your MALLET import file.\" ) opts = { \"input\" : mallet_file , \"num-topics\" : str ( num_topics ), \"num-iterations\" : str ( num_iterations ), \"optimize-interval\" : str ( optimize_interval ), \"random-seed\" : random_seed , \"output-state\" : f \" { self . model_dir } /state.gz\" , \"output-topic-keys\" : f \" { self . model_dir } /keys.txt\" , \"output-doc-topics\" : f \" { self . model_dir } /composition.txt\" , \"word-topic-counts-file\" : f \" { self . model_dir } /counts.txt\" , \"output-topic-docs\" : f \" { self . model_dir } /topic-docs.txt\" , \"diagnostics-file\" : f \" { self . model_dir } /diagnostics.xml\" } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if k == \"random-seed\" : v = str ( v ) if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) cmd_opts = \" \" . join ( cmd_opts ) mallet_cmd = f \" { self . mallet_path } /mallet train-topics { cmd_opts } \" msg . text ( f \"Running { mallet_cmd } \\n \" ) p = Popen ( mallet_cmd , stdout = PIPE , stderr = STDOUT , shell = True ) ll = [] prog = re . compile ( u '\\<([^\\)]+)\\>' ) while p . poll () is None : l = p . stdout . readline () . decode () print ( l , end = '' ) # Keep track of LL/topic. try : this_ll = float ( re . findall ( '([-+]\\d+\\.\\d+)' , l )[ 0 ]) ll . append ( this_ll ) except IndexError : # Not every line will match. pass # Keep track of modeling progress try : this_iter = float ( prog . match ( l ) . groups ()[ 0 ]) progress = int ( 100. * this_iter / num_iterations ) if progress % 10 == 0 : print ( 'Modeling progress: {0} %. \\r ' . format ( progress )), except AttributeError : # Not every line will match. pass def scale ( self , model_state_file : str = None , output_file : str = None ): \"\"\"Scale a model. Args: model_state_file (str): The path to a state_file. output_file (str): The path to an output file. \"\"\" msg = Printer () msg . text ( \"Processing...\" ) if not model_state_file : model_state_file = f \" { self . model_dir } /state.gz\" if not output_file : output_file = f \" { self . model_dir } /topic_scaled.csv\" # try: # Convert the mallet output_state file to a pyLDAvis data object converted_data = scale_model . convert_mallet_data ( model_state_file ) # Get the topic coordinates in a dataframe topic_coordinates = scale_model . get_topic_coordinates ( ** converted_data ) # Save the topic coordinates to a CSV file topic_coordinates . to_csv ( output_file , index = False , header = False ) msg . good ( \"Done!\" )","title":"Mallet"},{"location":"api/topic_model/#lexos.topic_model.mallet.Mallet.__init__","text":"Initialize the MALLET object. Parameters: Name Type Description Default model_dir str The directory to store the model. required mallet_path str The path to the MALLET binary. 'mallet' Source code in lexos\\topic_model\\mallet\\__init__.py 19 20 21 22 23 24 25 26 27 def __init__ ( self , model_dir : str , mallet_path : str = \"mallet\" ): \"\"\"Initialize the MALLET object. Args: model_dir (str): The directory to store the model. mallet_path (str): The path to the MALLET binary. \"\"\" self . model_dir = model_dir self . mallet_path = mallet_path","title":"__init__()"},{"location":"api/topic_model/#lexos.topic_model.mallet.Mallet.import_data","text":"Import data into MALLET. Parameters: Name Type Description Default docs List [ object ] A list of spaCy documents. required allowed List [ str ] A list of POS tags that are allowed. None remove_stops bool Whether to remove stop words. True remove_punct bool Whether to remove punctuation. True use_lemmas bool Whether to replace tokens with lemmas. False Notes Creates a file containing one doc per line with each doc consisting of space-separated terms repeated however many times they occurred in the source doc. This file is then over-written by the MALLET import-file command, potentially using any MALLET command flags that are passed in (although most of the work is done by the first step in the process). Source code in lexos\\topic_model\\mallet\\__init__.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def import_data ( self , docs : List [ object ], allowed : List [ str ] = None , remove_stops : bool = True , remove_punct : bool = True , use_lemmas : bool = False , ** kwargs ): \"\"\"Import data into MALLET. Args: docs (List[object]): A list of spaCy documents. allowed (List[str]): A list of POS tags that are allowed. remove_stops (bool): Whether to remove stop words. remove_punct (bool): Whether to remove punctuation. use_lemmas (bool): Whether to replace tokens with lemmas. Notes: Creates a file containing one doc per line with each doc consisting of space-separated terms repeated however many times they occurred in the source doc. This file is then over-written by the MALLET import-file command, potentially using any MALLET command flags that are passed in (although most of the work is done by the first step in the process). \"\"\" msg = Printer () if not Path ( f \" { self . model_dir } /data_skip.txt\" ) . is_file (): msg . text ( \"Bagifying data...\" ) # Set the allowable tokens if allowed : is_allowed_getter = lambda token : token . pos_ in allowed Token . set_extension ( \"is_allowed\" , getter = is_allowed_getter , force = True ) else : Token . set_extension ( \"is_allowed\" , default = True , force = True ) bags = [] # Get the token text for each doc for doc in docs : if use_lemmas : tokens = [ token . lemma_ for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] else : tokens = [ token . text for token in doc if token . _ . is_allowed and token . is_stop != remove_stops and token . is_punct != remove_punct ] # Get the token counts counts = dict ( Counter ( tokens )) # Create a bag with copies of each token occurring multiple times bag = [] for k , v in counts . items (): repeated = f \" { k } \" * v bag . append ( repeated . strip ()) bags . append ( \" \" . join ( bag )) # Write the data file with a bag for each document self . data_file = f \" { self . model_dir } /data.txt\" with open ( self . data_file , \"w\" , encoding = \"utf-8\" ) as f : f . write ( \" \\n \" . join ( bags )) else : self . data_file = f \" { self . model_dir } /data.txt\" self . mallet_file = f \" { self . model_dir } /import.mallet\" # Build the MALLET import command opts = { \"keep-sequence\" : True , \"preserve-case\" : True , \"remove-stopwords\" : False , \"extra-stopwords\" : False , \"token-regex\" : '\"\\S+\"' , \"stoplist-file\" : None , } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) mallet_cmd = f \" { self . mallet_path } /mallet import-file --input { self . data_file } --output { self . mallet_file } \" mallet_cmd += \" \" . join ( cmd_opts ) msg . text ( f \"Running { mallet_cmd } \" ) mallet_cmd = shlex . split ( mallet_cmd ) # Perform the import try : # shell=True required to handle backslashes in token-regex output = check_output ( mallet_cmd , stderr = STDOUT , shell = True , universal_newlines = True ) msg . good ( \"Import complete.\" ) except CalledProcessError as e : output = e . output #.decode() msg . fail ( output )","title":"import_data()"},{"location":"api/topic_model/#lexos.topic_model.mallet.Mallet.scale","text":"Scale a model. Parameters: Name Type Description Default model_state_file str The path to a state_file. None output_file str The path to an output file. None Source code in lexos\\topic_model\\mallet\\__init__.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def scale ( self , model_state_file : str = None , output_file : str = None ): \"\"\"Scale a model. Args: model_state_file (str): The path to a state_file. output_file (str): The path to an output file. \"\"\" msg = Printer () msg . text ( \"Processing...\" ) if not model_state_file : model_state_file = f \" { self . model_dir } /state.gz\" if not output_file : output_file = f \" { self . model_dir } /topic_scaled.csv\" # try: # Convert the mallet output_state file to a pyLDAvis data object converted_data = scale_model . convert_mallet_data ( model_state_file ) # Get the topic coordinates in a dataframe topic_coordinates = scale_model . get_topic_coordinates ( ** converted_data ) # Save the topic coordinates to a CSV file topic_coordinates . to_csv ( output_file , index = False , header = False ) msg . good ( \"Done!\" )","title":"scale()"},{"location":"api/topic_model/#lexos.topic_model.mallet.Mallet.train","text":"Train a model. Parameters: Name Type Description Default num_topics int The number of topics to train. 20 num_iterations int The number of iterations to train. 1000 optimize_interval int The number of iterations between optimization. 10 random_seed int The random seed to use. None Source code in lexos\\topic_model\\mallet\\__init__.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def train ( self , mallet_file : str = None , num_topics : int = 20 , num_iterations : int = 1000 , optimize_interval : int = 10 , random_seed : int = None , ** kwargs ): \"\"\"Train a model. Args: num_topics (int): The number of topics to train. num_iterations (int): The number of iterations to train. optimize_interval (int): The number of iterations between optimization. random_seed (int): The random seed to use. \"\"\" msg = Printer () # Set the options try : if not mallet_file : mallet_file = self . mallet_file except AttributeError : msg . fail ( \"Please supply an `input` argument with the path to your MALLET import file.\" ) opts = { \"input\" : mallet_file , \"num-topics\" : str ( num_topics ), \"num-iterations\" : str ( num_iterations ), \"optimize-interval\" : str ( optimize_interval ), \"random-seed\" : random_seed , \"output-state\" : f \" { self . model_dir } /state.gz\" , \"output-topic-keys\" : f \" { self . model_dir } /keys.txt\" , \"output-doc-topics\" : f \" { self . model_dir } /composition.txt\" , \"word-topic-counts-file\" : f \" { self . model_dir } /counts.txt\" , \"output-topic-docs\" : f \" { self . model_dir } /topic-docs.txt\" , \"diagnostics-file\" : f \" { self . model_dir } /diagnostics.xml\" } opts . update ( kwargs ) cmd_opts = [] for k , v in opts . items (): if v is not None : if k == \"random-seed\" : v = str ( v ) if v == True : cmd_opts . append ( f \"-- { k } \" ) elif isinstance ( v , str ): cmd_opts . append ( f \"-- { k } { v } \" ) cmd_opts = \" \" . join ( cmd_opts ) mallet_cmd = f \" { self . mallet_path } /mallet train-topics { cmd_opts } \" msg . text ( f \"Running { mallet_cmd } \\n \" ) p = Popen ( mallet_cmd , stdout = PIPE , stderr = STDOUT , shell = True ) ll = [] prog = re . compile ( u '\\<([^\\)]+)\\>' ) while p . poll () is None : l = p . stdout . readline () . decode () print ( l , end = '' ) # Keep track of LL/topic. try : this_ll = float ( re . findall ( '([-+]\\d+\\.\\d+)' , l )[ 0 ]) ll . append ( this_ll ) except IndexError : # Not every line will match. pass # Keep track of modeling progress try : this_iter = float ( prog . match ( l ) . groups ()[ 0 ]) progress = int ( 100. * this_iter / num_iterations ) if progress % 10 == 0 : print ( 'Modeling progress: {0} %. \\r ' . format ( progress )), except AttributeError : # Not every line will match. pass","title":"train()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.__num_dist_rows__","text":"Check that all rows in a matrix sum to 1. Source code in lexos\\topic_model\\mallet\\scale_model.py 22 23 24 def __num_dist_rows__ ( array , ndigits : int = 2 ): \"\"\"Check that all rows in a matrix sum to 1.\"\"\" return array . shape [ 0 ] - int (( pd . DataFrame ( array ) . sum ( axis = 1 ) < 0.999 ) . sum ())","title":"__num_dist_rows__()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.ValidationError","text":"Bases: ValueError Handle validation errors. Source code in lexos\\topic_model\\mallet\\scale_model.py 27 28 29 30 class ValidationError ( ValueError ): \"\"\"Handle validation errors.\"\"\" pass","title":"ValidationError"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model._input_check","text":"Check input for scale_model. Parameters: Name Type Description Default topic_term_dists pd . DataFrame Matrix of topic-term probabilities. required doc_topic_dists pd . DataFrame Matrix of document-topic probabilities. required doc_lengths list List of document lengths. required vocab list List of vocabulary. required term_frequency int Minimum number of times a term must appear in a document. required Returns: Name Type Description list list List of errors. Source code in lexos\\topic_model\\mallet\\scale_model.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def _input_check ( topic_term_dists : pd . DataFrame , doc_topic_dists : pd . DataFrame , doc_lengths : list , vocab : list , term_frequency : int , ) -> list : \"\"\"Check input for scale_model. Args: topic_term_dists (pd.DataFrame): Matrix of topic-term probabilities. doc_topic_dists (pd.DataFrame): Matrix of document-topic probabilities. doc_lengths (list): List of document lengths. vocab (list): List of vocabulary. term_frequency (int): Minimum number of times a term must appear in a document. Returns: list: List of errors. \"\"\" ttds = topic_term_dists . shape dtds = doc_topic_dists . shape errors = [] def err ( msg ): \"\"\"Append error message.\"\"\" errors . append ( msg ) if dtds [ 1 ] != ttds [ 0 ]: err ( \"Number of rows of topic_term_dists does not match number of columns of doc_topic_dists; both should be equal to the number of topics in the model.\" ) if len ( doc_lengths ) != dtds [ 0 ]: err ( \"Length of doc_lengths not equal to the number of rows in doc_topic_dists; both should be equal to the number of documents in the data.\" ) W = len ( vocab ) if ttds [ 1 ] != W : err ( \"Number of terms in vocabulary does not match the number of columns of topic_term_dists (where each row of topic_term_dists is a probability distribution of terms for a given topic).\" ) if len ( term_frequency ) != W : err ( \"Length of term_frequency not equal to the number of terms in the vocabulary (len of vocab).\" ) if __num_dist_rows__ ( topic_term_dists ) != ttds [ 0 ]: err ( \"Not all rows (distributions) in topic_term_dists sum to 1.\" ) if __num_dist_rows__ ( doc_topic_dists ) != dtds [ 0 ]: err ( \"Not all rows (distributions) in doc_topic_dists sum to 1.\" ) if len ( errors ) > 0 : return errors","title":"_input_check()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model._input_validate","text":"Check input for scale_model. Source code in lexos\\topic_model\\mallet\\scale_model.py 90 91 92 93 94 def _input_validate ( * args ) -> None : \"\"\"Check input for scale_model.\"\"\" res = _input_check ( * args ) if res : raise ValidationError ( \" \\n \" + \" \\n \" . join ([ \" * \" + s for s in res ]))","title":"_input_validate()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model._jensen_shannon","text":"Calculate Jensen-Shannon Divergence. Parameters: Name Type Description Default _P np . array Probability distribution. required _Q np . array Probability distribution. required Returns: Name Type Description float float Jensen-Shannon Divergence. Source code in lexos\\topic_model\\mallet\\scale_model.py 97 98 99 100 101 102 103 104 105 106 107 108 def _jensen_shannon ( _P : np . array , _Q : np . array ) -> float : \"\"\"Calculate Jensen-Shannon Divergence. Args: _P (np.array): Probability distribution. _Q (np.array): Probability distribution. Returns: float: Jensen-Shannon Divergence. \"\"\" _M = 0.5 * ( _P + _Q ) return 0.5 * ( entropy ( _P , _M ) + entropy ( _Q , _M ))","title":"_jensen_shannon()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model._pcoa","text":"Perform Principal Coordinate Analysis. AKA Classical Multidimensional Scaling Code referenced from skbio.stats.ordination.pcoa Parameters: Name Type Description Default pair_dists np . array Pairwise distances. required n_components int Number of dimensions to reduce to. 2 Returns: Type Description np . array np.array: PCoA matrix. Source code in lexos\\topic_model\\mallet\\scale_model.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def _pcoa ( pair_dists : np . array , n_components : int = 2 ) -> np . array : \"\"\"Perform Principal Coordinate Analysis. AKA Classical Multidimensional Scaling Code referenced from [skbio.stats.ordination.pcoa](https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py) Args: pair_dists (np.array): Pairwise distances. n_components (int): Number of dimensions to reduce to. Returns: np.array: PCoA matrix. \"\"\" # pairwise distance matrix is assumed symmetric pair_dists = np . asarray ( pair_dists , np . float64 ) # perform SVD on double centred distance matrix n = pair_dists . shape [ 0 ] H = np . eye ( n ) - np . ones (( n , n )) / n B = - H . dot ( pair_dists ** 2 ) . dot ( H ) / 2 eigvals , eigvecs = np . linalg . eig ( B ) # Take first n_components of eigenvalues and eigenvectors # sorted in decreasing order ix = eigvals . argsort ()[:: - 1 ][: n_components ] eigvals = eigvals [ ix ] eigvecs = eigvecs [:, ix ] # replace any remaining negative eigenvalues and associated eigenvectors with zeroes # at least 1 eigenvalue must be zero eigvals [ np . isclose ( eigvals , 0 )] = 0 if np . any ( eigvals < 0 ): ix_neg = eigvals < 0 eigvals [ ix_neg ] = np . zeros ( eigvals [ ix_neg ] . shape ) eigvecs [:, ix_neg ] = np . zeros ( eigvecs [:, ix_neg ] . shape ) return np . sqrt ( eigvals ) * eigvecs","title":"_pcoa()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.js_PCoA","text":"Perform dimension reduction. Works via Jensen-Shannon Divergence & Principal Coordinate Analysis (aka Classical Multidimensional Scaling) Parameters: Name Type Description Default distributions np . array (array-like, shape ( n_dists , k )): Matrix of distributions probabilities. required Returns: Name Type Description pcoa np . array (array, shape ( n_dists , 2)) Source code in lexos\\topic_model\\mallet\\scale_model.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def js_PCoA ( distributions : np . array ) -> np . array : \"\"\"Perform dimension reduction. Works via Jensen-Shannon Divergence & Principal Coordinate Analysis (aka Classical Multidimensional Scaling) Args: distributions: (array-like, shape (`n_dists`, `k`)): Matrix of distributions probabilities. Returns: pcoa (np.array): (array, shape (`n_dists`, 2)) \"\"\" dist_matrix = squareform ( pdist ( distributions , metric = _jensen_shannon )) return _pcoa ( dist_matrix )","title":"js_PCoA()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.js_MMDS","text":"Perform dimension reduction. Works via Jensen-Shannon Divergence & Metric Multidimensional Scaling Parameters: Name Type Description Default distributions np . array Matrix of distributions probabilities (array-like, shape ( n_dists , k )). required **kwargs dict Keyword argument to be passed to sklearn.manifold.MDS() {} Returns: Name Type Description mmds np . array (array, shape ( n_dists , 2)) Source code in lexos\\topic_model\\mallet\\scale_model.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def js_MMDS ( distributions : np . array , ** kwargs ) -> np . array : \"\"\"Perform dimension reduction. Works via Jensen-Shannon Divergence & Metric Multidimensional Scaling Args: distributions (np.array): Matrix of distributions probabilities (array-like, shape (`n_dists`, `k`)). **kwargs (dict): Keyword argument to be passed to `sklearn.manifold.MDS()` Returns: mmds (np.array): (array, shape (`n_dists`, 2)) \"\"\" dist_matrix = squareform ( pdist ( distributions , metric = _jensen_shannon )) model = MDS ( n_components = 2 , random_state = 0 , dissimilarity = \"precomputed\" , ** kwargs ) return model . fit_transform ( dist_matrix )","title":"js_MMDS()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.js_TSNE","text":"Perform dimension reduction. Works via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding Parameters: Name Type Description Default distributions np . array Matrix of distributions probabilities (array-like, shape ( n_dists , k )). required **kwargs dict Keyword argument to be passed to sklearn.manifold.MDS() {} Returns: Name Type Description tsne np . array (array, shape ( n_dists , 2)) Source code in lexos\\topic_model\\mallet\\scale_model.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def js_TSNE ( distributions , ** kwargs ) -> np . array : \"\"\"Perform dimension reduction. Works via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding Args: distributions (np.array): Matrix of distributions probabilities (array-like, shape (`n_dists`, `k`)). **kwargs (dict): Keyword argument to be passed to `sklearn.manifold.MDS()` Returns: tsne (np.array): (array, shape (`n_dists`, 2)) \"\"\" dist_matrix = squareform ( pdist ( distributions , metric = _jensen_shannon )) model = TSNE ( n_components = 2 , random_state = 0 , metric = \"precomputed\" , ** kwargs ) return model . fit_transform ( dist_matrix )","title":"js_TSNE()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model._df_with_names","text":"Get a dataframe with names. Parameters: Name Type Description Default data pd . DataFrame Dataframe. required index_name str Name of index. required columns_name str Name of columns. required Returns: Type Description pd . DataFrame pd.DataFrame: Dataframe with names. Source code in lexos\\topic_model\\mallet\\scale_model.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def _df_with_names ( data , index_name : str , columns_name : str ) -> pd . DataFrame : \"\"\"Get a dataframe with names. Args: data (pd.DataFrame): Dataframe. index_name (str): Name of index. columns_name (str): Name of columns. Returns: pd.DataFrame: Dataframe with names. \"\"\" if isinstance ( data , pd . DataFrame ): # we want our index to be numbered df = pd . DataFrame ( data . values ) else : df = pd . DataFrame ( data ) df . index . name = index_name df . columns . name = columns_name return df","title":"_df_with_names()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model._series_with_name","text":"Get a series with name. Parameters: Name Type Description Default data pd . Series Series. required name str Name of series. required Returns: Type Description pd . Series pd.Series: Series with name. Source code in lexos\\topic_model\\mallet\\scale_model.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def _series_with_name ( data , name ) -> pd . Series : \"\"\"Get a series with name. Args: data (pd.Series): Series. name (str): Name of series. Returns: pd.Series: Series with name. \"\"\" if isinstance ( data , pd . Series ): data . name = name # ensures a numeric index return data . reset_index ()[ name ] else : return pd . Series ( data , name = name )","title":"_series_with_name()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model._topic_coordinates","text":"Get coordinates for topics. Parameters: Name Type Description Default mds array, shape (`n_dists`, 2 MDS coordinates. required topic_term_dists array, shape (`n_topics`, `n_terms` Topic-term distributions. required topic_proportion array, shape (`n_topics` Topic proportions. required Returns: Type Description pd . DataFrame pd.DataFrame: Topic coordinates. Source code in lexos\\topic_model\\mallet\\scale_model.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def _topic_coordinates ( mds : np . array , topic_term_dists : np . array , topic_proportion : np . array ) -> pd . DataFrame : \"\"\"Get coordinates for topics. Args: mds (array, shape (`n_dists`, 2)): MDS coordinates. topic_term_dists (array, shape (`n_topics`, `n_terms`)): Topic-term distributions. topic_proportion (array, shape (`n_topics`)): Topic proportions. Returns: pd.DataFrame: Topic coordinates. \"\"\" K = topic_term_dists . shape [ 0 ] mds_res = mds ( topic_term_dists ) assert mds_res . shape == ( K , 2 ) mds_df = pd . DataFrame ( { \"x\" : mds_res [:, 0 ], \"y\" : mds_res [:, 1 ], \"topics\" : range ( 1 , K + 1 ), \"cluster\" : 1 , \"Freq\" : topic_proportion * 100 , } ) # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26 return mds_df","title":"_topic_coordinates()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.get_topic_coordinates","text":"Transform the topic model distributions and related corpus. Creates the data structures needed for topic bubbles. Parameters: Name Type Description Default topic_term_dists array-like, shape (`n_topics`, `n_terms` Matrix of topic-term probabilities where n_terms is len(vocab) . required doc_topic_dists array-like, shape (`n_docs`, `n_topics` Matrix of document-topic probabilities. required doc_lengths (array-like, shape n_docs ): The length of each document, i.e. the number of words in each document. The order of the numbers should be consistent with the ordering of the docs in doc_topic_dists . required vocab array-like, shape `n_terms` List of all the words in the corpus used to train the model. required term_frequency array-like, shape `n_terms` The count of each particular term over the entire corpus. The ordering of these counts should correspond with vocab and topic_term_dists . required mds Callable A function that takes topic_term_dists as an input and outputs a n_topics by 2 distance matrix. The output approximates the distance between topics. See js_PCoA() for details on the default function. A string representation currently accepts pcoa (or upper case variant), mmds (or upper case variant) and tsne (or upper case variant), if sklearn package is installed for the latter two. js_PCoA sort_topics bool Whether to sort topics by topic proportion (percentage of tokens covered). Set to False to to keep original topic order. True Returns: Name Type Description scaled_coordinates pd . DataFrame A pandas dataframe containing scaled x and y coordinates. Source code in lexos\\topic_model\\mallet\\scale_model.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def get_topic_coordinates ( topic_term_dists : np . array , doc_topic_dists : np . array , doc_lengths : list , vocab : list , term_frequency : list , mds : Callable = js_PCoA , sort_topics : bool = True , ) -> pd . DataFrame : \"\"\"Transform the topic model distributions and related corpus. Creates the data structures needed for topic bubbles. Args: topic_term_dists (array-like, shape (`n_topics`, `n_terms`)): Matrix of topic-term probabilities where `n_terms` is `len(vocab)`. doc_topic_dists (array-like, shape (`n_docs`, `n_topics`)): Matrix of document-topic probabilities. doc_lengths : (array-like, shape `n_docs`): The length of each document, i.e. the number of words in each document. The order of the numbers should be consistent with the ordering of the docs in `doc_topic_dists`. vocab (array-like, shape `n_terms`): List of all the words in the corpus used to train the model. term_frequency (array-like, shape `n_terms`): The count of each particular term over the entire corpus. The ordering of these counts should correspond with `vocab` and `topic_term_dists`. mds (Callable): A function that takes `topic_term_dists` as an input and outputs a `n_topics` by `2` distance matrix. The output approximates the distance between topics. See `js_PCoA()` for details on the default function. A string representation currently accepts `pcoa` (or upper case variant), `mmds` (or upper case variant) and `tsne` (or upper case variant), if `sklearn` package is installed for the latter two. sort_topics (bool): Whether to sort topics by topic proportion (percentage of tokens covered). Set to `False` to to keep original topic order. Returns: scaled_coordinates (pd.DataFrame): A pandas dataframe containing scaled x and y coordinates. \"\"\" # parse mds # if isinstance(mds, basestring): if isinstance ( mds , ( str , bytes )): mds = mds . lower () if mds == \"pcoa\" : mds = js_PCoA elif mds in ( \"mmds\" , \"tsne\" ): if sklearn_present : mds_opts = { \"mmds\" : js_MMDS , \"tsne\" : js_TSNE } mds = mds_opts [ mds ] else : logging . warning ( \"sklearn not present, switch to PCoA\" ) mds = js_PCoA else : logging . warning ( \"Unknown mds ` %s `, switch to PCoA\" % mds ) mds = js_PCoA topic_term_dists = _df_with_names ( topic_term_dists , \"topic\" , \"term\" ) doc_topic_dists = _df_with_names ( doc_topic_dists , \"doc\" , \"topic\" ) term_frequency = _series_with_name ( term_frequency , \"term_frequency\" ) doc_lengths = _series_with_name ( doc_lengths , \"doc_length\" ) vocab = _series_with_name ( vocab , \"vocab\" ) _input_validate ( topic_term_dists , doc_topic_dists , doc_lengths , vocab , term_frequency ) topic_freq = ( doc_topic_dists . T * doc_lengths ) . T . sum () if sort_topics : topic_proportion = ( topic_freq / topic_freq . sum ()) . sort_values ( ascending = False ) else : topic_proportion = topic_freq / topic_freq . sum () topic_order = topic_proportion . index topic_term_dists = topic_term_dists . iloc [ topic_order ] scaled_coordinates = _topic_coordinates ( mds , topic_term_dists , topic_proportion ) return scaled_coordinates","title":"get_topic_coordinates()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.extract_params","text":"Extract the alpha and beta values from the statefile. Parameters: Name Type Description Default statefile str Path to statefile produced by MALLET. required Returns: Name Type Description tuple tuple A tuple of (alpha (list), beta) Source code in lexos\\topic_model\\mallet\\scale_model.py 344 345 346 347 348 349 350 351 352 353 354 355 def extract_params ( statefile : str ) -> tuple : \"\"\"Extract the alpha and beta values from the statefile. Args: statefile (str): Path to statefile produced by MALLET. Returns: tuple: A tuple of (alpha (list), beta) \"\"\" with gzip . open ( statefile , \"r\" ) as state : params = [ x . decode ( \"utf8\" ) . strip () for x in state . readlines ()[ 1 : 3 ]] return ( list ( params [ 0 ] . split ( \":\" )[ 1 ] . split ( \" \" )), float ( params [ 1 ] . split ( \":\" )[ 1 ]))","title":"extract_params()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.state_to_df","text":"Transform state file into pandas dataframe. The MALLET statefile is tab-separated, and the first two rows contain the alpha and beta hypterparamters. Parameters: Name Type Description Default statefile str Path to statefile produced by MALLET. required Returns: Type Description pd . DataFrame pd.DataFrame: The topic assignment for each token in each document of the model. Source code in lexos\\topic_model\\mallet\\scale_model.py 358 359 360 361 362 363 364 365 366 367 368 369 def state_to_df ( statefile : str ) -> pd . DataFrame : \"\"\"Transform state file into pandas dataframe. The MALLET statefile is tab-separated, and the first two rows contain the alpha and beta hypterparamters. Args: statefile (str): Path to statefile produced by MALLET. Returns: pd.DataFrame: The topic assignment for each token in each document of the model. \"\"\" return pd . read_csv ( statefile , compression = \"gzip\" , sep = \" \" , skiprows = [ 1 , 2 ])","title":"state_to_df()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.pivot_and_smooth","text":"Turn the pandas dataframe into a data matrix. Parameters: Name Type Description Default df pd . DataFrame The aggregated dataframe. required smooth_value float Value to add to the matrix to account for the priors. required rows_variable str The name of the dataframe column to use as the rows in the matrix. required cols_variable str The name of the dataframe column to use as the columns in the matrix. required values_variable str The name of the dataframe column to use as the values in the matrix. required Returns: Type Description pd . DataFrame pd.DataFrame: A pandas matrix that has been normalized on the rows. Source code in lexos\\topic_model\\mallet\\scale_model.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def pivot_and_smooth ( df : pd . DataFrame , smooth_value : float , rows_variable : str , cols_variable : str , values_variable : str , ) -> pd . DataFrame : \"\"\"Turn the pandas dataframe into a data matrix. Args: df (pd.DataFrame): The aggregated dataframe. smooth_value (float): Value to add to the matrix to account for the priors. rows_variable (str): The name of the dataframe column to use as the rows in the matrix. cols_variable (str): The name of the dataframe column to use as the columns in the matrix. values_variable (str): The name of the dataframe column to use as the values in the matrix. Returns: pd.DataFrame: A pandas matrix that has been normalized on the rows. \"\"\" matrix = df . pivot ( index = rows_variable , columns = cols_variable , values = values_variable ) . fillna ( value = 0 ) matrix = matrix . values + smooth_value normed = sklearn . preprocessing . normalize ( matrix , norm = \"l1\" , axis = 1 ) return pd . DataFrame ( normed )","title":"pivot_and_smooth()"},{"location":"api/topic_model/#lexos.topic_model.mallet.scale_model.convert_mallet_data","text":"Convert Mallet data to a structure compatible with pyLDAvis. Parameters: Name Type Description Default state_file string Mallet state file required Returns: Name Type Description data dict A dict containing pandas dataframes for the pyLDAvis prepare method. Source code in lexos\\topic_model\\mallet\\scale_model.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def convert_mallet_data ( state_file : str ) -> dict : \"\"\"Convert Mallet data to a structure compatible with pyLDAvis. Args: state_file (string): Mallet state file Returns: data (dict): A dict containing pandas dataframes for the pyLDAvis prepare method. \"\"\" params = extract_params ( state_file ) alpha = [ float ( x ) for x in params [ 0 ][ 1 :]] beta = params [ 1 ] df = state_to_df ( state_file ) # Ensure that NaN is a string df [ \"type\" ] = df . type . astype ( str ) # Get document lengths from statefile docs = df . groupby ( \"#doc\" )[ \"type\" ] . count () . reset_index ( name = \"doc_length\" ) # Get vocab and term frequencies from statefile vocab = df [ \"type\" ] . value_counts () . reset_index () vocab . columns = [ \"type\" , \"term_freq\" ] vocab = vocab . sort_values ( by = \"type\" , ascending = True ) phi_df = ( df . groupby ([ \"topic\" , \"type\" ])[ \"type\" ] . count () . reset_index ( name = \"token_count\" ) ) phi_df = phi_df . sort_values ( by = \"type\" , ascending = True ) phi = pivot_and_smooth ( phi_df , beta , \"topic\" , \"type\" , \"token_count\" ) theta_df = ( df . groupby ([ \"#doc\" , \"topic\" ])[ \"topic\" ] . count () . reset_index ( name = \"topic_count\" ) ) theta = pivot_and_smooth ( theta_df , alpha , \"#doc\" , \"topic\" , \"topic_count\" ) data = { \"topic_term_dists\" : phi , \"doc_topic_dists\" : theta , \"doc_lengths\" : list ( docs [ \"doc_length\" ]), \"vocab\" : list ( vocab [ \"type\" ]), \"term_frequency\" : list ( vocab [ \"term_freq\" ]), } return data","title":"convert_mallet_data()"},{"location":"api/topic_model/#lexos.topic_model.dfr_browser.DfrBrowser","text":"DfrBrowser class. Source code in lexos\\topic_model\\dfr_browser\\__init__.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class DfrBrowser : \"\"\"DfrBrowser class.\"\"\" def __init__ ( self , model_dir : str = \".\" , model_state_file : str = \"state.gz\" , model_scaled_file : str = \"topic_scaled.csv\" , template_dir : str = TEMPLATE_DIR , ) -> None : \"\"\"Initialize DfrBrowser object.\"\"\" self . template_dir = template_dir self . model_dir = model_dir self . model_state_file = f \" { model_dir } / { model_state_file } \" self . model_scaled_file = f \" { model_dir } / { model_scaled_file } \" self . browser_dir = f \" { model_dir } /dfr_browser\" self . data_dir = f \" { self . browser_dir } /data\" self . num_topics = None # How to get this? # Make a browser directory and copy the template into it if not Path ( self . browser_dir ) . exists (): self . _copy_template () # Create dfr-browser files using python script self . _prepare_data () # Copy scaled file into data dir shutil . copy ( self . model_scaled_file , self . data_dir ) # Move meta.csv to data_dir, zip up, and rename, delete meta.csv copy self . _move_metadata () # Update assets self . _update_assets () def _copy_template ( self ): \"\"\"Copy the template directory to the browser directory.\"\"\" try : shutil . copytree ( Path ( self . template_dir ), Path ( self . browser_dir )) except FileNotFoundError as e : raise LexosException ( f \"Could not find dfr-browser template: { e } \" ) def _prepare_data ( self ): \"\"\"Prepare the data for the dfr-browser visualization.\"\"\" Path ( f \" { self . data_dir } \" ) . mkdir ( parents = True , exist_ok = True ) prepare_data_script = f \"python { self . browser_dir } /bin/prepare-data\" cmd = \" \" . join ( [ prepare_data_script , \"convert-state\" , self . model_state_file , \"--tw\" , f \" { self . data_dir } /tw.json\" , \"--dt\" , f \" { self . data_dir } /dt.json.zip\" , ] ) cmd = shlex . split ( cmd ) try : output = check_output ( cmd , stderr = STDOUT , shell = True , universal_newlines = True ) print ( output ) except CalledProcessError as e : raise LexosException ( e . output ) cmd = \" \" . join ( [ prepare_data_script , \"info-stub\" , \"-o\" , f \" { self . data_dir } /info.json\" ] ) cmd = shlex . split ( cmd ) try : output = check_output ( cmd , stderr = STDOUT , shell = True , universal_newlines = True ) print ( output ) except CalledProcessError as e : raise LexosException ( e . output ) def _move_metadata ( self ): \"\"\"Move meta.csv to data_dir, zip up, rename, and delete meta.csv copy.\"\"\" meta_zip = f \" { self . data_dir } /meta.csv.zip\" if Path ( meta_zip ) . exists (): Path ( meta_zip ) . unlink () browser_meta_file = f \" { self . model_dir } /meta.csv\" shutil . copy ( browser_meta_file , self . data_dir ) try : shutil . make_archive ( f \" { self . data_dir } /meta.csv\" , \"zip\" , self . data_dir , \"meta.csv\" ) except OSError as err : raise LexosException ( f \"Error writing meta.csv.zip: { err } \" ) def _update_assets ( self ): \"\"\"Update browser assets.\"\"\" # Tweak default index.html to link to JSON, not JSTOR with open ( f \" { self . browser_dir } /index.html\" , \"r\" ) as f : filedata = f . read () . replace ( \"on JSTOR\" , \"JSON\" ) with open ( f \" { self . browser_dir } /index.html\" , \"w\" ) as f : f . write ( filedata ) # Tweak js file to link to the domain with open ( f \" { self . browser_dir } /js/dfb.min.js.custom\" , \"r\" , encoding = \"utf-8\" ) as f : filedata = f . read () pat = r \"t\\.select\\( \\\" #doc_remark a\\.url \\\" \\).attr\\( \\\" href \\\" , .+?\\);\" new_pat = r 'var doc_url = document.URL.split(\"modules\")[0] + \"project_data\"; t.select(\"#doc_remark a.url\")' new_pat += r '.attr(\"href\", doc_url + \"/\" + e.url);' filedata = re . sub ( pat , new_pat , filedata ) with open ( f \" { self . browser_dir } /js/dfb.min.js\" , \"w\" , encoding = \"utf-8\" ) as f : f . write ( filedata ) def run ( self , port : int = 8080 ) -> None : \"\"\"Run the dfr-browser. This might work on the Jupyter port, but it might not. \"\"\" # run_server = f\"python {self.browser_dir}/bin/server\" import os import sys import threading import time import webbrowser as w from http.server import HTTPServer , SimpleHTTPRequestHandler # set up the HTTP server and start it in a separate daemon thread httpd = HTTPServer (( \"localhost\" , port ), SimpleHTTPRequestHandler ) thread = threading . Thread ( target = httpd . serve_forever ) thread . daemon = True # if startup time is too long we might want to be able to quit the program current_dir = os . getcwd () try : os . chdir ( self . browser_dir ) thread . start () except KeyboardInterrupt : httpd . shutdown () os . chdir ( current_dir ) sys . exit ( 0 ) # wait until the webserver finished starting up (maybe wait longer or shorter...) time . sleep ( 3 ) # start sending requests w . open ( f \"http://127.0.0.1: { port } /\" )","title":"DfrBrowser"},{"location":"api/topic_model/#lexos.topic_model.dfr_browser.DfrBrowser.__init__","text":"Initialize DfrBrowser object. Source code in lexos\\topic_model\\dfr_browser\\__init__.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , model_dir : str = \".\" , model_state_file : str = \"state.gz\" , model_scaled_file : str = \"topic_scaled.csv\" , template_dir : str = TEMPLATE_DIR , ) -> None : \"\"\"Initialize DfrBrowser object.\"\"\" self . template_dir = template_dir self . model_dir = model_dir self . model_state_file = f \" { model_dir } / { model_state_file } \" self . model_scaled_file = f \" { model_dir } / { model_scaled_file } \" self . browser_dir = f \" { model_dir } /dfr_browser\" self . data_dir = f \" { self . browser_dir } /data\" self . num_topics = None # How to get this? # Make a browser directory and copy the template into it if not Path ( self . browser_dir ) . exists (): self . _copy_template () # Create dfr-browser files using python script self . _prepare_data () # Copy scaled file into data dir shutil . copy ( self . model_scaled_file , self . data_dir ) # Move meta.csv to data_dir, zip up, and rename, delete meta.csv copy self . _move_metadata () # Update assets self . _update_assets ()","title":"__init__()"},{"location":"api/topic_model/#lexos.topic_model.dfr_browser.DfrBrowser.run","text":"Run the dfr-browser. This might work on the Jupyter port, but it might not. Source code in lexos\\topic_model\\dfr_browser\\__init__.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def run ( self , port : int = 8080 ) -> None : \"\"\"Run the dfr-browser. This might work on the Jupyter port, but it might not. \"\"\" # run_server = f\"python {self.browser_dir}/bin/server\" import os import sys import threading import time import webbrowser as w from http.server import HTTPServer , SimpleHTTPRequestHandler # set up the HTTP server and start it in a separate daemon thread httpd = HTTPServer (( \"localhost\" , port ), SimpleHTTPRequestHandler ) thread = threading . Thread ( target = httpd . serve_forever ) thread . daemon = True # if startup time is too long we might want to be able to quit the program current_dir = os . getcwd () try : os . chdir ( self . browser_dir ) thread . start () except KeyboardInterrupt : httpd . shutdown () os . chdir ( current_dir ) sys . exit ( 0 ) # wait until the webserver finished starting up (maybe wait longer or shorter...) time . sleep ( 3 ) # start sending requests w . open ( f \"http://127.0.0.1: { port } /\" )","title":"run()"},{"location":"api/visualization/","text":"Visualization \u00a4 This module contains the Lexos visualization functions. It has three submodules: bubbleviz , cloud , plotly , and seaborn . The first two create bubble charts and word clouds. The second two provide functions for using specific Python libraries. The plotly submodule has cloud and cluster methods for producing interactive word clouds and cluster plots (dendrograms and clustermaps). The seaborn module contains functions for producing static clustermaps.","title":"Overview"},{"location":"api/visualization/#visualization","text":"This module contains the Lexos visualization functions. It has three submodules: bubbleviz , cloud , plotly , and seaborn . The first two create bubble charts and word clouds. The second two provide functions for using specific Python libraries. The plotly submodule has cloud and cluster methods for producing interactive word clouds and cluster plots (dendrograms and clustermaps). The seaborn module contains functions for producing static clustermaps.","title":"Visualization"},{"location":"api/visualization/bubbleviz/","text":"Bubbleviz \u00a4 The Bubbleviz module produces bubble charts, a variant on word clouds, that produces circles (\"bubbles\") for each term based on the term's frequency in a text or collection of texts. lexos.visualization.bubbleviz.BubbleChart \u00a4 Bubble chart. Source code in lexos\\visualization\\bubbleviz\\__init__.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 class BubbleChart : \"\"\"Bubble chart.\"\"\" def __init__ ( self , BubbleChartModel : BubbleChartModel ): \"\"\"Instantiate a bubble chart from a BubbleChartModel. Args: BubbleChartModel (BubbleChartModel): A BubbleChartModel Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" self . model = BubbleChartModel # Reduce the area to the limited number of terms area = np . asarray ( self . model . area [: self . model . limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = self . model . bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass () def center_of_mass ( self ) -> int : \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ]) def center_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ]) def outline_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - bubbles [:, 2 ] - self . bubble_spacing def check_collisions ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ]) def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ] def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = self . bubbles [ i , : 2 ] + orth * self . step_dist new_point2 = self . bubbles [ i , : 2 ] - orth * self . step_dist dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2 def plot ( self , ax : object , labels : List [ str ], colors : List [ str ], font_family : str = \"Arial\" , ): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. font_family (str): The font family. \"\"\" plt . rcParams [ \"font.family\" ] = font_family color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ] ) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = \"center\" , verticalalignment = \"center\" ) __init__ ( BubbleChartModel ) \u00a4 Instantiate a bubble chart from a BubbleChartModel. Parameters: Name Type Description Default BubbleChartModel BubbleChartModel A BubbleChartModel required Notes If \"area\" is sorted, the results might look weird. If \"limit\" is raised too high, it will take a long time to generate the plot Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html . Source code in lexos\\visualization\\bubbleviz\\__init__.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __init__ ( self , BubbleChartModel : BubbleChartModel ): \"\"\"Instantiate a bubble chart from a BubbleChartModel. Args: BubbleChartModel (BubbleChartModel): A BubbleChartModel Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" self . model = BubbleChartModel # Reduce the area to the limited number of terms area = np . asarray ( self . model . area [: self . model . limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = self . model . bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass () center_distance ( bubble , bubbles ) \u00a4 Centre distance. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Type Description np . ndarray np.ndarray: The centre distance. Source code in lexos\\visualization\\bubbleviz\\__init__.py 109 110 111 112 113 114 115 116 117 118 119 def center_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ]) center_of_mass () \u00a4 Centre of mass. Returns: Name Type Description int int The centre of mass. Source code in lexos\\visualization\\bubbleviz\\__init__.py 101 102 103 104 105 106 107 def center_of_mass ( self ) -> int : \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ]) check_collisions ( bubble , bubbles ) \u00a4 Check collisions. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Name Type Description int int The length of the distance between bubbles. Source code in lexos\\visualization\\bubbleviz\\__init__.py 134 135 136 137 138 139 140 141 142 143 144 145 def check_collisions ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ]) collapse ( n_iterations = 50 ) \u00a4 Move bubbles to the center of mass. Parameters: Name Type Description Default n_iterations int Number of moves to perform. 50 Source code in lexos\\visualization\\bubbleviz\\__init__.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = self . bubbles [ i , : 2 ] + orth * self . step_dist new_point2 = self . bubbles [ i , : 2 ] - orth * self . step_dist dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2 collides_with ( bubble , bubbles ) \u00a4 Collide. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Name Type Description int int The minimum index. Source code in lexos\\visualization\\bubbleviz\\__init__.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ] outline_distance ( bubble , bubbles ) \u00a4 Outline distance. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Name Type Description int int The outline distance. Source code in lexos\\visualization\\bubbleviz\\__init__.py 121 122 123 124 125 126 127 128 129 130 131 132 def outline_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - bubbles [:, 2 ] - self . bubble_spacing plot ( ax , labels , colors , font_family = 'Arial' ) \u00a4 Draw the bubble plot. Parameters: Name Type Description Default ax matplotlib . axes . Axes The matplotlib axes. required labels List [ str ] The labels of the bubbles. required colors List [ str ] The colors of the bubbles. required font_family str The font family. 'Arial' Source code in lexos\\visualization\\bubbleviz\\__init__.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def plot ( self , ax : object , labels : List [ str ], colors : List [ str ], font_family : str = \"Arial\" , ): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. font_family (str): The font family. \"\"\" plt . rcParams [ \"font.family\" ] = font_family color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ] ) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = \"center\" , verticalalignment = \"center\" ) lexos.visualization.bubbleviz.BubbleChartModel \u00a4 Bases: BaseModel Ensure BubbleChart inputs are valid. Source code in lexos\\visualization\\bubbleviz\\__init__.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class BubbleChartModel ( BaseModel ): \"\"\"Ensure BubbleChart inputs are valid.\"\"\" terms : list area : list limit : Optional [ int ] = 100 title : Optional [ str ] = None bubble_spacing : Optional [ Union [ float , int ]] = 0.1 colors : Optional [ List [ str ]] = [ \"#5A69AF\" , \"#579E65\" , \"#F9C784\" , \"#FC944A\" , \"#F24C00\" , \"#00B825\" , ] figsize : Optional [ tuple ] = ( 15 , 15 ) font_family : Optional [ str ] = \"DejaVu Sans\" show : Optional [ bool ] = True filename : Optional [ str ] = None @validator ( \"terms\" ) def check_terms_not_empty ( cls , v ): \"\"\"Ensure `terms` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty term lists are not allowed.\" ) return v @validator ( \"area\" ) def check_area_not_empty ( cls , v ): \"\"\"Ensure `area` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty area lists are not allowed.\" ) return v check_area_not_empty ( v ) \u00a4 Ensure area is not empty. Source code in lexos\\visualization\\bubbleviz\\__init__.py 49 50 51 52 53 54 @validator ( \"area\" ) def check_area_not_empty ( cls , v ): \"\"\"Ensure `area` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty area lists are not allowed.\" ) return v check_terms_not_empty ( v ) \u00a4 Ensure terms is not empty. Source code in lexos\\visualization\\bubbleviz\\__init__.py 42 43 44 45 46 47 @validator ( \"terms\" ) def check_terms_not_empty ( cls , v ): \"\"\"Ensure `terms` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty term lists are not allowed.\" ) return v","title":"Bubbleviz"},{"location":"api/visualization/bubbleviz/#bubbleviz","text":"The Bubbleviz module produces bubble charts, a variant on word clouds, that produces circles (\"bubbles\") for each term based on the term's frequency in a text or collection of texts.","title":"Bubbleviz"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart","text":"Bubble chart. Source code in lexos\\visualization\\bubbleviz\\__init__.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 class BubbleChart : \"\"\"Bubble chart.\"\"\" def __init__ ( self , BubbleChartModel : BubbleChartModel ): \"\"\"Instantiate a bubble chart from a BubbleChartModel. Args: BubbleChartModel (BubbleChartModel): A BubbleChartModel Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" self . model = BubbleChartModel # Reduce the area to the limited number of terms area = np . asarray ( self . model . area [: self . model . limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = self . model . bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass () def center_of_mass ( self ) -> int : \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ]) def center_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ]) def outline_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - bubbles [:, 2 ] - self . bubble_spacing def check_collisions ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ]) def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ] def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = self . bubbles [ i , : 2 ] + orth * self . step_dist new_point2 = self . bubbles [ i , : 2 ] - orth * self . step_dist dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2 def plot ( self , ax : object , labels : List [ str ], colors : List [ str ], font_family : str = \"Arial\" , ): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. font_family (str): The font family. \"\"\" plt . rcParams [ \"font.family\" ] = font_family color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ] ) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = \"center\" , verticalalignment = \"center\" )","title":"BubbleChart"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.__init__","text":"Instantiate a bubble chart from a BubbleChartModel. Parameters: Name Type Description Default BubbleChartModel BubbleChartModel A BubbleChartModel required Notes If \"area\" is sorted, the results might look weird. If \"limit\" is raised too high, it will take a long time to generate the plot Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html . Source code in lexos\\visualization\\bubbleviz\\__init__.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __init__ ( self , BubbleChartModel : BubbleChartModel ): \"\"\"Instantiate a bubble chart from a BubbleChartModel. Args: BubbleChartModel (BubbleChartModel): A BubbleChartModel Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" self . model = BubbleChartModel # Reduce the area to the limited number of terms area = np . asarray ( self . model . area [: self . model . limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = self . model . bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass ()","title":"__init__()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.center_distance","text":"Centre distance. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Type Description np . ndarray np.ndarray: The centre distance. Source code in lexos\\visualization\\bubbleviz\\__init__.py 109 110 111 112 113 114 115 116 117 118 119 def center_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ])","title":"center_distance()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.center_of_mass","text":"Centre of mass. Returns: Name Type Description int int The centre of mass. Source code in lexos\\visualization\\bubbleviz\\__init__.py 101 102 103 104 105 106 107 def center_of_mass ( self ) -> int : \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ])","title":"center_of_mass()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.check_collisions","text":"Check collisions. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Name Type Description int int The length of the distance between bubbles. Source code in lexos\\visualization\\bubbleviz\\__init__.py 134 135 136 137 138 139 140 141 142 143 144 145 def check_collisions ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ])","title":"check_collisions()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.collapse","text":"Move bubbles to the center of mass. Parameters: Name Type Description Default n_iterations int Number of moves to perform. 50 Source code in lexos\\visualization\\bubbleviz\\__init__.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = self . bubbles [ i , : 2 ] + orth * self . step_dist new_point2 = self . bubbles [ i , : 2 ] - orth * self . step_dist dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2","title":"collapse()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.collides_with","text":"Collide. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Name Type Description int int The minimum index. Source code in lexos\\visualization\\bubbleviz\\__init__.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ]","title":"collides_with()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.outline_distance","text":"Outline distance. Parameters: Name Type Description Default bubble np . ndarray Bubble array. required bubbles np . ndarray Bubble array. required Returns: Name Type Description int int The outline distance. Source code in lexos\\visualization\\bubbleviz\\__init__.py 121 122 123 124 125 126 127 128 129 130 131 132 def outline_distance ( self , bubble : np . ndarray , bubbles : np . ndarray ) -> int : \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - bubbles [:, 2 ] - self . bubble_spacing","title":"outline_distance()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChart.plot","text":"Draw the bubble plot. Parameters: Name Type Description Default ax matplotlib . axes . Axes The matplotlib axes. required labels List [ str ] The labels of the bubbles. required colors List [ str ] The colors of the bubbles. required font_family str The font family. 'Arial' Source code in lexos\\visualization\\bubbleviz\\__init__.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def plot ( self , ax : object , labels : List [ str ], colors : List [ str ], font_family : str = \"Arial\" , ): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. font_family (str): The font family. \"\"\" plt . rcParams [ \"font.family\" ] = font_family color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ] ) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = \"center\" , verticalalignment = \"center\" )","title":"plot()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChartModel","text":"Bases: BaseModel Ensure BubbleChart inputs are valid. Source code in lexos\\visualization\\bubbleviz\\__init__.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class BubbleChartModel ( BaseModel ): \"\"\"Ensure BubbleChart inputs are valid.\"\"\" terms : list area : list limit : Optional [ int ] = 100 title : Optional [ str ] = None bubble_spacing : Optional [ Union [ float , int ]] = 0.1 colors : Optional [ List [ str ]] = [ \"#5A69AF\" , \"#579E65\" , \"#F9C784\" , \"#FC944A\" , \"#F24C00\" , \"#00B825\" , ] figsize : Optional [ tuple ] = ( 15 , 15 ) font_family : Optional [ str ] = \"DejaVu Sans\" show : Optional [ bool ] = True filename : Optional [ str ] = None @validator ( \"terms\" ) def check_terms_not_empty ( cls , v ): \"\"\"Ensure `terms` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty term lists are not allowed.\" ) return v @validator ( \"area\" ) def check_area_not_empty ( cls , v ): \"\"\"Ensure `area` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty area lists are not allowed.\" ) return v","title":"BubbleChartModel"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChartModel.check_area_not_empty","text":"Ensure area is not empty. Source code in lexos\\visualization\\bubbleviz\\__init__.py 49 50 51 52 53 54 @validator ( \"area\" ) def check_area_not_empty ( cls , v ): \"\"\"Ensure `area` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty area lists are not allowed.\" ) return v","title":"check_area_not_empty()"},{"location":"api/visualization/bubbleviz/#lexos.visualization.bubbleviz.BubbleChartModel.check_terms_not_empty","text":"Ensure terms is not empty. Source code in lexos\\visualization\\bubbleviz\\__init__.py 42 43 44 45 46 47 @validator ( \"terms\" ) def check_terms_not_empty ( cls , v ): \"\"\"Ensure `terms` is not empty.\"\"\" if v == []: raise ValueError ( \"Empty term lists are not allowed.\" ) return v","title":"check_terms_not_empty()"},{"location":"api/visualization/plotly/","text":"Plotly \u00a4 The plotly submodule contains methods of producing visualizations using the Plotly graphing library. Important There is currently a problem displaying Plotly plots in Jupyter notebooks (see issue #23 ). The workaround is to save the plot and open it in a different browser window. The plotly submodule contains components for generating word clouds and cluster analysis plots. Plotly Word Clouds \u00a4 lexos . visualization . plotly . cloud . wordcloud . make_wordcloud ( data , opts = None , round = None ) \u00a4 Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Parameters: Name Type Description Default data Union [ dict , list , object , str , tuple ] The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None Returns: Type Description object word cloud (object): A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\visualization\\plotly\\cloud\\wordcloud.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def make_wordcloud ( data : Union [ dict , list , object , str , tuple ], opts : dict = None , round : int = None ) -> object : \"\"\"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Args: data (Union[dict, list, object, str, tuple]): The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. Returns: word cloud (object): A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : if isinstance ( data , list ): data = { x [ 0 ]: x [ 1 ] for x in data } elif isinstance ( data , pd . DataFrame ): term_counts = data . to_dict ( orient = \"records\" ) try : data = { x [ \"terms\" ]: x [ \"count\" ] for x in term_counts } except KeyError : data = { x [ \"terms\" ]: x [ \"frequency\" ] for x in term_counts } wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) return wordcloud lexos . visualization . plotly . cloud . wordcloud . plot ( dtm , docs = None , opts = None , layout = None , show = True ) \u00a4 Convert a Python word cloud to a Plotly word cloud. This is some prototype code for generating word clouds in Plotly. Based on https://github.com/PrashantSaikia/Wordcloud-in-Plotly . This is really a case study because Plotly does not do good word clouds. One of the limitations is that WordCloud.layout_ always returns None for orientation and frequencies for counts. That limits the options for replicating its output. Run with: from lexos.visualization.plotly.cloud.wordcloud import plot plot ( dtm ) or wc = plot ( dtm , show = False ) wc . show () Parameters: Name Type Description Default dtm object A lexos.DTM object. required docs List [ str ] (List[str]): A list of document names to use. None opts dict (dict): A dict of options to pass to WordCloud. None layout dict (dict): A dict of options to pass to Plotly. None show bool (bool): Whether to show the plot. True Returns: Name Type Description object go . Figure A Plotly figure. Source code in lexos\\visualization\\plotly\\cloud\\wordcloud.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def plot ( dtm : object , docs : List [ str ] = None , opts : dict = None , layout : dict = None , show : bool = True , ) -> go . Figure : \"\"\"Convert a Python word cloud to a Plotly word cloud. This is some prototype code for generating word clouds in Plotly. Based on https://github.com/PrashantSaikia/Wordcloud-in-Plotly. This is really a case study because Plotly does not do good word clouds. One of the limitations is that `WordCloud.layout_` always returns `None` for orientation and frequencies for counts. That limits the options for replicating its output. Run with: ```python from lexos.visualization.plotly.cloud.wordcloud import plot plot(dtm) or wc = plot(dtm, show=False) wc.show() ``` Args: dtm (object): A lexos.DTM object. docs: (List[str]): A list of document names to use. opts: (dict): A dict of options to pass to WordCloud. layout: (dict): A dict of options to pass to Plotly. show: (bool): Whether to show the plot. Returns: object: A Plotly figure. \"\"\" word_list = [] freq_list = [] fontsize_list = [] position_list = [] orientation_list = [] color_list = [] layout_opts = { \"xaxis\" : { \"showgrid\" : False , \"showticklabels\" : False , \"zeroline\" : False }, \"yaxis\" : { \"showgrid\" : False , \"showticklabels\" : False , \"zeroline\" : False }, \"autosize\" : False , \"width\" : 750 , \"height\" : 750 , \"margin\" : { \"l\" : 50 , \"r\" : 50 , \"b\" : 100 , \"t\" : 100 , \"pad\" : 4 }, } if layout : for k , v in layout . items (): layout_opts [ k ] = v # Get the dtm table data = dtm . get_table () # Get the counts for the desired documents if docs : docs = [ \"terms\" ] + docs data = data [ docs ] . copy () # Create a new column with the total for each row data [ \"count\" ] = data . sum ( axis = 1 ) # Get the dtm sums else : data [ \"count\" ] = data . sum ( axis = 1 ) # data = data.rename({\"terms\": \"term\", \"sum\": \"count\"}, axis=1) # Ensure that the table only has terms and counts data = data [[ \"terms\" , \"count\" ]] . copy () # Create the word cloud if opts is None : opts = {} wc = make_wordcloud ( data , opts ) # Plot the word cloud for ( word , freq ), fontsize , position , orientation , color in wc . layout_ : word_list . append ( word ) freq_list . append ( freq ) fontsize_list . append ( fontsize ) position_list . append ( position ) orientation_list . append ( orientation ) color_list . append ( color ) # Get the positions x = [] y = [] for i in position_list : x . append ( i [ 0 ]) y . append ( i [ 1 ]) # Get the relative occurence frequencies new_freq_list = [] for i in freq_list : new_freq_list . append ( f \" { round ( i * 100 , 2 ) } %\" ) new_freq_list trace = go . Scatter ( x = x , y = y , textfont = dict ( size = fontsize_list , color = color_list ), hoverinfo = \"text\" , hovertext = [ f \" { w } : { f } \" for w , f in zip ( word_list , new_freq_list )], mode = \"text\" , text = word_list , ) # Set the laoyt and create the figure layout = go . Layout ( layout_opts ) fig = go . Figure ( data = [ trace ], layout = layout ) # Show the plot and/or return the figure if show : fig . show () return fig else : return fig Plotly Clustermaps \u00a4 lexos.visualization.plotly.cluster.clustermap.PlotlyClustermap \u00a4 PlotlyClustermap. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 class PlotlyClustermap (): \"\"\"PlotlyClustermap.\"\"\" def __init__ ( self , dtm : Any , metric : str = \"euclidean\" , method : str = \"average\" , hide_upper : bool = False , hide_side : bool = False , colorscale : str = \"Viridis\" , width : int = 600 , height : int = 600 , title : str = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), show : bool = False ): \"\"\"Initialise the Clustermap. Args: dtm (Any): The document-term-matrix \"\"\" self . dtm = dtm table = dtm . get_table () self . labels = table . columns . values . tolist ()[ 1 :] self . df = table . set_index ( \"terms\" ) . T self . metric = metric self . method = method self . hide_upper = hide_upper self . hide_side = hide_side self . colorscale = colorscale self . width = width self . height = height self . config = config self . title = title self . show = show self . build () def build ( self ) -> Any : \"\"\"Build a clustermap.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Initialize figure by creating upper dendrogram fig = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"bottom\" , labels = self . labels , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( fig [ \"data\" ])): fig [ \"data\" ][ i ][ \"yaxis\" ] = \"y2\" # Renders the upper dendrogram invisible # Also removes the labels, so you have to rely on hovertext if self . hide_upper : fig . for_each_trace ( lambda trace : trace . update ( visible = False )) # Create Side Dendrogram dendro_side = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"right\" , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( dendro_side [ \"data\" ])): dendro_side [ \"data\" ][ i ][ \"xaxis\" ] = \"x2\" # Add Side Dendrogram Data to Figure if not self . hide_side : for data in dendro_side [ \"data\" ]: fig . add_trace ( data ) # Create Heatmap dendro_leaves = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"ticktext\" ] dendro_leaves = list ( map ( int , dendro_leaves )) data_dist = pdist ( self . df ) heat_data = squareform ( data_dist ) heat_data = heat_data [ dendro_leaves , :] heat_data = heat_data [:, dendro_leaves ] num = len ( self . labels ) heatmap = [ go . Heatmap ( x = dendro_leaves , y = dendro_leaves , z = heat_data , colorscale = self . colorscale , hovertemplate = \"X: % {x} <br>Y: % {customdata} <br>Z: % {z} <extra></extra>\" , customdata = [[ label for x in range ( num )] for label in self . labels ] ) ] heatmap [ 0 ][ \"x\" ] = fig [ \"layout\" ][ \"xaxis\" ][ \"tickvals\" ] heatmap [ 0 ][ \"y\" ] = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"tickvals\" ] # Add Heatmap Data to Figure for data in heatmap : fig . add_trace ( data ) # Edit Layout fig . update_layout ({ \"width\" : self . width , \"height\" : self . height , \"showlegend\" : False , \"hovermode\" : \"closest\" , }) # Edit xaxis (dendrogram) if not self . hide_side : x = .15 else : x = 0 fig . update_layout ( xaxis = { \"domain\" : [ x , 1 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"ticks\" : \"\" }) # Edit xaxis2 (heatmap) fig . update_layout ( xaxis2 = { \"domain\" : [ 0 , .15 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) # Edit yaxis (heatmap) fig . update_layout ( yaxis = { \"domain\" : [ 0 , .85 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" , }) # Edit yaxis2 (dendrogram) fig . update_layout ( yaxis2 = { \"domain\" : [ .840 , .975 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) fig . update_layout ( margin = dict ( l = 0 ), paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , xaxis_tickfont = dict ( color = \"rgba(0,0,0,0)\" )) # Set the title if self . title : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Save the figure variable self . fig = fig # Show the plot if self . show : self . fig . show ( config = self . config ) def _get_colorscale ( self ) -> list : \"\"\"Get the colorscale as a list. Plotly continuous colorscales assign colors to the range [0, 1]. This function computes the intermediate color for any value in that range. Plotly doesn't make the colorscales directly accessible in a common format. Some are ready to use, and others are just swatche that need to be constructed into a colorscale. \"\"\" try : colorscale = plotly . colors . PLOTLY_SCALES [ self . colorscale ] except ValueError : swatch = getattr ( plotly . colors . sequential , self . colorscale ) colors , scale = plotly . colors . convert_colors_to_same_type ( swatch ) colorscale = plotly . colors . make_colorscale ( colors , scale = scale ) return colorscale def savefig ( self , filename : str ): \"\"\"Save the figure. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) def showfig ( self ): \"\"\"Show the figure.\"\"\" self . fig . show ( config = self . config ) def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) __init__ ( dtm , metric = 'euclidean' , method = 'average' , hide_upper = False , hide_side = False , colorscale = 'Viridis' , width = 600 , height = 600 , title = None , config = dict ( displaylogo = False , modeBarButtonsToRemove = [ 'toImage' , 'toggleSpikelines' ], scrollZoom = True ), show = False ) \u00a4 Initialise the Clustermap. Parameters: Name Type Description Default dtm Any The document-term-matrix required Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , dtm : Any , metric : str = \"euclidean\" , method : str = \"average\" , hide_upper : bool = False , hide_side : bool = False , colorscale : str = \"Viridis\" , width : int = 600 , height : int = 600 , title : str = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), show : bool = False ): \"\"\"Initialise the Clustermap. Args: dtm (Any): The document-term-matrix \"\"\" self . dtm = dtm table = dtm . get_table () self . labels = table . columns . values . tolist ()[ 1 :] self . df = table . set_index ( \"terms\" ) . T self . metric = metric self . method = method self . hide_upper = hide_upper self . hide_side = hide_side self . colorscale = colorscale self . width = width self . height = height self . config = config self . title = title self . show = show self . build () build () \u00a4 Build a clustermap. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def build ( self ) -> Any : \"\"\"Build a clustermap.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Initialize figure by creating upper dendrogram fig = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"bottom\" , labels = self . labels , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( fig [ \"data\" ])): fig [ \"data\" ][ i ][ \"yaxis\" ] = \"y2\" # Renders the upper dendrogram invisible # Also removes the labels, so you have to rely on hovertext if self . hide_upper : fig . for_each_trace ( lambda trace : trace . update ( visible = False )) # Create Side Dendrogram dendro_side = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"right\" , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( dendro_side [ \"data\" ])): dendro_side [ \"data\" ][ i ][ \"xaxis\" ] = \"x2\" # Add Side Dendrogram Data to Figure if not self . hide_side : for data in dendro_side [ \"data\" ]: fig . add_trace ( data ) # Create Heatmap dendro_leaves = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"ticktext\" ] dendro_leaves = list ( map ( int , dendro_leaves )) data_dist = pdist ( self . df ) heat_data = squareform ( data_dist ) heat_data = heat_data [ dendro_leaves , :] heat_data = heat_data [:, dendro_leaves ] num = len ( self . labels ) heatmap = [ go . Heatmap ( x = dendro_leaves , y = dendro_leaves , z = heat_data , colorscale = self . colorscale , hovertemplate = \"X: % {x} <br>Y: % {customdata} <br>Z: % {z} <extra></extra>\" , customdata = [[ label for x in range ( num )] for label in self . labels ] ) ] heatmap [ 0 ][ \"x\" ] = fig [ \"layout\" ][ \"xaxis\" ][ \"tickvals\" ] heatmap [ 0 ][ \"y\" ] = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"tickvals\" ] # Add Heatmap Data to Figure for data in heatmap : fig . add_trace ( data ) # Edit Layout fig . update_layout ({ \"width\" : self . width , \"height\" : self . height , \"showlegend\" : False , \"hovermode\" : \"closest\" , }) # Edit xaxis (dendrogram) if not self . hide_side : x = .15 else : x = 0 fig . update_layout ( xaxis = { \"domain\" : [ x , 1 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"ticks\" : \"\" }) # Edit xaxis2 (heatmap) fig . update_layout ( xaxis2 = { \"domain\" : [ 0 , .15 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) # Edit yaxis (heatmap) fig . update_layout ( yaxis = { \"domain\" : [ 0 , .85 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" , }) # Edit yaxis2 (dendrogram) fig . update_layout ( yaxis2 = { \"domain\" : [ .840 , .975 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) fig . update_layout ( margin = dict ( l = 0 ), paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , xaxis_tickfont = dict ( color = \"rgba(0,0,0,0)\" )) # Set the title if self . title : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Save the figure variable self . fig = fig # Show the plot if self . show : self . fig . show ( config = self . config ) savefig ( filename ) \u00a4 Save the figure. Parameters: Name Type Description Default filename str The name of the file to save. required Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 231 232 233 234 235 236 237 def savefig ( self , filename : str ): \"\"\"Save the figure. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) showfig () \u00a4 Show the figure. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 239 240 241 def showfig ( self ): \"\"\"Show the figure.\"\"\" self . fig . show ( config = self . config ) to_html ( show_link = False , output_type = 'div' , include_plotlyjs = False , filename = None , auto_open = False , config = None ) \u00a4 Convert the figure to HTML. Parameters: Name Type Description Default show_link bool For exporting to Plotly cloud services. Default is False . False output_type str If file , then the graph is saved as a standalone HTML file and plot returns None. If div , then plot returns a string that just contains the HTML that contains the graph and the script to generate the graph. Use file if you want to save and view a single graph at a time in a standalone HTML file. Use div if you are embedding these graphs in an existing HTML file. Default is div . 'div' include_plotlyjs bool If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a div . Default is False . False filename str The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is file . The default is temp-plot.html . None auto_open bool If True, open the saved file in a web browser after saving. This argument only applies if output_type is file . Default is False . False config dict A dict of parameters in the object's configuration. None Note This method uses plotly.offline.plot , which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/ . However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) lexos.visualization.plotly.cluster.dendrogram.PlotlyDendrogram \u00a4 PlotlyDendrogram. Typical usage: from lexos.visualization.plotly.cluster.dendrogram import PlotlyDendrogram dendrogram = PlotlyDendrogram ( dtm , show = True ) or dendrogram = PlotlyDendrogram ( dtm ) dendrogram . fig Needs some work in returning the figure as a figure and html and html div . Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class PlotlyDendrogram (): \"\"\"PlotlyDendrogram. Typical usage: ```python from lexos.visualization.plotly.cluster.dendrogram import PlotlyDendrogram dendrogram = PlotlyDendrogram(dtm, show=True) or dendrogram = PlotlyDendrogram(dtm) dendrogram.fig Needs some work in returning the figure as a figure and html and html div. ``` \"\"\" def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , get_leaves : bool = True , orientation : str = \"bottom\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , colorscale : List = None , hovertext : List = None , color_threshold : float = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), x_tickangle : int = 0 , y_tickangle : int = 0 , ** layout ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . get_leaves = get_leaves self . orientation = orientation self . title = title self . figsize = figsize self . show = show self . colorscale = colorscale self . hovertext = hovertext self . color_threshold = color_threshold self . config = config self . x_tickangle = x_tickangle self . y_tickangle = y_tickangle self . layout = layout # Get the dtm table self . df = self . dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . df . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . df = self . df . set_index ( \"terms\" ) . T # Build the dendrogram self . build () def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Create the figure self . fig = create_dendrogram ( self . df , labels = self . labels , distfun = distfun , linkagefun = linkagefun , orientation = self . orientation , colorscale = self . colorscale , hovertext = self . hovertext , color_threshold = self . color_threshold ) # Set the standard layout self . fig . update_layout ( margin = dict ( l = 0 , r = 0 , b = 0 , t = 0 , pad = 10 ), hovermode = 'x' , paper_bgcolor = \"rgba(0, 0, 0, 0)\" , plot_bgcolor = \"rgba(0, 0, 0, 0)\" , xaxis = dict ( showline = False , ticks = \"\" , tickangle = self . x_tickangle ), yaxis = dict ( showline = False , ticks = \"\" , tickangle = self . y_tickangle ) ) # Set the title if self . title is not None : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) self . fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Add user-configured layout self . fig . update_layout ( ** self . layout ) # Extend figure hack max_label_len = len ( max ( self . labels , key = len )) self . fig = _extend_figure ( self . fig , self . orientation , max_label_len ) if self . show : self . fig . show ( config = self . config ) def showfig ( self ): \"\"\"Show the figure. Calling `Dendrogram.fig` when the dendrogram has been set to `False` does not apply the config (there is no way to do this in Plotly. Calling `Dendrogram.showfig()` rebuilds the fig with the config applied. \"\"\" self . show = True self . build () def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) __init__ ( dtm , labels = None , metric = 'euclidean' , method = 'average' , truncate_mode = None , get_leaves = True , orientation = 'bottom' , title = None , figsize = ( 10 , 10 ), show = False , colorscale = None , hovertext = None , color_threshold = None , config = dict ( displaylogo = False , modeBarButtonsToRemove = [ 'toImage' , 'toggleSpikelines' ], scrollZoom = True ), x_tickangle = 0 , y_tickangle = 0 , ** layout ) \u00a4 Initialise the Dendrogram. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , get_leaves : bool = True , orientation : str = \"bottom\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , colorscale : List = None , hovertext : List = None , color_threshold : float = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), x_tickangle : int = 0 , y_tickangle : int = 0 , ** layout ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . get_leaves = get_leaves self . orientation = orientation self . title = title self . figsize = figsize self . show = show self . colorscale = colorscale self . hovertext = hovertext self . color_threshold = color_threshold self . config = config self . x_tickangle = x_tickangle self . y_tickangle = y_tickangle self . layout = layout # Get the dtm table self . df = self . dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . df . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . df = self . df . set_index ( \"terms\" ) . T # Build the dendrogram self . build () build () \u00a4 Build a dendrogram. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Create the figure self . fig = create_dendrogram ( self . df , labels = self . labels , distfun = distfun , linkagefun = linkagefun , orientation = self . orientation , colorscale = self . colorscale , hovertext = self . hovertext , color_threshold = self . color_threshold ) # Set the standard layout self . fig . update_layout ( margin = dict ( l = 0 , r = 0 , b = 0 , t = 0 , pad = 10 ), hovermode = 'x' , paper_bgcolor = \"rgba(0, 0, 0, 0)\" , plot_bgcolor = \"rgba(0, 0, 0, 0)\" , xaxis = dict ( showline = False , ticks = \"\" , tickangle = self . x_tickangle ), yaxis = dict ( showline = False , ticks = \"\" , tickangle = self . y_tickangle ) ) # Set the title if self . title is not None : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) self . fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Add user-configured layout self . fig . update_layout ( ** self . layout ) # Extend figure hack max_label_len = len ( max ( self . labels , key = len )) self . fig = _extend_figure ( self . fig , self . orientation , max_label_len ) if self . show : self . fig . show ( config = self . config ) showfig () \u00a4 Show the figure. Calling Dendrogram.fig when the dendrogram has been set to False does not apply the config (there is no way to do this in Plotly. Calling Dendrogram.showfig() rebuilds the fig with the config applied. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 168 169 170 171 172 173 174 175 176 177 def showfig ( self ): \"\"\"Show the figure. Calling `Dendrogram.fig` when the dendrogram has been set to `False` does not apply the config (there is no way to do this in Plotly. Calling `Dendrogram.showfig()` rebuilds the fig with the config applied. \"\"\" self . show = True self . build () to_html ( show_link = False , output_type = 'div' , include_plotlyjs = False , filename = None , auto_open = False , config = None ) \u00a4 Convert the figure to HTML. Parameters: Name Type Description Default show_link bool For exporting to Plotly cloud services. Default is False . False output_type str If file , then the graph is saved as a standalone HTML file and plot returns None. If div , then plot returns a string that just contains the HTML that contains the graph and the script to generate the graph. Use file if you want to save and view a single graph at a time in a standalone HTML file. Use div if you are embedding these graphs in an existing HTML file. Default is div . 'div' include_plotlyjs bool If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a div . Default is False . False filename str The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is file . The default is temp-plot.html . None auto_open bool If True, open the saved file in a web browser after saving. This argument only applies if output_type is file . Default is False . False config dict A dict of parameters in the object's configuration. None Note This method uses plotly.offline.plot , which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/ . However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) lexos . visualization . plotly . cluster . dendrogram . _get_dummy_scatter ( x_value ) \u00a4 Create a invisible scatter point at (x_value, 0). Use this function to help extend the margin of the dendrogram plot. Parameters: Name Type Description Default x_value float The desired x value we want to extend the margin to. required Returns: Name Type Description tuple Scatter An invisible scatter point at (x_value, 0). Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 def _get_dummy_scatter ( x_value : float ) -> Scatter : \"\"\"Create a invisible scatter point at (x_value, 0). Use this function to help extend the margin of the dendrogram plot. Args: x_value (float): The desired x value we want to extend the margin to. Returns: tuple: An invisible scatter point at (x_value, 0). \"\"\" return Scatter ( x = [ x_value ], y = [ 0 ], mode = \"markers\" , opacity = 0 , hoverinfo = \"skip\" )","title":"Plotly"},{"location":"api/visualization/plotly/#plotly","text":"The plotly submodule contains methods of producing visualizations using the Plotly graphing library. Important There is currently a problem displaying Plotly plots in Jupyter notebooks (see issue #23 ). The workaround is to save the plot and open it in a different browser window. The plotly submodule contains components for generating word clouds and cluster analysis plots.","title":"Plotly"},{"location":"api/visualization/plotly/#plotly-word-clouds","text":"","title":"Plotly Word Clouds"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cloud.wordcloud.make_wordcloud","text":"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Parameters: Name Type Description Default data Union [ dict , list , object , str , tuple ] The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None Returns: Type Description object word cloud (object): A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\visualization\\plotly\\cloud\\wordcloud.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def make_wordcloud ( data : Union [ dict , list , object , str , tuple ], opts : dict = None , round : int = None ) -> object : \"\"\"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Args: data (Union[dict, list, object, str, tuple]): The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. Returns: word cloud (object): A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : if isinstance ( data , list ): data = { x [ 0 ]: x [ 1 ] for x in data } elif isinstance ( data , pd . DataFrame ): term_counts = data . to_dict ( orient = \"records\" ) try : data = { x [ \"terms\" ]: x [ \"count\" ] for x in term_counts } except KeyError : data = { x [ \"terms\" ]: x [ \"frequency\" ] for x in term_counts } wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) return wordcloud","title":"make_wordcloud()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cloud.wordcloud.plot","text":"Convert a Python word cloud to a Plotly word cloud. This is some prototype code for generating word clouds in Plotly. Based on https://github.com/PrashantSaikia/Wordcloud-in-Plotly . This is really a case study because Plotly does not do good word clouds. One of the limitations is that WordCloud.layout_ always returns None for orientation and frequencies for counts. That limits the options for replicating its output. Run with: from lexos.visualization.plotly.cloud.wordcloud import plot plot ( dtm ) or wc = plot ( dtm , show = False ) wc . show () Parameters: Name Type Description Default dtm object A lexos.DTM object. required docs List [ str ] (List[str]): A list of document names to use. None opts dict (dict): A dict of options to pass to WordCloud. None layout dict (dict): A dict of options to pass to Plotly. None show bool (bool): Whether to show the plot. True Returns: Name Type Description object go . Figure A Plotly figure. Source code in lexos\\visualization\\plotly\\cloud\\wordcloud.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def plot ( dtm : object , docs : List [ str ] = None , opts : dict = None , layout : dict = None , show : bool = True , ) -> go . Figure : \"\"\"Convert a Python word cloud to a Plotly word cloud. This is some prototype code for generating word clouds in Plotly. Based on https://github.com/PrashantSaikia/Wordcloud-in-Plotly. This is really a case study because Plotly does not do good word clouds. One of the limitations is that `WordCloud.layout_` always returns `None` for orientation and frequencies for counts. That limits the options for replicating its output. Run with: ```python from lexos.visualization.plotly.cloud.wordcloud import plot plot(dtm) or wc = plot(dtm, show=False) wc.show() ``` Args: dtm (object): A lexos.DTM object. docs: (List[str]): A list of document names to use. opts: (dict): A dict of options to pass to WordCloud. layout: (dict): A dict of options to pass to Plotly. show: (bool): Whether to show the plot. Returns: object: A Plotly figure. \"\"\" word_list = [] freq_list = [] fontsize_list = [] position_list = [] orientation_list = [] color_list = [] layout_opts = { \"xaxis\" : { \"showgrid\" : False , \"showticklabels\" : False , \"zeroline\" : False }, \"yaxis\" : { \"showgrid\" : False , \"showticklabels\" : False , \"zeroline\" : False }, \"autosize\" : False , \"width\" : 750 , \"height\" : 750 , \"margin\" : { \"l\" : 50 , \"r\" : 50 , \"b\" : 100 , \"t\" : 100 , \"pad\" : 4 }, } if layout : for k , v in layout . items (): layout_opts [ k ] = v # Get the dtm table data = dtm . get_table () # Get the counts for the desired documents if docs : docs = [ \"terms\" ] + docs data = data [ docs ] . copy () # Create a new column with the total for each row data [ \"count\" ] = data . sum ( axis = 1 ) # Get the dtm sums else : data [ \"count\" ] = data . sum ( axis = 1 ) # data = data.rename({\"terms\": \"term\", \"sum\": \"count\"}, axis=1) # Ensure that the table only has terms and counts data = data [[ \"terms\" , \"count\" ]] . copy () # Create the word cloud if opts is None : opts = {} wc = make_wordcloud ( data , opts ) # Plot the word cloud for ( word , freq ), fontsize , position , orientation , color in wc . layout_ : word_list . append ( word ) freq_list . append ( freq ) fontsize_list . append ( fontsize ) position_list . append ( position ) orientation_list . append ( orientation ) color_list . append ( color ) # Get the positions x = [] y = [] for i in position_list : x . append ( i [ 0 ]) y . append ( i [ 1 ]) # Get the relative occurence frequencies new_freq_list = [] for i in freq_list : new_freq_list . append ( f \" { round ( i * 100 , 2 ) } %\" ) new_freq_list trace = go . Scatter ( x = x , y = y , textfont = dict ( size = fontsize_list , color = color_list ), hoverinfo = \"text\" , hovertext = [ f \" { w } : { f } \" for w , f in zip ( word_list , new_freq_list )], mode = \"text\" , text = word_list , ) # Set the laoyt and create the figure layout = go . Layout ( layout_opts ) fig = go . Figure ( data = [ trace ], layout = layout ) # Show the plot and/or return the figure if show : fig . show () return fig else : return fig","title":"plot()"},{"location":"api/visualization/plotly/#plotly-clustermaps","text":"","title":"Plotly Clustermaps"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.clustermap.PlotlyClustermap","text":"PlotlyClustermap. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 class PlotlyClustermap (): \"\"\"PlotlyClustermap.\"\"\" def __init__ ( self , dtm : Any , metric : str = \"euclidean\" , method : str = \"average\" , hide_upper : bool = False , hide_side : bool = False , colorscale : str = \"Viridis\" , width : int = 600 , height : int = 600 , title : str = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), show : bool = False ): \"\"\"Initialise the Clustermap. Args: dtm (Any): The document-term-matrix \"\"\" self . dtm = dtm table = dtm . get_table () self . labels = table . columns . values . tolist ()[ 1 :] self . df = table . set_index ( \"terms\" ) . T self . metric = metric self . method = method self . hide_upper = hide_upper self . hide_side = hide_side self . colorscale = colorscale self . width = width self . height = height self . config = config self . title = title self . show = show self . build () def build ( self ) -> Any : \"\"\"Build a clustermap.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Initialize figure by creating upper dendrogram fig = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"bottom\" , labels = self . labels , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( fig [ \"data\" ])): fig [ \"data\" ][ i ][ \"yaxis\" ] = \"y2\" # Renders the upper dendrogram invisible # Also removes the labels, so you have to rely on hovertext if self . hide_upper : fig . for_each_trace ( lambda trace : trace . update ( visible = False )) # Create Side Dendrogram dendro_side = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"right\" , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( dendro_side [ \"data\" ])): dendro_side [ \"data\" ][ i ][ \"xaxis\" ] = \"x2\" # Add Side Dendrogram Data to Figure if not self . hide_side : for data in dendro_side [ \"data\" ]: fig . add_trace ( data ) # Create Heatmap dendro_leaves = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"ticktext\" ] dendro_leaves = list ( map ( int , dendro_leaves )) data_dist = pdist ( self . df ) heat_data = squareform ( data_dist ) heat_data = heat_data [ dendro_leaves , :] heat_data = heat_data [:, dendro_leaves ] num = len ( self . labels ) heatmap = [ go . Heatmap ( x = dendro_leaves , y = dendro_leaves , z = heat_data , colorscale = self . colorscale , hovertemplate = \"X: % {x} <br>Y: % {customdata} <br>Z: % {z} <extra></extra>\" , customdata = [[ label for x in range ( num )] for label in self . labels ] ) ] heatmap [ 0 ][ \"x\" ] = fig [ \"layout\" ][ \"xaxis\" ][ \"tickvals\" ] heatmap [ 0 ][ \"y\" ] = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"tickvals\" ] # Add Heatmap Data to Figure for data in heatmap : fig . add_trace ( data ) # Edit Layout fig . update_layout ({ \"width\" : self . width , \"height\" : self . height , \"showlegend\" : False , \"hovermode\" : \"closest\" , }) # Edit xaxis (dendrogram) if not self . hide_side : x = .15 else : x = 0 fig . update_layout ( xaxis = { \"domain\" : [ x , 1 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"ticks\" : \"\" }) # Edit xaxis2 (heatmap) fig . update_layout ( xaxis2 = { \"domain\" : [ 0 , .15 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) # Edit yaxis (heatmap) fig . update_layout ( yaxis = { \"domain\" : [ 0 , .85 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" , }) # Edit yaxis2 (dendrogram) fig . update_layout ( yaxis2 = { \"domain\" : [ .840 , .975 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) fig . update_layout ( margin = dict ( l = 0 ), paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , xaxis_tickfont = dict ( color = \"rgba(0,0,0,0)\" )) # Set the title if self . title : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Save the figure variable self . fig = fig # Show the plot if self . show : self . fig . show ( config = self . config ) def _get_colorscale ( self ) -> list : \"\"\"Get the colorscale as a list. Plotly continuous colorscales assign colors to the range [0, 1]. This function computes the intermediate color for any value in that range. Plotly doesn't make the colorscales directly accessible in a common format. Some are ready to use, and others are just swatche that need to be constructed into a colorscale. \"\"\" try : colorscale = plotly . colors . PLOTLY_SCALES [ self . colorscale ] except ValueError : swatch = getattr ( plotly . colors . sequential , self . colorscale ) colors , scale = plotly . colors . convert_colors_to_same_type ( swatch ) colorscale = plotly . colors . make_colorscale ( colors , scale = scale ) return colorscale def savefig ( self , filename : str ): \"\"\"Save the figure. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) def showfig ( self ): \"\"\"Show the figure.\"\"\" self . fig . show ( config = self . config ) def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config )","title":"PlotlyClustermap"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.clustermap.PlotlyClustermap.__init__","text":"Initialise the Clustermap. Parameters: Name Type Description Default dtm Any The document-term-matrix required Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , dtm : Any , metric : str = \"euclidean\" , method : str = \"average\" , hide_upper : bool = False , hide_side : bool = False , colorscale : str = \"Viridis\" , width : int = 600 , height : int = 600 , title : str = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), show : bool = False ): \"\"\"Initialise the Clustermap. Args: dtm (Any): The document-term-matrix \"\"\" self . dtm = dtm table = dtm . get_table () self . labels = table . columns . values . tolist ()[ 1 :] self . df = table . set_index ( \"terms\" ) . T self . metric = metric self . method = method self . hide_upper = hide_upper self . hide_side = hide_side self . colorscale = colorscale self . width = width self . height = height self . config = config self . title = title self . show = show self . build ()","title":"__init__()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.clustermap.PlotlyClustermap.build","text":"Build a clustermap. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def build ( self ) -> Any : \"\"\"Build a clustermap.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Initialize figure by creating upper dendrogram fig = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"bottom\" , labels = self . labels , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( fig [ \"data\" ])): fig [ \"data\" ][ i ][ \"yaxis\" ] = \"y2\" # Renders the upper dendrogram invisible # Also removes the labels, so you have to rely on hovertext if self . hide_upper : fig . for_each_trace ( lambda trace : trace . update ( visible = False )) # Create Side Dendrogram dendro_side = create_dendrogram ( self . df , distfun = distfun , linkagefun = linkagefun , orientation = \"right\" , colorscale = self . _get_colorscale (), color_threshold = None ) for i in range ( len ( dendro_side [ \"data\" ])): dendro_side [ \"data\" ][ i ][ \"xaxis\" ] = \"x2\" # Add Side Dendrogram Data to Figure if not self . hide_side : for data in dendro_side [ \"data\" ]: fig . add_trace ( data ) # Create Heatmap dendro_leaves = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"ticktext\" ] dendro_leaves = list ( map ( int , dendro_leaves )) data_dist = pdist ( self . df ) heat_data = squareform ( data_dist ) heat_data = heat_data [ dendro_leaves , :] heat_data = heat_data [:, dendro_leaves ] num = len ( self . labels ) heatmap = [ go . Heatmap ( x = dendro_leaves , y = dendro_leaves , z = heat_data , colorscale = self . colorscale , hovertemplate = \"X: % {x} <br>Y: % {customdata} <br>Z: % {z} <extra></extra>\" , customdata = [[ label for x in range ( num )] for label in self . labels ] ) ] heatmap [ 0 ][ \"x\" ] = fig [ \"layout\" ][ \"xaxis\" ][ \"tickvals\" ] heatmap [ 0 ][ \"y\" ] = dendro_side [ \"layout\" ][ \"yaxis\" ][ \"tickvals\" ] # Add Heatmap Data to Figure for data in heatmap : fig . add_trace ( data ) # Edit Layout fig . update_layout ({ \"width\" : self . width , \"height\" : self . height , \"showlegend\" : False , \"hovermode\" : \"closest\" , }) # Edit xaxis (dendrogram) if not self . hide_side : x = .15 else : x = 0 fig . update_layout ( xaxis = { \"domain\" : [ x , 1 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"ticks\" : \"\" }) # Edit xaxis2 (heatmap) fig . update_layout ( xaxis2 = { \"domain\" : [ 0 , .15 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) # Edit yaxis (heatmap) fig . update_layout ( yaxis = { \"domain\" : [ 0 , .85 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" , }) # Edit yaxis2 (dendrogram) fig . update_layout ( yaxis2 = { \"domain\" : [ .840 , .975 ], \"mirror\" : False , \"showgrid\" : False , \"showline\" : False , \"zeroline\" : False , \"showticklabels\" : False , \"ticks\" : \"\" }) fig . update_layout ( margin = dict ( l = 0 ), paper_bgcolor = \"rgba(0,0,0,0)\" , plot_bgcolor = \"rgba(0,0,0,0)\" , xaxis_tickfont = dict ( color = \"rgba(0,0,0,0)\" )) # Set the title if self . title : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Save the figure variable self . fig = fig # Show the plot if self . show : self . fig . show ( config = self . config )","title":"build()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.clustermap.PlotlyClustermap.savefig","text":"Save the figure. Parameters: Name Type Description Default filename str The name of the file to save. required Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 231 232 233 234 235 236 237 def savefig ( self , filename : str ): \"\"\"Save the figure. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename )","title":"savefig()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.clustermap.PlotlyClustermap.showfig","text":"Show the figure. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 239 240 241 def showfig ( self ): \"\"\"Show the figure.\"\"\" self . fig . show ( config = self . config )","title":"showfig()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.clustermap.PlotlyClustermap.to_html","text":"Convert the figure to HTML. Parameters: Name Type Description Default show_link bool For exporting to Plotly cloud services. Default is False . False output_type str If file , then the graph is saved as a standalone HTML file and plot returns None. If div , then plot returns a string that just contains the HTML that contains the graph and the script to generate the graph. Use file if you want to save and view a single graph at a time in a standalone HTML file. Use div if you are embedding these graphs in an existing HTML file. Default is div . 'div' include_plotlyjs bool If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a div . Default is False . False filename str The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is file . The default is temp-plot.html . None auto_open bool If True, open the saved file in a web browser after saving. This argument only applies if output_type is file . Default is False . False config dict A dict of parameters in the object's configuration. None Note This method uses plotly.offline.plot , which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/ . However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. Source code in lexos\\visualization\\plotly\\cluster\\clustermap.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config )","title":"to_html()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.dendrogram.PlotlyDendrogram","text":"PlotlyDendrogram. Typical usage: from lexos.visualization.plotly.cluster.dendrogram import PlotlyDendrogram dendrogram = PlotlyDendrogram ( dtm , show = True ) or dendrogram = PlotlyDendrogram ( dtm ) dendrogram . fig Needs some work in returning the figure as a figure and html and html div . Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class PlotlyDendrogram (): \"\"\"PlotlyDendrogram. Typical usage: ```python from lexos.visualization.plotly.cluster.dendrogram import PlotlyDendrogram dendrogram = PlotlyDendrogram(dtm, show=True) or dendrogram = PlotlyDendrogram(dtm) dendrogram.fig Needs some work in returning the figure as a figure and html and html div. ``` \"\"\" def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , get_leaves : bool = True , orientation : str = \"bottom\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , colorscale : List = None , hovertext : List = None , color_threshold : float = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), x_tickangle : int = 0 , y_tickangle : int = 0 , ** layout ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . get_leaves = get_leaves self . orientation = orientation self . title = title self . figsize = figsize self . show = show self . colorscale = colorscale self . hovertext = hovertext self . color_threshold = color_threshold self . config = config self . x_tickangle = x_tickangle self . y_tickangle = y_tickangle self . layout = layout # Get the dtm table self . df = self . dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . df . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . df = self . df . set_index ( \"terms\" ) . T # Build the dendrogram self . build () def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Create the figure self . fig = create_dendrogram ( self . df , labels = self . labels , distfun = distfun , linkagefun = linkagefun , orientation = self . orientation , colorscale = self . colorscale , hovertext = self . hovertext , color_threshold = self . color_threshold ) # Set the standard layout self . fig . update_layout ( margin = dict ( l = 0 , r = 0 , b = 0 , t = 0 , pad = 10 ), hovermode = 'x' , paper_bgcolor = \"rgba(0, 0, 0, 0)\" , plot_bgcolor = \"rgba(0, 0, 0, 0)\" , xaxis = dict ( showline = False , ticks = \"\" , tickangle = self . x_tickangle ), yaxis = dict ( showline = False , ticks = \"\" , tickangle = self . y_tickangle ) ) # Set the title if self . title is not None : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) self . fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Add user-configured layout self . fig . update_layout ( ** self . layout ) # Extend figure hack max_label_len = len ( max ( self . labels , key = len )) self . fig = _extend_figure ( self . fig , self . orientation , max_label_len ) if self . show : self . fig . show ( config = self . config ) def showfig ( self ): \"\"\"Show the figure. Calling `Dendrogram.fig` when the dendrogram has been set to `False` does not apply the config (there is no way to do this in Plotly. Calling `Dendrogram.showfig()` rebuilds the fig with the config applied. \"\"\" self . show = True self . build () def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config )","title":"PlotlyDendrogram"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.dendrogram.PlotlyDendrogram.__init__","text":"Initialise the Dendrogram. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , dtm : Any , labels : List [ str ] = None , metric : str = \"euclidean\" , method : str = \"average\" , truncate_mode : str = None , get_leaves : bool = True , orientation : str = \"bottom\" , title : str = None , figsize : tuple = ( 10 , 10 ), show : bool = False , colorscale : List = None , hovertext : List = None , color_threshold : float = None , config : dict = dict ( displaylogo = False , modeBarButtonsToRemove = [ \"toImage\" , \"toggleSpikelines\" ], scrollZoom = True ), x_tickangle : int = 0 , y_tickangle : int = 0 , ** layout ) -> dict : \"\"\"Initialise the Dendrogram.\"\"\" # Create an empty plot for matplotlib self . dtm = dtm self . labels = labels self . metric = metric self . method = method self . truncate_mode = truncate_mode self . get_leaves = get_leaves self . orientation = orientation self . title = title self . figsize = figsize self . show = show self . colorscale = colorscale self . hovertext = hovertext self . color_threshold = color_threshold self . config = config self . x_tickangle = x_tickangle self . y_tickangle = y_tickangle self . layout = layout # Get the dtm table self . df = self . dtm . get_table () # Use default labels from the DTM table if self . labels is None : self . labels = self . df . columns . values . tolist ()[ 1 :] # Set \"terms\" as the index and transpose the table self . df = self . df . set_index ( \"terms\" ) . T # Build the dendrogram self . build ()","title":"__init__()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.dendrogram.PlotlyDendrogram.build","text":"Build a dendrogram. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def build ( self ): \"\"\"Build a dendrogram.\"\"\" # Set the distance and linkage metrics def distfun ( x ): \"\"\"Get the pairwise distance matrix. Args: x (Any): The distance matrix. Returns: Any: The pairwise distance matrix. \"\"\" return pdist ( x , metric = self . metric ) def linkagefun ( x ): \"\"\"Get the hierarchical clustering encoded as a linkage matrix. Args: x (Any): The pairwise distance matrix. Returns: Any: The linkage matrix. \"\"\" return sch . linkage ( x , self . method ) # Create the figure self . fig = create_dendrogram ( self . df , labels = self . labels , distfun = distfun , linkagefun = linkagefun , orientation = self . orientation , colorscale = self . colorscale , hovertext = self . hovertext , color_threshold = self . color_threshold ) # Set the standard layout self . fig . update_layout ( margin = dict ( l = 0 , r = 0 , b = 0 , t = 0 , pad = 10 ), hovermode = 'x' , paper_bgcolor = \"rgba(0, 0, 0, 0)\" , plot_bgcolor = \"rgba(0, 0, 0, 0)\" , xaxis = dict ( showline = False , ticks = \"\" , tickangle = self . x_tickangle ), yaxis = dict ( showline = False , ticks = \"\" , tickangle = self . y_tickangle ) ) # Set the title if self . title is not None : title = dict ( text = self . title , x = 0.5 , y = 0.95 , xanchor = \"center\" , yanchor = \"top\" ) self . fig . update_layout ( title = title , margin = dict ( t = 40 ) ) # Add user-configured layout self . fig . update_layout ( ** self . layout ) # Extend figure hack max_label_len = len ( max ( self . labels , key = len )) self . fig = _extend_figure ( self . fig , self . orientation , max_label_len ) if self . show : self . fig . show ( config = self . config )","title":"build()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.dendrogram.PlotlyDendrogram.showfig","text":"Show the figure. Calling Dendrogram.fig when the dendrogram has been set to False does not apply the config (there is no way to do this in Plotly. Calling Dendrogram.showfig() rebuilds the fig with the config applied. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 168 169 170 171 172 173 174 175 176 177 def showfig ( self ): \"\"\"Show the figure. Calling `Dendrogram.fig` when the dendrogram has been set to `False` does not apply the config (there is no way to do this in Plotly. Calling `Dendrogram.showfig()` rebuilds the fig with the config applied. \"\"\" self . show = True self . build ()","title":"showfig()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.dendrogram.PlotlyDendrogram.to_html","text":"Convert the figure to HTML. Parameters: Name Type Description Default show_link bool For exporting to Plotly cloud services. Default is False . False output_type str If file , then the graph is saved as a standalone HTML file and plot returns None. If div , then plot returns a string that just contains the HTML that contains the graph and the script to generate the graph. Use file if you want to save and view a single graph at a time in a standalone HTML file. Use div if you are embedding these graphs in an existing HTML file. Default is div . 'div' include_plotlyjs bool If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a div . Default is False . False filename str The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is file . The default is temp-plot.html . None auto_open bool If True, open the saved file in a web browser after saving. This argument only applies if output_type is file . Default is False . False config dict A dict of parameters in the object's configuration. None Note This method uses plotly.offline.plot , which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/ . However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def to_html ( self , show_link : bool = False , output_type : str = \"div\" , include_plotlyjs : bool = False , filename : str = None , auto_open : bool = False , config : dict = None ): \"\"\"Convert the figure to HTML. Args: show_link (bool): For exporting to Plotly cloud services. Default is `False`. output_type (str): If `file`, then the graph is saved as a standalone HTML file and plot returns None. If `div`, then plot returns a string that just contains the HTML <div> that contains the graph and the script to generate the graph. Use `file` if you want to save and view a single graph at a time in a standalone HTML file. Use `div` if you are embedding these graphs in an existing HTML file. Default is `div`. include_plotlyjs (bool): If True, include the plotly.js source code in the output file or string, which is good for standalone web pages but makes for very large files. If you are embedding the graph in a webpage, it is better to import the plotly.js library and use a `div`. Default is `False`. filename (str): The local filename to save the outputted chart to. If the filename already exists, it will be overwritten. This argument only applies if output_type is `file`. The default is `temp-plot.html`. auto_open (bool): If True, open the saved file in a web browser after saving. This argument only applies if output_type is `file`. Default is `False`. config (dict): A dict of parameters in the object's configuration. Note: This method uses `plotly.offline.plot`, which no longer appears to be documented. It has been replaced by renderers: https://plotly.com/python/renderers/. However, there does not appear to be an HTML renderer, so no attempt has been made to use the new functionality. \"\"\" if self . config : config = self . config if filename and output_type == \"file\" : return _plot ( self . fig , show_link = show_link , output_type = \"file\" , include_plotlyjs = include_plotlyjs , filename = filename , auto_open = auto_open , config = config ) elif filename and output_type == \"div\" : pl = _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config ) with open ( filename , \"w\" ) as f : f . write ( pl ) return pl else : return _plot ( self . fig , show_link = show_link , output_type = \"div\" , include_plotlyjs = include_plotlyjs , auto_open = auto_open , config = config )","title":"to_html()"},{"location":"api/visualization/plotly/#lexos.visualization.plotly.cluster.dendrogram._get_dummy_scatter","text":"Create a invisible scatter point at (x_value, 0). Use this function to help extend the margin of the dendrogram plot. Parameters: Name Type Description Default x_value float The desired x value we want to extend the margin to. required Returns: Name Type Description tuple Scatter An invisible scatter point at (x_value, 0). Source code in lexos\\visualization\\plotly\\cluster\\dendrogram.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 def _get_dummy_scatter ( x_value : float ) -> Scatter : \"\"\"Create a invisible scatter point at (x_value, 0). Use this function to help extend the margin of the dendrogram plot. Args: x_value (float): The desired x value we want to extend the margin to. Returns: tuple: An invisible scatter point at (x_value, 0). \"\"\" return Scatter ( x = [ x_value ], y = [ 0 ], mode = \"markers\" , opacity = 0 , hoverinfo = \"skip\" )","title":"_get_dummy_scatter()"},{"location":"api/visualization/seaborn/","text":"Seaborn \u00a4 The seaborn submodule contains methods of producing visualizations using the Seaborn visualization library. The seaborn submodule contains components for generating word clouds and cluster analysis plots. Seaborn Clustermaps \u00a4 lexos.visualization.seaborn.cluster.clustermap.ClusterMap \u00a4 ClusterMap. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 class ClusterMap : \"\"\"ClusterMap.\"\"\" def __init__ ( self , dtm : Any , z_score : int = 1 , labels : List [ str ] = None , pivot_kws : Dict [ str , str ] = None , method : str = \"average\" , metric : str = \"euclidean\" , standard_scale : int = None , figsize : tuple = ( 8 , 8 ), cbar_kws : dict = None , row_cluster : bool = True , col_cluster : bool = True , row_linkage : bool = None , col_linkage : bool = None , row_colors : Union [ list , pd . DataFrame , pd . Series ] = None , col_colors : Union [ list , pd . DataFrame , pd . Series ] = None , mask : Union [ np . ndarray , pd . DataFrame ] = None , dendrogram_ratio : Union [ float , Tuple [ float ]] = ( 0.1 , 0.2 ), colors_ratio : float = 0.03 , cbar_pos : Tuple [ str ] = ( 0.02 , 0.32 , 0.03 , 0.2 ), tree_kws : dict = None , center : int = 0 , cmap : str = \"vlag\" , linewidths : float = 0.75 , show : bool = False , title : str = None , ) -> figure . Figure : \"\"\"Initialize the ClusterMap object. Args: dtm (Any): The data to cluster. z_score (int, optional): The z-score to use. Defaults to 1. labels (List[str], optional): The labels to use. Defaults to None. pivot_kws (Dict[str, str], optional): The pivot kwargs. Defaults to None. method (str, optional): The method to use. Defaults to \"average\". metric (str, optional): The metric to use. Defaults to \"euclidean\". standard_scale (int, optional): The standard scale to use. Defaults to None. figsize (tuple, optional): The figure size to use. Defaults to (8, 8). cbar_kws (dict, optional): The cbar kwargs. Defaults to None. row_cluster (bool, optional): Whether to cluster the rows. Defaults to True. col_cluster (bool, optional): Whether to cluster the columns. Defaults to True. row_linkage (bool, optional): Whether to use row linkage. Defaults to None. col_linkage (bool, optional): Whether to use column linkage. Defaults to None. row_colors (Union[list, pd.DataFrame, pd.Series], optional): The row colors. Defaults to None. col_colors (Union[list, pd.DataFrame, pd.Series], optional): The column colors. Defaults to None. mask (Union[np.ndarray, pd.DataFrame], optional): The mask to use. Defaults to None. dendrogram_ratio (Union[float, Tuple[float]], optional): The dendrogram ratio to use. Defaults to (.1, .2). colors_ratio (float, optional): The colors ratio to use. Defaults to 0.03. cbar_pos (Tuple[str], optional): The cbar position to use. Defaults to (.02, .32, .03, .2). tree_kws (dict, optional): The tree kwargs. Defaults to None. center (int, optional): The center to use. Defaults to 0. cmap (str, optional): The cmap to use. Defaults to \"vlag\". linewidths (float, optional): The linewidths to use. Defaults to .75. show (bool, optional): Whether to show the figure. Defaults to False. title (str, optional): The title to use. Defaults to None. Returns: matplotlib.figure.Figure: The figure. \"\"\" self . dtm = dtm self . z_score = z_score self . labels = labels self . figsize = figsize self . show = show self . method = method self . metric = metric self . standard_scale = standard_scale self . title = title self . row_colors = row_colors self . col_colors = col_colors self . pivot_kws = pivot_kws self . cbar_kws = cbar_kws self . row_cluster = row_cluster self . col_cluster = col_cluster self . row_linkage = row_linkage self . col_linkage = col_linkage self . mask = mask self . dendrogram_ratio = dendrogram_ratio self . colors_ratio = colors_ratio self . cbar_pos = cbar_pos self . tree_kws = tree_kws self . center = center self . cmap = cmap self . linewidths = linewidths self . df = dtm . get_table () self . build () def build ( self ): \"\"\"Build the clustermap.\"\"\" # Get the labels if necessary if self . labels is None : # raise ValueError(\"Please provide labels.\") self . labels = self . df . columns . values . tolist ()[ 1 :] # Convert palette to vectors drawn on the side of the matrix if self . row_colors is None or self . col_colors is None : column_pal = sns . husl_palette ( 8 , s = 0.45 ) column_lut = dict ( zip ( map ( str , self . df ), column_pal )) column_colors = pd . Series ( self . labels , index = self . df . columns [ 1 :]) . map ( column_lut ) if self . row_colors is None : self . row_colors = column_colors if self . col_colors is None : self . col_colors = column_colors # Perform the cluster g = sns . clustermap ( self . df . corr (), cmap = self . cmap , method = self . method , metric = self . metric , figsize = self . figsize , row_colors = self . row_colors , col_colors = self . col_colors , row_cluster = self . row_cluster , col_cluster = self . col_cluster , center = self . center , linewidths = self . linewidths , z_score = self . z_score , pivot_kws = self . pivot_kws , standard_scale = self . standard_scale , cbar_kws = self . cbar_kws , row_linkage = self . row_linkage , col_linkage = self . col_linkage , mask = self . mask , dendrogram_ratio = self . dendrogram_ratio , colors_ratio = self . colors_ratio , cbar_pos = self . cbar_pos , tree_kws = self . tree_kws , ) # Remove the dendrogram on the left g . ax_row_dendrogram . remove () # Add the title if self . title : g . fig . suptitle ( self . title , y = 1.05 ) # Save the fig variable self . fig = g . fig # Suppress the output if not self . show : plt . close () return self . fig def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `ClusterMap.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig __init__ ( dtm , z_score = 1 , labels = None , pivot_kws = None , method = 'average' , metric = 'euclidean' , standard_scale = None , figsize = ( 8 , 8 ), cbar_kws = None , row_cluster = True , col_cluster = True , row_linkage = None , col_linkage = None , row_colors = None , col_colors = None , mask = None , dendrogram_ratio = ( 0.1 , 0.2 ), colors_ratio = 0.03 , cbar_pos = ( 0.02 , 0.32 , 0.03 , 0.2 ), tree_kws = None , center = 0 , cmap = 'vlag' , linewidths = 0.75 , show = False , title = None ) \u00a4 Initialize the ClusterMap object. Parameters: Name Type Description Default dtm Any The data to cluster. required z_score int The z-score to use. Defaults to 1. 1 labels List [ str ] The labels to use. Defaults to None. None pivot_kws Dict [ str , str ] The pivot kwargs. Defaults to None. None method str The method to use. Defaults to \"average\". 'average' metric str The metric to use. Defaults to \"euclidean\". 'euclidean' standard_scale int The standard scale to use. Defaults to None. None figsize tuple The figure size to use. Defaults to (8, 8). (8, 8) cbar_kws dict The cbar kwargs. Defaults to None. None row_cluster bool Whether to cluster the rows. Defaults to True. True col_cluster bool Whether to cluster the columns. Defaults to True. True row_linkage bool Whether to use row linkage. Defaults to None. None col_linkage bool Whether to use column linkage. Defaults to None. None row_colors Union [ list , pd . DataFrame , pd . Series ] The row colors. Defaults to None. None col_colors Union [ list , pd . DataFrame , pd . Series ] The column colors. Defaults to None. None mask Union [ np . ndarray , pd . DataFrame ] The mask to use. Defaults to None. None dendrogram_ratio Union [ float , Tuple [ float ]] The dendrogram ratio to use. Defaults to (.1, .2). (0.1, 0.2) colors_ratio float The colors ratio to use. Defaults to 0.03. 0.03 cbar_pos Tuple [ str ] The cbar position to use. Defaults to (.02, .32, .03, .2). (0.02, 0.32, 0.03, 0.2) tree_kws dict The tree kwargs. Defaults to None. None center int The center to use. Defaults to 0. 0 cmap str The cmap to use. Defaults to \"vlag\". 'vlag' linewidths float The linewidths to use. Defaults to .75. 0.75 show bool Whether to show the figure. Defaults to False. False title str The title to use. Defaults to None. None Returns: Type Description figure . Figure matplotlib.figure.Figure: The figure. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , dtm : Any , z_score : int = 1 , labels : List [ str ] = None , pivot_kws : Dict [ str , str ] = None , method : str = \"average\" , metric : str = \"euclidean\" , standard_scale : int = None , figsize : tuple = ( 8 , 8 ), cbar_kws : dict = None , row_cluster : bool = True , col_cluster : bool = True , row_linkage : bool = None , col_linkage : bool = None , row_colors : Union [ list , pd . DataFrame , pd . Series ] = None , col_colors : Union [ list , pd . DataFrame , pd . Series ] = None , mask : Union [ np . ndarray , pd . DataFrame ] = None , dendrogram_ratio : Union [ float , Tuple [ float ]] = ( 0.1 , 0.2 ), colors_ratio : float = 0.03 , cbar_pos : Tuple [ str ] = ( 0.02 , 0.32 , 0.03 , 0.2 ), tree_kws : dict = None , center : int = 0 , cmap : str = \"vlag\" , linewidths : float = 0.75 , show : bool = False , title : str = None , ) -> figure . Figure : \"\"\"Initialize the ClusterMap object. Args: dtm (Any): The data to cluster. z_score (int, optional): The z-score to use. Defaults to 1. labels (List[str], optional): The labels to use. Defaults to None. pivot_kws (Dict[str, str], optional): The pivot kwargs. Defaults to None. method (str, optional): The method to use. Defaults to \"average\". metric (str, optional): The metric to use. Defaults to \"euclidean\". standard_scale (int, optional): The standard scale to use. Defaults to None. figsize (tuple, optional): The figure size to use. Defaults to (8, 8). cbar_kws (dict, optional): The cbar kwargs. Defaults to None. row_cluster (bool, optional): Whether to cluster the rows. Defaults to True. col_cluster (bool, optional): Whether to cluster the columns. Defaults to True. row_linkage (bool, optional): Whether to use row linkage. Defaults to None. col_linkage (bool, optional): Whether to use column linkage. Defaults to None. row_colors (Union[list, pd.DataFrame, pd.Series], optional): The row colors. Defaults to None. col_colors (Union[list, pd.DataFrame, pd.Series], optional): The column colors. Defaults to None. mask (Union[np.ndarray, pd.DataFrame], optional): The mask to use. Defaults to None. dendrogram_ratio (Union[float, Tuple[float]], optional): The dendrogram ratio to use. Defaults to (.1, .2). colors_ratio (float, optional): The colors ratio to use. Defaults to 0.03. cbar_pos (Tuple[str], optional): The cbar position to use. Defaults to (.02, .32, .03, .2). tree_kws (dict, optional): The tree kwargs. Defaults to None. center (int, optional): The center to use. Defaults to 0. cmap (str, optional): The cmap to use. Defaults to \"vlag\". linewidths (float, optional): The linewidths to use. Defaults to .75. show (bool, optional): Whether to show the figure. Defaults to False. title (str, optional): The title to use. Defaults to None. Returns: matplotlib.figure.Figure: The figure. \"\"\" self . dtm = dtm self . z_score = z_score self . labels = labels self . figsize = figsize self . show = show self . method = method self . metric = metric self . standard_scale = standard_scale self . title = title self . row_colors = row_colors self . col_colors = col_colors self . pivot_kws = pivot_kws self . cbar_kws = cbar_kws self . row_cluster = row_cluster self . col_cluster = col_cluster self . row_linkage = row_linkage self . col_linkage = col_linkage self . mask = mask self . dendrogram_ratio = dendrogram_ratio self . colors_ratio = colors_ratio self . cbar_pos = cbar_pos self . tree_kws = tree_kws self . center = center self . cmap = cmap self . linewidths = linewidths self . df = dtm . get_table () self . build () build () \u00a4 Build the clustermap. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def build ( self ): \"\"\"Build the clustermap.\"\"\" # Get the labels if necessary if self . labels is None : # raise ValueError(\"Please provide labels.\") self . labels = self . df . columns . values . tolist ()[ 1 :] # Convert palette to vectors drawn on the side of the matrix if self . row_colors is None or self . col_colors is None : column_pal = sns . husl_palette ( 8 , s = 0.45 ) column_lut = dict ( zip ( map ( str , self . df ), column_pal )) column_colors = pd . Series ( self . labels , index = self . df . columns [ 1 :]) . map ( column_lut ) if self . row_colors is None : self . row_colors = column_colors if self . col_colors is None : self . col_colors = column_colors # Perform the cluster g = sns . clustermap ( self . df . corr (), cmap = self . cmap , method = self . method , metric = self . metric , figsize = self . figsize , row_colors = self . row_colors , col_colors = self . col_colors , row_cluster = self . row_cluster , col_cluster = self . col_cluster , center = self . center , linewidths = self . linewidths , z_score = self . z_score , pivot_kws = self . pivot_kws , standard_scale = self . standard_scale , cbar_kws = self . cbar_kws , row_linkage = self . row_linkage , col_linkage = self . col_linkage , mask = self . mask , dendrogram_ratio = self . dendrogram_ratio , colors_ratio = self . colors_ratio , cbar_pos = self . cbar_pos , tree_kws = self . tree_kws , ) # Remove the dendrogram on the left g . ax_row_dendrogram . remove () # Add the title if self . title : g . fig . suptitle ( self . title , y = 1.05 ) # Save the fig variable self . fig = g . fig # Suppress the output if not self . show : plt . close () return self . fig savefig ( filename ) \u00a4 Show the figure if it is hidden. Parameters: Name Type Description Default filename str The name of the file to save. required Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 165 166 167 168 169 170 171 def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) showfig () \u00a4 Show the figure if it is hidden. This is a helper method. You can also reference the figure using ClusterMap.fig . This will generally display in a Jupyter notebook. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 173 174 175 176 177 178 179 180 def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `ClusterMap.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig","title":"Seaborn"},{"location":"api/visualization/seaborn/#seaborn","text":"The seaborn submodule contains methods of producing visualizations using the Seaborn visualization library. The seaborn submodule contains components for generating word clouds and cluster analysis plots.","title":"Seaborn"},{"location":"api/visualization/seaborn/#seaborn-clustermaps","text":"","title":"Seaborn Clustermaps"},{"location":"api/visualization/seaborn/#lexos.visualization.seaborn.cluster.clustermap.ClusterMap","text":"ClusterMap. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 class ClusterMap : \"\"\"ClusterMap.\"\"\" def __init__ ( self , dtm : Any , z_score : int = 1 , labels : List [ str ] = None , pivot_kws : Dict [ str , str ] = None , method : str = \"average\" , metric : str = \"euclidean\" , standard_scale : int = None , figsize : tuple = ( 8 , 8 ), cbar_kws : dict = None , row_cluster : bool = True , col_cluster : bool = True , row_linkage : bool = None , col_linkage : bool = None , row_colors : Union [ list , pd . DataFrame , pd . Series ] = None , col_colors : Union [ list , pd . DataFrame , pd . Series ] = None , mask : Union [ np . ndarray , pd . DataFrame ] = None , dendrogram_ratio : Union [ float , Tuple [ float ]] = ( 0.1 , 0.2 ), colors_ratio : float = 0.03 , cbar_pos : Tuple [ str ] = ( 0.02 , 0.32 , 0.03 , 0.2 ), tree_kws : dict = None , center : int = 0 , cmap : str = \"vlag\" , linewidths : float = 0.75 , show : bool = False , title : str = None , ) -> figure . Figure : \"\"\"Initialize the ClusterMap object. Args: dtm (Any): The data to cluster. z_score (int, optional): The z-score to use. Defaults to 1. labels (List[str], optional): The labels to use. Defaults to None. pivot_kws (Dict[str, str], optional): The pivot kwargs. Defaults to None. method (str, optional): The method to use. Defaults to \"average\". metric (str, optional): The metric to use. Defaults to \"euclidean\". standard_scale (int, optional): The standard scale to use. Defaults to None. figsize (tuple, optional): The figure size to use. Defaults to (8, 8). cbar_kws (dict, optional): The cbar kwargs. Defaults to None. row_cluster (bool, optional): Whether to cluster the rows. Defaults to True. col_cluster (bool, optional): Whether to cluster the columns. Defaults to True. row_linkage (bool, optional): Whether to use row linkage. Defaults to None. col_linkage (bool, optional): Whether to use column linkage. Defaults to None. row_colors (Union[list, pd.DataFrame, pd.Series], optional): The row colors. Defaults to None. col_colors (Union[list, pd.DataFrame, pd.Series], optional): The column colors. Defaults to None. mask (Union[np.ndarray, pd.DataFrame], optional): The mask to use. Defaults to None. dendrogram_ratio (Union[float, Tuple[float]], optional): The dendrogram ratio to use. Defaults to (.1, .2). colors_ratio (float, optional): The colors ratio to use. Defaults to 0.03. cbar_pos (Tuple[str], optional): The cbar position to use. Defaults to (.02, .32, .03, .2). tree_kws (dict, optional): The tree kwargs. Defaults to None. center (int, optional): The center to use. Defaults to 0. cmap (str, optional): The cmap to use. Defaults to \"vlag\". linewidths (float, optional): The linewidths to use. Defaults to .75. show (bool, optional): Whether to show the figure. Defaults to False. title (str, optional): The title to use. Defaults to None. Returns: matplotlib.figure.Figure: The figure. \"\"\" self . dtm = dtm self . z_score = z_score self . labels = labels self . figsize = figsize self . show = show self . method = method self . metric = metric self . standard_scale = standard_scale self . title = title self . row_colors = row_colors self . col_colors = col_colors self . pivot_kws = pivot_kws self . cbar_kws = cbar_kws self . row_cluster = row_cluster self . col_cluster = col_cluster self . row_linkage = row_linkage self . col_linkage = col_linkage self . mask = mask self . dendrogram_ratio = dendrogram_ratio self . colors_ratio = colors_ratio self . cbar_pos = cbar_pos self . tree_kws = tree_kws self . center = center self . cmap = cmap self . linewidths = linewidths self . df = dtm . get_table () self . build () def build ( self ): \"\"\"Build the clustermap.\"\"\" # Get the labels if necessary if self . labels is None : # raise ValueError(\"Please provide labels.\") self . labels = self . df . columns . values . tolist ()[ 1 :] # Convert palette to vectors drawn on the side of the matrix if self . row_colors is None or self . col_colors is None : column_pal = sns . husl_palette ( 8 , s = 0.45 ) column_lut = dict ( zip ( map ( str , self . df ), column_pal )) column_colors = pd . Series ( self . labels , index = self . df . columns [ 1 :]) . map ( column_lut ) if self . row_colors is None : self . row_colors = column_colors if self . col_colors is None : self . col_colors = column_colors # Perform the cluster g = sns . clustermap ( self . df . corr (), cmap = self . cmap , method = self . method , metric = self . metric , figsize = self . figsize , row_colors = self . row_colors , col_colors = self . col_colors , row_cluster = self . row_cluster , col_cluster = self . col_cluster , center = self . center , linewidths = self . linewidths , z_score = self . z_score , pivot_kws = self . pivot_kws , standard_scale = self . standard_scale , cbar_kws = self . cbar_kws , row_linkage = self . row_linkage , col_linkage = self . col_linkage , mask = self . mask , dendrogram_ratio = self . dendrogram_ratio , colors_ratio = self . colors_ratio , cbar_pos = self . cbar_pos , tree_kws = self . tree_kws , ) # Remove the dendrogram on the left g . ax_row_dendrogram . remove () # Add the title if self . title : g . fig . suptitle ( self . title , y = 1.05 ) # Save the fig variable self . fig = g . fig # Suppress the output if not self . show : plt . close () return self . fig def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename ) def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `ClusterMap.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig","title":"ClusterMap"},{"location":"api/visualization/seaborn/#lexos.visualization.seaborn.cluster.clustermap.ClusterMap.__init__","text":"Initialize the ClusterMap object. Parameters: Name Type Description Default dtm Any The data to cluster. required z_score int The z-score to use. Defaults to 1. 1 labels List [ str ] The labels to use. Defaults to None. None pivot_kws Dict [ str , str ] The pivot kwargs. Defaults to None. None method str The method to use. Defaults to \"average\". 'average' metric str The metric to use. Defaults to \"euclidean\". 'euclidean' standard_scale int The standard scale to use. Defaults to None. None figsize tuple The figure size to use. Defaults to (8, 8). (8, 8) cbar_kws dict The cbar kwargs. Defaults to None. None row_cluster bool Whether to cluster the rows. Defaults to True. True col_cluster bool Whether to cluster the columns. Defaults to True. True row_linkage bool Whether to use row linkage. Defaults to None. None col_linkage bool Whether to use column linkage. Defaults to None. None row_colors Union [ list , pd . DataFrame , pd . Series ] The row colors. Defaults to None. None col_colors Union [ list , pd . DataFrame , pd . Series ] The column colors. Defaults to None. None mask Union [ np . ndarray , pd . DataFrame ] The mask to use. Defaults to None. None dendrogram_ratio Union [ float , Tuple [ float ]] The dendrogram ratio to use. Defaults to (.1, .2). (0.1, 0.2) colors_ratio float The colors ratio to use. Defaults to 0.03. 0.03 cbar_pos Tuple [ str ] The cbar position to use. Defaults to (.02, .32, .03, .2). (0.02, 0.32, 0.03, 0.2) tree_kws dict The tree kwargs. Defaults to None. None center int The center to use. Defaults to 0. 0 cmap str The cmap to use. Defaults to \"vlag\". 'vlag' linewidths float The linewidths to use. Defaults to .75. 0.75 show bool Whether to show the figure. Defaults to False. False title str The title to use. Defaults to None. None Returns: Type Description figure . Figure matplotlib.figure.Figure: The figure. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , dtm : Any , z_score : int = 1 , labels : List [ str ] = None , pivot_kws : Dict [ str , str ] = None , method : str = \"average\" , metric : str = \"euclidean\" , standard_scale : int = None , figsize : tuple = ( 8 , 8 ), cbar_kws : dict = None , row_cluster : bool = True , col_cluster : bool = True , row_linkage : bool = None , col_linkage : bool = None , row_colors : Union [ list , pd . DataFrame , pd . Series ] = None , col_colors : Union [ list , pd . DataFrame , pd . Series ] = None , mask : Union [ np . ndarray , pd . DataFrame ] = None , dendrogram_ratio : Union [ float , Tuple [ float ]] = ( 0.1 , 0.2 ), colors_ratio : float = 0.03 , cbar_pos : Tuple [ str ] = ( 0.02 , 0.32 , 0.03 , 0.2 ), tree_kws : dict = None , center : int = 0 , cmap : str = \"vlag\" , linewidths : float = 0.75 , show : bool = False , title : str = None , ) -> figure . Figure : \"\"\"Initialize the ClusterMap object. Args: dtm (Any): The data to cluster. z_score (int, optional): The z-score to use. Defaults to 1. labels (List[str], optional): The labels to use. Defaults to None. pivot_kws (Dict[str, str], optional): The pivot kwargs. Defaults to None. method (str, optional): The method to use. Defaults to \"average\". metric (str, optional): The metric to use. Defaults to \"euclidean\". standard_scale (int, optional): The standard scale to use. Defaults to None. figsize (tuple, optional): The figure size to use. Defaults to (8, 8). cbar_kws (dict, optional): The cbar kwargs. Defaults to None. row_cluster (bool, optional): Whether to cluster the rows. Defaults to True. col_cluster (bool, optional): Whether to cluster the columns. Defaults to True. row_linkage (bool, optional): Whether to use row linkage. Defaults to None. col_linkage (bool, optional): Whether to use column linkage. Defaults to None. row_colors (Union[list, pd.DataFrame, pd.Series], optional): The row colors. Defaults to None. col_colors (Union[list, pd.DataFrame, pd.Series], optional): The column colors. Defaults to None. mask (Union[np.ndarray, pd.DataFrame], optional): The mask to use. Defaults to None. dendrogram_ratio (Union[float, Tuple[float]], optional): The dendrogram ratio to use. Defaults to (.1, .2). colors_ratio (float, optional): The colors ratio to use. Defaults to 0.03. cbar_pos (Tuple[str], optional): The cbar position to use. Defaults to (.02, .32, .03, .2). tree_kws (dict, optional): The tree kwargs. Defaults to None. center (int, optional): The center to use. Defaults to 0. cmap (str, optional): The cmap to use. Defaults to \"vlag\". linewidths (float, optional): The linewidths to use. Defaults to .75. show (bool, optional): Whether to show the figure. Defaults to False. title (str, optional): The title to use. Defaults to None. Returns: matplotlib.figure.Figure: The figure. \"\"\" self . dtm = dtm self . z_score = z_score self . labels = labels self . figsize = figsize self . show = show self . method = method self . metric = metric self . standard_scale = standard_scale self . title = title self . row_colors = row_colors self . col_colors = col_colors self . pivot_kws = pivot_kws self . cbar_kws = cbar_kws self . row_cluster = row_cluster self . col_cluster = col_cluster self . row_linkage = row_linkage self . col_linkage = col_linkage self . mask = mask self . dendrogram_ratio = dendrogram_ratio self . colors_ratio = colors_ratio self . cbar_pos = cbar_pos self . tree_kws = tree_kws self . center = center self . cmap = cmap self . linewidths = linewidths self . df = dtm . get_table () self . build ()","title":"__init__()"},{"location":"api/visualization/seaborn/#lexos.visualization.seaborn.cluster.clustermap.ClusterMap.build","text":"Build the clustermap. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def build ( self ): \"\"\"Build the clustermap.\"\"\" # Get the labels if necessary if self . labels is None : # raise ValueError(\"Please provide labels.\") self . labels = self . df . columns . values . tolist ()[ 1 :] # Convert palette to vectors drawn on the side of the matrix if self . row_colors is None or self . col_colors is None : column_pal = sns . husl_palette ( 8 , s = 0.45 ) column_lut = dict ( zip ( map ( str , self . df ), column_pal )) column_colors = pd . Series ( self . labels , index = self . df . columns [ 1 :]) . map ( column_lut ) if self . row_colors is None : self . row_colors = column_colors if self . col_colors is None : self . col_colors = column_colors # Perform the cluster g = sns . clustermap ( self . df . corr (), cmap = self . cmap , method = self . method , metric = self . metric , figsize = self . figsize , row_colors = self . row_colors , col_colors = self . col_colors , row_cluster = self . row_cluster , col_cluster = self . col_cluster , center = self . center , linewidths = self . linewidths , z_score = self . z_score , pivot_kws = self . pivot_kws , standard_scale = self . standard_scale , cbar_kws = self . cbar_kws , row_linkage = self . row_linkage , col_linkage = self . col_linkage , mask = self . mask , dendrogram_ratio = self . dendrogram_ratio , colors_ratio = self . colors_ratio , cbar_pos = self . cbar_pos , tree_kws = self . tree_kws , ) # Remove the dendrogram on the left g . ax_row_dendrogram . remove () # Add the title if self . title : g . fig . suptitle ( self . title , y = 1.05 ) # Save the fig variable self . fig = g . fig # Suppress the output if not self . show : plt . close () return self . fig","title":"build()"},{"location":"api/visualization/seaborn/#lexos.visualization.seaborn.cluster.clustermap.ClusterMap.savefig","text":"Show the figure if it is hidden. Parameters: Name Type Description Default filename str The name of the file to save. required Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 165 166 167 168 169 170 171 def savefig ( self , filename : str ): \"\"\"Show the figure if it is hidden. Args: filename (str): The name of the file to save. \"\"\" self . fig . savefig ( filename )","title":"savefig()"},{"location":"api/visualization/seaborn/#lexos.visualization.seaborn.cluster.clustermap.ClusterMap.showfig","text":"Show the figure if it is hidden. This is a helper method. You can also reference the figure using ClusterMap.fig . This will generally display in a Jupyter notebook. Source code in lexos\\visualization\\seaborn\\cluster\\clustermap.py 173 174 175 176 177 178 179 180 def showfig ( self ): \"\"\"Show the figure if it is hidden. This is a helper method. You can also reference the figure using `ClusterMap.fig`. This will generally display in a Jupyter notebook. \"\"\" return self . fig","title":"showfig()"},{"location":"api/visualization/wordcloud/","text":"Wordcloud \u00a4 The Wordcloud module produces single word clouds and multiple word clouds (multiclouds) based on the term frequencies of a text or collection of texts. It uses the Python Wordcloud package. lexos . visualization . cloud . wordcloud . get_rows ( lst , n ) \u00a4 Yield successive n-sized rows from a list of documents. Parameters: Name Type Description Default lst list A list of documents. required n int The number of columns in the row. required Yields: Type Description Iterator [ int ] A generator with the documents separated into rows. Source code in lexos\\visualization\\cloud\\wordcloud.py 12 13 14 15 16 17 18 19 20 21 22 23 def get_rows ( lst , n ) -> Iterator [ int ]: \"\"\"Yield successive n-sized rows from a list of documents. Args: lst (list): A list of documents. n (int): The number of columns in the row. Yields: A generator with the documents separated into rows. \"\"\" for i in range ( 0 , len ( lst ), n ): yield lst [ i : i + n ] lexos . visualization . cloud . wordcloud . wordcloud ( dtm , docs = None , opts = None , show = True , figure_opts = None , round = None , filename = None ) \u00a4 Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Parameters: Name Type Description Default dtm Union [ dict , list , object , pd . DataFrame , str , tuple ] The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required docs list A list of documents to be selected from the DTM. None opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None filename str The filename to save the word cloud to. None Returns: Name Type Description object object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\visualization\\cloud\\wordcloud.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def wordcloud ( dtm : Union [ dict , list , object , pd . DataFrame , str , tuple ], docs : List [ str ] = None , opts : dict = None , show : bool = True , figure_opts : dict = None , round : int = None , filename : str = None , ) -> object : \"\"\"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Args: dtm (Union[dict, list, object, pd.DataFrame, str, tuple]): The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. docs (list): A list of documents to be selected from the DTM. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. filename (str): The filename to save the word cloud to. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if opts is None : opts = { \"background_color\" : \"white\" , \"max_words\" : 2000 , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" , } if figure_opts is None : figure_opts = {} if isinstance ( dtm , DTM ): # Get the dtm table data = dtm . get_table () # Get the counts for the desired documents if docs : docs = [ \"terms\" ] + docs data = data [ docs ] . copy () # Create a new column with the total for each row data [ \"count\" ] = data . sum ( axis = 1 , numeric_only = True ) # Get the dtm sums else : data [ \"count\" ] = data . sum ( axis = 1 , numeric_only = True ) # Ensure that the table only has terms and counts data = data [[ \"terms\" , \"count\" ]] . copy () else : data = dtm # Set the mask, if using if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask # Generate the WordCloud if isinstance ( data , str ): wc = WordCloud ( ** opts ) . generate_from_text ( data ) else : if isinstance ( data , list ): data = { x [ 0 ]: x [ 1 ] for x in data } elif isinstance ( data , pd . DataFrame ): term_counts = data . to_dict ( orient = \"records\" ) try : data = { x [ \"terms\" ]: x [ \"count\" ] for x in term_counts } except KeyError : data = { x [ \"terms\" ]: x [ \"frequency\" ] for x in term_counts } wc = WordCloud ( ** opts ) . generate_from_frequencies ( data ) # Plot or return the WordCloud if show : if figure_opts : plt . figure ( ** figure_opts ) plt . axis ( \"off\" ) # If a filename is provided, save the figure if filename : plt . to_file ( filename ) plt . imshow ( wc ) plt . show () else : return wc lexos . visualization . cloud . wordcloud . multicloud ( dtm , docs = None , opts = None , ncols = 3 , title = None , labels = None , show = True , figure_opts = None , round = None , filename = None ) \u00a4 Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by get_dtm_table() . Parameters: Name Type Description Default dtm List [ Union [ dict , object , str , tuple ]] The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" columns. required docs List [ str ] (List[str]): A list of documents to be selected from the DTM. None opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} None ncols int The number of columns in the grid. 3 title str The title of the grid. None labels List [ str ] The document labels for each subplot. None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None filename str The filename to save the figure to. None Returns: Name Type Description object object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\visualization\\cloud\\wordcloud.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def multicloud ( dtm : List [ Union [ dict , pd . DataFrame , object , str , tuple ]], docs : List [ str ] = None , opts : dict = None , ncols : int = 3 , title : str = None , labels : List [ str ] = None , show : bool = True , figure_opts : dict = None , round : int = None , filename : str = None , ) -> object : \"\"\"Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by `get_dtm_table()`. Args: dtm (List[Union[dict, object, str, tuple]]): The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" columns. docs: (List[str]): A list of documents to be selected from the DTM. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} ncols (int): The number of columns in the grid. title (str): The title of the grid. labels (List[str]): The document labels for each subplot. show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. filename (str): The filename to save the figure to. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if opts is None : opts = { \"background_color\" : \"white\" , \"max_words\" : 2000 , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" , } if figure_opts is None : figure_opts = {} if isinstance ( dtm , DTM ): # Get table data = dtm . get_table () # Get the counts for the desired documents if docs : data = data [[ \"terms\" ] + docs ] . copy () # Transposed table data = data . T else : data = dtm # Process the data data into a list if isinstance ( data , pd . DataFrame ): # Grab the first row for the header new_header = data . iloc [ 0 ] # Drop the first row data = data [ 1 :] # Set the header row as the df header data . columns = new_header # Return a dict data = data . to_dict ( orient = \"records\" ) # Ensure that anything that is not a list of strings is converted # to the appropriate format. elif isinstance ( data , list ): if all ( isinstance ( s , str ) for s in data ): pass else : data = [{ x [ 0 : 1 ]: x [ 1 : 2 ] for x in doc } for doc in data ] # List for multiple word clouds if they are to be returned. multiclouds = [] # Create a rounded mask if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask # Constrain the layout figure_opts [ \"constrained_layout\" ] = True # Create the figure fig = plt . figure ( ** figure_opts ) # Add the title if title : fig . suptitle ( title ) # Calculate the number of rows and columns nrows = int ( np . ceil ( len ( data ) / ncols )) spec = fig . add_gridspec ( nrows , ncols ) # Divide the data into rows rows = list ( get_rows ( data , ncols )) # Set an index for labels i = 0 # Loop through the rows for row , doc in enumerate ( rows ): # Loop through the documents in the row for col , data in enumerate ( doc ): # Create a subplot ax = fig . add_subplot ( spec [ row , col ]) # Generate the subplot's word cloud if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) # If `show=True`, show the word cloud if show : ax . imshow ( wordcloud ) ax . axis ( \"off\" ) # Set the image title from the label if labels : ax . set_title ( labels [ i ]) i += 1 # Otherwise, add the word cloud to the multiclouds list. else : multiclouds . append ( wordcloud ) # If a filename is provided, save the figure if filename : fig . savefig ( filename ) # If `show=False`, return the multiclouds list. if not show : return multiclouds","title":"Cloud"},{"location":"api/visualization/wordcloud/#wordcloud","text":"The Wordcloud module produces single word clouds and multiple word clouds (multiclouds) based on the term frequencies of a text or collection of texts. It uses the Python Wordcloud package.","title":"Wordcloud"},{"location":"api/visualization/wordcloud/#lexos.visualization.cloud.wordcloud.get_rows","text":"Yield successive n-sized rows from a list of documents. Parameters: Name Type Description Default lst list A list of documents. required n int The number of columns in the row. required Yields: Type Description Iterator [ int ] A generator with the documents separated into rows. Source code in lexos\\visualization\\cloud\\wordcloud.py 12 13 14 15 16 17 18 19 20 21 22 23 def get_rows ( lst , n ) -> Iterator [ int ]: \"\"\"Yield successive n-sized rows from a list of documents. Args: lst (list): A list of documents. n (int): The number of columns in the row. Yields: A generator with the documents separated into rows. \"\"\" for i in range ( 0 , len ( lst ), n ): yield lst [ i : i + n ]","title":"get_rows()"},{"location":"api/visualization/wordcloud/#lexos.visualization.cloud.wordcloud.wordcloud","text":"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Parameters: Name Type Description Default dtm Union [ dict , list , object , pd . DataFrame , str , tuple ] The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required docs list A list of documents to be selected from the DTM. None opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None filename str The filename to save the word cloud to. None Returns: Name Type Description object object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\visualization\\cloud\\wordcloud.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def wordcloud ( dtm : Union [ dict , list , object , pd . DataFrame , str , tuple ], docs : List [ str ] = None , opts : dict = None , show : bool = True , figure_opts : dict = None , round : int = None , filename : str = None , ) -> object : \"\"\"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Args: dtm (Union[dict, list, object, pd.DataFrame, str, tuple]): The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. docs (list): A list of documents to be selected from the DTM. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. filename (str): The filename to save the word cloud to. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if opts is None : opts = { \"background_color\" : \"white\" , \"max_words\" : 2000 , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" , } if figure_opts is None : figure_opts = {} if isinstance ( dtm , DTM ): # Get the dtm table data = dtm . get_table () # Get the counts for the desired documents if docs : docs = [ \"terms\" ] + docs data = data [ docs ] . copy () # Create a new column with the total for each row data [ \"count\" ] = data . sum ( axis = 1 , numeric_only = True ) # Get the dtm sums else : data [ \"count\" ] = data . sum ( axis = 1 , numeric_only = True ) # Ensure that the table only has terms and counts data = data [[ \"terms\" , \"count\" ]] . copy () else : data = dtm # Set the mask, if using if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask # Generate the WordCloud if isinstance ( data , str ): wc = WordCloud ( ** opts ) . generate_from_text ( data ) else : if isinstance ( data , list ): data = { x [ 0 ]: x [ 1 ] for x in data } elif isinstance ( data , pd . DataFrame ): term_counts = data . to_dict ( orient = \"records\" ) try : data = { x [ \"terms\" ]: x [ \"count\" ] for x in term_counts } except KeyError : data = { x [ \"terms\" ]: x [ \"frequency\" ] for x in term_counts } wc = WordCloud ( ** opts ) . generate_from_frequencies ( data ) # Plot or return the WordCloud if show : if figure_opts : plt . figure ( ** figure_opts ) plt . axis ( \"off\" ) # If a filename is provided, save the figure if filename : plt . to_file ( filename ) plt . imshow ( wc ) plt . show () else : return wc","title":"wordcloud()"},{"location":"api/visualization/wordcloud/#lexos.visualization.cloud.wordcloud.multicloud","text":"Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by get_dtm_table() . Parameters: Name Type Description Default dtm List [ Union [ dict , object , str , tuple ]] The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" columns. required docs List [ str ] (List[str]): A list of documents to be selected from the DTM. None opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} None ncols int The number of columns in the grid. 3 title str The title of the grid. None labels List [ str ] The document labels for each subplot. None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None filename str The filename to save the figure to. None Returns: Name Type Description object object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\visualization\\cloud\\wordcloud.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def multicloud ( dtm : List [ Union [ dict , pd . DataFrame , object , str , tuple ]], docs : List [ str ] = None , opts : dict = None , ncols : int = 3 , title : str = None , labels : List [ str ] = None , show : bool = True , figure_opts : dict = None , round : int = None , filename : str = None , ) -> object : \"\"\"Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by `get_dtm_table()`. Args: dtm (List[Union[dict, object, str, tuple]]): The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" columns. docs: (List[str]): A list of documents to be selected from the DTM. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_color\": \"steelblue\"} ncols (int): The number of columns in the grid. title (str): The title of the grid. labels (List[str]): The document labels for each subplot. show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. filename (str): The filename to save the figure to. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if opts is None : opts = { \"background_color\" : \"white\" , \"max_words\" : 2000 , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" , } if figure_opts is None : figure_opts = {} if isinstance ( dtm , DTM ): # Get table data = dtm . get_table () # Get the counts for the desired documents if docs : data = data [[ \"terms\" ] + docs ] . copy () # Transposed table data = data . T else : data = dtm # Process the data data into a list if isinstance ( data , pd . DataFrame ): # Grab the first row for the header new_header = data . iloc [ 0 ] # Drop the first row data = data [ 1 :] # Set the header row as the df header data . columns = new_header # Return a dict data = data . to_dict ( orient = \"records\" ) # Ensure that anything that is not a list of strings is converted # to the appropriate format. elif isinstance ( data , list ): if all ( isinstance ( s , str ) for s in data ): pass else : data = [{ x [ 0 : 1 ]: x [ 1 : 2 ] for x in doc } for doc in data ] # List for multiple word clouds if they are to be returned. multiclouds = [] # Create a rounded mask if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask # Constrain the layout figure_opts [ \"constrained_layout\" ] = True # Create the figure fig = plt . figure ( ** figure_opts ) # Add the title if title : fig . suptitle ( title ) # Calculate the number of rows and columns nrows = int ( np . ceil ( len ( data ) / ncols )) spec = fig . add_gridspec ( nrows , ncols ) # Divide the data into rows rows = list ( get_rows ( data , ncols )) # Set an index for labels i = 0 # Loop through the rows for row , doc in enumerate ( rows ): # Loop through the documents in the row for col , data in enumerate ( doc ): # Create a subplot ax = fig . add_subplot ( spec [ row , col ]) # Generate the subplot's word cloud if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) # If `show=True`, show the word cloud if show : ax . imshow ( wordcloud ) ax . axis ( \"off\" ) # Set the image title from the label if labels : ax . set_title ( labels [ i ]) i += 1 # Otherwise, add the word cloud to the multiclouds list. else : multiclouds . append ( wordcloud ) # If a filename is provided, save the figure if filename : fig . savefig ( filename ) # If `show=False`, return the multiclouds list. if not show : return multiclouds","title":"multicloud()"},{"location":"development/","text":"Development Notes \u00a4 Dependency management is handled with Poetry . The code is tested with pytest as a pre-commit hook. Before commit, I generally run isort and interrogate to ensure consistent imports and docstrings, but these are not currently implemented as pre-commit hooks. Docstrings are given an fully as possible in Google style, with as much type hinting as possible. Docstrings are used by mkdocs to auto-generate the documentation through the magic of mkdocstrings . A number of test scripts have been implemented, and they are used by the continuous integration process. However, the scripts are incomplete and intended primarily for quick testing from the command line. A fuller test suite is intended once the API is more complete. More information for developers will be added soon. In the meantime, see the GitHub Wiki for information on getting started.","title":"Index"},{"location":"development/#development-notes","text":"Dependency management is handled with Poetry . The code is tested with pytest as a pre-commit hook. Before commit, I generally run isort and interrogate to ensure consistent imports and docstrings, but these are not currently implemented as pre-commit hooks. Docstrings are given an fully as possible in Google style, with as much type hinting as possible. Docstrings are used by mkdocs to auto-generate the documentation through the magic of mkdocstrings . A number of test scripts have been implemented, and they are used by the continuous integration process. However, the scripts are incomplete and intended primarily for quick testing from the command line. A fuller test suite is intended once the API is more complete. More information for developers will be added soon. In the meantime, see the GitHub Wiki for information on getting started.","title":"Development Notes"},{"location":"tutorial/","text":"Overview \u00a4 This page is a rough overview of the usage of the API. Important Aspects of the API may change before the tutorial is updated. At this stage, the tutorial should only be taken as a general guideline to the API's usage.","title":"Overview"},{"location":"tutorial/#overview","text":"This page is a rough overview of the usage of the API. Important Aspects of the API may change before the tutorial is updated. At this stage, the tutorial should only be taken as a general guideline to the API's usage.","title":"Overview"},{"location":"tutorial/cutting_docs/","text":"Cutter is a module that divides files, texts, or documents into segments. It has three submodules: one for working with spaCy documents (codename Ginsu ), one for working with raw texts (codename Machete ), and a third class FileSplit (codename Chainsaw ) for splitting files based on bytesize. Ginsu \u00a4 The Ginsu class is used for splitting spaCy documents (pre-tokenized texts). Ginsu is the preferred method for creating segments because it can access information supplied by the language model. Ginsu has the following features: split() : Split by number of tokens (i.e. every N token). splitn() : Split by number of segments (i.e. return a predetermined number of segments). split_on_milestones() : Split on milestone tokens. split() and splitn() both have the ability to merge the list segment into the preceding one if it falls under a specific threshold (which can be adjusted with the threshold parameter). They can also generate overlapping segments with the overlap parameter, the value of which is the number of tokens in the overlap. All three methods return a list of lists, where each item in the sublist is a spaCy document. With split_on_milestones() , the user can choose whether or not to preserve the milestone token at the beginning of each segment. A milestone must be a single token and will generally match the token's text attribute. However, it can also match other attributes of the token if they are available in the language model used to produce the spaCy document. A query language (described below) is available for fine-grained matching. Splitting Documents by Number of Tokens \u00a4 To split documents every N tokens, use the split() method. Here are some examples: from lexos.cutter.ginsu import Ginsu cutter = Ginsu () segments = cutter . split ( doc ) segments = cutter . split ([ doc1 , doc2 ]) By default, the document will be split every 1000 tokens. Here is an example with the full range of options: segments = cutter . split ( doc , n = 500 , overlap = 5 , merge_threshold = 0.5 ) This will split the document every 500 tokens with each segment overlapping by 5 tokens. If the final segment is less than half the length of n , it will be merged with the previous segment (0.5 is the default). Splitting Documents into a Pre-Determined Number of Segments \u00a4 To split documents into a pre-determined number of segments, use the splitn() method. Here are some examples: from lexos.cutter.ginsu import Ginsu cutter = Ginsu () segments = cutter . splitn ( doc , n = 10 ) segments = cutter . splitn ([ doc1 , doc2 ], n = 10 , overlap = 5 , merge_threshold = 0.5 ) This will split the document(s) into 10 segments. The overlap and merge_threshold flags work exactly as they do for the split() method. Splitting Documents on Milestones \u00a4 A milestone is a pattern that serves as a boundary between segments of a document. The split_on_milestones() method accepts a pattern to match tokens to milestones. Typically a milestone pattern will be a simple string, and Cutter will split the spaCy doc whenever the text attribute of a token matches the pattern. As we will see below, more complex pattern matching methods are possible. from lexos.tokenizer import make_doc from lexos.cutter.ginsu import Ginsu text = \"\"\" It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. \"\"\" doc = make_doc ( text . strip ()) cutter = Ginsu () segments = cutter . split_on_milestones ( doc , milestone = \",\" ) print ( segments ) \"\"\" [ \"It is a truth universally acknowledged\", \"that a single man in possession of a good fortune\", \"must be in want of a wife.\" ] By default, the milestone is deleted from the result. If you wish to preserve it, use preserve_milestones=True . This milestone token will be preserved at the beginning of each segment. Note As shown above, it is possible to splitting documents on frequent tokens like commas. Examples like the one above are given because they can use a short text to illustrate the basic usage of spit_on_milestones() . A more common use case is to cut documents into segments based on markers like chapter headings. If the the milestone parameter is supplied with a list of strings, each item in the list will be treated as a milestone: segments = cutter . split_on_milestones ( doc , milestone = [ \"be\" , \"is\" ], preserve_milestones = True ) print ( segments ) \"\"\" [ \"It\", \"is a truth universally acknowledged, that a single man in possession of a good fortune, must\", \"be in want of a wife.\" ] Milestone patterns can also be matched using a query language supplied in the form of a dict, where the keyword is the name of a spaCy token attribute (e.g. text , pos_ , lemma_ , etc.) and the value is the value to match. By default a token matches a milestone if its value for the attribute is equivalent to the value given in the milestone dict. In other words, if the dict has the form {\"text\": \"chapter\"} , any token in the document for which the value of token.text is \"chapter\" will be treated as a milestone. As result, the following commands are functionally equivalent: segments = cutter . split_on_milestones ( doc , milestone = \",\" ) segments = cutter . split_on_milestones ( doc , milestone = { \"text\" : \",\" }) However, the following will treat all varieties of \"chapter\" (\"chapters\", \"chapter's\", \"Chapter\", etc.) as milestones: segments = cutter . split_on_milestones ( doc , milestone = { \"lemma_\" : \"chapter\" }) Note For further information on spaCy token attributes, see Tokenising Texts . Milestone dict values can also be given as tuples consisting of a pattern and an operator. The operator is the method used to perform the match. Currently, the following operators are available: in : If the pattern is given as a list of strings, any item in the list will be treated as a milestone. not_in : If the pattern is given as a list of strings, any item not in the list will be treated as a milestone. starts_with : Any item starting with the string pattern will be treated as a milestone (uses the Python startswith() function). ends_with : Any item ending with the string pattern will be treated as a milestone (uses the Python endswith() function). re_match : Any item starting with the regular expression pattern will be treated as a milestone (uses the Python re.match() function). re_search : Any item containing with the regular expression pattern will be treated as a milestone (uses the Python re.search() function). For example, the following command will treat any adjective or noun as a milestone. segments = cutter . split_on_milestones ( doc , milestone = { \"pos_\" : ([ \"ADJ\" , \"NOUN\" ], \"in\" )} ) More complex queries can be built using the and and or keywords. For instance, the following pattern is the equivalent of the previous example. segments = cutter . split_on_milestones ( doc , milestone = { \"and\" : { \"pos_\" : \"ADJ\" }, \"or\" : { \"pos_\" : \"NOUN\" } } ) In the example below, the word \"can\" would be treated as a milestone only when it functions as a verb: segments = cutter . split_on_milestones ( doc , milestone = { \"and\" : { \"text\" : \"can\" }, \"and\" : { \"pos_\" : \"VERB\" } } ) Important Filtering based on parts of speech, lemmas, and some other attributes is only possible if you have tokenized your documents using a language model that contains the relevant attribute. The Milestone Class \u00a4 The Ginsu split functions match tokens to milestones patterns and cut documents on the fly. However, it is also possible to use the lexos.cutter.milestones.Milestones class to preprocess documents. This adds the custom extension token._.is_milestone (by default False ) to each token in the document and uses the same query language to allow the user to match tokens where the value should be True . If documents are pre-processed in this way, Ginsu 's split_on_milestones() method can leverage that information. from lexos.tokenizer import make_doc from lexos.cutter.ginsu import Ginsu from lexos.cutter.milestones import Milestones text = \"\"\" It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. \"\"\" doc = make_doc ( text . strip ()) # Set commas as milestones Milestones () . set ( doc , \",\" ) cutter = Ginsu () segments = cutter . split_on_milestones ( doc , milestone = { \"is_milestone\" : True }) print ( segments ) \"\"\" [ \"It is a truth universally acknowledged\", \"that a single man in possession of a good fortune\", \"must be in want of a wife.\" ] Note Custom extension attributes like token._.is_milestone are normally referenced with the _. prefix. However, in milestone dicts, only the name \"is_milestone\" is given, parallelling built-in attributes like \"is_punct\". The same is true for any other custom extension available in the token's attributes. An obvious advantage of preprocessing milestones is that the custom attribute can be saved. If the user chooses to preserve milestones when using Ginsu.split_on_milestones , the milestone will appear at the beginning of each document, but will not have the _.is_milestone attribute. In the future, it is hoped that we will incorporate the ability to set milestones on the fly. Machete \u00a4 Machete is a cruder method of cutting raw text into segments without the benefit of a language model. It may be particularly valuable as a standalone method of segmenting texts for outside applications. The Machete Tokenizer \u00a4 Machete works in a manner similar to Ginsu and has all the same functionality. However, before splitting the text it applies a makeshift tokenizer function and then splits the text based on the resulting list of tokens. The Lexos API has three tokenizer functions in the cutter function registry: \"whitespace\" (splits on whitespace, the default), \"character\" (splits into single-character tokens), and \"linebreaks\" (splits on linebreaks). A Machete object can be initialized with one of the tokenizers or the tokenizer can be passed to the split() , splitn() , and split_on_milestones() methods using the tokenizer parameter. What if I don't like the tokenizer? You can supply a custom function after first adding it to the registry. Here is an example: from lexos.cutter.machete import Machete import lexos.cutter.registry def custom_punctuation_tokenizer ( text : str ) -> str : \"\"\"Split the text on punctuation or whitespace.\"\"\" return re . split ( r \"(\\W+)\" , text ) # Register the custom function registry . tokenizers . register ( \"custom_punctuation_tokenizer\" , func = custom_punctuation_tokenizer ) # Create a `Machete` object machete = Machete () # Split the texts into 5 segments result = machete . splitn ( texts , n = 5 , tokenizer = \"custom_punctuation_tokenizer\" ) Splitting Documents with Machete \u00a4 split() , splitn() , and split_on_milestones() return a list of lists, where each item in the outer list corresponds to a text and each sublist contains the text's segments. split() , splitn() take the same merge_threshold and overlap parameters as in the Ginsu class. split_on_milestones() is more limited than its Ginsu equivalent. Milestone patterns are evaluated as regular expressions and searched from the beginning of the token string using Python's re.match() function. By default, all three methods return segments as lists of strings. In the example below, we get results with the default as_string=True : from lexos.cutter.machete import Machete text = \"\"\" It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. \"\"\" cutter = Machete () segments = cutter . split_on_milestones ( text . strip (), n = 2 ) print ( segments ) \"\"\" [ [ \"It is a truth universally acknowledged, that a single man in possession\", \"of a good fortune, must be in want of a wife.\" ] The basis for the division can be seen by displaying the segments as lists of tokens using as_string=False : segments = cutter . split_on_milestones ( text . strip (), n = 2 , as_string = False ) print ( segments ) \"\"\" [ ['It ', 'is ', 'a ', 'truth ', 'universally ', 'acknowledged, ', 'that ', 'a ', 'single ', 'man \\n ', 'in ', 'possession '], ['of ', 'a ', 'good ', 'fortune, ', 'must ', 'be ', 'in ', 'want ', 'of ', 'a ', 'wife. \\n '] ] The default \"whitespace\" tokenizer does not strip whitespace around words and does not treat punctuation as separate functions. This makes it very easy to reconstitute the text as a string, but it may not be the desired behaviour for all applications. You may need to apply a custom tokenizer as described above. For specific languages, more reliable results may be obtained by using a language model as described in Tokenizing Texts . Splitting Lists of Tokens with Machete \u00a4 Sometimes data will be available as lists of tokens, rather than as strings (for instance, if you have already tokenized your texts using a tool like NLTK ). In this case, you can cut your texts using the split_list() method. It works just like split() , except that it takes a list of tokens as input. def split_list( self, texts: List[str], n: int = 1000, merge_threshold: float = 0.5, overlap: int = None, as_strin from lexos.cutter.machete import Machete text = [ \"It\" , \"is\" , \"a\" , \"truth\" , \"universally\" , \"acknowledged\" , \"that\" , \"a\" , \"single\" , \"man\" , \"in\" , \"possession\" , \"of\" , \"a\" , \"good\" , \"fortune\" , \"must\" , \"be\" , \"in\" , \"want\" , \"of\" , \"a\" , \"wife\" ] # Pad each token with a following space text = \" \" . join ( text ) . split () cutter = Machete () segments = cutter . split_list ( text , n = 12 ) print ( segments ) \"\"\" [ [ \"It is a truth universally acknowledged that a single man in possession\", \"of a good fortune must be in want of a wife.\" ] Note Machete assumes that spaces and punctuation between words are preserved, which makes it easier to return the segments as a human-readable string. In the example above, our token list did not preserve spaces or punctuation between words. As a result, it was necessary to take the extra step of padding each token (although the resulting segments have still lost the original punctuation). Steps like this may be necessary when using lists of tokens, depending on how they are created. It should be noted that, whilst of the Machete functions will accept either a single text string or a list of text strings, split_list() accepts only a list of token strings. If you have multiple lists, you should handle them in a loop (or equivalent list comprehension) as follows: from lexos.cutter.machete import Machete texts = [ [ \"It\" , \"is\" , \"a\" , \"truth\" , \"universally\" , \"acknowledged\" , \"that\" , \"a\" , \"single\" , \"man\" ], [ \"in\" , \"possession\" , \"of\" , \"a\" , \"good\" , \"fortune\" , \"must\" , \"be\" , \"in\" , \"want\" , \"of\" , \"a\" , \"wife\" ] ] # Pad each token with a following space texts = [ \" \" . join ( text ) . split () for text in texts ] cutter = Machete () all_segments = [ cutter . split_list ( text , n = 6 ) for text in texts ] print ( all_segments ) \"\"\" [ [ \"It is a truth universally acknowledged\", \"that a single man in possession\" ], [ \"of a good fortune must be\", \"in want of a wife.\" ] ] Filesplit (codename Chainsaw ) \u00a4 The lexos.cutter.filesplit.Filesplit class allows the user to cut binary files into numbered file segments with the format filename_1.txt , filename_2.txt , etc. The source file is divided by number of bytes. This class would typically be used as a first step in a workflow if a large file needs to be divided into many smaller files. The class is a fork of Ram Prakash Jayapalan's filesplit module with a few minor tweaks. The most important is that the split function takes a sep argument to allow the user to specify the separator between the filename and number in each generated file. Typical usage is as follows: from lexos.cutter.filesplit import Filesplit fs = Filesplit () fs . split ( file = \"/filesplit_test/longfile.txt\" , split_size = 30000000 , output_dir = \"/filesplit_test/splits/\" , sep = \"__\" ) The split_size parameter indicates the size of the split files in in bytes. The Filesplit class generates a manifest file called fs_manifest.csv in the output directory. This can be used to re-merge the files, if desired: fs . merge ( \"/filesplit_test/splits/\" , cleanup = True ) Setting the cleanup parameter to True causes the manifest file and all split files to be deleted after the merged file is created. Merging Other Types of Segments \u00a4 If you have not previously split files using Filesplit , Cutter has some standalone functions for merging other types of documents. The merge() function accepts a list of strings or spaCy documents and merges them into a single string or Doc object. If you are merging strings, you can use the sep parameter to control the \"boundary\" between the merged texts. There is no equivalent if you are merging spaCy docs. from cutter.merge import merge merged_text = merge ([ text1 , text2 ], sep = \" \" ) merged_doc = merge ([ doc1 , doc2 ]) Note that merge() is also a method of the Ginsu and Machete classes, so you can do the following: cutter = Ginsu () merged_docs = cutter . merge ([ doc1 , doc2 ]) There is also a merge_files() function for merging files on disk and saving the results as a new file: from cutter.merge import merge_files merge_files ( [ \"filename1.txt\" , \"filename1.txt\" ], output_file = \"merged_file.txt\" , binary = False ) If you wish to read and write binary files, set binary=True .","title":"Cutting Documents"},{"location":"tutorial/cutting_docs/#ginsu","text":"The Ginsu class is used for splitting spaCy documents (pre-tokenized texts). Ginsu is the preferred method for creating segments because it can access information supplied by the language model. Ginsu has the following features: split() : Split by number of tokens (i.e. every N token). splitn() : Split by number of segments (i.e. return a predetermined number of segments). split_on_milestones() : Split on milestone tokens. split() and splitn() both have the ability to merge the list segment into the preceding one if it falls under a specific threshold (which can be adjusted with the threshold parameter). They can also generate overlapping segments with the overlap parameter, the value of which is the number of tokens in the overlap. All three methods return a list of lists, where each item in the sublist is a spaCy document. With split_on_milestones() , the user can choose whether or not to preserve the milestone token at the beginning of each segment. A milestone must be a single token and will generally match the token's text attribute. However, it can also match other attributes of the token if they are available in the language model used to produce the spaCy document. A query language (described below) is available for fine-grained matching.","title":"Ginsu"},{"location":"tutorial/cutting_docs/#splitting-documents-by-number-of-tokens","text":"To split documents every N tokens, use the split() method. Here are some examples: from lexos.cutter.ginsu import Ginsu cutter = Ginsu () segments = cutter . split ( doc ) segments = cutter . split ([ doc1 , doc2 ]) By default, the document will be split every 1000 tokens. Here is an example with the full range of options: segments = cutter . split ( doc , n = 500 , overlap = 5 , merge_threshold = 0.5 ) This will split the document every 500 tokens with each segment overlapping by 5 tokens. If the final segment is less than half the length of n , it will be merged with the previous segment (0.5 is the default).","title":"Splitting Documents by Number of Tokens"},{"location":"tutorial/cutting_docs/#splitting-documents-into-a-pre-determined-number-of-segments","text":"To split documents into a pre-determined number of segments, use the splitn() method. Here are some examples: from lexos.cutter.ginsu import Ginsu cutter = Ginsu () segments = cutter . splitn ( doc , n = 10 ) segments = cutter . splitn ([ doc1 , doc2 ], n = 10 , overlap = 5 , merge_threshold = 0.5 ) This will split the document(s) into 10 segments. The overlap and merge_threshold flags work exactly as they do for the split() method.","title":"Splitting Documents into a Pre-Determined Number of Segments"},{"location":"tutorial/cutting_docs/#splitting-documents-on-milestones","text":"A milestone is a pattern that serves as a boundary between segments of a document. The split_on_milestones() method accepts a pattern to match tokens to milestones. Typically a milestone pattern will be a simple string, and Cutter will split the spaCy doc whenever the text attribute of a token matches the pattern. As we will see below, more complex pattern matching methods are possible. from lexos.tokenizer import make_doc from lexos.cutter.ginsu import Ginsu text = \"\"\" It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. \"\"\" doc = make_doc ( text . strip ()) cutter = Ginsu () segments = cutter . split_on_milestones ( doc , milestone = \",\" ) print ( segments ) \"\"\" [ \"It is a truth universally acknowledged\", \"that a single man in possession of a good fortune\", \"must be in want of a wife.\" ] By default, the milestone is deleted from the result. If you wish to preserve it, use preserve_milestones=True . This milestone token will be preserved at the beginning of each segment. Note As shown above, it is possible to splitting documents on frequent tokens like commas. Examples like the one above are given because they can use a short text to illustrate the basic usage of spit_on_milestones() . A more common use case is to cut documents into segments based on markers like chapter headings. If the the milestone parameter is supplied with a list of strings, each item in the list will be treated as a milestone: segments = cutter . split_on_milestones ( doc , milestone = [ \"be\" , \"is\" ], preserve_milestones = True ) print ( segments ) \"\"\" [ \"It\", \"is a truth universally acknowledged, that a single man in possession of a good fortune, must\", \"be in want of a wife.\" ] Milestone patterns can also be matched using a query language supplied in the form of a dict, where the keyword is the name of a spaCy token attribute (e.g. text , pos_ , lemma_ , etc.) and the value is the value to match. By default a token matches a milestone if its value for the attribute is equivalent to the value given in the milestone dict. In other words, if the dict has the form {\"text\": \"chapter\"} , any token in the document for which the value of token.text is \"chapter\" will be treated as a milestone. As result, the following commands are functionally equivalent: segments = cutter . split_on_milestones ( doc , milestone = \",\" ) segments = cutter . split_on_milestones ( doc , milestone = { \"text\" : \",\" }) However, the following will treat all varieties of \"chapter\" (\"chapters\", \"chapter's\", \"Chapter\", etc.) as milestones: segments = cutter . split_on_milestones ( doc , milestone = { \"lemma_\" : \"chapter\" }) Note For further information on spaCy token attributes, see Tokenising Texts . Milestone dict values can also be given as tuples consisting of a pattern and an operator. The operator is the method used to perform the match. Currently, the following operators are available: in : If the pattern is given as a list of strings, any item in the list will be treated as a milestone. not_in : If the pattern is given as a list of strings, any item not in the list will be treated as a milestone. starts_with : Any item starting with the string pattern will be treated as a milestone (uses the Python startswith() function). ends_with : Any item ending with the string pattern will be treated as a milestone (uses the Python endswith() function). re_match : Any item starting with the regular expression pattern will be treated as a milestone (uses the Python re.match() function). re_search : Any item containing with the regular expression pattern will be treated as a milestone (uses the Python re.search() function). For example, the following command will treat any adjective or noun as a milestone. segments = cutter . split_on_milestones ( doc , milestone = { \"pos_\" : ([ \"ADJ\" , \"NOUN\" ], \"in\" )} ) More complex queries can be built using the and and or keywords. For instance, the following pattern is the equivalent of the previous example. segments = cutter . split_on_milestones ( doc , milestone = { \"and\" : { \"pos_\" : \"ADJ\" }, \"or\" : { \"pos_\" : \"NOUN\" } } ) In the example below, the word \"can\" would be treated as a milestone only when it functions as a verb: segments = cutter . split_on_milestones ( doc , milestone = { \"and\" : { \"text\" : \"can\" }, \"and\" : { \"pos_\" : \"VERB\" } } ) Important Filtering based on parts of speech, lemmas, and some other attributes is only possible if you have tokenized your documents using a language model that contains the relevant attribute.","title":"Splitting Documents on Milestones"},{"location":"tutorial/cutting_docs/#the-milestone-class","text":"The Ginsu split functions match tokens to milestones patterns and cut documents on the fly. However, it is also possible to use the lexos.cutter.milestones.Milestones class to preprocess documents. This adds the custom extension token._.is_milestone (by default False ) to each token in the document and uses the same query language to allow the user to match tokens where the value should be True . If documents are pre-processed in this way, Ginsu 's split_on_milestones() method can leverage that information. from lexos.tokenizer import make_doc from lexos.cutter.ginsu import Ginsu from lexos.cutter.milestones import Milestones text = \"\"\" It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. \"\"\" doc = make_doc ( text . strip ()) # Set commas as milestones Milestones () . set ( doc , \",\" ) cutter = Ginsu () segments = cutter . split_on_milestones ( doc , milestone = { \"is_milestone\" : True }) print ( segments ) \"\"\" [ \"It is a truth universally acknowledged\", \"that a single man in possession of a good fortune\", \"must be in want of a wife.\" ] Note Custom extension attributes like token._.is_milestone are normally referenced with the _. prefix. However, in milestone dicts, only the name \"is_milestone\" is given, parallelling built-in attributes like \"is_punct\". The same is true for any other custom extension available in the token's attributes. An obvious advantage of preprocessing milestones is that the custom attribute can be saved. If the user chooses to preserve milestones when using Ginsu.split_on_milestones , the milestone will appear at the beginning of each document, but will not have the _.is_milestone attribute. In the future, it is hoped that we will incorporate the ability to set milestones on the fly.","title":"The Milestone Class"},{"location":"tutorial/cutting_docs/#machete","text":"Machete is a cruder method of cutting raw text into segments without the benefit of a language model. It may be particularly valuable as a standalone method of segmenting texts for outside applications.","title":"Machete"},{"location":"tutorial/cutting_docs/#the-machete-tokenizer","text":"Machete works in a manner similar to Ginsu and has all the same functionality. However, before splitting the text it applies a makeshift tokenizer function and then splits the text based on the resulting list of tokens. The Lexos API has three tokenizer functions in the cutter function registry: \"whitespace\" (splits on whitespace, the default), \"character\" (splits into single-character tokens), and \"linebreaks\" (splits on linebreaks). A Machete object can be initialized with one of the tokenizers or the tokenizer can be passed to the split() , splitn() , and split_on_milestones() methods using the tokenizer parameter. What if I don't like the tokenizer? You can supply a custom function after first adding it to the registry. Here is an example: from lexos.cutter.machete import Machete import lexos.cutter.registry def custom_punctuation_tokenizer ( text : str ) -> str : \"\"\"Split the text on punctuation or whitespace.\"\"\" return re . split ( r \"(\\W+)\" , text ) # Register the custom function registry . tokenizers . register ( \"custom_punctuation_tokenizer\" , func = custom_punctuation_tokenizer ) # Create a `Machete` object machete = Machete () # Split the texts into 5 segments result = machete . splitn ( texts , n = 5 , tokenizer = \"custom_punctuation_tokenizer\" )","title":"The Machete Tokenizer"},{"location":"tutorial/cutting_docs/#splitting-documents-with-machete","text":"split() , splitn() , and split_on_milestones() return a list of lists, where each item in the outer list corresponds to a text and each sublist contains the text's segments. split() , splitn() take the same merge_threshold and overlap parameters as in the Ginsu class. split_on_milestones() is more limited than its Ginsu equivalent. Milestone patterns are evaluated as regular expressions and searched from the beginning of the token string using Python's re.match() function. By default, all three methods return segments as lists of strings. In the example below, we get results with the default as_string=True : from lexos.cutter.machete import Machete text = \"\"\" It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. \"\"\" cutter = Machete () segments = cutter . split_on_milestones ( text . strip (), n = 2 ) print ( segments ) \"\"\" [ [ \"It is a truth universally acknowledged, that a single man in possession\", \"of a good fortune, must be in want of a wife.\" ] The basis for the division can be seen by displaying the segments as lists of tokens using as_string=False : segments = cutter . split_on_milestones ( text . strip (), n = 2 , as_string = False ) print ( segments ) \"\"\" [ ['It ', 'is ', 'a ', 'truth ', 'universally ', 'acknowledged, ', 'that ', 'a ', 'single ', 'man \\n ', 'in ', 'possession '], ['of ', 'a ', 'good ', 'fortune, ', 'must ', 'be ', 'in ', 'want ', 'of ', 'a ', 'wife. \\n '] ] The default \"whitespace\" tokenizer does not strip whitespace around words and does not treat punctuation as separate functions. This makes it very easy to reconstitute the text as a string, but it may not be the desired behaviour for all applications. You may need to apply a custom tokenizer as described above. For specific languages, more reliable results may be obtained by using a language model as described in Tokenizing Texts .","title":"Splitting Documents with Machete"},{"location":"tutorial/cutting_docs/#splitting-lists-of-tokens-with-machete","text":"Sometimes data will be available as lists of tokens, rather than as strings (for instance, if you have already tokenized your texts using a tool like NLTK ). In this case, you can cut your texts using the split_list() method. It works just like split() , except that it takes a list of tokens as input. def split_list( self, texts: List[str], n: int = 1000, merge_threshold: float = 0.5, overlap: int = None, as_strin from lexos.cutter.machete import Machete text = [ \"It\" , \"is\" , \"a\" , \"truth\" , \"universally\" , \"acknowledged\" , \"that\" , \"a\" , \"single\" , \"man\" , \"in\" , \"possession\" , \"of\" , \"a\" , \"good\" , \"fortune\" , \"must\" , \"be\" , \"in\" , \"want\" , \"of\" , \"a\" , \"wife\" ] # Pad each token with a following space text = \" \" . join ( text ) . split () cutter = Machete () segments = cutter . split_list ( text , n = 12 ) print ( segments ) \"\"\" [ [ \"It is a truth universally acknowledged that a single man in possession\", \"of a good fortune must be in want of a wife.\" ] Note Machete assumes that spaces and punctuation between words are preserved, which makes it easier to return the segments as a human-readable string. In the example above, our token list did not preserve spaces or punctuation between words. As a result, it was necessary to take the extra step of padding each token (although the resulting segments have still lost the original punctuation). Steps like this may be necessary when using lists of tokens, depending on how they are created. It should be noted that, whilst of the Machete functions will accept either a single text string or a list of text strings, split_list() accepts only a list of token strings. If you have multiple lists, you should handle them in a loop (or equivalent list comprehension) as follows: from lexos.cutter.machete import Machete texts = [ [ \"It\" , \"is\" , \"a\" , \"truth\" , \"universally\" , \"acknowledged\" , \"that\" , \"a\" , \"single\" , \"man\" ], [ \"in\" , \"possession\" , \"of\" , \"a\" , \"good\" , \"fortune\" , \"must\" , \"be\" , \"in\" , \"want\" , \"of\" , \"a\" , \"wife\" ] ] # Pad each token with a following space texts = [ \" \" . join ( text ) . split () for text in texts ] cutter = Machete () all_segments = [ cutter . split_list ( text , n = 6 ) for text in texts ] print ( all_segments ) \"\"\" [ [ \"It is a truth universally acknowledged\", \"that a single man in possession\" ], [ \"of a good fortune must be\", \"in want of a wife.\" ] ]","title":"Splitting Lists of Tokens with Machete"},{"location":"tutorial/cutting_docs/#filesplit-codename-chainsaw","text":"The lexos.cutter.filesplit.Filesplit class allows the user to cut binary files into numbered file segments with the format filename_1.txt , filename_2.txt , etc. The source file is divided by number of bytes. This class would typically be used as a first step in a workflow if a large file needs to be divided into many smaller files. The class is a fork of Ram Prakash Jayapalan's filesplit module with a few minor tweaks. The most important is that the split function takes a sep argument to allow the user to specify the separator between the filename and number in each generated file. Typical usage is as follows: from lexos.cutter.filesplit import Filesplit fs = Filesplit () fs . split ( file = \"/filesplit_test/longfile.txt\" , split_size = 30000000 , output_dir = \"/filesplit_test/splits/\" , sep = \"__\" ) The split_size parameter indicates the size of the split files in in bytes. The Filesplit class generates a manifest file called fs_manifest.csv in the output directory. This can be used to re-merge the files, if desired: fs . merge ( \"/filesplit_test/splits/\" , cleanup = True ) Setting the cleanup parameter to True causes the manifest file and all split files to be deleted after the merged file is created.","title":"Filesplit (codename Chainsaw)"},{"location":"tutorial/cutting_docs/#merging-other-types-of-segments","text":"If you have not previously split files using Filesplit , Cutter has some standalone functions for merging other types of documents. The merge() function accepts a list of strings or spaCy documents and merges them into a single string or Doc object. If you are merging strings, you can use the sep parameter to control the \"boundary\" between the merged texts. There is no equivalent if you are merging spaCy docs. from cutter.merge import merge merged_text = merge ([ text1 , text2 ], sep = \" \" ) merged_doc = merge ([ doc1 , doc2 ]) Note that merge() is also a method of the Ginsu and Machete classes, so you can do the following: cutter = Ginsu () merged_docs = cutter . merge ([ doc1 , doc2 ]) There is also a merge_files() function for merging files on disk and saving the results as a new file: from cutter.merge import merge_files merge_files ( [ \"filename1.txt\" , \"filename1.txt\" ], output_file = \"merged_file.txt\" , binary = False ) If you wish to read and write binary files, set binary=True .","title":"Merging Other Types of Segments"},{"location":"tutorial/getting_started/","text":"Overview \u00a4 Lexos is a library for constructing text analysis workflows. This normally means a step-by-step pipeline of collecting, processing, analyzing, and visualizating data. (The distinction between the analysis and visualization, however, is often blurred because most visualizations require some form of analysis.) Lexos offers different modules for performing these steps. The Loader and Corpus modules collect and create containers for storing and accessing data. The Scrubber module enables you to perform preprocessing steps on the texts in your data, such as normalizing whitespace or removing certain character patterns. The Tokenizer module uses Natural Language Processing (NLP) tools to extract features from your data \u2014 most importantly, countable tokens. This can be transformed into a document-term matrix with the DTM module. A typical workflow is shown below (dotted lines indicate optional steps). flowchart LR id1{Data} --> id2(((Loader))) & id3[(Corpus)]-. Preprocessing .-> id4{Scrubber}-. Feature Recognition .-> id5{Tokenizer} --> id6{DTM} The DTM module allows you to extract basic statistics which you can use to interpret your data. Lexos modules do not always have to be used in a strict sequential order. For instance, you can feed scrubbed or tokenized texts back into a corpus. You can also split your data at any time in the workflow with the Cutter module. The workflow above might be supplemented by another leading to analysis and visualization. flowchart LR id1{DTM}-.->id2(Analysis) & id3([Visualization]) & id4{Export} Before You Get Started \u00a4 Before you get started, make sure that you have installed Lexos . Basic Usage \u00a4 Lexos workflows can be run conveniently in Jupyter notebooks simply by importing the relevant module (or the required functions and classes from the module). For instance, you can import the Loader with # Import the Lexos smart loader from lexos.io.smart import Loader # Instantiate the Loader object loader = Loader () # Load a text file loader . load ( \"myfile.txt\" ) This will work in a standalone script as well. Any errors will be printed to your notebook or console. If you are designing an app that uses Lexos \"under the hood\", it is good practice to import the LexosException class and re-write the last line above in a try...except clause: from lexos.exceptions import LexosException try : loader . load ( \"myfile.txt\" ) except LexosException as e : print ( e ) This will enable your application to handle errors without stopping the program. To learn about each of the individual modules in the Lexos API, browse through the pages in this tutorial, which take you through the modules and their applications one by one.","title":"Getting Started"},{"location":"tutorial/getting_started/#overview","text":"Lexos is a library for constructing text analysis workflows. This normally means a step-by-step pipeline of collecting, processing, analyzing, and visualizating data. (The distinction between the analysis and visualization, however, is often blurred because most visualizations require some form of analysis.) Lexos offers different modules for performing these steps. The Loader and Corpus modules collect and create containers for storing and accessing data. The Scrubber module enables you to perform preprocessing steps on the texts in your data, such as normalizing whitespace or removing certain character patterns. The Tokenizer module uses Natural Language Processing (NLP) tools to extract features from your data \u2014 most importantly, countable tokens. This can be transformed into a document-term matrix with the DTM module. A typical workflow is shown below (dotted lines indicate optional steps). flowchart LR id1{Data} --> id2(((Loader))) & id3[(Corpus)]-. Preprocessing .-> id4{Scrubber}-. Feature Recognition .-> id5{Tokenizer} --> id6{DTM} The DTM module allows you to extract basic statistics which you can use to interpret your data. Lexos modules do not always have to be used in a strict sequential order. For instance, you can feed scrubbed or tokenized texts back into a corpus. You can also split your data at any time in the workflow with the Cutter module. The workflow above might be supplemented by another leading to analysis and visualization. flowchart LR id1{DTM}-.->id2(Analysis) & id3([Visualization]) & id4{Export}","title":"Overview"},{"location":"tutorial/getting_started/#before-you-get-started","text":"Before you get started, make sure that you have installed Lexos .","title":"Before You Get Started"},{"location":"tutorial/getting_started/#basic-usage","text":"Lexos workflows can be run conveniently in Jupyter notebooks simply by importing the relevant module (or the required functions and classes from the module). For instance, you can import the Loader with # Import the Lexos smart loader from lexos.io.smart import Loader # Instantiate the Loader object loader = Loader () # Load a text file loader . load ( \"myfile.txt\" ) This will work in a standalone script as well. Any errors will be printed to your notebook or console. If you are designing an app that uses Lexos \"under the hood\", it is good practice to import the LexosException class and re-write the last line above in a try...except clause: from lexos.exceptions import LexosException try : loader . load ( \"myfile.txt\" ) except LexosException as e : print ( e ) This will enable your application to handle errors without stopping the program. To learn about each of the individual modules in the Lexos API, browse through the pages in this tutorial, which take you through the modules and their applications one by one.","title":"Basic Usage"},{"location":"tutorial/loading_texts/","text":"A typical workflow would create a Loader object and call loader.load() to load the data from disk or download it from the internet. You can access all loaded texts by calling Loader.texts . Note It is more efficient simply to use Python's open() to load texts into a list if you know the file's encoding. The advantage of the Loader class is that it automatically coerces the data to Unicode and it allows you to use the same method regardless of the file's format or whether it is on your local machine or downloaded from the internet. When you use a Loader , all your data is stored in memory for use in a Lexos workflow. You can save it to disk, but it is largely up to you to keep track of your data folder(s) and file locations. If you wish to have a more sophisticated system for managing your data, look at Managing a Corpus . Lexos has multiple Loader classes found in the IO module. The simplest to use is the smart loader, described below. The Smart Loader \u00a4 Here is a sample of the code for loading a single text file: #import Loader from lexos.io.smart import Loader # Create the loader and load the data loader = Loader () loader . load ( \"myfile.txt) # Print the first text in the Loader text = loader . texts [ 0 ] print ( text ) The load() function accepts filepaths, urls, or lists of either. If urls are submitted, the content will be downloaded automatically. Valid formats are .txt files, .docx files, and .pdf files, as well as directories or .zip files containing only files of these types. A Loader object has six properties: source : The filepath or url of the last item added to the Loader . names : A list of the names of all items added to the Loader . This will normally be the filenames without the extensions, unless you change them. locations : A list of the filepaths or urls of all items added to the Loader . texts : A list contain the full text of all the items added to the Loader . errors : A list of filepaths or urls for which loading failed. As you can see from the example above, each of these properties can be accessed by called Loader.names , Loader.texts , etc. You can also iterate through a Loader and get the name , location , and text of each item: for item in loader : print ( item . name ) print ( item . text ) The Dataset Loader \u00a4 A \"dataset\" refers to a collection of documents which are often stored and meant to be accessed from a single file. Lexos has a DatasetLoader class designed to work with these data sources. Here is an example in which a single plain text file containing one document per line is loaded. #import Loader from lexos.io.dataset import DatasetLoader # Create the loader and load the data dataset_loader = Loader () dataset_loader . load ( \"myfile.txt\" , labels = [ \"Doc1\" , \"Doc2\" ]) Each line in the file is added to the dataset.texts list. Since we cannot use the filename to generate names for our documents, you need to supply a list of names using the labels parameter. These values will then be accessible in dataset.names . The DatasetLoader.load() method accepts files, urls, and directories of files in .txt , .csv , .tsv , .xlsx , json , and jsonl format, as well zip archives containing files in those formats. As shown above, .txt files must be line-delimited, without a header, and must be accompanied by a list of labels . .csv , .tsv , and .xlsx files must have a header line containing the values title and text . Lexos will use these columns to assign your documents' name and text values. If your source file has a different header, you can tell Lexos which headers to use, as in the following example: dataset_loader . load ( \"myfile.tsv\" , title_col = \"label\" , text_col = \"content\" , sep = \" \\t \" ) The example above also tells Lexos to use a tab as the separator between columns since the file being loaded is a tab-separated value file. Under the hood, Lexos reads the data with the Pandas library's read_csv , read_excel , and read_json file, and you can pass along any keywords accepted by those methods. The sep keyword in the example above is an example. For JSON-formatted files, use title_field and text_field to assign which columns should be read by Lexos. If your file is in newline-delimited JSON (JSONL) format, add the parameter lines=True . Once loaded, texts and their metadata can be accessed with the DatasetLoader.data property. This is a list of dicts where each document dict has keywords for title and text . To access the first document's title, you would use Dataset.data[0][\"title\"] . When iterating through the dataset, the data property is optional: for item in dataset : print ( item [ \"title\" ]) produces the same result as for item in dataset . data : print ( item [ \"title\" ]) Warning Notice that iterating through the DatasetLoader requires that you reference keywords of a dict ( item[\"text\"] , where as the smart loader yields an object, allowing you to reference item.text . We hope to make this behaviour more consistent in the future. The Dataset Class \u00a4 Internally, the DatasetLoader detects the format of the input data and then calls the appropriate method of the Dataset class. For instance, if the file is a CSV file, the Dataset.parse_csv() method will be used. In most case, it makes sense to take advantage of the DatasetLoader 's format detection so that you can use the same syntax for all inputs, but in some circumstances, it may be useful to call Dataset directly. Here is an example of how you would do it: from lexos.io.dataset import Dataset dataset = Dataset . parse_csv ( \"myfile.csv\" ) for item in dataset : print ( item [ \"title\" ]) Dataset.parse_csv() takes the same text_col and title_col arguments that you would pass to the DatasetLoader . Here is a list of the main Dataset methods and the arguments they take: parse_string() : Parses line-delimited text files. Requires labels . parse_csv() : Parses a CSV file. Requires text_col and title_col if there are no text and title headers. Requires sep=\"\\t\" is the file is a tab-separated value file. parse_excel() : Parses an Excel file. Requires text_col and title_col if there are no text and title headers. parse_json() : Parses a JSON file. Requires text_field and title_field if there are no text and title fields. parse_jsonl() : Parses a JSONL file. Requires text_field and title_field if there are no text and title fields. Adding Datasets to a Standard Lexos Loader \u00a4 If you already have a Loader, it is easy to add datasets to it. # Import the loaders from lexos.io.smart import Loader from lexos.io.dataset import Dataset , DatasetLoader # Create and empty `Loader` loader = Loader () # Create a `DatasetLoader` and load a dataset dataset_loader = DatasetLoader ( \"myfile1.csv\" ) # Load a dataset with `Dataset` dataset = Dataset . parse_csv ( \"myfile1.csv\" ) # Add the text and names for each dataset to the standard loader for item in [ dataset_loader , dataset ]: loader . names . extend ( item . names ) loader . texts . extend ( item . texts ) Once you have all your data in a Loader , you can manipulate the text. Almost inevitably, some of the text you have loaded will be \"dirty\" \u2014 meaning that it is not quite in the shape you want it in for further analysis. This may be a moment to do some preprocessing with the Scrubber module.","title":"Loading Texts"},{"location":"tutorial/loading_texts/#the-smart-loader","text":"Here is a sample of the code for loading a single text file: #import Loader from lexos.io.smart import Loader # Create the loader and load the data loader = Loader () loader . load ( \"myfile.txt) # Print the first text in the Loader text = loader . texts [ 0 ] print ( text ) The load() function accepts filepaths, urls, or lists of either. If urls are submitted, the content will be downloaded automatically. Valid formats are .txt files, .docx files, and .pdf files, as well as directories or .zip files containing only files of these types. A Loader object has six properties: source : The filepath or url of the last item added to the Loader . names : A list of the names of all items added to the Loader . This will normally be the filenames without the extensions, unless you change them. locations : A list of the filepaths or urls of all items added to the Loader . texts : A list contain the full text of all the items added to the Loader . errors : A list of filepaths or urls for which loading failed. As you can see from the example above, each of these properties can be accessed by called Loader.names , Loader.texts , etc. You can also iterate through a Loader and get the name , location , and text of each item: for item in loader : print ( item . name ) print ( item . text )","title":"The Smart Loader"},{"location":"tutorial/loading_texts/#the-dataset-loader","text":"A \"dataset\" refers to a collection of documents which are often stored and meant to be accessed from a single file. Lexos has a DatasetLoader class designed to work with these data sources. Here is an example in which a single plain text file containing one document per line is loaded. #import Loader from lexos.io.dataset import DatasetLoader # Create the loader and load the data dataset_loader = Loader () dataset_loader . load ( \"myfile.txt\" , labels = [ \"Doc1\" , \"Doc2\" ]) Each line in the file is added to the dataset.texts list. Since we cannot use the filename to generate names for our documents, you need to supply a list of names using the labels parameter. These values will then be accessible in dataset.names . The DatasetLoader.load() method accepts files, urls, and directories of files in .txt , .csv , .tsv , .xlsx , json , and jsonl format, as well zip archives containing files in those formats. As shown above, .txt files must be line-delimited, without a header, and must be accompanied by a list of labels . .csv , .tsv , and .xlsx files must have a header line containing the values title and text . Lexos will use these columns to assign your documents' name and text values. If your source file has a different header, you can tell Lexos which headers to use, as in the following example: dataset_loader . load ( \"myfile.tsv\" , title_col = \"label\" , text_col = \"content\" , sep = \" \\t \" ) The example above also tells Lexos to use a tab as the separator between columns since the file being loaded is a tab-separated value file. Under the hood, Lexos reads the data with the Pandas library's read_csv , read_excel , and read_json file, and you can pass along any keywords accepted by those methods. The sep keyword in the example above is an example. For JSON-formatted files, use title_field and text_field to assign which columns should be read by Lexos. If your file is in newline-delimited JSON (JSONL) format, add the parameter lines=True . Once loaded, texts and their metadata can be accessed with the DatasetLoader.data property. This is a list of dicts where each document dict has keywords for title and text . To access the first document's title, you would use Dataset.data[0][\"title\"] . When iterating through the dataset, the data property is optional: for item in dataset : print ( item [ \"title\" ]) produces the same result as for item in dataset . data : print ( item [ \"title\" ]) Warning Notice that iterating through the DatasetLoader requires that you reference keywords of a dict ( item[\"text\"] , where as the smart loader yields an object, allowing you to reference item.text . We hope to make this behaviour more consistent in the future.","title":"The Dataset Loader"},{"location":"tutorial/loading_texts/#the-dataset-class","text":"Internally, the DatasetLoader detects the format of the input data and then calls the appropriate method of the Dataset class. For instance, if the file is a CSV file, the Dataset.parse_csv() method will be used. In most case, it makes sense to take advantage of the DatasetLoader 's format detection so that you can use the same syntax for all inputs, but in some circumstances, it may be useful to call Dataset directly. Here is an example of how you would do it: from lexos.io.dataset import Dataset dataset = Dataset . parse_csv ( \"myfile.csv\" ) for item in dataset : print ( item [ \"title\" ]) Dataset.parse_csv() takes the same text_col and title_col arguments that you would pass to the DatasetLoader . Here is a list of the main Dataset methods and the arguments they take: parse_string() : Parses line-delimited text files. Requires labels . parse_csv() : Parses a CSV file. Requires text_col and title_col if there are no text and title headers. Requires sep=\"\\t\" is the file is a tab-separated value file. parse_excel() : Parses an Excel file. Requires text_col and title_col if there are no text and title headers. parse_json() : Parses a JSON file. Requires text_field and title_field if there are no text and title fields. parse_jsonl() : Parses a JSONL file. Requires text_field and title_field if there are no text and title fields.","title":"The Dataset Class"},{"location":"tutorial/loading_texts/#adding-datasets-to-a-standard-lexos-loader","text":"If you already have a Loader, it is easy to add datasets to it. # Import the loaders from lexos.io.smart import Loader from lexos.io.dataset import Dataset , DatasetLoader # Create and empty `Loader` loader = Loader () # Create a `DatasetLoader` and load a dataset dataset_loader = DatasetLoader ( \"myfile1.csv\" ) # Load a dataset with `Dataset` dataset = Dataset . parse_csv ( \"myfile1.csv\" ) # Add the text and names for each dataset to the standard loader for item in [ dataset_loader , dataset ]: loader . names . extend ( item . names ) loader . texts . extend ( item . texts ) Once you have all your data in a Loader , you can manipulate the text. Almost inevitably, some of the text you have loaded will be \"dirty\" \u2014 meaning that it is not quite in the shape you want it in for further analysis. This may be a moment to do some preprocessing with the Scrubber module.","title":"Adding Datasets to a Standard Lexos Loader"},{"location":"tutorial/mallet_topic_models/","text":"MALLET Topic Models in Lexos \u00a4 Topic modelling is a widely-used method of exploring the semantic and discursive concepts, or \"topics\", within collections of texts. Wikipedia defines a topic model as follows: In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is. MALLET is the most widely used topic modelling tool in the Digital Humanities, both because it is very performant and because its implementation of the Latent Dirichlet Allocation (LDA) algorithm tends to produce quality topics. MALLET is a command-line tool written in Java. It is independent of Lexos and must be installed separately. User-friendly instructions for installing and using MALLET can be found in the Programming Historian tutorial Getting Started with Topic Modeling and MALLET . One of the difficulties of using MALLET is that its output is relatively difficult to manipulate into data structures useful for visualization. This tutorial is a proof of concept for how we might use Lexos to generate a MALLET topic model and then use the model to create a visualization with Andrew Goldstone's dfr-browser . Note Much of the legwork for this procedure was done for the WhatEvery1Says Project , which established the basic workflow. Before Getting Started \u00a4 Before you get started, make sure that you have a working installation of MALLET by following the instructions in the Programming Historian tutorial Getting Started with Topic Modeling and MALLET . Make sure that you know the path to the MALLET binary file. Next, make a new folder for your topic model. In this tutorial, we will locate our new folder at ../topic_model , which indicates that the folder is at the same level as the Lexos API's lexos folder. Import Some Data \u00a4 For this tutorial, we'll use the MALLET English-language sample data. This a very small dataset and should run very quickly. It should have been downloaded when you installed MALLET. Change the data_path to wherever your data is located. The code below simply reads a folder of text files and adds each text to a list. data_path = \"C:/mallet/mallet-2.0.8/sample-data/web/en\" data = [] for file in os . listdir ( data_path ): with open ( f \" { data_path } / { file } \" , \"r\" ) as f : data . append ( f . read ()) Create Metadata \u00a4 Although not required for topic modelling, metadata is required to generate a dfr-browser. Dfr-browser was originally designed for displaying models of journal articles in the JSTOR database, so you need to supply metadata fields with the categories it expects. These categories are id , title , publication , authors , volume , issue , year , and pagerange . If these categories are not appropriate to your data, you can leave them blank (as an empty string). You can also include additional fields (e.g. file or url ), although they may not be displayed in the dfr-browser. Further information on customizing metadata can be found in the dfr-browser documentation . Metadata should be stored in a CSV file with no headings called meta.csv . Scrub the Data \u00a4 Now we will use Lexos to scrub the data. We import the Scrubber components, make a pipeline, and run the pipeline on each text. The components here are just random samples of the possible options. from lexos.scrubber.pipeline import make_pipeline , pipe from lexos.scrubber.registry import scrubber_components , load_components emails , new_lines , pattern = load_components (( \"emails\" , \"new_lines\" , \"pattern\" )) scrub = make_pipeline ( emails , new_lines , pipe ( pattern , pattern = \" \\' \" ) ) data = [ scrub ( item ) for item in data ] Tokenize the Data \u00a4 We will import the Lexos tokenizer and create a list of spaCy docs. In the example below, we use spaCy's \"en_core_web_sm\" language model, and we'll add \"gorillas\" as an arbitrary extra stop word. Keep in mind that each token in the doc is annotated with its part of speech, whether or not it is a stop word, and whether or not it is a punctuation mark (to name a few examples). We will use these properties below. Note that because tokenization also involves adding these annotations, it may take a long time for large datasets. from lexos import tokenizer docs = tokenizer . make_docs ( data , model = \"en_core_web_sm\" , add_stopwords = [ \"gorillas\" ] ) Topic Modelling \u00a4 We are now ready to create the topic model. We start by creating a Mallet object, pointing it to a directory where we would like the model to be saved, and supplying the path to our MALLET installation. from lexos.topic_model.mallet import Mallet model = Mallet ( model_dir = \"../topic_model\" , mallet_path = \"C:/mallet/mallet-2.0.8/bin\" ) Import the Data \u00a4 We use our Mallet object to import our tokenized docs. In the example below, we will import only tokens labelled as nouns. The default behaviour is to skip stop words and punctuation. This process creates two files in the model directory. The first is called data.txt . This file contains all our doc tokens with one doc per line. Each doc is a bag of words (meaning token order is lost). The second file is called import.mallet . This contains the information in data.txt , imported into a binary format. It will also have the method's default settings overridden by any MALLET parameters you supply. However, we'll stick with the defaults below. model . import_data ( docs , allowed = [ \"NOUN\" ]) Note You can override the default settings by creating a dict of keyword-value pairs based on MALLET settings and then passing the dict to the import_data() function. For instance, say you wanted to use an external stop word file called stoplist.txt : opts = { \"remove-stopwords\" : True , \"stoplist-file\" : \"stoplist.txt\" } model . import_data ( docs , ** opts ) This feature is currently not fully developed, but it should work for some basic procedures. Train the Model \u00a4 If we've followed the procedure above, we can simply call model.train() . If for some reason, we need to re-instantiate the Mallet object, we can do so and skip the import step above. In this case, we would call model.train(mallet_file=\"import.mallet\") . model . train () Important The progress of the modelling task is monitored by continuous output to the console. If you are running the process in a Jupyter notebook, you may wish to put %%capture at the beginning of the cell so that the progress output is not printed to the cell's output, which may overwhelm the memory buffer. Eventually, we will create a progress bar option to avoid this issue. Once the model is complete (which may take a long time if you have a lot of data but should take seconds for the MALLET sample data), it is worth inspecting the model. Navigate to your models keys.txt file and open it. If some topics have no keywords, that is a sign that something has gone wrong with your model. If everything looks good, you're ready for the next step. Create the Topic Scale File \u00a4 dfr-browser requires an additional CSV file containing topic scaling data. To produce this, we just need to call model.scale() model . scale () Dfr-Browser \u00a4 We can now generate a dfr-browser from our topic model. We import the DfrBrowser class and create a DfrBrowser object. This will create a dfr_browser directory in your model's folder where all the necessary files are housed. Make sure that your meta.csv file is in the root of your topic model folder. from lexos.topic_model.dfr_browser import DfrBrowser browser = DfrBrowser ( model_dir = \"../topic_model\" ) Open the Dfr-Browser \u00a4 When the process is complete, you will need to start a local server. Open a command prompt and cd to your model's dfr_browser folder. Then type python -m http.server 8080 . If you are already running a local server on port 8080, you can change it to something else. Then point your browser to http://localhost:8080/ , and the dfr-browser should load. Note that some features of dfr-browser may not work if you do not have appropriate metadata. When you are finished, remember to go back to the command prompt and type Ctr+C to shutdown the server.","title":"Topic Modelling with MALLET"},{"location":"tutorial/mallet_topic_models/#mallet-topic-models-in-lexos","text":"Topic modelling is a widely-used method of exploring the semantic and discursive concepts, or \"topics\", within collections of texts. Wikipedia defines a topic model as follows: In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is. MALLET is the most widely used topic modelling tool in the Digital Humanities, both because it is very performant and because its implementation of the Latent Dirichlet Allocation (LDA) algorithm tends to produce quality topics. MALLET is a command-line tool written in Java. It is independent of Lexos and must be installed separately. User-friendly instructions for installing and using MALLET can be found in the Programming Historian tutorial Getting Started with Topic Modeling and MALLET . One of the difficulties of using MALLET is that its output is relatively difficult to manipulate into data structures useful for visualization. This tutorial is a proof of concept for how we might use Lexos to generate a MALLET topic model and then use the model to create a visualization with Andrew Goldstone's dfr-browser . Note Much of the legwork for this procedure was done for the WhatEvery1Says Project , which established the basic workflow.","title":"MALLET Topic Models in Lexos"},{"location":"tutorial/mallet_topic_models/#before-getting-started","text":"Before you get started, make sure that you have a working installation of MALLET by following the instructions in the Programming Historian tutorial Getting Started with Topic Modeling and MALLET . Make sure that you know the path to the MALLET binary file. Next, make a new folder for your topic model. In this tutorial, we will locate our new folder at ../topic_model , which indicates that the folder is at the same level as the Lexos API's lexos folder.","title":"Before Getting Started"},{"location":"tutorial/mallet_topic_models/#import-some-data","text":"For this tutorial, we'll use the MALLET English-language sample data. This a very small dataset and should run very quickly. It should have been downloaded when you installed MALLET. Change the data_path to wherever your data is located. The code below simply reads a folder of text files and adds each text to a list. data_path = \"C:/mallet/mallet-2.0.8/sample-data/web/en\" data = [] for file in os . listdir ( data_path ): with open ( f \" { data_path } / { file } \" , \"r\" ) as f : data . append ( f . read ())","title":"Import Some Data"},{"location":"tutorial/mallet_topic_models/#create-metadata","text":"Although not required for topic modelling, metadata is required to generate a dfr-browser. Dfr-browser was originally designed for displaying models of journal articles in the JSTOR database, so you need to supply metadata fields with the categories it expects. These categories are id , title , publication , authors , volume , issue , year , and pagerange . If these categories are not appropriate to your data, you can leave them blank (as an empty string). You can also include additional fields (e.g. file or url ), although they may not be displayed in the dfr-browser. Further information on customizing metadata can be found in the dfr-browser documentation . Metadata should be stored in a CSV file with no headings called meta.csv .","title":"Create Metadata"},{"location":"tutorial/mallet_topic_models/#scrub-the-data","text":"Now we will use Lexos to scrub the data. We import the Scrubber components, make a pipeline, and run the pipeline on each text. The components here are just random samples of the possible options. from lexos.scrubber.pipeline import make_pipeline , pipe from lexos.scrubber.registry import scrubber_components , load_components emails , new_lines , pattern = load_components (( \"emails\" , \"new_lines\" , \"pattern\" )) scrub = make_pipeline ( emails , new_lines , pipe ( pattern , pattern = \" \\' \" ) ) data = [ scrub ( item ) for item in data ]","title":"Scrub the Data"},{"location":"tutorial/mallet_topic_models/#tokenize-the-data","text":"We will import the Lexos tokenizer and create a list of spaCy docs. In the example below, we use spaCy's \"en_core_web_sm\" language model, and we'll add \"gorillas\" as an arbitrary extra stop word. Keep in mind that each token in the doc is annotated with its part of speech, whether or not it is a stop word, and whether or not it is a punctuation mark (to name a few examples). We will use these properties below. Note that because tokenization also involves adding these annotations, it may take a long time for large datasets. from lexos import tokenizer docs = tokenizer . make_docs ( data , model = \"en_core_web_sm\" , add_stopwords = [ \"gorillas\" ] )","title":"Tokenize the Data"},{"location":"tutorial/mallet_topic_models/#topic-modelling","text":"We are now ready to create the topic model. We start by creating a Mallet object, pointing it to a directory where we would like the model to be saved, and supplying the path to our MALLET installation. from lexos.topic_model.mallet import Mallet model = Mallet ( model_dir = \"../topic_model\" , mallet_path = \"C:/mallet/mallet-2.0.8/bin\" )","title":"Topic Modelling"},{"location":"tutorial/mallet_topic_models/#import-the-data","text":"We use our Mallet object to import our tokenized docs. In the example below, we will import only tokens labelled as nouns. The default behaviour is to skip stop words and punctuation. This process creates two files in the model directory. The first is called data.txt . This file contains all our doc tokens with one doc per line. Each doc is a bag of words (meaning token order is lost). The second file is called import.mallet . This contains the information in data.txt , imported into a binary format. It will also have the method's default settings overridden by any MALLET parameters you supply. However, we'll stick with the defaults below. model . import_data ( docs , allowed = [ \"NOUN\" ]) Note You can override the default settings by creating a dict of keyword-value pairs based on MALLET settings and then passing the dict to the import_data() function. For instance, say you wanted to use an external stop word file called stoplist.txt : opts = { \"remove-stopwords\" : True , \"stoplist-file\" : \"stoplist.txt\" } model . import_data ( docs , ** opts ) This feature is currently not fully developed, but it should work for some basic procedures.","title":"Import the Data"},{"location":"tutorial/mallet_topic_models/#train-the-model","text":"If we've followed the procedure above, we can simply call model.train() . If for some reason, we need to re-instantiate the Mallet object, we can do so and skip the import step above. In this case, we would call model.train(mallet_file=\"import.mallet\") . model . train () Important The progress of the modelling task is monitored by continuous output to the console. If you are running the process in a Jupyter notebook, you may wish to put %%capture at the beginning of the cell so that the progress output is not printed to the cell's output, which may overwhelm the memory buffer. Eventually, we will create a progress bar option to avoid this issue. Once the model is complete (which may take a long time if you have a lot of data but should take seconds for the MALLET sample data), it is worth inspecting the model. Navigate to your models keys.txt file and open it. If some topics have no keywords, that is a sign that something has gone wrong with your model. If everything looks good, you're ready for the next step.","title":"Train the Model"},{"location":"tutorial/mallet_topic_models/#create-the-topic-scale-file","text":"dfr-browser requires an additional CSV file containing topic scaling data. To produce this, we just need to call model.scale() model . scale ()","title":"Create the Topic Scale File"},{"location":"tutorial/mallet_topic_models/#dfr-browser","text":"We can now generate a dfr-browser from our topic model. We import the DfrBrowser class and create a DfrBrowser object. This will create a dfr_browser directory in your model's folder where all the necessary files are housed. Make sure that your meta.csv file is in the root of your topic model folder. from lexos.topic_model.dfr_browser import DfrBrowser browser = DfrBrowser ( model_dir = \"../topic_model\" )","title":"Dfr-Browser"},{"location":"tutorial/mallet_topic_models/#open-the-dfr-browser","text":"When the process is complete, you will need to start a local server. Open a command prompt and cd to your model's dfr_browser folder. Then type python -m http.server 8080 . If you are already running a local server on port 8080, you can change it to something else. Then point your browser to http://localhost:8080/ , and the dfr-browser should load. Note that some features of dfr-browser may not work if you do not have appropriate metadata. When you are finished, remember to go back to the command prompt and type Ctr+C to shutdown the server.","title":"Open the Dfr-Browser"},{"location":"tutorial/managing_a_corpus/","text":"Overview \u00a4 The Corpus module consists of a Corpus class that helps you manage assets in your workflow and serialize them to disk for later use. It is strictly optional; you may find it sufficient to load your documents into memory with a Loader or to manage your corpus assets independently through a different application. It is important to realise that a Lexos Corpus is primarily a manager for project assets; it is not used for acquiring those assets and is not used for analysing them, apart from the generation of a few statistics. In general, using a Corpus will require a workflow like the following: Use Loader to acquire texts. Optionally, use Scrubber to perform any required preprocessing on the texts in Loader.texts . Optionally, use Tokenizer to convert the original or scrubbed texts to spaCy Doc objects. Add the documents (texts or spaCy Doc objects) to the Corpus . If you wished to analyse the documents, you would get them from the Corpus and run them through Tokenizer if you did not do so before adding them. From this workflow, you should be able to see that you can skip the Corpus entirely. The Corpus simply allows you to attach metadata to the documents, such as a name, description, or classification label, and to save them to and retrieve them from disk easily. One of the important metadata categories is whether or not a document is active. A Corpus allows you to retrieve subsets of your documents based on this and other metadata categories. Creating a Corpus \u00a4 Begin by importing the Corpus module: import lexos.corpus as cp We use the cp alias so that we can use corpus as a variable below: corpus = cp . Corpus ( name = \"My Corpus\" , description = \"My test corpus\" , corpus_dir = \"corpus\" ) The name , description , and corpus_dir arguments are all optional. corpus_dir is the directory where the corpus will be stored, and the default is \"corpus\" in the current directory. You can use additional keywords to instantiate the corpus with other metadata such as author or creation date. When you run the code above, the corpus directory will be created if it does not already exist. You can view your corpus metadata in a number of ways: Corpus.meta_table() will return an overview of your corpus as a pandas dataframe. You can also view this information as a dict with corpus.meta . The following individual properties can also be viewed: ids : A list of document ids in the Corpus. names : A list of document names in the Corpus. docs : A dict of document ids and docs in the Corpus if you have opted to cache them to RAM. num_docs : The number of docs in the Corpus. num_active_docs : The number of active docs in the Corpus. num_tokens : The number of tokens in the Corpus. num_terms : The number of terms in the Corpus. terms : A set of unique terms in the Corpus. Call these functions with code like Corpus.num_tokens . You can also get a Python collections Counter object containing the corpus term counts with Corpus.get_term_counts() . These attributes should all be empty or 0 when the corpus is first created. Note The Corpus class is constructed using Pydantic's BaseModel class . This means that it has access to any of Pydantic's attributes and methods, such as dict() and json() . Corpus Records \u00a4 The basic unit of storage in a Corpus is a Record object. This is a Python object that provides access to the record's content and its metadata. Constructing a Record is simple. You just have to feed it some content and, in most cases, give it a name: record = Record(content=mydoc, name=\"greeting\") Behind the scenes, the Record class will give the record a default id of 1 (unless you specify a different integer) and set the is_active property to True (unless you set instantiate the object with it set to False ). See lexos.corpus.Record for other arguments that can be passed to the Record class. You can also create a Record from a dict using Pydantic's parse_obj() method: record = Record . parse_obj ({ \"content\" : mydoc , \"name\" : \"greeting\" }) See the Pydantic documentation for helper functions for parsing json or file content into objects. Once instantiated, a record provides access to the following information: preview : A preview of the first 50 characters of the record's text. terms : A set of unique terms in the record's content. text : The full text of the record's content. tokens : A list of tokens in the record's content. num_terms : The number of unique terms in the record's content. num_tokens : The number of tokens in the record's content. Important Term counts do not collapse upper- and lower-case words, so, if this is important, you must get the tokens, convert to lower case, and then generate the list of terms yourself. Alternatively, you may use Scrubber to preprocess your data before creating the Record object. Record.set() allows you to set arbitrary Record attributes (such as author or date), and Record.save() allows you to save the file to disk. Important When a Record object is saved to disk, it is serialized as a binary pickle file, which is not human readable. To restore it, you use a normal Python method of reading a binary file: with open ( filename , \"rb\" ) as f : record = pickle . load ( f ) The pickle format is not considered secure, so never unpickle a file you do not trust. In the latest version of spaCy, it is possible to serialize to JSON, but these methods have not yet been integrated in the Lexos API. The Record class accepts content only in the form of a pre-tokenized spaCy doc. However, it is possible to store an untokenized text by creating a blank spaCy language model and feeding it the lexos.corpus.NullTokenizer class. This simply returns a spaCy doc with the text as a single token. nlp = spacy . blank ( \"xx\" ) nlp . tokenizer = NullTokenizer ( nlp . vocab ) content = nlp ( content ) record = Record ( content ) Note that the entire text will be counted as a single token and a single term, so it is preferable to tokenize the text first or to plan to do so later. If the content is already a tokenized document, it is necessary to label it as such in the metadata. Here is an example of how you would do it: import spacy nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"Hi there!\" ) record = Record ( content = doc , name = \"greeting\" , is_parsed = True ) The is_parsed attribute allows Corpus to know that it is dealing with a tokenized document. You can still access the full text by calling record.text , but you can also access individual tokens by calling record.content[0] (to get the first token). If you want a dictionary with a record's full metadata, probably the easiest method is metadata = record.dict().remove(\"content\") . Why serialize records with pickle ? A Lexos Record is a Python object which contains a spaCy Doc object which contains spaCy Token objects. This complex structure creates a scenario which cannot be handled by other serialization formats without some serious hacks. There are some concerns about whether serialization and de-serialization will be fast enough when working with many records in a corpus (and lesser concerns about the security of the format), but for the moment it is the easiest and most straightforward format to work with. This is something to be revisited at a future date, especially now that spaCy has added a Doc.to_json() method. Adding Records to a Corpus \u00a4 Adding records to a corpus is simple with Corpus.add_record() : record = Record ( content = doc , name = \"greeting\" , is_parsed = True ) corpus . add_record ( record , cache = True ) There is also a Corpus.add_records() , which takes a list of records. By default, the record's content is not cached in memory; instead, the entire record is serialized to disk. If you want to keep it in memory, you can set cache=True (as above). This will allow you to access the record from corpus.docs without having to fetch the record from disk. Note At present the docs property in the Corpus class is the only place where a clear distinction between a \"record\" and a \"document\" is made. Adding Documents to a Corpus \u00a4 It is not necessary to pre-generate records from documents before adding them to a corpus. You can also use Corpus.add() to add a document directly: # Use a text string unparsed_doc = \"Hi there!\" corpus . add_record ( unparsed_doc , name = \"greeting\" ) # Create parsed_doc = nlp ( \"This is a full sentence.\" ) corpus . add ( content = parsed_doc , name = \"sentence\" , is_parsed = True ) By default, the is_active attribute is True . You can set additional metadata properties by supplying a metadata dict: metadata = { \"author\" : \"John Smith\" , \"date\" : \"2011\" } corpus . add ( content = parsed_doc , name = \"sentence\" , is_parsed = True , metadata = metadata , cache = True ) The corresponding Corpus.add_docs() allows you to insert multiple documents. The format is a little more complicated. It takes a list of dicts with the document as the value of the content property: docs = [ { \"content\" : doc1 , \"name\" : \"Doc 1\" }, { \"content\" : doc2 , \"name\" : \"Doc 2\" }, ] corpus . add_docs ( docs , cache = True ) All the arguments accepted by Corpus.add() can be set as keys in the docs dictionary. Important Whether you are adding documents or records to a corpus, a check is made to ensure that the records stored have unique id , name , and filename attributes. If you do not specify a name for a document or record, a UUID will be used instead and will be used to generate a corresponding filename. The results of this can be unwieldy. In the future, some other method of ensuring uniqueness will be explored. Getting Records from the Corpus \u00a4 Individual records can be fetched using Corpus.get() with a record id : record = corpus . get ( 1 ) doc = record . content The second line above extracts the spaCy doc from the records, and it can be treated like any spaCy doc. You can also supply a list of ids to Corpus.get_records() . If you pass nothing to the method, all the records in the corpus will be retrieved. If you do not know the id(s) of the document(s) you want, you can provide a query for Corpus.get_records() : records = corpus . get_records ( query = \"id < 10\" ) for record in records : print ( record . name ) This will yield a generator with each of the records with an id less than 10. Note On the back end, Corpus.get() and Corpus.get_records() call Corpus.meta , which contains a subset of the metadata for each record. A pandas dataframe is constructed from this metadata. The query can therefore be anything acceptable to pandas.DataFrame.query() . This allows complex queries to be performed on the corpus. If you want just the metadata for a record, probably the easiest method is metadata = corpus.get(1).dict().remove(\"content\") . Viewing the Records Table \u00a4 Corpus.records_table() generates a pandas dataframe with each record in a separate row. By default, the id , name , filename , num_tokens , num_terms , is_active , and is_parsed attributes are displayed columns. You can supply your own list of columns with the columns argument, or you can exclude specific columns with the exclude argument. Setting Record Properties \u00a4 After a corpus is instantiated, you can set the properties of individual records with Corpus.set() : corpus . set ( 1 , { \"name\" : \"John Smith\" }) Removing Records \u00a4 Corpus.remove() and Corpus.remove_records() can be used to remove records from a corpus. The former takes an id number and the latter takes a list of ids. Using Records \u00a4 Typically, you would retrieve records using Corpus.get_records() and then pass their content to another Lexos module. For example, here is how you would create a document-term matrix: # Get the records records = corpus . get_records () # Extract the documents and labels docs = [ record . content for record in records ] labels = [ record . name for record in records ] # Import the dtm module and generate a document-term matrix from lexos.dtm import DTM # Build the DTM dtm = DTM ( docs , labels )","title":"Managing a Corpus"},{"location":"tutorial/managing_a_corpus/#overview","text":"The Corpus module consists of a Corpus class that helps you manage assets in your workflow and serialize them to disk for later use. It is strictly optional; you may find it sufficient to load your documents into memory with a Loader or to manage your corpus assets independently through a different application. It is important to realise that a Lexos Corpus is primarily a manager for project assets; it is not used for acquiring those assets and is not used for analysing them, apart from the generation of a few statistics. In general, using a Corpus will require a workflow like the following: Use Loader to acquire texts. Optionally, use Scrubber to perform any required preprocessing on the texts in Loader.texts . Optionally, use Tokenizer to convert the original or scrubbed texts to spaCy Doc objects. Add the documents (texts or spaCy Doc objects) to the Corpus . If you wished to analyse the documents, you would get them from the Corpus and run them through Tokenizer if you did not do so before adding them. From this workflow, you should be able to see that you can skip the Corpus entirely. The Corpus simply allows you to attach metadata to the documents, such as a name, description, or classification label, and to save them to and retrieve them from disk easily. One of the important metadata categories is whether or not a document is active. A Corpus allows you to retrieve subsets of your documents based on this and other metadata categories.","title":"Overview"},{"location":"tutorial/managing_a_corpus/#creating-a-corpus","text":"Begin by importing the Corpus module: import lexos.corpus as cp We use the cp alias so that we can use corpus as a variable below: corpus = cp . Corpus ( name = \"My Corpus\" , description = \"My test corpus\" , corpus_dir = \"corpus\" ) The name , description , and corpus_dir arguments are all optional. corpus_dir is the directory where the corpus will be stored, and the default is \"corpus\" in the current directory. You can use additional keywords to instantiate the corpus with other metadata such as author or creation date. When you run the code above, the corpus directory will be created if it does not already exist. You can view your corpus metadata in a number of ways: Corpus.meta_table() will return an overview of your corpus as a pandas dataframe. You can also view this information as a dict with corpus.meta . The following individual properties can also be viewed: ids : A list of document ids in the Corpus. names : A list of document names in the Corpus. docs : A dict of document ids and docs in the Corpus if you have opted to cache them to RAM. num_docs : The number of docs in the Corpus. num_active_docs : The number of active docs in the Corpus. num_tokens : The number of tokens in the Corpus. num_terms : The number of terms in the Corpus. terms : A set of unique terms in the Corpus. Call these functions with code like Corpus.num_tokens . You can also get a Python collections Counter object containing the corpus term counts with Corpus.get_term_counts() . These attributes should all be empty or 0 when the corpus is first created. Note The Corpus class is constructed using Pydantic's BaseModel class . This means that it has access to any of Pydantic's attributes and methods, such as dict() and json() .","title":"Creating a Corpus"},{"location":"tutorial/managing_a_corpus/#corpus-records","text":"The basic unit of storage in a Corpus is a Record object. This is a Python object that provides access to the record's content and its metadata. Constructing a Record is simple. You just have to feed it some content and, in most cases, give it a name: record = Record(content=mydoc, name=\"greeting\") Behind the scenes, the Record class will give the record a default id of 1 (unless you specify a different integer) and set the is_active property to True (unless you set instantiate the object with it set to False ). See lexos.corpus.Record for other arguments that can be passed to the Record class. You can also create a Record from a dict using Pydantic's parse_obj() method: record = Record . parse_obj ({ \"content\" : mydoc , \"name\" : \"greeting\" }) See the Pydantic documentation for helper functions for parsing json or file content into objects. Once instantiated, a record provides access to the following information: preview : A preview of the first 50 characters of the record's text. terms : A set of unique terms in the record's content. text : The full text of the record's content. tokens : A list of tokens in the record's content. num_terms : The number of unique terms in the record's content. num_tokens : The number of tokens in the record's content. Important Term counts do not collapse upper- and lower-case words, so, if this is important, you must get the tokens, convert to lower case, and then generate the list of terms yourself. Alternatively, you may use Scrubber to preprocess your data before creating the Record object. Record.set() allows you to set arbitrary Record attributes (such as author or date), and Record.save() allows you to save the file to disk. Important When a Record object is saved to disk, it is serialized as a binary pickle file, which is not human readable. To restore it, you use a normal Python method of reading a binary file: with open ( filename , \"rb\" ) as f : record = pickle . load ( f ) The pickle format is not considered secure, so never unpickle a file you do not trust. In the latest version of spaCy, it is possible to serialize to JSON, but these methods have not yet been integrated in the Lexos API. The Record class accepts content only in the form of a pre-tokenized spaCy doc. However, it is possible to store an untokenized text by creating a blank spaCy language model and feeding it the lexos.corpus.NullTokenizer class. This simply returns a spaCy doc with the text as a single token. nlp = spacy . blank ( \"xx\" ) nlp . tokenizer = NullTokenizer ( nlp . vocab ) content = nlp ( content ) record = Record ( content ) Note that the entire text will be counted as a single token and a single term, so it is preferable to tokenize the text first or to plan to do so later. If the content is already a tokenized document, it is necessary to label it as such in the metadata. Here is an example of how you would do it: import spacy nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"Hi there!\" ) record = Record ( content = doc , name = \"greeting\" , is_parsed = True ) The is_parsed attribute allows Corpus to know that it is dealing with a tokenized document. You can still access the full text by calling record.text , but you can also access individual tokens by calling record.content[0] (to get the first token). If you want a dictionary with a record's full metadata, probably the easiest method is metadata = record.dict().remove(\"content\") . Why serialize records with pickle ? A Lexos Record is a Python object which contains a spaCy Doc object which contains spaCy Token objects. This complex structure creates a scenario which cannot be handled by other serialization formats without some serious hacks. There are some concerns about whether serialization and de-serialization will be fast enough when working with many records in a corpus (and lesser concerns about the security of the format), but for the moment it is the easiest and most straightforward format to work with. This is something to be revisited at a future date, especially now that spaCy has added a Doc.to_json() method.","title":"Corpus Records"},{"location":"tutorial/managing_a_corpus/#adding-records-to-a-corpus","text":"Adding records to a corpus is simple with Corpus.add_record() : record = Record ( content = doc , name = \"greeting\" , is_parsed = True ) corpus . add_record ( record , cache = True ) There is also a Corpus.add_records() , which takes a list of records. By default, the record's content is not cached in memory; instead, the entire record is serialized to disk. If you want to keep it in memory, you can set cache=True (as above). This will allow you to access the record from corpus.docs without having to fetch the record from disk. Note At present the docs property in the Corpus class is the only place where a clear distinction between a \"record\" and a \"document\" is made.","title":"Adding Records to a Corpus"},{"location":"tutorial/managing_a_corpus/#adding-documents-to-a-corpus","text":"It is not necessary to pre-generate records from documents before adding them to a corpus. You can also use Corpus.add() to add a document directly: # Use a text string unparsed_doc = \"Hi there!\" corpus . add_record ( unparsed_doc , name = \"greeting\" ) # Create parsed_doc = nlp ( \"This is a full sentence.\" ) corpus . add ( content = parsed_doc , name = \"sentence\" , is_parsed = True ) By default, the is_active attribute is True . You can set additional metadata properties by supplying a metadata dict: metadata = { \"author\" : \"John Smith\" , \"date\" : \"2011\" } corpus . add ( content = parsed_doc , name = \"sentence\" , is_parsed = True , metadata = metadata , cache = True ) The corresponding Corpus.add_docs() allows you to insert multiple documents. The format is a little more complicated. It takes a list of dicts with the document as the value of the content property: docs = [ { \"content\" : doc1 , \"name\" : \"Doc 1\" }, { \"content\" : doc2 , \"name\" : \"Doc 2\" }, ] corpus . add_docs ( docs , cache = True ) All the arguments accepted by Corpus.add() can be set as keys in the docs dictionary. Important Whether you are adding documents or records to a corpus, a check is made to ensure that the records stored have unique id , name , and filename attributes. If you do not specify a name for a document or record, a UUID will be used instead and will be used to generate a corresponding filename. The results of this can be unwieldy. In the future, some other method of ensuring uniqueness will be explored.","title":"Adding Documents to a Corpus"},{"location":"tutorial/managing_a_corpus/#getting-records-from-the-corpus","text":"Individual records can be fetched using Corpus.get() with a record id : record = corpus . get ( 1 ) doc = record . content The second line above extracts the spaCy doc from the records, and it can be treated like any spaCy doc. You can also supply a list of ids to Corpus.get_records() . If you pass nothing to the method, all the records in the corpus will be retrieved. If you do not know the id(s) of the document(s) you want, you can provide a query for Corpus.get_records() : records = corpus . get_records ( query = \"id < 10\" ) for record in records : print ( record . name ) This will yield a generator with each of the records with an id less than 10. Note On the back end, Corpus.get() and Corpus.get_records() call Corpus.meta , which contains a subset of the metadata for each record. A pandas dataframe is constructed from this metadata. The query can therefore be anything acceptable to pandas.DataFrame.query() . This allows complex queries to be performed on the corpus. If you want just the metadata for a record, probably the easiest method is metadata = corpus.get(1).dict().remove(\"content\") .","title":"Getting Records from the Corpus"},{"location":"tutorial/managing_a_corpus/#viewing-the-records-table","text":"Corpus.records_table() generates a pandas dataframe with each record in a separate row. By default, the id , name , filename , num_tokens , num_terms , is_active , and is_parsed attributes are displayed columns. You can supply your own list of columns with the columns argument, or you can exclude specific columns with the exclude argument.","title":"Viewing the Records Table"},{"location":"tutorial/managing_a_corpus/#setting-record-properties","text":"After a corpus is instantiated, you can set the properties of individual records with Corpus.set() : corpus . set ( 1 , { \"name\" : \"John Smith\" })","title":"Setting Record Properties"},{"location":"tutorial/managing_a_corpus/#removing-records","text":"Corpus.remove() and Corpus.remove_records() can be used to remove records from a corpus. The former takes an id number and the latter takes a list of ids.","title":"Removing Records"},{"location":"tutorial/managing_a_corpus/#using-records","text":"Typically, you would retrieve records using Corpus.get_records() and then pass their content to another Lexos module. For example, here is how you would create a document-term matrix: # Get the records records = corpus . get_records () # Extract the documents and labels docs = [ record . content for record in records ] labels = [ record . name for record in records ] # Import the dtm module and generate a document-term matrix from lexos.dtm import DTM # Build the DTM dtm = DTM ( docs , labels )","title":"Using Records"},{"location":"tutorial/scrubbing_texts/","text":"About Scrubber \u00a4 Scrubber can be defined as a destructive preprocessor. In other words, it changes the text as loaded in ways that potentially make mapping the results onto the original text potentially impossible. It is therefore best used before other procedures so that the scrubbed text is essentially treated as the \"original\" text. The importance of this will be seen below when we see the implementation of the tokeniser. But, to be short, the Lexos API differs from the web app in that Scrubber does not play a role in tokenisation by separating tokens by whitespace. Scrubbing works by applying a single function or a pipeline of functions to the text. As a reminder, we need to load the scrubber components registry with from lexos.scrubber.registry import scrubber_components , load_components Scrubber Components \u00a4 Scrubber components are divided into three categories: Normalize components are used to manipulate text into a standardized form. Remove components are used to remove strings and patterns from text. Replace components are used to replace strings and patterns in text. Follow these links to view all of the default scrubber components. Loading Scrubber Components \u00a4 Components must be loaded before they can be used. We can load them individually, as in the first example below, or we can specify multiple components in a tuple, as in the second example. In both cases, the returned variable is a function, which we can then feed to a scrubbing pipeline. # Load a single component from the registry lower_case = scrubber_components . get ( \"lower_case\" ) or lower_case = load_component ( \"lower_Case\" ) Or, if you want to do several at once: punctuation , remove_digits = load_components (( \"punctuation\" , \"digits\" )) In the first example, a component is loaded using the registry's built-in get method. It is also possible to load a single component with the registry's load_component() helper function. There is a parallel load_components() function for multiple components. Using Components \u00a4 Loaded component functions can be called like any normal function. For example: scrubbed_text = remove_digits(\"Lexos123\", only=[\"2\", \"3\"]) will return \"Lexos1\". If you are intending to apply multiple components to a single piece of text, the more efficient method is to use a pipeline. Making a Pipeline \u00a4 Now let's make the pipeline. We simply feed our component function names into the make_pipeline() function in the order we want them to be implemented. Notice that remove_digits has to be passed through the pipe() function. This is because the digits function requires extra arguments, and pipe() allows those arguments to be passed to the main pipeline function. # Make the pipeline scrub = make_pipeline ( lower_case , punctuation , pipe ( remove_digits , only = [ \"1\" ]) ) The value returned is a function that implements the full pipeline when called on a text, as shown below. # Scrub the text scrubbed_text = scrub ( \"Lexos is the number 12 text analysis tool!!\" ) This will return \"lexos is the number 2 text analysis tool\". Custom Scrubbing Components \u00a4 Users can write and use custom scrubbing functions. The function is written like a normal function, and to use it like a scrubber component it must be added to the registry. Below is an example with a custom title_case function. # Define the custom function def title_case ( text : str ) -> str : \"\"\"Our custom function to convert text to title case.\"\"\" return text . title () # Register the custom function scrubber_components . register ( \"title_case\" , func = title_case ) Users can add whatever scrubbing functions they want. For development purposes, we can start by creating custom functions, and, if we use them a lot, migrate them to the permanent registry. Important To use a custom scrubbing function, you must register it before you call load_component() or load_components() .","title":"Scrubbing Texts"},{"location":"tutorial/scrubbing_texts/#about-scrubber","text":"Scrubber can be defined as a destructive preprocessor. In other words, it changes the text as loaded in ways that potentially make mapping the results onto the original text potentially impossible. It is therefore best used before other procedures so that the scrubbed text is essentially treated as the \"original\" text. The importance of this will be seen below when we see the implementation of the tokeniser. But, to be short, the Lexos API differs from the web app in that Scrubber does not play a role in tokenisation by separating tokens by whitespace. Scrubbing works by applying a single function or a pipeline of functions to the text. As a reminder, we need to load the scrubber components registry with from lexos.scrubber.registry import scrubber_components , load_components","title":"About Scrubber"},{"location":"tutorial/scrubbing_texts/#scrubber-components","text":"Scrubber components are divided into three categories: Normalize components are used to manipulate text into a standardized form. Remove components are used to remove strings and patterns from text. Replace components are used to replace strings and patterns in text. Follow these links to view all of the default scrubber components.","title":"Scrubber Components"},{"location":"tutorial/scrubbing_texts/#loading-scrubber-components","text":"Components must be loaded before they can be used. We can load them individually, as in the first example below, or we can specify multiple components in a tuple, as in the second example. In both cases, the returned variable is a function, which we can then feed to a scrubbing pipeline. # Load a single component from the registry lower_case = scrubber_components . get ( \"lower_case\" ) or lower_case = load_component ( \"lower_Case\" ) Or, if you want to do several at once: punctuation , remove_digits = load_components (( \"punctuation\" , \"digits\" )) In the first example, a component is loaded using the registry's built-in get method. It is also possible to load a single component with the registry's load_component() helper function. There is a parallel load_components() function for multiple components.","title":"Loading Scrubber Components"},{"location":"tutorial/scrubbing_texts/#using-components","text":"Loaded component functions can be called like any normal function. For example: scrubbed_text = remove_digits(\"Lexos123\", only=[\"2\", \"3\"]) will return \"Lexos1\". If you are intending to apply multiple components to a single piece of text, the more efficient method is to use a pipeline.","title":"Using Components"},{"location":"tutorial/scrubbing_texts/#making-a-pipeline","text":"Now let's make the pipeline. We simply feed our component function names into the make_pipeline() function in the order we want them to be implemented. Notice that remove_digits has to be passed through the pipe() function. This is because the digits function requires extra arguments, and pipe() allows those arguments to be passed to the main pipeline function. # Make the pipeline scrub = make_pipeline ( lower_case , punctuation , pipe ( remove_digits , only = [ \"1\" ]) ) The value returned is a function that implements the full pipeline when called on a text, as shown below. # Scrub the text scrubbed_text = scrub ( \"Lexos is the number 12 text analysis tool!!\" ) This will return \"lexos is the number 2 text analysis tool\".","title":"Making a Pipeline"},{"location":"tutorial/scrubbing_texts/#custom-scrubbing-components","text":"Users can write and use custom scrubbing functions. The function is written like a normal function, and to use it like a scrubber component it must be added to the registry. Below is an example with a custom title_case function. # Define the custom function def title_case ( text : str ) -> str : \"\"\"Our custom function to convert text to title case.\"\"\" return text . title () # Register the custom function scrubber_components . register ( \"title_case\" , func = title_case ) Users can add whatever scrubbing functions they want. For development purposes, we can start by creating custom functions, and, if we use them a lot, migrate them to the permanent registry. Important To use a custom scrubbing function, you must register it before you call load_component() or load_components() .","title":"Custom Scrubbing Components"},{"location":"tutorial/the_document_term_matrix/","text":"About the Document-Term Matrix \u00a4 A document-term matrix (DTM) is the standard interface for analysis and information of document data. It consists in its raw form of a list of token counts per document in the corpus. Each unique token form is called a term. Thus it is really a list of term counts per document, arranged as matrix. In the Lexos App, sklearn's CountVectorizer is used to produce the DTM. In the Lexos API, Textacy's Vectorizer is the default vectorizer. Here is a vanilla implication to get a DTM containing the raw term counts for each document. from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = \"linear\" , idf_type = None , norm = None ) tokenized_docs = ( LexosDoc ( doc ) . get_tokens () for doc in docs ) doc_term_matrix = vectorizer . fit_transform ( tokenized_docs ) The main advantage of this procedure is that sklearn's CountVectorizer employs a regular expression pattern to tokenize the text and has very limited functionality to implement the kind of language-specific knowledge available in a document tokenised with a language model. Getting Term Counts and Frequencies \u00a4 The Lexos API provides an easy method of using the Vectorizer to retrieve term counts or frequencies from a single document and returning the results in a pandas dataframe. from lexos.dtm import get_doc_term_counts df = get_doc_term_counts ( docs , as_df = True ) Setting normalize=True will return relative frequencies instead of raw counts. dtm.get_doc_term_counts() has various parameters for limiting and filtering the output based on token labels or regex patterns. The DTM Class \u00a4 Lexos also wraps Textacy's Vectorizer in the DTM class with greater functionality. You can import it with from lexos.dtm import DTM Most work will leverage the DTM class to builds a document-term matrix and provide methods for manipulating the information held therein. The standard method of creating a DTM object is as follows: labels = [ \"Pride_and_Prejudice\" , \"Sense_and_Sensibility\" ] dtm = DTM ( docs , labels ) The labels are human-readable names for the documents which would otherwise be referenced by numeric indices. Instantiating a DTM object creates a vectorizer. By default, this is a Textacy Vectorizer object with parameters set to produce raw counts. The vectorizer settings can be viewed by calling lexos.dtm.vectorizer_settings and they can be adjusted by calling set_vectorizer() . The vectorizer is an object, so you can also inspect individual vectorizer settings with calls like DTM.vectorizer.idf_type . Important After changing the settings of an object, you must call DTM.build() to rebuild the document-term matrix. Getting a Term Counts Table \u00a4 The DTM class method for getting a table of raw term counts is DTM.get_table(). You can also call DTM.table , which will return a table based on state after the last time DTM.build()` was called. The options are as follows: # Get a table of counts with documents as columns and terms as rows df = dtm . get_table () # Get a table of counts with terms as columns and documents as rows df = dtm . get_table ( transpose = True ) The second option is equivalent to calling dtm.get_table().T , using pandas notation. The dtm.get_table() output is generally intended to allow you to use the pandas API once you have the data in the form of a pandas dataframe. If you change vectorizer settings, remember to rebuild the DTM. For instance, you want to use the Lexos app's implementation of TF-IDF, you would use the following: dtm . set_vectorizer ( tf_type = \"log\" , idf_type = \"smooth\" , norm = \"l2\" ) dtm . build () df = dtm . get_table () Important Currently, DTM.build() resets dtm.table=None , so you will need to call DTM.get_table() to use the new vectorizer. This is intended to reduce overhead if an app only needs to interact directly with the vectorizer. Perhaps down the line, it might be advisable to give DTM.build() a boolean parameter to allow the user to decide whether the table gets regenerated automatically. Note The Lexos culling function is now handled by the min_df parameter and extended by the max_df parameter in the vectorizer. The Lexos most frequent words function is handled by max_n_terms . But see the section below. Getting Statistics from the DTM \u00a4 Pandas has methods for calculating the sum, mean, and median of rows in the table. However, to save users from Googling, the DTM class has the DTM.get_stats_table() method that calculates these statistics and adds them to the columns in the default DTM table. stats_table = dtm . get_stats_table ([ \"sum\" , \"mean\" , \"median\" ]) Once the new dataframe is generated, it is easy to extract the data to a list with standard pandas syntax like stats_table[\"sum\"].values.tolist() . The table can also be sorted using pandas to get the most or least frequent terms: most_frequent = stats_table . sort_values ( by = \"sum\" , ascending = False ) . head () least_frequent = stats_table . sort_values ( by = \"sum\" , ascending = False ) . head () Getting Relative Frequencies \u00a4 DTM.get_freq_table() converts the raw counts in the default DTM table to relative frequencies. Since the resulting values are typically floats, there is an option to set the number of digits used for rounding. frequency_table = dtm . get_freq_table ( rounding = 2 , as_percent = True ) The setting as_percent=True multiples the frequencies by 100. The default is False . Getting Lists of Terms and Term Counts \u00a4 By default, most of the DTM methods return a pandas dataframe. Two methods provide output in the form of lists. DTM.get_terms() provides a simple, alphabetised list of terms in the document-term matrix. DTM.get_counts() returns a list of tuples with terms as the first element and sums (the total number of occurrences of the term in all documents) as the second element. This method has parameters for sorting by column and direction. By default, terms are sorted by natsort.ns.LOCALE (i.e. the computer's locale is used for the sorting algorithm). This can be configured using the options in the Python natsort reference . Visualising the DTM \u00a4 Once a document-term matrix table has been generated as a pandas dataframe, it becomes possible to use any of the pandas.DataFrame.plot methods, or to export the data for use with other tools. However, the Lexos API has two built-in visualisations: word clouds and bubble charts. Word clouds can be generated for the entire DTM or for individual documents. Multiple word clouds arrange for comparison are referred to as multiclouds. For information on generating these and other visualizations, see the Visualization page .","title":"The Document-Term Matrix"},{"location":"tutorial/the_document_term_matrix/#about-the-document-term-matrix","text":"A document-term matrix (DTM) is the standard interface for analysis and information of document data. It consists in its raw form of a list of token counts per document in the corpus. Each unique token form is called a term. Thus it is really a list of term counts per document, arranged as matrix. In the Lexos App, sklearn's CountVectorizer is used to produce the DTM. In the Lexos API, Textacy's Vectorizer is the default vectorizer. Here is a vanilla implication to get a DTM containing the raw term counts for each document. from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = \"linear\" , idf_type = None , norm = None ) tokenized_docs = ( LexosDoc ( doc ) . get_tokens () for doc in docs ) doc_term_matrix = vectorizer . fit_transform ( tokenized_docs ) The main advantage of this procedure is that sklearn's CountVectorizer employs a regular expression pattern to tokenize the text and has very limited functionality to implement the kind of language-specific knowledge available in a document tokenised with a language model.","title":"About the Document-Term Matrix"},{"location":"tutorial/the_document_term_matrix/#getting-term-counts-and-frequencies","text":"The Lexos API provides an easy method of using the Vectorizer to retrieve term counts or frequencies from a single document and returning the results in a pandas dataframe. from lexos.dtm import get_doc_term_counts df = get_doc_term_counts ( docs , as_df = True ) Setting normalize=True will return relative frequencies instead of raw counts. dtm.get_doc_term_counts() has various parameters for limiting and filtering the output based on token labels or regex patterns.","title":"Getting Term Counts and Frequencies"},{"location":"tutorial/the_document_term_matrix/#the-dtm-class","text":"Lexos also wraps Textacy's Vectorizer in the DTM class with greater functionality. You can import it with from lexos.dtm import DTM Most work will leverage the DTM class to builds a document-term matrix and provide methods for manipulating the information held therein. The standard method of creating a DTM object is as follows: labels = [ \"Pride_and_Prejudice\" , \"Sense_and_Sensibility\" ] dtm = DTM ( docs , labels ) The labels are human-readable names for the documents which would otherwise be referenced by numeric indices. Instantiating a DTM object creates a vectorizer. By default, this is a Textacy Vectorizer object with parameters set to produce raw counts. The vectorizer settings can be viewed by calling lexos.dtm.vectorizer_settings and they can be adjusted by calling set_vectorizer() . The vectorizer is an object, so you can also inspect individual vectorizer settings with calls like DTM.vectorizer.idf_type . Important After changing the settings of an object, you must call DTM.build() to rebuild the document-term matrix.","title":"The DTM Class"},{"location":"tutorial/the_document_term_matrix/#getting-a-term-counts-table","text":"The DTM class method for getting a table of raw term counts is DTM.get_table(). You can also call DTM.table , which will return a table based on state after the last time DTM.build()` was called. The options are as follows: # Get a table of counts with documents as columns and terms as rows df = dtm . get_table () # Get a table of counts with terms as columns and documents as rows df = dtm . get_table ( transpose = True ) The second option is equivalent to calling dtm.get_table().T , using pandas notation. The dtm.get_table() output is generally intended to allow you to use the pandas API once you have the data in the form of a pandas dataframe. If you change vectorizer settings, remember to rebuild the DTM. For instance, you want to use the Lexos app's implementation of TF-IDF, you would use the following: dtm . set_vectorizer ( tf_type = \"log\" , idf_type = \"smooth\" , norm = \"l2\" ) dtm . build () df = dtm . get_table () Important Currently, DTM.build() resets dtm.table=None , so you will need to call DTM.get_table() to use the new vectorizer. This is intended to reduce overhead if an app only needs to interact directly with the vectorizer. Perhaps down the line, it might be advisable to give DTM.build() a boolean parameter to allow the user to decide whether the table gets regenerated automatically. Note The Lexos culling function is now handled by the min_df parameter and extended by the max_df parameter in the vectorizer. The Lexos most frequent words function is handled by max_n_terms . But see the section below.","title":"Getting a Term Counts Table"},{"location":"tutorial/the_document_term_matrix/#getting-statistics-from-the-dtm","text":"Pandas has methods for calculating the sum, mean, and median of rows in the table. However, to save users from Googling, the DTM class has the DTM.get_stats_table() method that calculates these statistics and adds them to the columns in the default DTM table. stats_table = dtm . get_stats_table ([ \"sum\" , \"mean\" , \"median\" ]) Once the new dataframe is generated, it is easy to extract the data to a list with standard pandas syntax like stats_table[\"sum\"].values.tolist() . The table can also be sorted using pandas to get the most or least frequent terms: most_frequent = stats_table . sort_values ( by = \"sum\" , ascending = False ) . head () least_frequent = stats_table . sort_values ( by = \"sum\" , ascending = False ) . head ()","title":"Getting Statistics from the DTM"},{"location":"tutorial/the_document_term_matrix/#getting-relative-frequencies","text":"DTM.get_freq_table() converts the raw counts in the default DTM table to relative frequencies. Since the resulting values are typically floats, there is an option to set the number of digits used for rounding. frequency_table = dtm . get_freq_table ( rounding = 2 , as_percent = True ) The setting as_percent=True multiples the frequencies by 100. The default is False .","title":"Getting Relative Frequencies"},{"location":"tutorial/the_document_term_matrix/#getting-lists-of-terms-and-term-counts","text":"By default, most of the DTM methods return a pandas dataframe. Two methods provide output in the form of lists. DTM.get_terms() provides a simple, alphabetised list of terms in the document-term matrix. DTM.get_counts() returns a list of tuples with terms as the first element and sums (the total number of occurrences of the term in all documents) as the second element. This method has parameters for sorting by column and direction. By default, terms are sorted by natsort.ns.LOCALE (i.e. the computer's locale is used for the sorting algorithm). This can be configured using the options in the Python natsort reference .","title":"Getting Lists of Terms and Term Counts"},{"location":"tutorial/the_document_term_matrix/#visualising-the-dtm","text":"Once a document-term matrix table has been generated as a pandas dataframe, it becomes possible to use any of the pandas.DataFrame.plot methods, or to export the data for use with other tools. However, the Lexos API has two built-in visualisations: word clouds and bubble charts. Word clouds can be generated for the entire DTM or for individual documents. Multiple word clouds arrange for comparison are referred to as multiclouds. For information on generating these and other visualizations, see the Visualization page .","title":"Visualising the DTM"},{"location":"tutorial/tokenizing_texts/","text":"Language Models \u00a4 The tokenizer module is a big change for Lexos, as it formally separates tokenization from preprocessing. In the Lexos app, users employ Scrubber to massage the text into shape using their implicit knowledge about the text's language. Tokenization then takes place by splitting the text according to a regular expression pattern (normally whitespace). By contrast, the Lexos API uses a language model that formalizes the implicit rules and thus automates the tokenization process. Language models can implement both rule-based and probabilistic strategies for separating document strings into tokens. Because they have built-in procedures appropriate to specific languages, language models can often do a better job of tokenization than the approach used in the Lexos app. Important There are some trade-offs to using language models. Because the algorithm does more than split strings, processing times can be greater. In addition, tokenization is no longer (explicitly) language agnostic. A language model is \"opinionated\" and it may overfit the data. At the same time, if no language model exists for the language being tokenized, the results may not be satisfactory. The Lexos strategy for handling this situation is described below. Tokenized Documents \u00a4 A tokenized document can be defined as a text split into tokens in which each token is stored with any number of annotations assigned by the model. These annotations are token \"attributes\". The structure of a tokenized document can then be conceived in theory as a list of dicts like the following, where each keyword is an attribute. tokenized_doc = [ { \"text\" : \"The\" , \"part_of_speech\" : \"noun\" , \"is_stopword\" : \"True\" }, { \"text\" : \"end\" , \"part_of_speech\" : \"noun\" , \"is_stopword\" : \"False\" } ] It is then a simple matter to iterate through the document and retrieve all the tokens that are not stopwords using a Python list comprehension. non_stopwords = [ token for token in tokenized_doc if token [ \"is_stopword\" ] == False ] Many filtering procedures are easy to implement in this way. For languages such as Modern English, language models exist that can automatically annotate tokens with information like parts of speech, lemmas, stop words, and other information. However, token attributes can also be set after the text has been tokenized. If no language model exists for the text's language, it will only be possible to tokenize using general rules, and it will not be possible to add other annotations (at the tokenization stage). But new language models, including models for historical languages, are being produced all the time, and this is a growing area of interest in the Digital Humanities. spaCy Docs \u00a4 The Lexos API wraps the spaCy Natural Language Processing (NLP) library for loading language models and tokenizing texts. Because spaCy has excellent documentation and fairly wide acceptance in the Digital Humanities community, it is a good tool to use under the bonnet. spaCy has a growing number of language models in a number of languages, as well as wrappers for loading models from other common NLP libraries such as Stanford Stanza. Note The architecture of the Scrubber module is partially built on top of the preprocessing functions in Textacy , which also accesses and extends spaCy. In spaCy, texts are parsed into spacy.Doc objects consisting of sequences of annotated tokens. Note In order to formalize the difference between a text string that has been scrubbed and one that has been tokenized, we refer wherever possible to the string as a \"text\" and to the tokenized spacy.Doc object as a \"document\" (or just \"doc\"). We continue to refer to the individual items as \"documents\" if we are not concerned with their data type. Each token is spacy.Token object which stores all the token's attributes. The Lexos API wraps this procedure in the tokenizer.make_doc() function: from lexos import tokenizer doc = tokenizer . make_doc ( text ) This returns a spacy.Doc object. By default the tokenizer uses spaCy's \"xx_sent_ud_sm\" language model, which has been trained for tokenization and sentence segmentation on multiple languages. This model performs statistical sentence segmentation and possesses general rules for token segmentation that work well for a variety of languages. If you were making a document from a text in a language which rquired a more language-specific model, you would specify the model to be used. For instance, to use spaCy's small English model trained on web texts, you would call doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" ) tokenizer also has a make_docs() function to parse a list of texts into spaCy docs. Important Tokenization using spaCy uses a lot of memory. For a small English-language model, the parser and named entity recognizer (NER) can require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the memory limit with the max_length parameter. The limit is in number of characters (the default is set to 2,000,000 for Lexos), so you can check whether your inputs are too long by checking len(text) . If you are not using RAM-hungry pipeline components, you can disable or exclude them to avoid errors an increase efficiency (see the discussion on the spaCy pipeline below). In some cases, it may also be possible to cut the texts into segments before tokenization. A list of individual tokens can be obtained by iterating over the spaCy doc: # Get a list of tokens tokens = [ token . text for token in doc ] Here the text attribute stores the original text form of the token. SpaCy docs are non-destructive because they preserve the original text alongside the list of tokens and their attributes. You can access the original text of the entire doc by calling doc.text (assuming you have assigned the Doc object to the doc variable). Indeed, calling doc.to_json() will return a JSON representation which gives the start and end position of each token in the original text! As mentioned above, you can use a Python list comprehension to filter the the contents of the doc using information in the document's attributes. For instance: # Get a list of non-punctuation tokens non_punct_tokens = [ token . text for token in doc if not token . is_punct ] The example above leverages the built-in is_punct attribute to indicate whether the token is defined as (or predicted to be) a punctuation mark in the language model. SpaCy docs have a number of built-in attributes, which are described in the spaCy API reference . Note It is possible to extend spaCy's Doc object with its extension attribute. Lexos has a sample is_fruit extension (borrowed from the spaCy docs), which is illustrated below. Note that extensions are accessed via the underscore prefix, as shown. # Indicate whether the token is labelled as fruit for token in doc : print ( token . _ . is_fruit ) The sample extension can be found in lexos.tokenizer.extensions . The spaCy Pipeline \u00a4 Once spaCy tokenizes a text, it normally passes the resulting document to a pipeline of functions to parse it for other features. Typically, these functions will perform actions such as part-of-speech tagging, labelling syntactic dependencies, and identifying named entities. Processing times can be increased by disabling pipeline components if they are unavailable in the language model or not needed for the application's purposes. make_doc() and make_docs() will automatically run all pipeline components in the model unless they are disabled or excluded with the disable or exclude parameter. Check the model's documentation for the names of the components it includes. It is also possible to include custom pipeline components, which can be inserted at any point in the pipeline order. Custom components are supplied with the pipeline_components parameter, which takes a dictionary containing the keyword \"custom\". The value is a list of dictionaries where each dictionary contains information about the component as described in spaCy's documentation . Note The pipeline_components dict also contains disable and exclude keywords. The values are lists of components which will be merged with any components supplied in the disable or exclude paramaters of make_doc() and make_docs() . The ability to add custom pipeline components is valuable for certain language- or application-specific scenarios. However, it also opens Lexos up to the wealth of third-part pipeline components available through the spaCy Universe . Handling Stop Words \u00a4 Every token in a spaCy doc has an is_stop attribute. Most language models will have a list of default stop words, and this list is used to set the is_stop attribute True for every token when the document is parsed. It is possible to add stop words to the default list by passing a list to make_doc() and make_docs() with the add_stopwords argument: doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" , add_stopwords = [ \"yes\" , \"no\" , \"maybe\" ] ) The remove_stopwords argument removes stop words from the default list. If remove_stopwords=True , all stop words are removed. Important add_stopwords and remove_stopwords do not remove stop word tokens from the doc; rather, they modify the stop word list used to set the is_stop attribute of individual tokens. To get a list of tokens without stop words, you must do something like [token for token in doc if not token.is_stop] . If you a are producing a corpus of documents in which the documents will be processed by different models, it is most efficient to process the documents in batches, one batch for each model. LexosDocs \u00a4 The Lexos API also has a LexosDoc class, which provides a wrapper for spaCy docs. Its use is illustrated below. from lexos.tokenizer.lexosdoc import LexosDoc lexos_doc = LexosDoc ( doc ) tokens = lexos_doc . get_tokens () This example just returns [token.text for token in doc] , so it is not strictly necessary. But using the LexosDoc wrapper can be useful for producing clean code. In other cases, it might be useful to manipulate spaCy docs with methods that do not access their built-in or extended attributes or method. For instance, LexosDoc.get_token_attrs() shows what attributes are available for tokens in the doc and LexosDoc.to_dataframe() exports the tokens and their attributes to a pandas dataframe. Ngrams \u00a4 Both texts and documents can be parsed into sequences of two or more tokens called ngrams. Many spaCy models can identify syntactic units such as noun chunks. These capabilities are not covered here since they are language specific. Instead, the section below describe how to obtain more general ngram sequences. Generating Word Ngrams \u00a4 The easiest method of obtaining ngrams from a text is to create a spaCy doc and then call Textacy's textacy.extract.basics.ngrams method: import spacy import textacy.extract.basics.ngrams as ng nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = list ( ng ( doc , 2 , min_freq = 1 )) This will produce [The end, end is, is nigh] . The output is a list of spaCy tokens. (An additional [token.text for token in ngrams] is required to ensure that you have quoted strings: [\"The end\", \"end is\", \"is nigh\"] ). Textacy has a lot of additional options, which are documented in the Textacy API reference under textacy.extract.basics.ngrams . However, if you do not need these options, you can use Tokenizer 's helper function ngrams_from_doc() : import spacy nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = ngrams_from_doc ( doc , size = 2 ) Notice that in both cases, the output will be a list of overlapping ngrams generated by a rolling window across the pre-tokenized document. If you want your document to contain ngrams as tokens, you will need to create a new document using Tokenizer 's doc_from_ngrams() function: doc = doc_from_ngrams ( ngrams , strict = True ) Note Setting strict=False will preserve all the whitespace in the ngrams; otherwise, your language model may modify the output by doing things like splitting punctuation into separate tokens. There is also a doc_from_ngrams() function to which you can feed multiple lists of ngrams. A possible workflow might call Textacy directly to take advantage of some its filters, when generating ngrams and then calling doc_from_ngrams() to pipe the extracted tokens back into a doc. textacy.extract.basics.ngrams has sister functions that do things like extract noun chunks (if available in the language model), making this a very powerful approach generating ngrams with semantic information. Generating Character Ngrams \u00a4 Character ngrams at their most basic level split the untokenized string every N characters. So \"The end is nigh.\" would produce something like [\"Th\", \"e \", \"nd\", \" i\", \"s \", \"ni\", \"gh\", \".\"] (if we wanted to preserve the whitespace). Tokenizer does this with the generate_ngrams]() function: text = \"The end is nigh.\" ngrams = generate_character_ngrams ( text , 2 , drop_whitespace = False ) This will produce the output shown above. If we wanted to output [\"Th\", \"en\", \"di\", \"sn\", \"ig\", \"h.\"] , we would use drop_whitespace=True (which is the default). Note generate_character_ngrams() is a wrapper for Python's textwrap.wrap method, which can also be called directly. Once you have produced a list of ngrams, you can create a doc from them using ngrams_from_doc() , as shown above. Use generate_character_ngrams() (a) when you simply want a list of non-overlapping ngrams, or (b) when you want to produce docs with non-overlapping ngrams as tokens. Note that your language model may not be able apply labels effectively to ngram tokens, so working with character ngrams is primarily useful if you are planning to work with the token forms only, or if the ngram size you use maps closely to character lengths of words in the language you are working in.","title":"Tokenizing Texts"},{"location":"tutorial/tokenizing_texts/#language-models","text":"The tokenizer module is a big change for Lexos, as it formally separates tokenization from preprocessing. In the Lexos app, users employ Scrubber to massage the text into shape using their implicit knowledge about the text's language. Tokenization then takes place by splitting the text according to a regular expression pattern (normally whitespace). By contrast, the Lexos API uses a language model that formalizes the implicit rules and thus automates the tokenization process. Language models can implement both rule-based and probabilistic strategies for separating document strings into tokens. Because they have built-in procedures appropriate to specific languages, language models can often do a better job of tokenization than the approach used in the Lexos app. Important There are some trade-offs to using language models. Because the algorithm does more than split strings, processing times can be greater. In addition, tokenization is no longer (explicitly) language agnostic. A language model is \"opinionated\" and it may overfit the data. At the same time, if no language model exists for the language being tokenized, the results may not be satisfactory. The Lexos strategy for handling this situation is described below.","title":"Language Models"},{"location":"tutorial/tokenizing_texts/#tokenized-documents","text":"A tokenized document can be defined as a text split into tokens in which each token is stored with any number of annotations assigned by the model. These annotations are token \"attributes\". The structure of a tokenized document can then be conceived in theory as a list of dicts like the following, where each keyword is an attribute. tokenized_doc = [ { \"text\" : \"The\" , \"part_of_speech\" : \"noun\" , \"is_stopword\" : \"True\" }, { \"text\" : \"end\" , \"part_of_speech\" : \"noun\" , \"is_stopword\" : \"False\" } ] It is then a simple matter to iterate through the document and retrieve all the tokens that are not stopwords using a Python list comprehension. non_stopwords = [ token for token in tokenized_doc if token [ \"is_stopword\" ] == False ] Many filtering procedures are easy to implement in this way. For languages such as Modern English, language models exist that can automatically annotate tokens with information like parts of speech, lemmas, stop words, and other information. However, token attributes can also be set after the text has been tokenized. If no language model exists for the text's language, it will only be possible to tokenize using general rules, and it will not be possible to add other annotations (at the tokenization stage). But new language models, including models for historical languages, are being produced all the time, and this is a growing area of interest in the Digital Humanities.","title":"Tokenized Documents"},{"location":"tutorial/tokenizing_texts/#spacy-docs","text":"The Lexos API wraps the spaCy Natural Language Processing (NLP) library for loading language models and tokenizing texts. Because spaCy has excellent documentation and fairly wide acceptance in the Digital Humanities community, it is a good tool to use under the bonnet. spaCy has a growing number of language models in a number of languages, as well as wrappers for loading models from other common NLP libraries such as Stanford Stanza. Note The architecture of the Scrubber module is partially built on top of the preprocessing functions in Textacy , which also accesses and extends spaCy. In spaCy, texts are parsed into spacy.Doc objects consisting of sequences of annotated tokens. Note In order to formalize the difference between a text string that has been scrubbed and one that has been tokenized, we refer wherever possible to the string as a \"text\" and to the tokenized spacy.Doc object as a \"document\" (or just \"doc\"). We continue to refer to the individual items as \"documents\" if we are not concerned with their data type. Each token is spacy.Token object which stores all the token's attributes. The Lexos API wraps this procedure in the tokenizer.make_doc() function: from lexos import tokenizer doc = tokenizer . make_doc ( text ) This returns a spacy.Doc object. By default the tokenizer uses spaCy's \"xx_sent_ud_sm\" language model, which has been trained for tokenization and sentence segmentation on multiple languages. This model performs statistical sentence segmentation and possesses general rules for token segmentation that work well for a variety of languages. If you were making a document from a text in a language which rquired a more language-specific model, you would specify the model to be used. For instance, to use spaCy's small English model trained on web texts, you would call doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" ) tokenizer also has a make_docs() function to parse a list of texts into spaCy docs. Important Tokenization using spaCy uses a lot of memory. For a small English-language model, the parser and named entity recognizer (NER) can require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the memory limit with the max_length parameter. The limit is in number of characters (the default is set to 2,000,000 for Lexos), so you can check whether your inputs are too long by checking len(text) . If you are not using RAM-hungry pipeline components, you can disable or exclude them to avoid errors an increase efficiency (see the discussion on the spaCy pipeline below). In some cases, it may also be possible to cut the texts into segments before tokenization. A list of individual tokens can be obtained by iterating over the spaCy doc: # Get a list of tokens tokens = [ token . text for token in doc ] Here the text attribute stores the original text form of the token. SpaCy docs are non-destructive because they preserve the original text alongside the list of tokens and their attributes. You can access the original text of the entire doc by calling doc.text (assuming you have assigned the Doc object to the doc variable). Indeed, calling doc.to_json() will return a JSON representation which gives the start and end position of each token in the original text! As mentioned above, you can use a Python list comprehension to filter the the contents of the doc using information in the document's attributes. For instance: # Get a list of non-punctuation tokens non_punct_tokens = [ token . text for token in doc if not token . is_punct ] The example above leverages the built-in is_punct attribute to indicate whether the token is defined as (or predicted to be) a punctuation mark in the language model. SpaCy docs have a number of built-in attributes, which are described in the spaCy API reference . Note It is possible to extend spaCy's Doc object with its extension attribute. Lexos has a sample is_fruit extension (borrowed from the spaCy docs), which is illustrated below. Note that extensions are accessed via the underscore prefix, as shown. # Indicate whether the token is labelled as fruit for token in doc : print ( token . _ . is_fruit ) The sample extension can be found in lexos.tokenizer.extensions .","title":"spaCy Docs"},{"location":"tutorial/tokenizing_texts/#the-spacy-pipeline","text":"Once spaCy tokenizes a text, it normally passes the resulting document to a pipeline of functions to parse it for other features. Typically, these functions will perform actions such as part-of-speech tagging, labelling syntactic dependencies, and identifying named entities. Processing times can be increased by disabling pipeline components if they are unavailable in the language model or not needed for the application's purposes. make_doc() and make_docs() will automatically run all pipeline components in the model unless they are disabled or excluded with the disable or exclude parameter. Check the model's documentation for the names of the components it includes. It is also possible to include custom pipeline components, which can be inserted at any point in the pipeline order. Custom components are supplied with the pipeline_components parameter, which takes a dictionary containing the keyword \"custom\". The value is a list of dictionaries where each dictionary contains information about the component as described in spaCy's documentation . Note The pipeline_components dict also contains disable and exclude keywords. The values are lists of components which will be merged with any components supplied in the disable or exclude paramaters of make_doc() and make_docs() . The ability to add custom pipeline components is valuable for certain language- or application-specific scenarios. However, it also opens Lexos up to the wealth of third-part pipeline components available through the spaCy Universe .","title":"The spaCy Pipeline"},{"location":"tutorial/tokenizing_texts/#handling-stop-words","text":"Every token in a spaCy doc has an is_stop attribute. Most language models will have a list of default stop words, and this list is used to set the is_stop attribute True for every token when the document is parsed. It is possible to add stop words to the default list by passing a list to make_doc() and make_docs() with the add_stopwords argument: doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" , add_stopwords = [ \"yes\" , \"no\" , \"maybe\" ] ) The remove_stopwords argument removes stop words from the default list. If remove_stopwords=True , all stop words are removed. Important add_stopwords and remove_stopwords do not remove stop word tokens from the doc; rather, they modify the stop word list used to set the is_stop attribute of individual tokens. To get a list of tokens without stop words, you must do something like [token for token in doc if not token.is_stop] . If you a are producing a corpus of documents in which the documents will be processed by different models, it is most efficient to process the documents in batches, one batch for each model.","title":"Handling Stop Words"},{"location":"tutorial/tokenizing_texts/#lexosdocs","text":"The Lexos API also has a LexosDoc class, which provides a wrapper for spaCy docs. Its use is illustrated below. from lexos.tokenizer.lexosdoc import LexosDoc lexos_doc = LexosDoc ( doc ) tokens = lexos_doc . get_tokens () This example just returns [token.text for token in doc] , so it is not strictly necessary. But using the LexosDoc wrapper can be useful for producing clean code. In other cases, it might be useful to manipulate spaCy docs with methods that do not access their built-in or extended attributes or method. For instance, LexosDoc.get_token_attrs() shows what attributes are available for tokens in the doc and LexosDoc.to_dataframe() exports the tokens and their attributes to a pandas dataframe.","title":"LexosDocs"},{"location":"tutorial/tokenizing_texts/#ngrams","text":"Both texts and documents can be parsed into sequences of two or more tokens called ngrams. Many spaCy models can identify syntactic units such as noun chunks. These capabilities are not covered here since they are language specific. Instead, the section below describe how to obtain more general ngram sequences.","title":"Ngrams"},{"location":"tutorial/tokenizing_texts/#generating-word-ngrams","text":"The easiest method of obtaining ngrams from a text is to create a spaCy doc and then call Textacy's textacy.extract.basics.ngrams method: import spacy import textacy.extract.basics.ngrams as ng nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = list ( ng ( doc , 2 , min_freq = 1 )) This will produce [The end, end is, is nigh] . The output is a list of spaCy tokens. (An additional [token.text for token in ngrams] is required to ensure that you have quoted strings: [\"The end\", \"end is\", \"is nigh\"] ). Textacy has a lot of additional options, which are documented in the Textacy API reference under textacy.extract.basics.ngrams . However, if you do not need these options, you can use Tokenizer 's helper function ngrams_from_doc() : import spacy nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = ngrams_from_doc ( doc , size = 2 ) Notice that in both cases, the output will be a list of overlapping ngrams generated by a rolling window across the pre-tokenized document. If you want your document to contain ngrams as tokens, you will need to create a new document using Tokenizer 's doc_from_ngrams() function: doc = doc_from_ngrams ( ngrams , strict = True ) Note Setting strict=False will preserve all the whitespace in the ngrams; otherwise, your language model may modify the output by doing things like splitting punctuation into separate tokens. There is also a doc_from_ngrams() function to which you can feed multiple lists of ngrams. A possible workflow might call Textacy directly to take advantage of some its filters, when generating ngrams and then calling doc_from_ngrams() to pipe the extracted tokens back into a doc. textacy.extract.basics.ngrams has sister functions that do things like extract noun chunks (if available in the language model), making this a very powerful approach generating ngrams with semantic information.","title":"Generating Word Ngrams"},{"location":"tutorial/tokenizing_texts/#generating-character-ngrams","text":"Character ngrams at their most basic level split the untokenized string every N characters. So \"The end is nigh.\" would produce something like [\"Th\", \"e \", \"nd\", \" i\", \"s \", \"ni\", \"gh\", \".\"] (if we wanted to preserve the whitespace). Tokenizer does this with the generate_ngrams]() function: text = \"The end is nigh.\" ngrams = generate_character_ngrams ( text , 2 , drop_whitespace = False ) This will produce the output shown above. If we wanted to output [\"Th\", \"en\", \"di\", \"sn\", \"ig\", \"h.\"] , we would use drop_whitespace=True (which is the default). Note generate_character_ngrams() is a wrapper for Python's textwrap.wrap method, which can also be called directly. Once you have produced a list of ngrams, you can create a doc from them using ngrams_from_doc() , as shown above. Use generate_character_ngrams() (a) when you simply want a list of non-overlapping ngrams, or (b) when you want to produce docs with non-overlapping ngrams as tokens. Note that your language model may not be able apply labels effectively to ngram tokens, so working with character ngrams is primarily useful if you are planning to work with the token forms only, or if the ngram size you use maps closely to character lengths of words in the language you are working in.","title":"Generating Character Ngrams"},{"location":"tutorial/training_a_language_model/","text":"The Lexos tokenizer module now uses language models for a richer and more flexible way of accessing the linguistic features of texts. Token segmentation and feature assignment are performed under the hood by the spaCy , and, for some users, spaCy's pre-built language models may be sufficient. For others, models trained on their data may be necessary, especially if they are working with a language for which no pre-built model exists. The spaCy library provides a rich environment for training language models, and the Lexos API provides a thin wrapper around its functionality for greater ease of use and integration into applications. Language models in spaCy are built on pipelines of configurable components that handle tagging different types of linguistic features. The most basic is the \"tagger\", which labels parts of speech, but other pipeline components handle morphological analysis, lemmatization, and named entity recognition. Components are registered in a configuration (config) file which exposes the dizzying array of options available in a machine learning workflow and serves as the \"single source of truth\" for how the model was trained. The Lexos API maintains and provides access to the config file whilst allowing the user to train a language model with minimal need to edit it. Language models can be instantiated with configuration \"recipes\" that provide recommended configuration values for various use cases. Note In spaCy, the term \"pipeline\" refers to a configuration of components, rules or algorithms for assigning feature labels to texts, whereas the term \"model\" refers to the statistical weights (vectors) these algorithms produce that are used to make predictions about labels. Since trained pipelines generate statistical models, there is inevitably some overlap in the terminology. The LanguageModel Class and the Training Workflow \u00a4 Most of the work can be performed using the LanguageModel class, which can be instantiated with model = LanguageModel() . This will create a new language model folder with various subfolders and a configuration file. In most cases, the user will want to pass certain information on instantiation, such as the desired path to the model folder. This procedure will be illustrated below. Once a LanguageModel object is instantiated, the next step is to copy assets into the model's assets folder. The assets are normally three files, one for training, one for development, and one for testing. These three files are then converted to binary spaCy format, stored in the corpus folder, with LanguageModel.convert_assets . Once the assets are in spaCy format, the training and development files are used to train the model with LanguageModel.train() . The trained model is stored in the training folder. Once complete, the model can be evaluated with LanguageModel.evaluate() . This provides various statistical measures of the model's accuracy by testing its predictions against the labels in the testing file. Results are saved in the metrics folder. If the user deems the model to be sufficiently accurate, they can then package it with LanguageModel.package() . This saves the model, its configuration file, and all its pipeline components in the packages folder. The path to the new model can now be passed to the Tokenizer , and the new model will be loaded for parsing texts into spaCy docs. A more in-depth, step-by-step explanation of the procedure is described below. Before Getting Started \u00a4 Training a model requires you to have a set of training data, which is a series of sentences (or sentence-like units) with pre-assigned labels. Producing training data can be a time-consuming process. If the language you are working in is close to that of a pre-built model, it is probably easiest to use Tokenizer to generate labels on some sample texts from your data and then correct the labels as appropriate. For most purposes, it will be easiest to work in the CONLL-U format because of its convenient columnar format and because spaCy has a built-in converter for CONLL-U. This tutorial does not cover data preparation for training features, such as named entities, which are not included in CONLL-U. That said, the conversion process converts the CONLL-U text to a set of spaCy docs, so, if the desired labels are available in a spaCy doc, it is possible to proceed. More information on this is given below. Important Note that spaCy functions sometimes assume that you are working on the command line and can provide output that does not make sense if you are working in, say, a Jupyter notebook. In most cases, this will take the form of references to command-line flags, but other idiosyncratic behaviours may occur. For instance, when evaluating a model, if there is a single error, the spaCy function calls sys.exit() causing SystemExit: 1 error code with no indication to the user that the script did not exit in the middle of execution! However, such glitches are rare. Instantiating a LanguageModel Object \u00a4 By default, calling model = LanguageModel() will generate a folder called language_model in the current directory. However, it is possible to add arguments to override the default settings. Here is an example: model = LanguageModel ( project_dir = \"update_en_core_web_sm\" , recipe = \"project/recipes/update_en_core_web_sm.cfg\" , force = True ) This will create the \"update_en_core_web_sm\" folder. Instead of auto-generating a default config file, it will copy the \"recipe\" config file for updating spaCy's \"en_core_web_sm\" model. Setting force=True will cause Lexos to overwrite and pre-existing configuration if the model folder was created previously. See the API documentation for the full set of arguments that can be passed to the LanguageModel class. We will demonstrate some of them below. A note on model folder paths When a LanguageModel object is instantiated, most of the folders necessary for managing the workflow are created. By default, each subfolder contains the name of the model's language (the default is multilingual \"xx\"). Most of the model's methods will then assume that files are written to this language-specific subfolder. This organizational structure is largely based on the spaCy tagger_parser_ud project , and it remains to be seen if it should be a best practice for a more generic application. spaCy projects also store configuration files in a configs folder, which has not been deemed necessary for the LanguageModel class. As an instance should only have one configuration at a time, the LanguageModel 's config file is stored in its folder root. These decisions are subject to re-evaluation. Working with the Configuration \u00a4 Once the LanguageModel object is instantiated, the model folder will contain a configuration file with default configuration values or values you have assigned using arguments passed during instantiation. Although the configuration file is the \"single source of truth\" for the model, the LanguageModel object maintains a copy as a class attribute. Internally, this copy is parsed as a modified Python dict, which can be accessed by calling model.config . To see a string representation, call model.config.to_str() . Note The \"modified Python dict\" is actually a Thinc Config object (Thinc is the machine-learning library used internally by spaCy), so any of the Config object's methods as described in the Thinc documentation can be used. If you wish to change a value in your configuration you can simply modify the dictionary. For instance, if you wanted to change the language to English, you would use: model.config[\"nlp\"][\"lang\"] = \"en\" . This only modifies the config attribute. To ensure that your changes are made in the config file , you must then call model.save_config() . You can also replace the current config file and ( config attribute) by calling model.load_config() with a path to the desired config file. This is the equivalent of instantiating the LanguageModel object with a recipe path and force=True . Copying Assets \u00a4 Copying assets involves copying your training and testing data from another location into the models assets folder. This is done with the following code: project . copy_assets ( training_file = \"path/to/train.conllu\" , dev_file = \"path/to/dev.conllu\" , test_file = \"path/to/test.conllu\" ) In addition to local filepaths, copy_assets() also accepts urls. Asset files are not used directly when training a model; they are primarily for archival purposes. This is especially useful if you have downloaded them since you do not have to download them again if you need to re-instantiate the model object. Asset files are assumed to be in CONLL-U format, which will be converted to spaCy binary format by LanguageModel.convert_assets() (see below). If your training and testing files are not in CONLL-U format, you may still manually copy them into the assets folder for archival purposes, and you do not need to call LanguageModel.copy_assets() . Converting Assets to spaCy Format \u00a4 The LanguageModel.convert_assets() method assumes that you have CONLL-U formatted files in the model's assets folder and automatically converts them to spaCy binary formatted files in the model's corpus folder. If your assets are not in CONLL-U format, you can skip this step, but you are responsible for converting them to spaCy binary format and depositing them in the corpus folder by some other means. Information on how to do that can be found in the spaCy documentation . Important The model's training_file , dev_file , and test_file attributes are used to locate files for conversion in the assets folder. If the files do not have the same names, an error will occur. You can see (or set) these attribute values with model.training_file , model.dev_file , and model.test_file . If you have not set them when you instantiated the LanguageModel object. Debugging Configuration and Data \u00a4 Before training your model, or if you encounter a problem, it is a good idea to try debugging with debug_config(path/to/config/file) or debug_data(path/to/config/file) . This will generate reports identifying potential problems. Important LanguageModel.debug_config() and LanguageModel.debug_data() are not part of the LanguageModel class, so do not call them without the dot notation. This means that you can debug a config file without an instantiated LanguageModel object. Selecting Pipeline Components \u00a4 Before training your model, you must also ensure that you have specified which pipeline components you wish to train. By default, the LanguageModel object configures just the \"tagger\", which labels parts of speech. Other common pipeline components are \"tok2vec\", \"attribute_ruler\", \"lemmatizer\", \"parser\", \"ner\". For an overview of pipeline components, see the spaCy documentation . You can set pipeline components when instantiating a LanguageModel object with the components argument, which takes a list of components. You can also set them afterwards by modifying model.config[\"nlp\"][\"pipeline\"] , or even by editing the config file itself. Warning Setting pipeline components opens a bewildering away of possibilities for configuring the components, and it is very likely to generate invalid config files. It is highly recommended that you use LanguageModel.debug_config() if this happens but even more highly recommended that you rely on trusted recipes, especially if you are developing an interface for user-defined configuration. This will minimize the chance that you will encounter errors during training. Most components are independent of each other, but some share a \"token-to-vector\" component like \"tok2vec\". It is possible to speed up training by setting some components to be \"frozen\" and by indicating which components will be used for annotation. The following example assumes a model with the pipeline components \"tagger\", \"attribute_ruler\", \"lemmatizer\", \"parser\", and \"ner\". However, it skips training \"attribute_ruler\", \"lemmatizer\", \"parser\", and \"ner\". If the source of the components is a pre-existing model, the pre-existing state is simply passed to the new model. However, in the new model, only the \"tagger\", \"attribute_rule\", and \"lemmatizer\" components will have annotations. model . config [ \"training\" ][ \"frozen_components\" ] = [ \"attribute_ruler\" , \"lemmatizer\" , \"parser\" , \"ner\" ] model . config [ \"training\" ][ \"annotating_components\" ] = [ \"tagger\" , \"attribute_ruler\" , \"lemmatizer\" ] model . save_config () Again, it is safest to use a trusted recipe to ensure that all components are properly configured. Training a Model \u00a4 If everything looks good, you are ready to train your model. To do this, simply call LanguageModel.train() . Although it is possible to pass arguments to override the settings in the config file, in most cases you will not want to do so. Once training begins, its progress will be logged to the screen, and you can watch it train. Be aware that training can take a very (and unpredictably) long time for large data sets. Be patient! Balancing Efficiency and Accuracy \u00a4 By default, models are generated with efficiency in mind, but you can change this by setting optimize=\"accuracy\" when instantiating your model. This will cause the LanguageModel object to generate a different config file, one that may produce more accurate models, but with potentially longer training times and bigger output model sizes. Internally, these config files are generated by an evolving set of language-specific recommendations maintained by spaCy. You can, of course, override these recommendations or use a recipe config file to bypass them entirely. If you are using a language for which spaCy has a pre-built model, it can be useful to generate a config file with the default recommendations and then modify it for your own purposes as necessary. Training with a GPU and Transformer Models \u00a4 Using a GPU to train your model can be 2-3 times faster than using your computer's CPU. You can substitute a GPU for your computer's CPU simply by instantiating your LanguageModel object with gpu set to your GPU's id. However, this may not be worth it, given that setting up a GPU can be very fiddly. Where it truly becomes necessary is if you want to take advantage of pre-built transformer models such as BERT or GPT-2. For an introduction to the use of transformer models, see spaCy's documentation and the spacy-transformers plugin. Warning Setting up a GPU and transformer models is not for the faint of heart. Most likely, this will be done by application developers on the back end so that individual users do not have to do so on their own machines. Evaluating a Model \u00a4 Once a model has completed training, the output is stored in the training folder. You can then evaluate it by calling LanguageModel.evaluate() with the path to the trained model folder and the path to your test file in the corpus folder. model . evaluate ( model = \"training/en/model-best\" , testfile = \"corpus/en/test.spacy\" ) You will receive a report showing how accurately the model predicted the labels in your test data. A copy of the report in json format is saved to the metrics folder. Note If you run into a problem, you can also debug your model by calling LanguageModel.debug_model(\"path/to/config/file\") . If you are unhappy with the level of accuracy, you can try tweaking your configuration or, with a greater likelihood of success, adding more training data. Re-train the model and test again. Packaging the Model \u00a4 To make the model useable, it must first be packaged. Once packaged, the model can be used for parsing texts using either the Lexos API or using spaCy directly: # Using the Lexos API from lexos import tokenizer doc = tokenizer . make_doc ( text , model = \"path/to/model\" ) # Using spaCy import spacy nlp = spacy . load ( \"path/to/model\" ) doc = nlp ( text ) The path to the model package should point to the directory containing the packages config.cfg file. Note So far, I have only tried loading model packages using the path the package folder. It should also be possible to use the package name, but this needs further testing. To package a model, call LanguageModel.package() , providing the input folder for the model in the training folder and the path to the output folder. You can also provide a name and version for the model if you did not instantiation the LanguageModel object with package_name and package_version values. Setting force=True will overwrite any previous package in the output directory. model . package ( input_dir = \"path/to/model-best\" , output_dir = \"path/to/output/directory\" , name = \"en_test_sm\" , version = \"1.0.0\" , force = True ) It is a good idea to store packages in a folder called packages inside the model folder, although they can be stored in a separate location for distribution.","title":"Training a Language Model"},{"location":"tutorial/training_a_language_model/#the-languagemodel-class-and-the-training-workflow","text":"Most of the work can be performed using the LanguageModel class, which can be instantiated with model = LanguageModel() . This will create a new language model folder with various subfolders and a configuration file. In most cases, the user will want to pass certain information on instantiation, such as the desired path to the model folder. This procedure will be illustrated below. Once a LanguageModel object is instantiated, the next step is to copy assets into the model's assets folder. The assets are normally three files, one for training, one for development, and one for testing. These three files are then converted to binary spaCy format, stored in the corpus folder, with LanguageModel.convert_assets . Once the assets are in spaCy format, the training and development files are used to train the model with LanguageModel.train() . The trained model is stored in the training folder. Once complete, the model can be evaluated with LanguageModel.evaluate() . This provides various statistical measures of the model's accuracy by testing its predictions against the labels in the testing file. Results are saved in the metrics folder. If the user deems the model to be sufficiently accurate, they can then package it with LanguageModel.package() . This saves the model, its configuration file, and all its pipeline components in the packages folder. The path to the new model can now be passed to the Tokenizer , and the new model will be loaded for parsing texts into spaCy docs. A more in-depth, step-by-step explanation of the procedure is described below.","title":"The LanguageModel Class and the Training Workflow"},{"location":"tutorial/training_a_language_model/#before-getting-started","text":"Training a model requires you to have a set of training data, which is a series of sentences (or sentence-like units) with pre-assigned labels. Producing training data can be a time-consuming process. If the language you are working in is close to that of a pre-built model, it is probably easiest to use Tokenizer to generate labels on some sample texts from your data and then correct the labels as appropriate. For most purposes, it will be easiest to work in the CONLL-U format because of its convenient columnar format and because spaCy has a built-in converter for CONLL-U. This tutorial does not cover data preparation for training features, such as named entities, which are not included in CONLL-U. That said, the conversion process converts the CONLL-U text to a set of spaCy docs, so, if the desired labels are available in a spaCy doc, it is possible to proceed. More information on this is given below. Important Note that spaCy functions sometimes assume that you are working on the command line and can provide output that does not make sense if you are working in, say, a Jupyter notebook. In most cases, this will take the form of references to command-line flags, but other idiosyncratic behaviours may occur. For instance, when evaluating a model, if there is a single error, the spaCy function calls sys.exit() causing SystemExit: 1 error code with no indication to the user that the script did not exit in the middle of execution! However, such glitches are rare.","title":"Before Getting Started"},{"location":"tutorial/training_a_language_model/#instantiating-a-languagemodel-object","text":"By default, calling model = LanguageModel() will generate a folder called language_model in the current directory. However, it is possible to add arguments to override the default settings. Here is an example: model = LanguageModel ( project_dir = \"update_en_core_web_sm\" , recipe = \"project/recipes/update_en_core_web_sm.cfg\" , force = True ) This will create the \"update_en_core_web_sm\" folder. Instead of auto-generating a default config file, it will copy the \"recipe\" config file for updating spaCy's \"en_core_web_sm\" model. Setting force=True will cause Lexos to overwrite and pre-existing configuration if the model folder was created previously. See the API documentation for the full set of arguments that can be passed to the LanguageModel class. We will demonstrate some of them below. A note on model folder paths When a LanguageModel object is instantiated, most of the folders necessary for managing the workflow are created. By default, each subfolder contains the name of the model's language (the default is multilingual \"xx\"). Most of the model's methods will then assume that files are written to this language-specific subfolder. This organizational structure is largely based on the spaCy tagger_parser_ud project , and it remains to be seen if it should be a best practice for a more generic application. spaCy projects also store configuration files in a configs folder, which has not been deemed necessary for the LanguageModel class. As an instance should only have one configuration at a time, the LanguageModel 's config file is stored in its folder root. These decisions are subject to re-evaluation.","title":"Instantiating a LanguageModel Object"},{"location":"tutorial/training_a_language_model/#working-with-the-configuration","text":"Once the LanguageModel object is instantiated, the model folder will contain a configuration file with default configuration values or values you have assigned using arguments passed during instantiation. Although the configuration file is the \"single source of truth\" for the model, the LanguageModel object maintains a copy as a class attribute. Internally, this copy is parsed as a modified Python dict, which can be accessed by calling model.config . To see a string representation, call model.config.to_str() . Note The \"modified Python dict\" is actually a Thinc Config object (Thinc is the machine-learning library used internally by spaCy), so any of the Config object's methods as described in the Thinc documentation can be used. If you wish to change a value in your configuration you can simply modify the dictionary. For instance, if you wanted to change the language to English, you would use: model.config[\"nlp\"][\"lang\"] = \"en\" . This only modifies the config attribute. To ensure that your changes are made in the config file , you must then call model.save_config() . You can also replace the current config file and ( config attribute) by calling model.load_config() with a path to the desired config file. This is the equivalent of instantiating the LanguageModel object with a recipe path and force=True .","title":"Working with the Configuration"},{"location":"tutorial/training_a_language_model/#copying-assets","text":"Copying assets involves copying your training and testing data from another location into the models assets folder. This is done with the following code: project . copy_assets ( training_file = \"path/to/train.conllu\" , dev_file = \"path/to/dev.conllu\" , test_file = \"path/to/test.conllu\" ) In addition to local filepaths, copy_assets() also accepts urls. Asset files are not used directly when training a model; they are primarily for archival purposes. This is especially useful if you have downloaded them since you do not have to download them again if you need to re-instantiate the model object. Asset files are assumed to be in CONLL-U format, which will be converted to spaCy binary format by LanguageModel.convert_assets() (see below). If your training and testing files are not in CONLL-U format, you may still manually copy them into the assets folder for archival purposes, and you do not need to call LanguageModel.copy_assets() .","title":"Copying Assets"},{"location":"tutorial/training_a_language_model/#converting-assets-to-spacy-format","text":"The LanguageModel.convert_assets() method assumes that you have CONLL-U formatted files in the model's assets folder and automatically converts them to spaCy binary formatted files in the model's corpus folder. If your assets are not in CONLL-U format, you can skip this step, but you are responsible for converting them to spaCy binary format and depositing them in the corpus folder by some other means. Information on how to do that can be found in the spaCy documentation . Important The model's training_file , dev_file , and test_file attributes are used to locate files for conversion in the assets folder. If the files do not have the same names, an error will occur. You can see (or set) these attribute values with model.training_file , model.dev_file , and model.test_file . If you have not set them when you instantiated the LanguageModel object.","title":"Converting Assets to spaCy Format"},{"location":"tutorial/training_a_language_model/#debugging-configuration-and-data","text":"Before training your model, or if you encounter a problem, it is a good idea to try debugging with debug_config(path/to/config/file) or debug_data(path/to/config/file) . This will generate reports identifying potential problems. Important LanguageModel.debug_config() and LanguageModel.debug_data() are not part of the LanguageModel class, so do not call them without the dot notation. This means that you can debug a config file without an instantiated LanguageModel object.","title":"Debugging Configuration and Data"},{"location":"tutorial/training_a_language_model/#selecting-pipeline-components","text":"Before training your model, you must also ensure that you have specified which pipeline components you wish to train. By default, the LanguageModel object configures just the \"tagger\", which labels parts of speech. Other common pipeline components are \"tok2vec\", \"attribute_ruler\", \"lemmatizer\", \"parser\", \"ner\". For an overview of pipeline components, see the spaCy documentation . You can set pipeline components when instantiating a LanguageModel object with the components argument, which takes a list of components. You can also set them afterwards by modifying model.config[\"nlp\"][\"pipeline\"] , or even by editing the config file itself. Warning Setting pipeline components opens a bewildering away of possibilities for configuring the components, and it is very likely to generate invalid config files. It is highly recommended that you use LanguageModel.debug_config() if this happens but even more highly recommended that you rely on trusted recipes, especially if you are developing an interface for user-defined configuration. This will minimize the chance that you will encounter errors during training. Most components are independent of each other, but some share a \"token-to-vector\" component like \"tok2vec\". It is possible to speed up training by setting some components to be \"frozen\" and by indicating which components will be used for annotation. The following example assumes a model with the pipeline components \"tagger\", \"attribute_ruler\", \"lemmatizer\", \"parser\", and \"ner\". However, it skips training \"attribute_ruler\", \"lemmatizer\", \"parser\", and \"ner\". If the source of the components is a pre-existing model, the pre-existing state is simply passed to the new model. However, in the new model, only the \"tagger\", \"attribute_rule\", and \"lemmatizer\" components will have annotations. model . config [ \"training\" ][ \"frozen_components\" ] = [ \"attribute_ruler\" , \"lemmatizer\" , \"parser\" , \"ner\" ] model . config [ \"training\" ][ \"annotating_components\" ] = [ \"tagger\" , \"attribute_ruler\" , \"lemmatizer\" ] model . save_config () Again, it is safest to use a trusted recipe to ensure that all components are properly configured.","title":"Selecting Pipeline Components"},{"location":"tutorial/training_a_language_model/#training-a-model","text":"If everything looks good, you are ready to train your model. To do this, simply call LanguageModel.train() . Although it is possible to pass arguments to override the settings in the config file, in most cases you will not want to do so. Once training begins, its progress will be logged to the screen, and you can watch it train. Be aware that training can take a very (and unpredictably) long time for large data sets. Be patient!","title":"Training a Model"},{"location":"tutorial/training_a_language_model/#balancing-efficiency-and-accuracy","text":"By default, models are generated with efficiency in mind, but you can change this by setting optimize=\"accuracy\" when instantiating your model. This will cause the LanguageModel object to generate a different config file, one that may produce more accurate models, but with potentially longer training times and bigger output model sizes. Internally, these config files are generated by an evolving set of language-specific recommendations maintained by spaCy. You can, of course, override these recommendations or use a recipe config file to bypass them entirely. If you are using a language for which spaCy has a pre-built model, it can be useful to generate a config file with the default recommendations and then modify it for your own purposes as necessary.","title":"Balancing Efficiency and Accuracy"},{"location":"tutorial/training_a_language_model/#training-with-a-gpu-and-transformer-models","text":"Using a GPU to train your model can be 2-3 times faster than using your computer's CPU. You can substitute a GPU for your computer's CPU simply by instantiating your LanguageModel object with gpu set to your GPU's id. However, this may not be worth it, given that setting up a GPU can be very fiddly. Where it truly becomes necessary is if you want to take advantage of pre-built transformer models such as BERT or GPT-2. For an introduction to the use of transformer models, see spaCy's documentation and the spacy-transformers plugin. Warning Setting up a GPU and transformer models is not for the faint of heart. Most likely, this will be done by application developers on the back end so that individual users do not have to do so on their own machines.","title":"Training with a GPU and Transformer Models"},{"location":"tutorial/training_a_language_model/#evaluating-a-model","text":"Once a model has completed training, the output is stored in the training folder. You can then evaluate it by calling LanguageModel.evaluate() with the path to the trained model folder and the path to your test file in the corpus folder. model . evaluate ( model = \"training/en/model-best\" , testfile = \"corpus/en/test.spacy\" ) You will receive a report showing how accurately the model predicted the labels in your test data. A copy of the report in json format is saved to the metrics folder. Note If you run into a problem, you can also debug your model by calling LanguageModel.debug_model(\"path/to/config/file\") . If you are unhappy with the level of accuracy, you can try tweaking your configuration or, with a greater likelihood of success, adding more training data. Re-train the model and test again.","title":"Evaluating a Model"},{"location":"tutorial/training_a_language_model/#packaging-the-model","text":"To make the model useable, it must first be packaged. Once packaged, the model can be used for parsing texts using either the Lexos API or using spaCy directly: # Using the Lexos API from lexos import tokenizer doc = tokenizer . make_doc ( text , model = \"path/to/model\" ) # Using spaCy import spacy nlp = spacy . load ( \"path/to/model\" ) doc = nlp ( text ) The path to the model package should point to the directory containing the packages config.cfg file. Note So far, I have only tried loading model packages using the path the package folder. It should also be possible to use the package name, but this needs further testing. To package a model, call LanguageModel.package() , providing the input folder for the model in the training folder and the path to the output folder. You can also provide a name and version for the model if you did not instantiation the LanguageModel object with package_name and package_version values. Setting force=True will overwrite any previous package in the output directory. model . package ( input_dir = \"path/to/model-best\" , output_dir = \"path/to/output/directory\" , name = \"en_test_sm\" , version = \"1.0.0\" , force = True ) It is a good idea to store packages in a folder called packages inside the model folder, although they can be stored in a separate location for distribution.","title":"Packaging the Model"},{"location":"tutorial/visualization/","text":"Overview \u00a4 Once you have generated a document-term matrix, it becomes easy to convert it to pandas dataframe and then to use any of the pandas.DataFrame.plot methods, or to export the data for use with other tools. However, the Lexos API has a number of built-in visualizations to make the process easier. These are accessed through the lexos.visualization module. By default, all Lexos visualizations are static plots produced with the Python matplotlib library. However, both static and interactive plots can be produced using third-party libraries like Plotly and Seaborn. These can be imported from their own folders within the visualization module. For instance, the Plotly dendrogram module can be imported with from lexos.visualization.plotly.cluster import dendrogram . Each of the available visualization types is described below. Warning Currently, there is some inconsistency with respect to the format of the input data for dendrograms. Wherever possible, the document-term matrix (the output of lexos.dtm.DTM ) is used. Generally, the visualization function will then call DTM.get_table() to retrieve the data as a pandas dataframe. However, in some cases, the visualization function requires the dataframe table to be format of input document. Eventually, these functions will be made consistent. Word Clouds (and Variants) \u00a4 Single Word Clouds \u00a4 The simplest way to generate a word cloud is to import the wordcloud function and pass it a document-term matrix. from lexos.visualization.cloud.wordcloud import wordcloud dtm = DTM ( segments , labels ) wordcloud ( dtm , opts , figure_opts ) Wordclouds are generated by the Python Wordcloud library. Options can be passed to WordCloud using the opts parameter. Figures are generated using Python's matplotlib , and its options can be passed using figure_opts . Both take a dictionary of options, as in the example below: opts = { \"max_words\" : 2000 , \"background_color\" : \"white\" , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" } figure_opts = { \"figsize\" : ( 15 , 8 )} wordcloud = wordcloud ( dtm , opts = opts , figure_opts = figure_opts , round = 150 , show = True , filename = \"wordcloud.png\" ) The round parameter (normally between 100 and 300) will add various degrees of rounded corners to the word cloud. If a filename is provided, the plot will be saved to the specified file. By default, show=True , and the wordcloud will be plotted to the screen if the environment is appropriate. If show=False , the WordCloud object will be returned. In this instance, you can still save the word cloud by calling wordcloud.to_file(filename) . By default, wordcloud() , creates a word cloud based on the total term counts for all documents. If you wish to use a single or a subset of documents, use the docs parameter. wordcloud ( dtm , docs = [ \"doc1\" , \"doc2\" , etc . ]) Note wordcloud() takes a number of other input formats, including raw text, but a lexos.dtm.DTM is by far the easiest method to generate data from pre-tokenised texts. Multiclouds \u00a4 Multiclouds are grid-organized word clouds of individual documents, which allow you to compare the document clouds side by side. The method of generating multiclouds is similar to word clouds. The basic input is a lexos.dtm.DTM object, where one word cloud will be generated for each document. If a subset of documents is required, the docs parameter shown above should be used. Once the data is prepared, the multiclouds are generated as shown below: from lexos.visualization.cloud.wordcloud import multicloud labels = dtm . get_table () . columns . tolist ()[ 1 :] multicloud ( dtm , title = \"My Multicloud\" , labels = labels , ncols = 3 ) Since multicloud produce multiple subplots, there is a title parameter to give the entire figure a title and a labels parameter, which includes a list labels to be assigned to each subplot. In the example above, we are just taking the labels from the DTM, minus the first \"terms\" column. The ncols parameter sets the number of subplots per row. If a filename is provided, the entire plot will be saved. If show=False , multicloud() returns a list of word clouds. These can be saved individually by calling to_file() on them. Note As with word clouds, the multicloud() function takes a number of different input formats, but pandas dataframes are the easiest to work with. Bubble Charts \u00a4 Bubble charts (known as \"bubbleviz\" in Lexos) are produced as follows: from lexos.visualization.bubbleviz import bubbleviz bubbleviz ( dtm ) See lexos.visualization.bubbleviz.BubbleChart for a description of the various options. Warning The algorithm to produce bubble charts in pure Python is experimental and not nearly as good as the Javascript implementation used in the Lexos app. Dendrograms \u00a4 Static dendrograms based on hierarchical agglomerative clustering are produced using the Dendrogram class. It operates directly on the document-term matrix. dendrogram = Dendrogram ( dtm ) dendrogram . fig or dendrogram = Dendrogram ( dtm , show = True ) The dendrogram plot is not shown by default, so you need to use one of the methods above to display it. The class is a wrapper around scipy.cluster.hierarchy.dendrogram , and you can change any of its options by calling them, e.g. Dendrogram.orientation = bottom . If you change any of the options, you must then rebuild the dendrogram by calling Dendrogram.build() . The distance title, distance metric, and linkage method, of the dendrogram can be set in the same way by passing title , metric , and method when instantiating the class or by setting them afterwards in the same manner as shown above. There is also a savefig() method which takes a filename or filepath to save the file. The image format is detected automatically from the extension type. Plotly Dendrograms \u00a4 To create a dendrogram in plotly, do the following: from lexos.visualization.plotly.cluster.dendrogram import PlotlyDendrogram layout = dict ( margin = dict ( l = 20 )) dendrogram = PlotlyDendrogram ( dtm , title = \"Plotly Dendrogram\" , x_tickangle = 45 , ** layout ) The plot() function accepts the labels , colorscale , hovertext , and color_threshold parameters in the plotly.figure_factory.create_dendrogram function. However, it requires strings, rather than functions, for the names of distance metric and linkage method, as shown above. Use the x_tickangle parameter to change the rotation of the leaf labels. A dictionary of Plotly configuration options can be passed to the config parameter. Likewise, Plotly layout options can be passed using the layout parameter, as shown in the example above. Once the dendrogram object has been instantiated, it can be displayed with the Dendrogram.showfig() method it can also be converted to HTML with Dendrogram.to_html() . By default, this will return an HTML div element, but the output_type can also be set to \"file\" and a filename supplied to save the HTML string as a file. Warning If you create a dendrogram with something like dendrogram = Dendrogram(dtm, show=False) and then call dendrogram.fig , you will get a plot, but it will not have any configurations you have specified applied to it. This includes the default configurations such as removing the Plotly logo from the menubar. This is due to a flaw in Plotly's API. Accessing the dendrogram figure with dendrogram.showfig() avoids this problem. Clustermaps \u00a4 Using Seaborn \u00a4 A clustermap is a dendrogram attached to a heatmap, showing the relative similarity of documents using a colour scale. Lexos can generate static clustermap images using the Python Seaborn library. To generate a clustermap, use the following code: from lexos.visualization.seaborn.cluster.clustermap import ClusterMap cluster_map = ClusterMap ( dtm , title = \"My Clustermap\" ) lexos.visualization.seaborn.cluster.clustermap.ClusterMap accepts any Seaborn.clustermap parameter. The distance title, distance metric, and linkage method, of the dendrogram can be set in the same way by passing title , metric , and method when instantiating the class or by setting them afterwards calling ClusterMap.build() . The clustermap plot is not shown by default. To display the plot, generate it with show=True or refernce it with ClusterMap.fig . If you change any of the options, you must then rebuild the dendrogram by calling ClusterMap.build() . There is also a savefig() method which takes a filename or filepath to save the file. The image format is detected automatically from the extension type. Using Plotly \u00a4 Plotly clustermaps are somewhat experimental and may not render plots that are as informative as Seaborn clustermaps. One advantage they have is that, instead of providing labels for each document at the bottom of the graph, they provide the document labels on the x and y axes, as well as the z (distance) score in the hovertext. This allows you to mouse over individual sections of the heatmap to see which documents are represented by that particular section. Plotly clustermaps are constructed in the same manner to Plotly dendrograms: from lexos.visualization.seaborn.cluster.clustermap import PlotlyClustermap cluster_map = PlotlyClustermap ( dtm ) cluster_map . showfig () All the options for Plotly dendrograms are available with the following differences: Figure size is determined by configuring the width and height parameters. colorscale is the name of a built-in Plotly colorscale . This is applied to the heatmap and converted internally to a list of colorus to apply to the dendrograms. Two additional parameters, hide_upper and hide_side allow you to hide the individual dendrograms. Warning Once the clustermap plot has been generated, it is inadvisable to use the modebar zoom and pan buttons because this tends to separate the heatmap from the dendrogram leaves. It may even be advisable to remove these buttons from the modebar by default.","title":"Visualization"},{"location":"tutorial/visualization/#overview","text":"Once you have generated a document-term matrix, it becomes easy to convert it to pandas dataframe and then to use any of the pandas.DataFrame.plot methods, or to export the data for use with other tools. However, the Lexos API has a number of built-in visualizations to make the process easier. These are accessed through the lexos.visualization module. By default, all Lexos visualizations are static plots produced with the Python matplotlib library. However, both static and interactive plots can be produced using third-party libraries like Plotly and Seaborn. These can be imported from their own folders within the visualization module. For instance, the Plotly dendrogram module can be imported with from lexos.visualization.plotly.cluster import dendrogram . Each of the available visualization types is described below. Warning Currently, there is some inconsistency with respect to the format of the input data for dendrograms. Wherever possible, the document-term matrix (the output of lexos.dtm.DTM ) is used. Generally, the visualization function will then call DTM.get_table() to retrieve the data as a pandas dataframe. However, in some cases, the visualization function requires the dataframe table to be format of input document. Eventually, these functions will be made consistent.","title":"Overview"},{"location":"tutorial/visualization/#word-clouds-and-variants","text":"","title":"Word Clouds (and Variants)"},{"location":"tutorial/visualization/#single-word-clouds","text":"The simplest way to generate a word cloud is to import the wordcloud function and pass it a document-term matrix. from lexos.visualization.cloud.wordcloud import wordcloud dtm = DTM ( segments , labels ) wordcloud ( dtm , opts , figure_opts ) Wordclouds are generated by the Python Wordcloud library. Options can be passed to WordCloud using the opts parameter. Figures are generated using Python's matplotlib , and its options can be passed using figure_opts . Both take a dictionary of options, as in the example below: opts = { \"max_words\" : 2000 , \"background_color\" : \"white\" , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" } figure_opts = { \"figsize\" : ( 15 , 8 )} wordcloud = wordcloud ( dtm , opts = opts , figure_opts = figure_opts , round = 150 , show = True , filename = \"wordcloud.png\" ) The round parameter (normally between 100 and 300) will add various degrees of rounded corners to the word cloud. If a filename is provided, the plot will be saved to the specified file. By default, show=True , and the wordcloud will be plotted to the screen if the environment is appropriate. If show=False , the WordCloud object will be returned. In this instance, you can still save the word cloud by calling wordcloud.to_file(filename) . By default, wordcloud() , creates a word cloud based on the total term counts for all documents. If you wish to use a single or a subset of documents, use the docs parameter. wordcloud ( dtm , docs = [ \"doc1\" , \"doc2\" , etc . ]) Note wordcloud() takes a number of other input formats, including raw text, but a lexos.dtm.DTM is by far the easiest method to generate data from pre-tokenised texts.","title":"Single Word Clouds"},{"location":"tutorial/visualization/#multiclouds","text":"Multiclouds are grid-organized word clouds of individual documents, which allow you to compare the document clouds side by side. The method of generating multiclouds is similar to word clouds. The basic input is a lexos.dtm.DTM object, where one word cloud will be generated for each document. If a subset of documents is required, the docs parameter shown above should be used. Once the data is prepared, the multiclouds are generated as shown below: from lexos.visualization.cloud.wordcloud import multicloud labels = dtm . get_table () . columns . tolist ()[ 1 :] multicloud ( dtm , title = \"My Multicloud\" , labels = labels , ncols = 3 ) Since multicloud produce multiple subplots, there is a title parameter to give the entire figure a title and a labels parameter, which includes a list labels to be assigned to each subplot. In the example above, we are just taking the labels from the DTM, minus the first \"terms\" column. The ncols parameter sets the number of subplots per row. If a filename is provided, the entire plot will be saved. If show=False , multicloud() returns a list of word clouds. These can be saved individually by calling to_file() on them. Note As with word clouds, the multicloud() function takes a number of different input formats, but pandas dataframes are the easiest to work with.","title":"Multiclouds"},{"location":"tutorial/visualization/#bubble-charts","text":"Bubble charts (known as \"bubbleviz\" in Lexos) are produced as follows: from lexos.visualization.bubbleviz import bubbleviz bubbleviz ( dtm ) See lexos.visualization.bubbleviz.BubbleChart for a description of the various options. Warning The algorithm to produce bubble charts in pure Python is experimental and not nearly as good as the Javascript implementation used in the Lexos app.","title":"Bubble Charts"},{"location":"tutorial/visualization/#dendrograms","text":"Static dendrograms based on hierarchical agglomerative clustering are produced using the Dendrogram class. It operates directly on the document-term matrix. dendrogram = Dendrogram ( dtm ) dendrogram . fig or dendrogram = Dendrogram ( dtm , show = True ) The dendrogram plot is not shown by default, so you need to use one of the methods above to display it. The class is a wrapper around scipy.cluster.hierarchy.dendrogram , and you can change any of its options by calling them, e.g. Dendrogram.orientation = bottom . If you change any of the options, you must then rebuild the dendrogram by calling Dendrogram.build() . The distance title, distance metric, and linkage method, of the dendrogram can be set in the same way by passing title , metric , and method when instantiating the class or by setting them afterwards in the same manner as shown above. There is also a savefig() method which takes a filename or filepath to save the file. The image format is detected automatically from the extension type.","title":"Dendrograms"},{"location":"tutorial/visualization/#plotly-dendrograms","text":"To create a dendrogram in plotly, do the following: from lexos.visualization.plotly.cluster.dendrogram import PlotlyDendrogram layout = dict ( margin = dict ( l = 20 )) dendrogram = PlotlyDendrogram ( dtm , title = \"Plotly Dendrogram\" , x_tickangle = 45 , ** layout ) The plot() function accepts the labels , colorscale , hovertext , and color_threshold parameters in the plotly.figure_factory.create_dendrogram function. However, it requires strings, rather than functions, for the names of distance metric and linkage method, as shown above. Use the x_tickangle parameter to change the rotation of the leaf labels. A dictionary of Plotly configuration options can be passed to the config parameter. Likewise, Plotly layout options can be passed using the layout parameter, as shown in the example above. Once the dendrogram object has been instantiated, it can be displayed with the Dendrogram.showfig() method it can also be converted to HTML with Dendrogram.to_html() . By default, this will return an HTML div element, but the output_type can also be set to \"file\" and a filename supplied to save the HTML string as a file. Warning If you create a dendrogram with something like dendrogram = Dendrogram(dtm, show=False) and then call dendrogram.fig , you will get a plot, but it will not have any configurations you have specified applied to it. This includes the default configurations such as removing the Plotly logo from the menubar. This is due to a flaw in Plotly's API. Accessing the dendrogram figure with dendrogram.showfig() avoids this problem.","title":"Plotly Dendrograms"},{"location":"tutorial/visualization/#clustermaps","text":"","title":"Clustermaps"},{"location":"tutorial/visualization/#using-seaborn","text":"A clustermap is a dendrogram attached to a heatmap, showing the relative similarity of documents using a colour scale. Lexos can generate static clustermap images using the Python Seaborn library. To generate a clustermap, use the following code: from lexos.visualization.seaborn.cluster.clustermap import ClusterMap cluster_map = ClusterMap ( dtm , title = \"My Clustermap\" ) lexos.visualization.seaborn.cluster.clustermap.ClusterMap accepts any Seaborn.clustermap parameter. The distance title, distance metric, and linkage method, of the dendrogram can be set in the same way by passing title , metric , and method when instantiating the class or by setting them afterwards calling ClusterMap.build() . The clustermap plot is not shown by default. To display the plot, generate it with show=True or refernce it with ClusterMap.fig . If you change any of the options, you must then rebuild the dendrogram by calling ClusterMap.build() . There is also a savefig() method which takes a filename or filepath to save the file. The image format is detected automatically from the extension type.","title":"Using Seaborn"},{"location":"tutorial/visualization/#using-plotly","text":"Plotly clustermaps are somewhat experimental and may not render plots that are as informative as Seaborn clustermaps. One advantage they have is that, instead of providing labels for each document at the bottom of the graph, they provide the document labels on the x and y axes, as well as the z (distance) score in the hovertext. This allows you to mouse over individual sections of the heatmap to see which documents are represented by that particular section. Plotly clustermaps are constructed in the same manner to Plotly dendrograms: from lexos.visualization.seaborn.cluster.clustermap import PlotlyClustermap cluster_map = PlotlyClustermap ( dtm ) cluster_map . showfig () All the options for Plotly dendrograms are available with the following differences: Figure size is determined by configuring the width and height parameters. colorscale is the name of a built-in Plotly colorscale . This is applied to the heatmap and converted internally to a list of colorus to apply to the dendrograms. Two additional parameters, hide_upper and hide_side allow you to hide the individual dendrograms. Warning Once the clustermap plot has been generated, it is inadvisable to use the modebar zoom and pan buttons because this tends to separate the heatmap from the dendrogram leaves. It may even be advisable to remove these buttons from the modebar by default.","title":"Using Plotly"}]}