{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00a4 The Lexos API is a library of methods for programmatically implementating and extending the functionality found in the Lexos web app. Eventually, the web app will be rewritten to use the API directly. The goal of this stage of development is to reproduce (and in some cases extend) the functionality of the current web app. For the moment, much of the thinking behind the API's architecture is explained in the Tutorial . Current Status (v0.0.1) \u00a4 So far, only the basic architecture of the API has been built. The Loader class will accept any local file, regardless of format, and it will also download text from URLs. Obviously, there are some security features that need to be added. It would also be nice to load from different file formats (json, docx, zip, etc.), which is not currently supported. All of the functionality of the Lexos app's scrubber module has been ported over, and the basic tokenizer module works. However, there needs to be some error checking in both modules. The cutter module will need some consideration, as it will probably require a combination of features from scrubber and tokenizer , depending on whether the user wants to cut based on some pattern or cut by token ranges. Development Notes \u00a4 Dependency management is handled with Poetry . The code is tested with pytest as a pre-commit hook. Before commit, I generally run isort and interrogate to ensure consistent imports and docstrings, but these are not currently implemented as pre-commit hooks. Docstrings are given an fully as possible in Google style, with as much type hinting as possible. Docstrings are used by mkdocs to auto-generate the documentation through the magic of mkdocstrings . A number of test scripts have been implemented, and they are used by the continuous integration process. However, the scripts are incomplete and intended primarily for quick testing from the command line. A fuller test suite is intended once the API is more complete.","title":"Home"},{"location":"#introduction","text":"The Lexos API is a library of methods for programmatically implementating and extending the functionality found in the Lexos web app. Eventually, the web app will be rewritten to use the API directly. The goal of this stage of development is to reproduce (and in some cases extend) the functionality of the current web app. For the moment, much of the thinking behind the API's architecture is explained in the Tutorial .","title":"Introduction"},{"location":"#current-status-v001","text":"So far, only the basic architecture of the API has been built. The Loader class will accept any local file, regardless of format, and it will also download text from URLs. Obviously, there are some security features that need to be added. It would also be nice to load from different file formats (json, docx, zip, etc.), which is not currently supported. All of the functionality of the Lexos app's scrubber module has been ported over, and the basic tokenizer module works. However, there needs to be some error checking in both modules. The cutter module will need some consideration, as it will probably require a combination of features from scrubber and tokenizer , depending on whether the user wants to cut based on some pattern or cut by token ranges.","title":"Current Status (v0.0.1)"},{"location":"#development-notes","text":"Dependency management is handled with Poetry . The code is tested with pytest as a pre-commit hook. Before commit, I generally run isort and interrogate to ensure consistent imports and docstrings, but these are not currently implemented as pre-commit hooks. Docstrings are given an fully as possible in Google style, with as much type hinting as possible. Docstrings are used by mkdocs to auto-generate the documentation through the magic of mkdocstrings . A number of test scripts have been implemented, and they are used by the continuous integration process. However, the scripts are incomplete and intended primarily for quick testing from the command line. A fuller test suite is intended once the API is more complete.","title":"Development Notes"},{"location":"installation/","text":"Installation \u00a4 During the development process, dependency management and packaging are handled using Poetry . When the API is released, you will be able to install it with pip install lexos-api . Other methods of installation will be added to this page at a later date. Installing the Lexos API with Poetry \u00a4 A Mac-centric approach to installing Poetry is as follows: $ brew install poetry $ poetry --version Poetry version 1.1.11 The Poetry website contains extensive documentation on how to install it on different operating systems. My running theory is that the best way to install the Lexos API prior to release is to complete following steps: Install Poetry. Clone the repo. cd to the local repo directory. Run poetry init . This is as yet untested. If you are only evaluating the API and not modifying the code base, you can probably just clone the repo and run the library locally by following the instructions below. Running the Development Library Locally \u00a4 Although you can call the Lexos API from command-line scripts or the Python command line, the easiest method is to call it from a Jupyter notebook. However, you may find that Jupyter has trouble finding the module if it has not been added to your Python environment. There is an easy workaround for this. cd to the directory containing the API. If you cloned the repository, this will be the lexos folder (not the lexos subfolder inside it). Then fire up jupyter notebook or jupyter lab . In the first cell of your notebook, run the following code: import os import sys LEXOS_PATH = \"lexos\" if \"NOTEBOOK_INITIATED_FLAG\" not in globals (): NOTEBOOK_INITIATED_FLAG = True try : module_path = os . path . join ( os . path . dirname ( __file__ ), os . pardir ) except : module_path = os . path . abspath ( os . path . join ( LEXOS_PATH )) % cd lexos % pwd if module_path not in sys . path : sys . path . append ( module_path ) If you are starting your notebook from another directory, you can modify LEXOS_PATH to point to the lexos subfolder. You should now be able to import the lexos API module from your local directory.","title":"Installation"},{"location":"installation/#installation","text":"During the development process, dependency management and packaging are handled using Poetry . When the API is released, you will be able to install it with pip install lexos-api . Other methods of installation will be added to this page at a later date.","title":"Installation"},{"location":"installation/#installing-the-lexos-api-with-poetry","text":"A Mac-centric approach to installing Poetry is as follows: $ brew install poetry $ poetry --version Poetry version 1.1.11 The Poetry website contains extensive documentation on how to install it on different operating systems. My running theory is that the best way to install the Lexos API prior to release is to complete following steps: Install Poetry. Clone the repo. cd to the local repo directory. Run poetry init . This is as yet untested. If you are only evaluating the API and not modifying the code base, you can probably just clone the repo and run the library locally by following the instructions below.","title":"Installing the Lexos API with Poetry"},{"location":"installation/#running-the-development-library-locally","text":"Although you can call the Lexos API from command-line scripts or the Python command line, the easiest method is to call it from a Jupyter notebook. However, you may find that Jupyter has trouble finding the module if it has not been added to your Python environment. There is an easy workaround for this. cd to the directory containing the API. If you cloned the repository, this will be the lexos folder (not the lexos subfolder inside it). Then fire up jupyter notebook or jupyter lab . In the first cell of your notebook, run the following code: import os import sys LEXOS_PATH = \"lexos\" if \"NOTEBOOK_INITIATED_FLAG\" not in globals (): NOTEBOOK_INITIATED_FLAG = True try : module_path = os . path . join ( os . path . dirname ( __file__ ), os . pardir ) except : module_path = os . path . abspath ( os . path . join ( LEXOS_PATH )) % cd lexos % pwd if module_path not in sys . path : sys . path . append ( module_path ) If you are starting your notebook from another directory, you can modify LEXOS_PATH to point to the lexos subfolder. You should now be able to import the lexos API module from your local directory.","title":"Running the Development Library Locally"},{"location":"api/","text":"API \u00a4 A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. io A set of functions for handling input-output processes. utils A set of utility functions shared by multiple modules.","title":"Index"},{"location":"api/#api","text":"A full explanation will be added soon. In the meantime, here is a table of the Lexos API modules: scrubber A destructive preprocessor normally used on texts before they are tokenised. tokenizer A set of functions used to convert texts into spaCy tokenised spaCy docs and to manipulate those docs. io A set of functions for handling input-output processes. utils A set of utility functions shared by multiple modules.","title":"API"},{"location":"api/utils/","text":"Utils \u00a4 This module contains helper functions used by multiple modules. lexos . utils . _decode_bytes ( raw_bytes ) private \u00a4 Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: Type Description str The decoded string. Source code in lexos\\utils.py def _decode_bytes ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: The decoded string. \"\"\" if isinstance ( raw_bytes , bytes ): try : decoded_str = _try_decode_bytes_ ( raw_bytes ) except ( UnicodeDecodeError , TypeError ): raise LexosException ( 'Chardet failed to detect encoding of your ' 'file. Please make sure your file is in ' 'utf-8 encoding.' ) else : decoded_str = raw_bytes return decoded_str lexos . utils . _try_decode_bytes_ ( raw_bytes ) private \u00a4 Try to decode raw bytes (helper function for decode_bytes(). Parameters: Name Type Description Default raw_bytes bytes The bytes you want to decode to string. required Returns: Type Description str A decoded string. Source code in lexos\\utils.py def _try_decode_bytes_ ( raw_bytes : bytes ) -> str : \"\"\"Try to decode raw bytes (helper function for decode_bytes(). Args: raw_bytes (bytes): The bytes you want to decode to string. Returns: A decoded string. \"\"\" # Detect the encoding with only the first couple of bytes encoding_detect = chardet . detect ( raw_bytes [: constants . MIN_ENCODING_DETECT ]) # Get the encoding encoding_type = encoding_detect [ 'encoding' ] if encoding_type is None : encoding_detect = chardet . detect ( raw_bytes ) encoding_type = encoding_detect [ 'encoding' ] try : # Try to decode the string using the original encoding decoded_string = raw_bytes . decode ( encoding_type ) except ( UnicodeDecodeError , TypeError ): # Try UnicodeDammit if chardet didn't work if encoding_type == \"ascii\" : dammit = UnicodeDammit ( raw_bytes , [ \"iso-8859-1\" , \"iso-8859-15\" , \"windows-1252\" ]) else : dammit = UnicodeDammit ( raw_bytes ) decoded_string = dammit . unicode_markup return decoded_string lexos . utils . ensure_list ( item ) \u00a4 Ensure string is converted to a Path. Parameters: Name Type Description Default item Any Anything. required Returns: Type Description List The item inside a list if it is not already a list. Source code in lexos\\utils.py def ensure_list ( item : Any ) -> List : \"\"\"Ensure string is converted to a Path. Args: item (Any): Anything. Returns: The item inside a list if it is not already a list. \"\"\" if not isinstance ( item , list ): item = [ item ] return item lexos . utils . ensure_path ( path ) \u00a4 Ensure string is converted to a Path. Parameters: Name Type Description Default path Any Anything. If string, it's converted to Path. required Returns: Type Description Any Path or original argument. Source code in lexos\\utils.py def ensure_path ( path : Any ) -> Any : \"\"\"Ensure string is converted to a Path. Args: path (Any): Anything. If string, it's converted to Path. Returns: Path or original argument. \"\"\" if isinstance ( path , str ): return Path ( path . replace ( ' \\\\ ' , '/' )) else : return path lexos . utils . is_url ( s ) \u00a4 Check if string is a URL. Source code in lexos\\utils.py def is_url ( s : str ) -> bool : \"\"\"Check if string is a URL.\"\"\" return bool ( re . match ( r \"(https?|ftp)://\" # protocol r \"(\\w+(\\-\\w+)*\\.)?\" # host (optional) r \"((\\w+(\\-\\w+)*)\\.(\\w+))\" # domain r \"(\\.\\w+)*\" # top-level domain (optional, can have > 1) r \"([\\w\\-\\._\\~/]*)*(?<!\\.)\" # path, params, anchors, etc. (optional) , s )) lexos . utils . unzip_archive ( archive_path , extract_dir ) \u00a4 Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python . Parameters: Name Type Description Default archive_path str The path to the archive file to be unzipped. required extract_dir str The path to folder where the archive will be extracted. required Source code in lexos\\utils.py def unzip_archive ( archive_path : str , extract_dir : str ): \"\"\"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python. Args: archive_path (str): The path to the archive file to be unzipped. extract_dir (str): The path to folder where the archive will be extracted. \"\"\" zf = zipfile . ZipFile ( archive_path , 'r' ) progress = Progress () with progress : for file in progress . track ( zf . infolist (), description = \"Processing...\" ): zf . extract ( file , path = extract_dir ) sleep ( 0.1 ) lexos . utils . zip_folder ( source_dir , archive_file ) \u00a4 Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Parameters: Name Type Description Default source_dir Path The path to the source directory. required archive_file Path The path to the archive file to be created (including file extension). required Source code in lexos\\utils.py def zip_folder ( source_dir : Path , archive_file : Path ): \"\"\"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Args: source_dir (Path): The path to the source directory. archive_file (Path): The path to the archive file to be created (including file extension). \"\"\" progress = Progress () with zipfile . ZipFile ( archive_file , mode = \"w\" , compression = zipfile . ZIP_DEFLATED , compresslevel = 7 ) as zip : files = list ( source_dir . rglob ( \"*\" )) with progress : for file in progress . track ( files , description = \"Processing...\" ): relative_path = file . relative_to ( source_dir ) zip . write ( file , arcname = relative_path ) sleep ( 0.1 ) lexos . utils . get_encoding ( input_string ) \u00a4 Use chardet to return the encoding type of a string. Parameters: Name Type Description Default input_string bytes A bytestring. required Returns: Type Description str The string's encoding type. Source code in lexos\\utils.py def get_encoding ( input_string : bytes ) -> str : \"\"\"Use chardet to return the encoding type of a string. Args: input_string (bytes): A bytestring. Returns: The string's encoding type. \"\"\" encoding_detect = chardet . detect ( input_string [ : constants . MIN_ENCODING_DETECT ]) encoding_type = encoding_detect [ 'encoding' ] return encoding_type lexos . utils . normalize ( raw_bytes ) \u00a4 Normalise a string to LexosFile format. Parameters: Name Type Description Default raw_bytes str The input string. required Returns: Type Description str Normalised version of the input string. Source code in lexos\\utils.py def normalize ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Normalise a string to LexosFile format. Args: raw_bytes (str): The input string. Returns: Normalised version of the input string. \"\"\" s = _decode_bytes ( raw_bytes ) return s lexos . utils . normalize_files ( filepaths , destination_dir = '.' ) \u00a4 Normalise a list of files to LexosFile format and save the files. Parameters: Name Type Description Default filepaths list The list of paths to input files. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_files ( filepaths : List [ Union [ Path , str ]], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a list of files to LexosFile format and save the files. Args: filepaths (list): The list of paths to input files. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" for filepath in filepaths : filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc )) lexos . utils . normalize_file ( filepath , destination_dir = '.' ) \u00a4 Normalise a file to LexosFile format and save the file. Parameters: Name Type Description Default filepath Path, str The path to the input file. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_file ( filepath : Union [ Path , str ], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a file to LexosFile format and save the file. Args: filepath (Path, str): The path to the input file. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc )) lexos . utils . normalize_strings ( strings ) \u00a4 Normalise a list of strings to LexosFile format. Parameters: Name Type Description Default strings list The list of input strings. required Returns: Type Description str A list of normalised versions of the input strings. Source code in lexos\\utils.py def normalize_strings ( strings : List [ Union [ bytes , str ]]) -> str : \"\"\"Normalise a list of strings to LexosFile format. Args: strings (list): The list of input strings. Returns: A list of normalised versions of the input strings. \"\"\" normalized_strings = [] for s in strings : normalized_strings . append ( normalize ( s )) return normalized_strings","title":"Utils"},{"location":"api/utils/#utils","text":"This module contains helper functions used by multiple modules.","title":"Utils"},{"location":"api/utils/#lexos.utils._decode_bytes","text":"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: Type Description str The decoded string. Source code in lexos\\utils.py def _decode_bytes ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Decode raw bytes from a user's file into a string. Args raw_bytes (bytes, str): The bytes to be decoded to a python string. Returns: The decoded string. \"\"\" if isinstance ( raw_bytes , bytes ): try : decoded_str = _try_decode_bytes_ ( raw_bytes ) except ( UnicodeDecodeError , TypeError ): raise LexosException ( 'Chardet failed to detect encoding of your ' 'file. Please make sure your file is in ' 'utf-8 encoding.' ) else : decoded_str = raw_bytes return decoded_str","title":"_decode_bytes()"},{"location":"api/utils/#lexos.utils._try_decode_bytes_","text":"Try to decode raw bytes (helper function for decode_bytes(). Parameters: Name Type Description Default raw_bytes bytes The bytes you want to decode to string. required Returns: Type Description str A decoded string. Source code in lexos\\utils.py def _try_decode_bytes_ ( raw_bytes : bytes ) -> str : \"\"\"Try to decode raw bytes (helper function for decode_bytes(). Args: raw_bytes (bytes): The bytes you want to decode to string. Returns: A decoded string. \"\"\" # Detect the encoding with only the first couple of bytes encoding_detect = chardet . detect ( raw_bytes [: constants . MIN_ENCODING_DETECT ]) # Get the encoding encoding_type = encoding_detect [ 'encoding' ] if encoding_type is None : encoding_detect = chardet . detect ( raw_bytes ) encoding_type = encoding_detect [ 'encoding' ] try : # Try to decode the string using the original encoding decoded_string = raw_bytes . decode ( encoding_type ) except ( UnicodeDecodeError , TypeError ): # Try UnicodeDammit if chardet didn't work if encoding_type == \"ascii\" : dammit = UnicodeDammit ( raw_bytes , [ \"iso-8859-1\" , \"iso-8859-15\" , \"windows-1252\" ]) else : dammit = UnicodeDammit ( raw_bytes ) decoded_string = dammit . unicode_markup return decoded_string","title":"_try_decode_bytes_()"},{"location":"api/utils/#lexos.utils.ensure_list","text":"Ensure string is converted to a Path. Parameters: Name Type Description Default item Any Anything. required Returns: Type Description List The item inside a list if it is not already a list. Source code in lexos\\utils.py def ensure_list ( item : Any ) -> List : \"\"\"Ensure string is converted to a Path. Args: item (Any): Anything. Returns: The item inside a list if it is not already a list. \"\"\" if not isinstance ( item , list ): item = [ item ] return item","title":"ensure_list()"},{"location":"api/utils/#lexos.utils.ensure_path","text":"Ensure string is converted to a Path. Parameters: Name Type Description Default path Any Anything. If string, it's converted to Path. required Returns: Type Description Any Path or original argument. Source code in lexos\\utils.py def ensure_path ( path : Any ) -> Any : \"\"\"Ensure string is converted to a Path. Args: path (Any): Anything. If string, it's converted to Path. Returns: Path or original argument. \"\"\" if isinstance ( path , str ): return Path ( path . replace ( ' \\\\ ' , '/' )) else : return path","title":"ensure_path()"},{"location":"api/utils/#lexos.utils.is_url","text":"Check if string is a URL. Source code in lexos\\utils.py def is_url ( s : str ) -> bool : \"\"\"Check if string is a URL.\"\"\" return bool ( re . match ( r \"(https?|ftp)://\" # protocol r \"(\\w+(\\-\\w+)*\\.)?\" # host (optional) r \"((\\w+(\\-\\w+)*)\\.(\\w+))\" # domain r \"(\\.\\w+)*\" # top-level domain (optional, can have > 1) r \"([\\w\\-\\._\\~/]*)*(?<!\\.)\" # path, params, anchors, etc. (optional) , s ))","title":"is_url()"},{"location":"api/utils/#lexos.utils.unzip_archive","text":"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python . Parameters: Name Type Description Default archive_path str The path to the archive file to be unzipped. required extract_dir str The path to folder where the archive will be extracted. required Source code in lexos\\utils.py def unzip_archive ( archive_path : str , extract_dir : str ): \"\"\"Extract a zip archive. For adding a progress indicator, see https://stackoverflow.com/questions/4006970/monitor-zip-file-extraction-python. Args: archive_path (str): The path to the archive file to be unzipped. extract_dir (str): The path to folder where the archive will be extracted. \"\"\" zf = zipfile . ZipFile ( archive_path , 'r' ) progress = Progress () with progress : for file in progress . track ( zf . infolist (), description = \"Processing...\" ): zf . extract ( file , path = extract_dir ) sleep ( 0.1 )","title":"unzip_archive()"},{"location":"api/utils/#lexos.utils.zip_folder","text":"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Parameters: Name Type Description Default source_dir Path The path to the source directory. required archive_file Path The path to the archive file to be created (including file extension). required Source code in lexos\\utils.py def zip_folder ( source_dir : Path , archive_file : Path ): \"\"\"Zip a folder recursively with no extra root folder in the archive. Works with a progress indicator. Args: source_dir (Path): The path to the source directory. archive_file (Path): The path to the archive file to be created (including file extension). \"\"\" progress = Progress () with zipfile . ZipFile ( archive_file , mode = \"w\" , compression = zipfile . ZIP_DEFLATED , compresslevel = 7 ) as zip : files = list ( source_dir . rglob ( \"*\" )) with progress : for file in progress . track ( files , description = \"Processing...\" ): relative_path = file . relative_to ( source_dir ) zip . write ( file , arcname = relative_path ) sleep ( 0.1 )","title":"zip_folder()"},{"location":"api/utils/#lexos.utils.get_encoding","text":"Use chardet to return the encoding type of a string. Parameters: Name Type Description Default input_string bytes A bytestring. required Returns: Type Description str The string's encoding type. Source code in lexos\\utils.py def get_encoding ( input_string : bytes ) -> str : \"\"\"Use chardet to return the encoding type of a string. Args: input_string (bytes): A bytestring. Returns: The string's encoding type. \"\"\" encoding_detect = chardet . detect ( input_string [ : constants . MIN_ENCODING_DETECT ]) encoding_type = encoding_detect [ 'encoding' ] return encoding_type","title":"get_encoding()"},{"location":"api/utils/#lexos.utils.normalize","text":"Normalise a string to LexosFile format. Parameters: Name Type Description Default raw_bytes str The input string. required Returns: Type Description str Normalised version of the input string. Source code in lexos\\utils.py def normalize ( raw_bytes : Union [ bytes , str ]) -> str : \"\"\"Normalise a string to LexosFile format. Args: raw_bytes (str): The input string. Returns: Normalised version of the input string. \"\"\" s = _decode_bytes ( raw_bytes ) return s","title":"normalize()"},{"location":"api/utils/#lexos.utils.normalize_files","text":"Normalise a list of files to LexosFile format and save the files. Parameters: Name Type Description Default filepaths list The list of paths to input files. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_files ( filepaths : List [ Union [ Path , str ]], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a list of files to LexosFile format and save the files. Args: filepaths (list): The list of paths to input files. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" for filepath in filepaths : filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc ))","title":"normalize_files()"},{"location":"api/utils/#lexos.utils.normalize_file","text":"Normalise a file to LexosFile format and save the file. Parameters: Name Type Description Default filepath Path, str The path to the input file. required destination_dir path, str The path to the directory where the files will be saved. '.' Source code in lexos\\utils.py def normalize_file ( filepath : Union [ Path , str ], destination_dir : Union [ Path , str ] = '.' ) -> str : \"\"\"Normalise a file to LexosFile format and save the file. Args: filepath (Path, str): The path to the input file. destination_dir (path, str): The path to the directory where the files will be saved. \"\"\" filepath = ensure_path ( filepath ) with open ( filepath , 'rb' ) as f : doc = f . read () with open ( destination_dir / filepath . name , 'wb' ) as f : f . write ( normalize ( doc ))","title":"normalize_file()"},{"location":"api/utils/#lexos.utils.normalize_strings","text":"Normalise a list of strings to LexosFile format. Parameters: Name Type Description Default strings list The list of input strings. required Returns: Type Description str A list of normalised versions of the input strings. Source code in lexos\\utils.py def normalize_strings ( strings : List [ Union [ bytes , str ]]) -> str : \"\"\"Normalise a list of strings to LexosFile format. Args: strings (list): The list of input strings. Returns: A list of normalised versions of the input strings. \"\"\" normalized_strings = [] for s in strings : normalized_strings . append ( normalize ( s )) return normalized_strings","title":"normalize_strings()"},{"location":"api/dtm/","text":"DTM \u00a4 The DTM module contains a basic DTM class. lexos.dtm.DTM \u00a4 Class for a document-term matrix. Source code in lexos\\dtm\\__init__.py class DTM (): \"\"\"Class for a document-term matrix.\"\"\" def __init__ ( self , docs = List [ object ], labels = List [ str ]): \"\"\"Initialise the DTM.\"\"\" self . docs = docs self . table = None self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build () def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_tokens = [[ token . text for token in doc ] for doc in self . docs ] self . matrix = self . vectorizer . fit_transform ( doc_tokens ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index () def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 ) tmp [ \"median\" ] = median . round ( rounding ) return tmp def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )] def least_frequent ( self , max_n_terms : int = 100 , start : int = - 1 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) return df [ start : max_n_terms ] def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ] def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer __init__ ( self , docs = typing . List [ object ], labels = typing . List [ str ]) special \u00a4 Initialise the DTM. Source code in lexos\\dtm\\__init__.py def __init__ ( self , docs = List [ object ], labels = List [ str ]): \"\"\"Initialise the DTM.\"\"\" self . docs = docs self . table = None self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build () build ( self ) \u00a4 Build a new DTM matrix based on the current vectorizer. Source code in lexos\\dtm\\__init__.py def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_tokens = [[ token . text for token in doc ] for doc in self . docs ] self . matrix = self . vectorizer . fit_transform ( doc_tokens ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None get_freq_table ( self , rounding = 3 , as_percent = False ) \u00a4 Get a table with the relative frequencies of terms in each document. Parameters: Name Type Description Default rounding int The number of digits to round floats. 3 as_percent bool Whether to return the frequencies as percentages. False Returns: Type Description pd.DataFrame A dataframe with the relative frequencies. Source code in lexos\\dtm\\__init__.py def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index () get_stats_table ( self , stats = 'sum' , rounding = 3 ) \u00a4 Get a table with the sum, mean, and/or median calculated for each row. Parameters: Name Type Description Default stats Union[List[str], str] One or more of \"sum\", \"mean\", and/or \"median\". 'sum' rounding int The number of digits to round floats. 3 Returns: Type Description pd.DataFrame A dataframe with the calculated statistics. Source code in lexos\\dtm\\__init__.py def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 ) tmp [ \"median\" ] = median . round ( rounding ) return tmp get_table ( self , transpose = False ) \u00a4 Get a Textacy document-term matrix as a pandas dataframe. Parameters: Name Type Description Default transpose bool If True, terms are columns and docs are rows. False Returns: Type Description DataFrame pd.Dataframe Source code in lexos\\dtm\\__init__.py def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df get_term_counts ( self , sort_by = [ 'terms' , 'sum' ], ascending = True , alg = functools . partial ( < function natsort_key at 0x000001FFDD447160 > , key = None , string_func =< function parse_string_factory .< locals >. func at 0x000001FFDFB57EE0 > , bytes_func =< function parse_bytes_factory .< locals >.< lambda > at 0x000001FFDFB57DC0 > , num_func =< function parse_number_or_none_factory .< locals >. func at 0x000001FFDFB57D30 > )) \u00a4 Get a list of term counts with optional sorting. Parameters: Name Type Description Default sort_by Union[list, List[str]] The column(s) to sort by in order of preference. ['terms', 'sum'] ascending Union[bool, List[bool]] Whether to sort values in ascending or descending order. True Returns: Type Description List(tuple) A list of tuples containing terms and counts. Source code in lexos\\dtm\\__init__.py def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )] get_terms ( self ) \u00a4 Get an alphabetical list of terms. Source code in lexos\\dtm\\__init__.py def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms least_frequent ( self , max_n_terms = 100 , start =- 1 ) \u00a4 Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. -1 Returns: Type Description pd.DataFrame The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py def least_frequent ( self , max_n_terms : int = 100 , start : int = - 1 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) return df [ start : max_n_terms ] most_frequent ( self , max_n_terms = 100 , start = 0 ) \u00a4 Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. 0 Returns: Type Description pd.DataFrame The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ] set_vectorizer ( self , tf_type = 'linear' , idf_type = None , dl_type = None , norm = None , min_df = 1 , max_df = 1.0 , max_n_terms = None , vocabulary_terms = None , new = False ) \u00a4 Set the vectorizer. By default, returns a vectorizer that gets raw counts. Source code in lexos\\dtm\\__init__.py def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer There is also a separate get_doc_term_counts function. lexos . dtm . get_doc_term_counts ( docs , limit = None , start = 0 , end = None , filters = None , regex = False , normalize = False , normalize_with_filters = False , as_df = False ) \u00a4 Get a list of word counts for each token in the doc. Parameters: Name Type Description Default docs List[object] A list of spaCy docs. required limit int The maximum number of tokens to count. None start int The index of the first token to count. 0 end int The index of the last token to count after limit is applied. None filters List[Union[Dict[str, str], str]] A list of Doc attributes to ignore. None regex bool Whether to match the dictionary value using regex. False normalize bool Whether to return raw counts or relative frequencies. False normalize_with_filters bool Whether to normalize based on the number of tokens after filters are applied. False as_df bool Whether to return a pandas dataframe. False Returns: Type Description Union[List, pd.DataFrame] A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. Source code in lexos\\dtm\\__init__.py def get_doc_term_counts ( docs : List [ object ], limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: docs (List[object]): A list of spaCy docs. limit (int): The maximum number of tokens to count. start (int): The index of the first token to count. end (int): The index of the last token to count after limit is applied. filters (List[Union[Dict[str, str], str]]): A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df (bool): Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for doc in docs for token in doc if _bool_filter ( token , bool_filters ) and _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = sum ([ len ( doc ) for doc in docs ]) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return pd . DataFrame ( term_counts , columns = columns ) else : return term_counts It depends on two helper functions. lexos . dtm . _bool_filter ( token , filters ) private \u00a4 Filter a token based on a list of boolean filters. Parameters: Name Type Description Default token object A spaCy token. required filters str A list of boolean filters (the names of spaCy token attributes). required Returns: Type Description bool Whether the token passes the filters. Source code in lexos\\dtm\\__init__.py def _bool_filter ( token : object , filters : List [ str ]) -> bool : \"\"\"Filter a token based on a list of boolean filters. Args: token (object): A spaCy token. filters (str): A list of boolean filters (the names of spaCy token attributes). Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != []: for filter in filters : if getattr ( token , filter ): return False else : return True else : return True lexos . dtm . _dict_filter ( token , filters , regex = False ) private \u00a4 Filter a token based on a list of dictionary filters. Parameters: Name Type Description Default token object A spaCy token. required filters List[Dict[str, str]] A list of filter dictionaries with keys as spaCy token attributes. required regex bool Whether to match the dictionary value using regex. False Returns: Type Description bool Whether the token passes the filters. Source code in lexos\\dtm\\__init__.py def _dict_filter ( token , filters : List [ Dict [ str , str ]], regex : bool = False ) -> bool : \"\"\"Filter a token based on a list of dictionary filters. Args: token (object): A spaCy token. filters (List[Dict[str, str]]): A list of filter dictionaries with keys as spaCy token attributes. regex (bool): Whether to match the dictionary value using regex. Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != {}: for filter , value in filters . items (): if regex and re . search ( re . compile ( value ), getattr ( token , filter )) is not None : return False elif getattr ( token , filter ) == value : return False else : return True else : return True","title":"Index"},{"location":"api/dtm/#dtm","text":"The DTM module contains a basic DTM class.","title":"DTM"},{"location":"api/dtm/#lexos.dtm.DTM","text":"Class for a document-term matrix. Source code in lexos\\dtm\\__init__.py class DTM (): \"\"\"Class for a document-term matrix.\"\"\" def __init__ ( self , docs = List [ object ], labels = List [ str ]): \"\"\"Initialise the DTM.\"\"\" self . docs = docs self . table = None self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build () def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_tokens = [[ token . text for token in doc ] for doc in self . docs ] self . matrix = self . vectorizer . fit_transform ( doc_tokens ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index () def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 ) tmp [ \"median\" ] = median . round ( rounding ) return tmp def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )] def least_frequent ( self , max_n_terms : int = 100 , start : int = - 1 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) return df [ start : max_n_terms ] def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ] def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer","title":"DTM"},{"location":"api/dtm/#lexos.dtm.DTM.__init__","text":"Initialise the DTM. Source code in lexos\\dtm\\__init__.py def __init__ ( self , docs = List [ object ], labels = List [ str ]): \"\"\"Initialise the DTM.\"\"\" self . docs = docs self . table = None self . labels = labels self . vectorizer_settings = {} self . vectorizer = self . set_vectorizer ( new = True ) self . build ()","title":"__init__()"},{"location":"api/dtm/#lexos.dtm.DTM.build","text":"Build a new DTM matrix based on the current vectorizer. Source code in lexos\\dtm\\__init__.py def build ( self ): \"\"\"Build a new DTM matrix based on the current vectorizer.\"\"\" doc_tokens = [[ token . text for token in doc ] for doc in self . docs ] self . matrix = self . vectorizer . fit_transform ( doc_tokens ) # Require explicit calling of get_table after each build to ensure table is up to date. # Ensures that the two processes can be kept separate if desired. self . table = None","title":"build()"},{"location":"api/dtm/#lexos.dtm.DTM.get_freq_table","text":"Get a table with the relative frequencies of terms in each document. Parameters: Name Type Description Default rounding int The number of digits to round floats. 3 as_percent bool Whether to return the frequencies as percentages. False Returns: Type Description pd.DataFrame A dataframe with the relative frequencies. Source code in lexos\\dtm\\__init__.py def get_freq_table ( self , rounding : int = 3 , as_percent : bool = False ) -> pd . DataFrame : \"\"\"Get a table with the relative frequencies of terms in each document. Args: rounding (int): The number of digits to round floats. as_percent (bool): Whether to return the frequencies as percentages. Returns: pd.DataFrame: A dataframe with the relative frequencies. \"\"\" df = self . get_table () . copy () df . set_index ( \"terms\" , inplace = True ) if as_percent : return df . apply ( lambda row : (( row / row . sum ()) * 100 ) . round ( rounding ), axis = 1 ) . reset_index () else : return df . apply ( lambda row : row / row . sum () . round ( rounding ), axis = 1 ) . reset_index ()","title":"get_freq_table()"},{"location":"api/dtm/#lexos.dtm.DTM.get_stats_table","text":"Get a table with the sum, mean, and/or median calculated for each row. Parameters: Name Type Description Default stats Union[List[str], str] One or more of \"sum\", \"mean\", and/or \"median\". 'sum' rounding int The number of digits to round floats. 3 Returns: Type Description pd.DataFrame A dataframe with the calculated statistics. Source code in lexos\\dtm\\__init__.py def get_stats_table ( self , stats : Union [ List [ str ], str ] = \"sum\" , rounding : int = 3 ) -> pd . DataFrame : \"\"\"Get a table with the sum, mean, and/or median calculated for each row. Args: stats (Union[List[str], str]): One or more of \"sum\", \"mean\", and/or \"median\". rounding (int): The number of digits to round floats. Returns: pd.DataFrame: A dataframe with the calculated statistics. \"\"\" df = self . get_table () tmp = df . copy () if \"sum\" in stats : tmp [ \"sum\" ] = df . sum ( axis = 1 ) if \"mean\" in stats : tmp [ \"mean\" ] = df . mean ( axis = 1 ) . round ( rounding ) if \"median\" in stats : median = df . median ( axis = 1 ) tmp [ \"median\" ] = median . round ( rounding ) return tmp","title":"get_stats_table()"},{"location":"api/dtm/#lexos.dtm.DTM.get_table","text":"Get a Textacy document-term matrix as a pandas dataframe. Parameters: Name Type Description Default transpose bool If True, terms are columns and docs are rows. False Returns: Type Description DataFrame pd.Dataframe Source code in lexos\\dtm\\__init__.py def get_table ( self , transpose : bool = False ) -> pd . DataFrame : \"\"\"Get a Textacy document-term matrix as a pandas dataframe. Args: transpose (bool): If True, terms are columns and docs are rows. Returns: pd.Dataframe \"\"\" if self . table is not None : return self . table else : rows = [] for term in self . vectorizer . terms_list : row = [ term ] terms = self . vectorizer . vocabulary_terms [ term ] freq = self . matrix [ 0 :, terms ] . toarray () [ row . append ( item [ 0 ]) for item in freq ] rows . append ( row ) df = pd . DataFrame ( rows , columns = [ \"terms\" ] + self . labels ) if transpose : df . rename ({ \"terms\" : \"docs\" }, axis = 1 , inplace = True ) df = df . T self . table = df return df","title":"get_table()"},{"location":"api/dtm/#lexos.dtm.DTM.get_term_counts","text":"Get a list of term counts with optional sorting. Parameters: Name Type Description Default sort_by Union[list, List[str]] The column(s) to sort by in order of preference. ['terms', 'sum'] ascending Union[bool, List[bool]] Whether to sort values in ascending or descending order. True Returns: Type Description List(tuple) A list of tuples containing terms and counts. Source code in lexos\\dtm\\__init__.py def get_term_counts ( self , sort_by : Union [ list , List [ str ]] = [ \"terms\" , \"sum\" ], ascending : Union [ bool , List [ bool ]] = True , alg = SORTING_ALGORITHM ) -> List [ tuple ]: \"\"\"Get a list of term counts with optional sorting. Args: sort_by Union[list, List[str]]): The column(s) to sort by in order of preference. ascending (Union[bool, List[bool]]): Whether to sort values in ascending or descending order. Returns: List(tuple): A list of tuples containing terms and counts. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = sort_by , ascending = ascending , key = alg ) terms = df [ \"terms\" ] . values . tolist () sums = df [ \"sum\" ] . values . tolist () return [( terms [ i ], sums [ i ]) for i , _ in enumerate ( terms )]","title":"get_term_counts()"},{"location":"api/dtm/#lexos.dtm.DTM.get_terms","text":"Get an alphabetical list of terms. Source code in lexos\\dtm\\__init__.py def get_terms ( self ): \"\"\"Get an alphabetical list of terms.\"\"\" return self . vectorizer . vocabulary_terms","title":"get_terms()"},{"location":"api/dtm/#lexos.dtm.DTM.least_frequent","text":"Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. -1 Returns: Type Description pd.DataFrame The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py def least_frequent ( self , max_n_terms : int = 100 , start : int = - 1 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = True ) return df [ start : max_n_terms ]","title":"least_frequent()"},{"location":"api/dtm/#lexos.dtm.DTM.most_frequent","text":"Get the most frequent terms in the DTM. Parameters: Name Type Description Default max_n_terms int The number of terms to return. 100 start int int = 0: The start index in the DTM table. 0 Returns: Type Description pd.DataFrame The reduced DTM table. the vectorizer because the table will be cut twice. Source code in lexos\\dtm\\__init__.py def most_frequent ( self , max_n_terms : int = 100 , start : int = 0 ) -> pd . DataFrame : \"\"\"Get the most frequent terms in the DTM. Args: max_n_terms (int): The number of terms to return. start: int = 0: The start index in the DTM table. Returns: pd.DataFrame: The reduced DTM table. Note: This function should not be used if `min_df` or `max_df` is set in the vectorizer because the table will be cut twice. \"\"\" df = self . get_stats_table ( \"sum\" ) . sort_values ( by = \"sum\" , ascending = False ) return df [ start : max_n_terms ]","title":"most_frequent()"},{"location":"api/dtm/#lexos.dtm.DTM.set_vectorizer","text":"Set the vectorizer. By default, returns a vectorizer that gets raw counts. Source code in lexos\\dtm\\__init__.py def set_vectorizer ( self , tf_type : str = \"linear\" , idf_type : str = None , dl_type : str = None , norm : Union [ list , str ] = None , min_df : Union [ float , int ] = 1 , max_df : Union [ float , int ] = 1.0 , max_n_terms : int = None , vocabulary_terms : Union [ list , str ] = None , new : bool = False ): \"\"\"Set the vectorizer. By default, returns a vectorizer that gets raw counts. \"\"\" from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = tf_type , idf_type = idf_type , dl_type = dl_type , norm = norm , min_df = min_df , max_df = max_df , max_n_terms = max_n_terms , vocabulary_terms = vocabulary_terms ) self . vectorizer_settings = { \"tf_type\" : tf_type , \"idf_type\" : idf_type , \"norm\" : norm , \"min_df\" : min_df , \"max_df\" : max_df , \"max_n_terms\" : max_n_terms , } if new : return vectorizer else : self . vectorizer = vectorizer There is also a separate get_doc_term_counts function.","title":"set_vectorizer()"},{"location":"api/dtm/#lexos.dtm.get_doc_term_counts","text":"Get a list of word counts for each token in the doc. Parameters: Name Type Description Default docs List[object] A list of spaCy docs. required limit int The maximum number of tokens to count. None start int The index of the first token to count. 0 end int The index of the last token to count after limit is applied. None filters List[Union[Dict[str, str], str]] A list of Doc attributes to ignore. None regex bool Whether to match the dictionary value using regex. False normalize bool Whether to return raw counts or relative frequencies. False normalize_with_filters bool Whether to normalize based on the number of tokens after filters are applied. False as_df bool Whether to return a pandas dataframe. False Returns: Type Description Union[List, pd.DataFrame] A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. Source code in lexos\\dtm\\__init__.py def get_doc_term_counts ( docs : List [ object ], limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: docs (List[object]): A list of spaCy docs. limit (int): The maximum number of tokens to count. start (int): The index of the first token to count. end (int): The index of the last token to count after limit is applied. filters (List[Union[Dict[str, str], str]]): A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df (bool): Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for doc in docs for token in doc if _bool_filter ( token , bool_filters ) and _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = sum ([ len ( doc ) for doc in docs ]) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return pd . DataFrame ( term_counts , columns = columns ) else : return term_counts It depends on two helper functions.","title":"get_doc_term_counts()"},{"location":"api/dtm/#lexos.dtm._bool_filter","text":"Filter a token based on a list of boolean filters. Parameters: Name Type Description Default token object A spaCy token. required filters str A list of boolean filters (the names of spaCy token attributes). required Returns: Type Description bool Whether the token passes the filters. Source code in lexos\\dtm\\__init__.py def _bool_filter ( token : object , filters : List [ str ]) -> bool : \"\"\"Filter a token based on a list of boolean filters. Args: token (object): A spaCy token. filters (str): A list of boolean filters (the names of spaCy token attributes). Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != []: for filter in filters : if getattr ( token , filter ): return False else : return True else : return True","title":"_bool_filter()"},{"location":"api/dtm/#lexos.dtm._dict_filter","text":"Filter a token based on a list of dictionary filters. Parameters: Name Type Description Default token object A spaCy token. required filters List[Dict[str, str]] A list of filter dictionaries with keys as spaCy token attributes. required regex bool Whether to match the dictionary value using regex. False Returns: Type Description bool Whether the token passes the filters. Source code in lexos\\dtm\\__init__.py def _dict_filter ( token , filters : List [ Dict [ str , str ]], regex : bool = False ) -> bool : \"\"\"Filter a token based on a list of dictionary filters. Args: token (object): A spaCy token. filters (List[Dict[str, str]]): A list of filter dictionaries with keys as spaCy token attributes. regex (bool): Whether to match the dictionary value using regex. Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != {}: for filter , value in filters . items (): if regex and re . search ( re . compile ( value ), getattr ( token , filter )) is not None : return False elif getattr ( token , filter ) == value : return False else : return True else : return True","title":"_dict_filter()"},{"location":"api/dtm/bubbleviz/","text":"Bubble Charts \u00a4 The lexos.dtm.bubbleviz module contains classes and functions that facilitate the production of bubble chart images directly from the document-term matrix. Images are produced with matplotlib and the Python Wordcloud library. lexos.dtm.bubbleviz.BubbleChart \u00a4 Bubble chart. Source code in lexos\\dtm\\bubbleviz.py class BubbleChart : \"\"\"Bubble chart.\"\"\" def __init__ ( self , area : list , bubble_spacing : Union [ float , int ] = 0 , limit : int = 100 ): \"\"\"Setup for bubble collapse. Args: area (list): List of counts or frequencies bubble_spacing: (Union[float, int]): The spacing between bubbles after collapsing. limit (int): The maximum number of bubbles to display. Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" # Reduce the area to the limited number of terms area = np . asarray ( area [ 0 : limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass () def center_of_mass ( self ): \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ] ) def center_distance ( self , bubble , bubbles ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ]) def outline_distance ( self , bubble , bubbles ): \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - \\ bubbles [:, 2 ] - self . bubble_spacing def check_collisions ( self , bubble , bubbles ): \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ]) def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ): \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ] def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = ( self . bubbles [ i , : 2 ] + orth * self . step_dist ) new_point2 = ( self . bubbles [ i , : 2 ] - orth * self . step_dist ) dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2 def plot ( self , ax : object , labels : List [ str ], colors : List [ str ]): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. \"\"\" color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ]) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = 'center' , verticalalignment = 'center' ) __init__ ( self , area , bubble_spacing = 0 , limit = 100 ) special \u00a4 Setup for bubble collapse. Parameters: Name Type Description Default area list List of counts or frequencies required bubble_spacing Union[float, int] (Union[float, int]): The spacing between bubbles after collapsing. 0 limit int The maximum number of bubbles to display. 100 Notes If \"area\" is sorted, the results might look weird. If \"limit\" is raised too high, it will take a long time to generate the plot Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html . Source code in lexos\\dtm\\bubbleviz.py def __init__ ( self , area : list , bubble_spacing : Union [ float , int ] = 0 , limit : int = 100 ): \"\"\"Setup for bubble collapse. Args: area (list): List of counts or frequencies bubble_spacing: (Union[float, int]): The spacing between bubbles after collapsing. limit (int): The maximum number of bubbles to display. Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" # Reduce the area to the limited number of terms area = np . asarray ( area [ 0 : limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass () center_distance ( self , bubble , bubbles ) \u00a4 Centre distance. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description np.ndarray The centre distance. Source code in lexos\\dtm\\bubbleviz.py def center_distance ( self , bubble , bubbles ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ]) center_of_mass ( self ) \u00a4 Centre of mass. Returns: Type Description int The centre of mass. Source code in lexos\\dtm\\bubbleviz.py def center_of_mass ( self ): \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ] ) check_collisions ( self , bubble , bubbles ) \u00a4 Check collisions. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description int The length of the distance between bubbles. Source code in lexos\\dtm\\bubbleviz.py def check_collisions ( self , bubble , bubbles ): \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ]) collapse ( self , n_iterations = 50 ) \u00a4 Move bubbles to the center of mass. Parameters: Name Type Description Default n_iterations int Number of moves to perform. 50 Source code in lexos\\dtm\\bubbleviz.py def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = ( self . bubbles [ i , : 2 ] + orth * self . step_dist ) new_point2 = ( self . bubbles [ i , : 2 ] - orth * self . step_dist ) dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2 collides_with ( self , bubble , bubbles ) \u00a4 Collide. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description int The minimum index. Source code in lexos\\dtm\\bubbleviz.py def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ): \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ] outline_distance ( self , bubble , bubbles ) \u00a4 Outline distance. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description int The outline distance. Source code in lexos\\dtm\\bubbleviz.py def outline_distance ( self , bubble , bubbles ): \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - \\ bubbles [:, 2 ] - self . bubble_spacing plot ( self , ax , labels , colors ) \u00a4 Draw the bubble plot. Parameters: Name Type Description Default ax matplotlib.axes.Axes The matplotlib axes. required labels List[str] The labels of the bubbles. required colors List[str] The colors of the bubbles. required Source code in lexos\\dtm\\bubbleviz.py def plot ( self , ax : object , labels : List [ str ], colors : List [ str ]): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. \"\"\" color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ]) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = 'center' , verticalalignment = 'center' ) lexos . dtm . bubbleviz . make_bubble_chart ( terms , area , limit = 100 , title = None , bubble_spacing = 0.1 , colors = [ '#5A69AF' , '#579E65' , '#F9C784' , '#FC944A' , '#F24C00' , '#00B825' ], figsize = ( 15 , 15 ), show = True , filename = None ) \u00a4 Make bubble chart. Parameters: Name Type Description Default terms List[str] The terms to plot. required area List[Union[float, int]] The area of the bubbles. required limit int The maximum number of bubbles to plot. 100 title str The title of the plot. None bubble_spacing Union[float, int] The spacing between bubbles. 0.1 colors List[str] The colors of the bubbles. ['#5A69AF', '#579E65', '#F9C784', '#FC944A', '#F24C00', '#00B825'] figsize tuple The size of the figure. (15, 15) show bool Whether to show the plot. True filename str The filename to save the plot to. None Source code in lexos\\dtm\\bubbleviz.py def make_bubble_chart ( terms : List [ str ], area : List [ Union [ float , int ]], limit : int = 100 , title : str = None , bubble_spacing : Union [ float , int ] = 0.1 , colors : List [ str ] = [ \"#5A69AF\" , \"#579E65\" , \"#F9C784\" , \"#FC944A\" , \"#F24C00\" , \"#00B825\" ], figsize : tuple = ( 15 , 15 ), show : bool = True , filename : str = None ): \"\"\"Make bubble chart. Args: terms (List[str]): The terms to plot. area (List[Union[float, int]]): The area of the bubbles. limit (int): The maximum number of bubbles to plot. title (str): The title of the plot. bubble_spacing (Union[float, int]): The spacing between bubbles. colors (List[str]): The colors of the bubbles. figsize (tuple): The size of the figure. show (bool): Whether to show the plot. filename (str): The filename to save the plot to. \"\"\" bubble_chart = BubbleChart ( area = area , bubble_spacing = bubble_spacing , limit = limit ) bubble_chart . collapse () fig , ax = plt . subplots ( subplot_kw = dict ( aspect = \"equal\" ), figsize = figsize ) bubble_chart . plot ( ax , terms , colors = colors ) ax . axis ( \"off\" ) ax . relim () ax . autoscale_view () if title : ax . set_title ( title ) if show : plt . show () if filename : plt . to_file ( filename )","title":"Bubble Charts"},{"location":"api/dtm/bubbleviz/#bubble-charts","text":"The lexos.dtm.bubbleviz module contains classes and functions that facilitate the production of bubble chart images directly from the document-term matrix. Images are produced with matplotlib and the Python Wordcloud library.","title":"Bubble Charts"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart","text":"Bubble chart. Source code in lexos\\dtm\\bubbleviz.py class BubbleChart : \"\"\"Bubble chart.\"\"\" def __init__ ( self , area : list , bubble_spacing : Union [ float , int ] = 0 , limit : int = 100 ): \"\"\"Setup for bubble collapse. Args: area (list): List of counts or frequencies bubble_spacing: (Union[float, int]): The spacing between bubbles after collapsing. limit (int): The maximum number of bubbles to display. Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" # Reduce the area to the limited number of terms area = np . asarray ( area [ 0 : limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass () def center_of_mass ( self ): \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ] ) def center_distance ( self , bubble , bubbles ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ]) def outline_distance ( self , bubble , bubbles ): \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - \\ bubbles [:, 2 ] - self . bubble_spacing def check_collisions ( self , bubble , bubbles ): \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ]) def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ): \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ] def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = ( self . bubbles [ i , : 2 ] + orth * self . step_dist ) new_point2 = ( self . bubbles [ i , : 2 ] - orth * self . step_dist ) dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2 def plot ( self , ax : object , labels : List [ str ], colors : List [ str ]): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. \"\"\" color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ]) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = 'center' , verticalalignment = 'center' )","title":"BubbleChart"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.__init__","text":"Setup for bubble collapse. Parameters: Name Type Description Default area list List of counts or frequencies required bubble_spacing Union[float, int] (Union[float, int]): The spacing between bubbles after collapsing. 0 limit int The maximum number of bubbles to display. 100 Notes If \"area\" is sorted, the results might look weird. If \"limit\" is raised too high, it will take a long time to generate the plot Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html . Source code in lexos\\dtm\\bubbleviz.py def __init__ ( self , area : list , bubble_spacing : Union [ float , int ] = 0 , limit : int = 100 ): \"\"\"Setup for bubble collapse. Args: area (list): List of counts or frequencies bubble_spacing: (Union[float, int]): The spacing between bubbles after collapsing. limit (int): The maximum number of bubbles to display. Notes: - If \"area\" is sorted, the results might look weird. - If \"limit\" is raised too high, it will take a long time to generate the plot - Based on https://matplotlib.org/stable/gallery/misc/packed_bubbles.html. \"\"\" # Reduce the area to the limited number of terms area = np . asarray ( area [ 0 : limit ]) r = np . sqrt ( area / np . pi ) self . bubble_spacing = bubble_spacing self . bubbles = np . ones (( len ( area ), 4 )) self . bubbles [:, 2 ] = r self . bubbles [:, 3 ] = area self . maxstep = 2 * self . bubbles [:, 2 ] . max () + self . bubble_spacing self . step_dist = self . maxstep / 2 # Calculate initial grid layout for bubbles length = np . ceil ( np . sqrt ( len ( self . bubbles ))) grid = np . arange ( length ) * self . maxstep gx , gy = np . meshgrid ( grid , grid ) self . bubbles [:, 0 ] = gx . flatten ()[: len ( self . bubbles )] self . bubbles [:, 1 ] = gy . flatten ()[: len ( self . bubbles )] self . com = self . center_of_mass ()","title":"__init__()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.center_distance","text":"Centre distance. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description np.ndarray The centre distance. Source code in lexos\\dtm\\bubbleviz.py def center_distance ( self , bubble , bubbles ) -> np . ndarray : \"\"\"Centre distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: np.ndarray: The centre distance. \"\"\" return np . hypot ( bubble [ 0 ] - bubbles [:, 0 ], bubble [ 1 ] - bubbles [:, 1 ])","title":"center_distance()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.center_of_mass","text":"Centre of mass. Returns: Type Description int The centre of mass. Source code in lexos\\dtm\\bubbleviz.py def center_of_mass ( self ): \"\"\"Centre of mass. Returns: int: The centre of mass. \"\"\" return np . average ( self . bubbles [:, : 2 ], axis = 0 , weights = self . bubbles [:, 3 ] )","title":"center_of_mass()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.check_collisions","text":"Check collisions. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description int The length of the distance between bubbles. Source code in lexos\\dtm\\bubbleviz.py def check_collisions ( self , bubble , bubbles ): \"\"\"Check collisions. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The length of the distance between bubbles. \"\"\" distance = self . outline_distance ( bubble , bubbles ) return len ( distance [ distance < 0 ])","title":"check_collisions()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.collapse","text":"Move bubbles to the center of mass. Parameters: Name Type Description Default n_iterations int Number of moves to perform. 50 Source code in lexos\\dtm\\bubbleviz.py def collapse ( self , n_iterations : int = 50 ): \"\"\"Move bubbles to the center of mass. Args: n_iterations (int): Number of moves to perform. \"\"\" for _i in range ( n_iterations ): moves = 0 for i in range ( len ( self . bubbles )): rest_bub = np . delete ( self . bubbles , i , 0 ) # Try to move directly towards the center of mass # Direction vector from bubble to the center of mass dir_vec = self . com - self . bubbles [ i , : 2 ] # Shorten direction vector to have length of 1 dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate new bubble position new_point = self . bubbles [ i , : 2 ] + dir_vec * self . step_dist new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) # Check whether new bubble collides with other bubbles if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () moves += 1 else : # Try to move around a bubble that you collide with # Find colliding bubble for colliding in self . collides_with ( new_bubble , rest_bub ): # Calculate direction vector dir_vec = rest_bub [ colliding , : 2 ] - self . bubbles [ i , : 2 ] dir_vec = dir_vec / np . sqrt ( dir_vec . dot ( dir_vec )) # Calculate orthogonal vector orth = np . array ([ dir_vec [ 1 ], - dir_vec [ 0 ]]) # test which direction to go new_point1 = ( self . bubbles [ i , : 2 ] + orth * self . step_dist ) new_point2 = ( self . bubbles [ i , : 2 ] - orth * self . step_dist ) dist1 = self . center_distance ( self . com , np . array ([ new_point1 ])) dist2 = self . center_distance ( self . com , np . array ([ new_point2 ])) new_point = new_point1 if dist1 < dist2 else new_point2 new_bubble = np . append ( new_point , self . bubbles [ i , 2 : 4 ]) if not self . check_collisions ( new_bubble , rest_bub ): self . bubbles [ i , :] = new_bubble self . com = self . center_of_mass () if moves / len ( self . bubbles ) < 0.1 : self . step_dist = self . step_dist / 2","title":"collapse()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.collides_with","text":"Collide. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description int The minimum index. Source code in lexos\\dtm\\bubbleviz.py def collides_with ( self , bubble : np . ndarray , bubbles : np . ndarray ): \"\"\"Collide. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The minimum index. \"\"\" distance = self . outline_distance ( bubble , bubbles ) idx_min = np . argmin ( distance ) return idx_min if type ( idx_min ) == np . ndarray else [ idx_min ]","title":"collides_with()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.outline_distance","text":"Outline distance. Parameters: Name Type Description Default bubble np.ndarray Bubble array. required bubbles np.ndarray Bubble array. required Returns: Type Description int The outline distance. Source code in lexos\\dtm\\bubbleviz.py def outline_distance ( self , bubble , bubbles ): \"\"\"Outline distance. Args: bubble (np.ndarray): Bubble array. bubbles (np.ndarray): Bubble array. Returns: int: The outline distance. \"\"\" center_distance = self . center_distance ( bubble , bubbles ) return center_distance - bubble [ 2 ] - \\ bubbles [:, 2 ] - self . bubble_spacing","title":"outline_distance()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.BubbleChart.plot","text":"Draw the bubble plot. Parameters: Name Type Description Default ax matplotlib.axes.Axes The matplotlib axes. required labels List[str] The labels of the bubbles. required colors List[str] The colors of the bubbles. required Source code in lexos\\dtm\\bubbleviz.py def plot ( self , ax : object , labels : List [ str ], colors : List [ str ]): \"\"\"Draw the bubble plot. Args: ax (matplotlib.axes.Axes): The matplotlib axes. labels (List[str]): The labels of the bubbles. colors (List[str]): The colors of the bubbles. \"\"\" color_num = 0 for i in range ( len ( self . bubbles )): if color_num == len ( colors ) - 1 : color_num = 0 else : color_num += 1 circ = plt . Circle ( self . bubbles [ i , : 2 ], self . bubbles [ i , 2 ], color = colors [ color_num ]) ax . add_patch ( circ ) ax . text ( * self . bubbles [ i , : 2 ], labels [ i ], horizontalalignment = 'center' , verticalalignment = 'center' )","title":"plot()"},{"location":"api/dtm/bubbleviz/#lexos.dtm.bubbleviz.make_bubble_chart","text":"Make bubble chart. Parameters: Name Type Description Default terms List[str] The terms to plot. required area List[Union[float, int]] The area of the bubbles. required limit int The maximum number of bubbles to plot. 100 title str The title of the plot. None bubble_spacing Union[float, int] The spacing between bubbles. 0.1 colors List[str] The colors of the bubbles. ['#5A69AF', '#579E65', '#F9C784', '#FC944A', '#F24C00', '#00B825'] figsize tuple The size of the figure. (15, 15) show bool Whether to show the plot. True filename str The filename to save the plot to. None Source code in lexos\\dtm\\bubbleviz.py def make_bubble_chart ( terms : List [ str ], area : List [ Union [ float , int ]], limit : int = 100 , title : str = None , bubble_spacing : Union [ float , int ] = 0.1 , colors : List [ str ] = [ \"#5A69AF\" , \"#579E65\" , \"#F9C784\" , \"#FC944A\" , \"#F24C00\" , \"#00B825\" ], figsize : tuple = ( 15 , 15 ), show : bool = True , filename : str = None ): \"\"\"Make bubble chart. Args: terms (List[str]): The terms to plot. area (List[Union[float, int]]): The area of the bubbles. limit (int): The maximum number of bubbles to plot. title (str): The title of the plot. bubble_spacing (Union[float, int]): The spacing between bubbles. colors (List[str]): The colors of the bubbles. figsize (tuple): The size of the figure. show (bool): Whether to show the plot. filename (str): The filename to save the plot to. \"\"\" bubble_chart = BubbleChart ( area = area , bubble_spacing = bubble_spacing , limit = limit ) bubble_chart . collapse () fig , ax = plt . subplots ( subplot_kw = dict ( aspect = \"equal\" ), figsize = figsize ) bubble_chart . plot ( ax , terms , colors = colors ) ax . axis ( \"off\" ) ax . relim () ax . autoscale_view () if title : ax . set_title ( title ) if show : plt . show () if filename : plt . to_file ( filename )","title":"make_bubble_chart()"},{"location":"api/dtm/wordcloud/","text":"Word Clouds \u00a4 The lexos.dtm.wordcloud module contains classes and functions that facilitate the production of word cloud images directly from the document-term matrix. Images are produced with matplotlib and the Python Wordcloud library. lexos . dtm . wordcloud . get_rows ( lst , n ) \u00a4 Yield successive n-sized rows from a list of documents. Parameters: Name Type Description Default lst list A list of documents. required n int The number of columns in the row. required Yields: Type Description list A generator with the documents separated into rows. Source code in lexos\\dtm\\wordcloud.py def get_rows ( lst , n ): \"\"\"Yield successive n-sized rows from a list of documents. Args: lst (list): A list of documents. n (int): The number of columns in the row. Yields: list: A generator with the documents separated into rows. \"\"\" for i in range ( 0 , len ( lst ), n ): yield lst [ i : i + n ] lexos . dtm . wordcloud . make_wordcloud ( data , opts = None , show = True , figure_opts = None , round = None ) \u00a4 Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Parameters: Name Type Description Default data Union[dict, list, object, str, tuple] The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None Returns: Type Description object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\dtm\\wordcloud.py def make_wordcloud ( data : Union [ dict , list , object , str , tuple ], opts : dict = None , show : bool = True , figure_opts : dict = None , round : int = None ): \"\"\"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Args: data (Union[dict, list, object, str, tuple]): The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : if isinstance ( data , list ): data = { x [ 0 ]: x [ 1 ] for x in data } elif isinstance ( data , pd . DataFrame ): term_counts = data . to_dict ( orient = \"records\" ) try : data = { x [ \"term\" ]: x [ \"count\" ] for x in term_counts } except KeyError : data = { x [ \"term\" ]: x [ \"frequency\" ] for x in term_counts } wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) if show : if figure_opts : plt . figure ( ** figure_opts ) plt . axis ( \"off\" ) plt . imshow ( wordcloud ) plt . show () else : return wordcloud lexos . dtm . wordcloud . make_multiclouds ( docs , opts = None , ncols = 3 , title = None , labels = None , show = True , figure_opts = None , round = None ) \u00a4 Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by get_dtm_table() . Parameters: Name Type Description Default docs List[Union[dict, object, str, tuple]] The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} None ncols int The number of columns in the grid. 3 title str The title of the grid. None labels List[str] The document labels for each subplot. None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None Returns: Type Description object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\dtm\\wordcloud.py def make_multiclouds ( docs : List [ Union [ dict , object , str , tuple ]], opts : dict = None , ncols : int = 3 , title : str = None , labels : List [ str ] = None , show : bool = True , figure_opts : dict = None , round : int = None ): \"\"\"Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by `get_dtm_table()`. Args: docs (List[Union[dict, object, str, tuple]]): The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} ncols (int): The number of columns in the grid. title (str): The title of the grid. labels (List[str]): The document labels for each subplot. show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" # Process the docs data into a list if isinstance ( docs , pd . core . frame . DataFrame ): # Assumes a df with columns: Terms, Doc_Label, DocLabel,... # Transpose the df docs = docs . T # Grab the first row for the header new_header = docs . iloc [ 0 ] # Drop the first row docs = docs [ 1 :] # Set the header row as the df header docs . columns = new_header # Return a dict docs = docs . to_dict ( orient = \"records\" ) # Ensure that anything that is not a list of strings is converted # to the appropriate format. elif isinstance ( docs , list ): if all ( isinstance ( s , str ) for s in docs ): pass else : docs = [{ x [ 0 : 1 ]: x [ 1 : 2 ] for x in data } for data in docs ] # List for multiple word clouds if they are to be returned. multiclouds = [] # Create a rounded mask. if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask # Constrain the layout figure_opts [ \"constrained_layout\" ] = True # Create the figure. fig = plt . figure ( ** figure_opts ) # Add the title if title : fig . suptitle ( title ) # Calculate the number of rows and columns. nrows = int ( np . ceil ( len ( docs ) / ncols )) spec = fig . add_gridspec ( nrows , ncols ) # Divide the data into rows. rows = list ( get_rows ( docs , ncols )) # Set an index for labels i = 0 # Loop through the rows. for row , doc in enumerate ( rows ): # Loop through the documents in the row. for col , data in enumerate ( doc ): # Create a subplot. ax = fig . add_subplot ( spec [ row , col ]) # Generate the subplot's word cloud. if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) # If `show=True`, show the word cloud. if show : ax . imshow ( wordcloud ) ax . axis ( \"off\" ) # Set the image title from the label if labels : ax . set_title ( labels [ i ]) i += 1 # Otherwise, add the word cloud to the multiclouds list. else : multiclouds . append ( wordcloud ) # If `show=False`, return the multiclouds list. if not show : return multiclouds","title":"Wordcloud"},{"location":"api/dtm/wordcloud/#word-clouds","text":"The lexos.dtm.wordcloud module contains classes and functions that facilitate the production of word cloud images directly from the document-term matrix. Images are produced with matplotlib and the Python Wordcloud library.","title":"Word Clouds"},{"location":"api/dtm/wordcloud/#lexos.dtm.wordcloud.get_rows","text":"Yield successive n-sized rows from a list of documents. Parameters: Name Type Description Default lst list A list of documents. required n int The number of columns in the row. required Yields: Type Description list A generator with the documents separated into rows. Source code in lexos\\dtm\\wordcloud.py def get_rows ( lst , n ): \"\"\"Yield successive n-sized rows from a list of documents. Args: lst (list): A list of documents. n (int): The number of columns in the row. Yields: list: A generator with the documents separated into rows. \"\"\" for i in range ( 0 , len ( lst ), n ): yield lst [ i : i + n ]","title":"get_rows()"},{"location":"api/dtm/wordcloud/#lexos.dtm.wordcloud.make_wordcloud","text":"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Parameters: Name Type Description Default data Union[dict, list, object, str, tuple] The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None Returns: Type Description object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\dtm\\wordcloud.py def make_wordcloud ( data : Union [ dict , list , object , str , tuple ], opts : dict = None , show : bool = True , figure_opts : dict = None , round : int = None ): \"\"\"Make a word cloud. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. Args: data (Union[dict, list, object, str, tuple]): The data. Accepts a text string, a list of lists or tuples, a dict with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : if isinstance ( data , list ): data = { x [ 0 ]: x [ 1 ] for x in data } elif isinstance ( data , pd . DataFrame ): term_counts = data . to_dict ( orient = \"records\" ) try : data = { x [ \"term\" ]: x [ \"count\" ] for x in term_counts } except KeyError : data = { x [ \"term\" ]: x [ \"frequency\" ] for x in term_counts } wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) if show : if figure_opts : plt . figure ( ** figure_opts ) plt . axis ( \"off\" ) plt . imshow ( wordcloud ) plt . show () else : return wordcloud","title":"make_wordcloud()"},{"location":"api/dtm/wordcloud/#lexos.dtm.wordcloud.make_multiclouds","text":"Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by get_dtm_table() . Parameters: Name Type Description Default docs List[Union[dict, object, str, tuple]] The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. required opts dict The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} None ncols int The number of columns in the grid. 3 title str The title of the grid. None labels List[str] The document labels for each subplot. None show bool Whether to show the plotted word cloud or return it as a WordCloud object. True figure_opts dict A dict of matplotlib figure options. None round int An integer (generally between 100-300) to apply a mask that rounds the word cloud. None Returns: Type Description object A WordCloud object if show is set to False. Notes For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud . If show=False the function expects to be called with something like wordcloud = make_wordcloud(data, show=False) . This returns WordCloud object which can be manipulated by any of its methods, such as to_file() . See the WordCloud documentation for a list of methods. Source code in lexos\\dtm\\wordcloud.py def make_multiclouds ( docs : List [ Union [ dict , object , str , tuple ]], opts : dict = None , ncols : int = 3 , title : str = None , labels : List [ str ] = None , show : bool = True , figure_opts : dict = None , round : int = None ): \"\"\"Make multiclouds. Accepts data from a string, list of lists or tuples, a dict with terms as keys and counts/frequencies as values, or a dataframe. The best input is a dtm produced by `get_dtm_table()`. Args: docs (List[Union[dict, object, str, tuple]]): The data. Accepts a list of text strings, a list of tuples, or dicts with the terms as keys and the counts/frequencies as values, or a dataframe with \"term\" and \"count\" or \"frequency\" columns. opts (dict): The WordCloud() options. For testing, try {\"background_color\": \"white\", \"max_words\": 2000, \"contour_width\": 3, \"contour_width\": \"steelblue\"} ncols (int): The number of columns in the grid. title (str): The title of the grid. labels (List[str]): The document labels for each subplot. show (bool): Whether to show the plotted word cloud or return it as a WordCloud object. figure_opts (dict): A dict of matplotlib figure options. round (int): An integer (generally between 100-300) to apply a mask that rounds the word cloud. Returns: object: A WordCloud object if show is set to False. Notes: - For a full list of options, see https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud-wordcloud. - If `show=False` the function expects to be called with something like `wordcloud = make_wordcloud(data, show=False)`. This returns WordCloud object which can be manipulated by any of its methods, such as `to_file()`. See the WordCloud documentation for a list of methods. \"\"\" # Process the docs data into a list if isinstance ( docs , pd . core . frame . DataFrame ): # Assumes a df with columns: Terms, Doc_Label, DocLabel,... # Transpose the df docs = docs . T # Grab the first row for the header new_header = docs . iloc [ 0 ] # Drop the first row docs = docs [ 1 :] # Set the header row as the df header docs . columns = new_header # Return a dict docs = docs . to_dict ( orient = \"records\" ) # Ensure that anything that is not a list of strings is converted # to the appropriate format. elif isinstance ( docs , list ): if all ( isinstance ( s , str ) for s in docs ): pass else : docs = [{ x [ 0 : 1 ]: x [ 1 : 2 ] for x in data } for data in docs ] # List for multiple word clouds if they are to be returned. multiclouds = [] # Create a rounded mask. if round : x , y = np . ogrid [: 300 , : 300 ] mask = ( x - 150 ) ** 2 + ( y - 150 ) ** 2 > round ** 2 mask = 255 * mask . astype ( int ) opts [ \"mask\" ] = mask # Constrain the layout figure_opts [ \"constrained_layout\" ] = True # Create the figure. fig = plt . figure ( ** figure_opts ) # Add the title if title : fig . suptitle ( title ) # Calculate the number of rows and columns. nrows = int ( np . ceil ( len ( docs ) / ncols )) spec = fig . add_gridspec ( nrows , ncols ) # Divide the data into rows. rows = list ( get_rows ( docs , ncols )) # Set an index for labels i = 0 # Loop through the rows. for row , doc in enumerate ( rows ): # Loop through the documents in the row. for col , data in enumerate ( doc ): # Create a subplot. ax = fig . add_subplot ( spec [ row , col ]) # Generate the subplot's word cloud. if isinstance ( data , str ): wordcloud = WordCloud ( ** opts ) . generate_from_text ( data ) else : wordcloud = WordCloud ( ** opts ) . generate_from_frequencies ( data ) # If `show=True`, show the word cloud. if show : ax . imshow ( wordcloud ) ax . axis ( \"off\" ) # Set the image title from the label if labels : ax . set_title ( labels [ i ]) i += 1 # Otherwise, add the word cloud to the multiclouds list. else : multiclouds . append ( wordcloud ) # If `show=False`, return the multiclouds list. if not show : return multiclouds","title":"make_multiclouds()"},{"location":"api/io/","text":"IO \u00a4 The IO module manages input and output functions. So far, it just contains basic loading functions.","title":"Index"},{"location":"api/io/#io","text":"The IO module manages input and output functions. So far, it just contains basic loading functions.","title":"IO"},{"location":"api/io/basic/","text":"Basic \u00a4 Currently, the IO module contains a basic Loader class. lexos.io.basic.Loader \u00a4 Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\basic.py class Loader (): \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text.\"\"\" return utils . _decode_bytes ( text ) def _download_text ( self , url : str ) -> str : \"\"\"Download a text from a url.\"\"\" try : r = requests . get ( url ) r . raise_for_status () return self . _decode ( r . text ) except requests . exceptions . HTTPError as e : raise LexosException ( e . response . text ) def _validate_source ( self , source : Any ) -> bool : \"\"\"Validate a source.\"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): self . errors . append ( source ) return False else : return True def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" ) __init__ ( self ) special \u00a4 init method. Source code in lexos\\io\\basic.py def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True load ( self , source , decode = True ) \u00a4 Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union[List[Path, str], Path, str] A source or list of sources. required decode bool Whether to decode the source. True Exceptions: Type Description LexosException An error message. Source code in lexos\\io\\basic.py def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"Basic"},{"location":"api/io/basic/#basic","text":"Currently, the IO module contains a basic Loader class.","title":"Basic"},{"location":"api/io/basic/#lexos.io.basic.Loader","text":"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. Source code in lexos\\io\\basic.py class Loader (): \"\"\"Loader class. Handles the queue for assets to be pipelined from their sources to text processing tools. \"\"\" def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True def _decode ( self , text : Union [ bytes , str ]) -> str : \"\"\"Decode a text.\"\"\" return utils . _decode_bytes ( text ) def _download_text ( self , url : str ) -> str : \"\"\"Download a text from a url.\"\"\" try : r = requests . get ( url ) r . raise_for_status () return self . _decode ( r . text ) except requests . exceptions . HTTPError as e : raise LexosException ( e . response . text ) def _validate_source ( self , source : Any ) -> bool : \"\"\"Validate a source.\"\"\" if not isinstance ( source , str ) and not isinstance ( source , Path ): self . errors . append ( source ) return False else : return True def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"Loader"},{"location":"api/io/basic/#lexos.io.basic.Loader.__init__","text":"init method. Source code in lexos\\io\\basic.py def __init__ ( self ): \"\"\"__init__ method.\"\"\" self . source = None self . texts = [] self . errors = [] self . decode = True","title":"__init__()"},{"location":"api/io/basic/#lexos.io.basic.Loader.load","text":"Load the source into a list of bytes and strings. Parameters: Name Type Description Default source Union[List[Path, str], Path, str] A source or list of sources. required decode bool Whether to decode the source. True Exceptions: Type Description LexosException An error message. Source code in lexos\\io\\basic.py def load ( self , source : Union [ List [ Union [ Path , str ]], Path , str ], decode : bool = True ) -> List [ str ]: \"\"\"Load the source into a list of bytes and strings. Args: source (Union[List[Path, str], Path, str]): A source or list of sources. decode (bool): Whether to decode the source. Raises: LexosException: An error message. \"\"\" if not source : raise LexosException ( LANG [ \"no_source\" ]) else : self . source = source self . decode = decode if isinstance ( self . source , str ) or isinstance ( self . source , Path ): self . source = [ self . source ] for item in self . source : if self . _validate_source ( item ): if utils . is_url ( item ): self . texts . append ( self . _download_text ( item )) elif utils . ensure_path ( item ) . is_file (): with open ( utils . ensure_path ( item ), \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) elif utils . ensure_path ( item ) . is_dir (): for filepath in utils . ensure_path ( item ) . rglob ( \"*\" ): with open ( filepath , \"rb\" ) as f : self . texts . append ( self . _decode ( f . read ())) # Failsafe else : raise LexosException ( f ' { LANG [ \"bad_source\" ] } : { item } ' ) else : pass if len ( self . errors ) > 0 : print ( \"The following items were not loaded:\" ) for source in self . errors : print ( f \"Error: { source } \" )","title":"load()"},{"location":"api/scrubber/","text":"Scrubber \u00a4 Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into five submodules: normalize A set of functions for massaging text into standardised forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Constains the lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Index"},{"location":"api/scrubber/#scrubber","text":"Scrubber is a destructive preprocessing module that contains a set of functions for manipulating text. It leans heavily on the code base for Textacy but tweaks some of that library's functions in order to modify or extend the functionality. Scrubber is divided into five submodules: normalize A set of functions for massaging text into standardised forms. pipeline A set of functions for feeding multiple components into a scrubbing function. registry A registry of scrubbing functions that can be accessed to reference functions by name. remove A set of functions for removing strings and patterns from text. replace A set of functions for replacing strings and patterns from text. resources A set of constants, classes, and functions used by the other components of the Scrubber module. scrubber Constains the lexos.scrubber.scrubber.Scrub class for managing scrubbing pipelines. utils A set of utility functions shared by the other components of the Scrubber module.","title":"Scrubber"},{"location":"api/scrubber/normalize/","text":"Normalize \u00a4 The normalize component of Scrubber contains functions to perform a variety of text manipulations. The functions are frequently applied at the beginning of a scrubbing pipeline. lexos . scrubber . normalize . bullet_points ( text ) \u00a4 Normalize bullet points. Normalises all \"fancy\" bullet point symbols in text to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's utils.normalize_bullets . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def bullet_points ( text : str ) -> str : \"\"\"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in `text` to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's `utils.normalize_bullets`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_BULLET_POINTS . sub ( r \"\\1-\" , text ) lexos . scrubber . normalize . hyphenated_words ( text ) \u00a4 Normalize hyphenated words. Normalize words in text that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's utils.normalize_hyphens . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def hyphenated_words ( text : str ) -> str : \"\"\"Normalize hyphenated words. Normalize words in `text` that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's `utils.normalize_hyphens`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_HYPHENATED_WORD . sub ( r \"\\1\\2\" , text ) lexos . scrubber . normalize . lower_case ( text ) \u00a4 Convert text to lower case. Parameters: Name Type Description Default text str The text to convert to lower case. required Returns: Type Description str The converted text. Source code in lexos\\scrubber\\normalize.py def lower_case ( text : str ) -> str : \"\"\"Convert `text` to lower case. Args: text (str): The text to convert to lower case. Returns: The converted text. \"\"\" return text . lower () lexos . scrubber . normalize . quotation_marks ( text ) \u00a4 Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in text to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's utils.normalize_quotation_marks . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def quotation_marks ( text : str ) -> str : \"\"\"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in `text` to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's `utils.normalize_quotation_marks`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return text . translate ( resources . QUOTE_TRANSLATION_TABLE ) lexos . scrubber . normalize . repeating_chars ( text , * , chars , maxn = 1 ) \u00a4 Normalize repeating characters in text . Truncating their number of consecutive repetitions to maxn . Duplicates Textacy's utils.normalize_repeating_chars . Parameters: Name Type Description Default text str The text to normalize. required chars str One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". required maxn int Maximum number of consecutive repetitions of chars to which longer repetitions will be truncated. 1 Returns: Type Description str str Source code in lexos\\scrubber\\normalize.py def repeating_chars ( text : str , * , chars : str , maxn : int = 1 ) -> str : \"\"\"Normalize repeating characters in `text`. Truncating their number of consecutive repetitions to `maxn`. Duplicates Textacy's `utils.normalize_repeating_chars`. Args: text (str): The text to normalize. chars: One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". maxn: Maximum number of consecutive repetitions of `chars` to which longer repetitions will be truncated. Returns: str \"\"\" return re . sub ( r \"( {} ){{ {} ,}}\" . format ( re . escape ( chars ), maxn + 1 ), chars * maxn , text ) lexos . scrubber . normalize . unicode ( text , * , form = 'NFC' ) \u00a4 Normalize unicode characters in text into canonical forms. Duplicates Textacy's utils.normalize_unicode . Parameters: Name Type Description Default text str The text to normalize. required form Literal['NFC', 'NFD', 'NFKC', 'NFKD'] Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. 'NFC' See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize Source code in lexos\\scrubber\\normalize.py def unicode ( text : str , * , form : Literal [ \"NFC\" , \"NFD\" , \"NFKC\" , \"NFKD\" ] = \"NFC\" ) -> str : \"\"\"Normalize unicode characters in `text` into canonical forms. Duplicates Textacy's `utils.normalize_unicode`. Args: text (str): The text to normalize. form: Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"\"\" return unicodedata . normalize ( form , text ) lexos . scrubber . normalize . whitespace ( text ) \u00a4 Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def whitespace ( text : str ) -> str : \"\"\"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" text = resources . RE_ZWSP . sub ( \"\" , text ) text = resources . RE_LINEBREAK . sub ( r \"\\n\" , text ) text = resources . RE_NONBREAKING_SPACE . sub ( \" \" , text ) return text . strip ()","title":"Normalize"},{"location":"api/scrubber/normalize/#normalize","text":"The normalize component of Scrubber contains functions to perform a variety of text manipulations. The functions are frequently applied at the beginning of a scrubbing pipeline.","title":"Normalize"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.bullet_points","text":"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in text to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's utils.normalize_bullets . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def bullet_points ( text : str ) -> str : \"\"\"Normalize bullet points. Normalises all \"fancy\" bullet point symbols in `text` to just the basic ASCII \"-\", provided they are the first non-whitespace characters on a new line (like a list of items). Duplicates Textacy's `utils.normalize_bullets`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_BULLET_POINTS . sub ( r \"\\1-\" , text )","title":"bullet_points()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.hyphenated_words","text":"Normalize hyphenated words. Normalize words in text that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's utils.normalize_hyphens . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def hyphenated_words ( text : str ) -> str : \"\"\"Normalize hyphenated words. Normalize words in `text` that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace. Duplicates Textacy's `utils.normalize_hyphens`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return resources . RE_HYPHENATED_WORD . sub ( r \"\\1\\2\" , text )","title":"hyphenated_words()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.lower_case","text":"Convert text to lower case. Parameters: Name Type Description Default text str The text to convert to lower case. required Returns: Type Description str The converted text. Source code in lexos\\scrubber\\normalize.py def lower_case ( text : str ) -> str : \"\"\"Convert `text` to lower case. Args: text (str): The text to convert to lower case. Returns: The converted text. \"\"\" return text . lower ()","title":"lower_case()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.quotation_marks","text":"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in text to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's utils.normalize_quotation_marks . Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def quotation_marks ( text : str ) -> str : \"\"\"Normalize quotation marks. Normalize all \"fancy\" single- and double-quotation marks in `text` to just the basic ASCII equivalents. Note that this will also normalize fancy apostrophes, which are typically represented as single quotation marks. Duplicates Textacy's `utils.normalize_quotation_marks`. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" return text . translate ( resources . QUOTE_TRANSLATION_TABLE )","title":"quotation_marks()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.repeating_chars","text":"Normalize repeating characters in text . Truncating their number of consecutive repetitions to maxn . Duplicates Textacy's utils.normalize_repeating_chars . Parameters: Name Type Description Default text str The text to normalize. required chars str One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". required maxn int Maximum number of consecutive repetitions of chars to which longer repetitions will be truncated. 1 Returns: Type Description str str Source code in lexos\\scrubber\\normalize.py def repeating_chars ( text : str , * , chars : str , maxn : int = 1 ) -> str : \"\"\"Normalize repeating characters in `text`. Truncating their number of consecutive repetitions to `maxn`. Duplicates Textacy's `utils.normalize_repeating_chars`. Args: text (str): The text to normalize. chars: One or more characters whose consecutive repetitions are to be normalized, e.g. \".\" or \"?!\". maxn: Maximum number of consecutive repetitions of `chars` to which longer repetitions will be truncated. Returns: str \"\"\" return re . sub ( r \"( {} ){{ {} ,}}\" . format ( re . escape ( chars ), maxn + 1 ), chars * maxn , text )","title":"repeating_chars()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.unicode","text":"Normalize unicode characters in text into canonical forms. Duplicates Textacy's utils.normalize_unicode . Parameters: Name Type Description Default text str The text to normalize. required form Literal['NFC', 'NFD', 'NFKC', 'NFKD'] Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. 'NFC' See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize Source code in lexos\\scrubber\\normalize.py def unicode ( text : str , * , form : Literal [ \"NFC\" , \"NFD\" , \"NFKC\" , \"NFKD\" ] = \"NFC\" ) -> str : \"\"\"Normalize unicode characters in `text` into canonical forms. Duplicates Textacy's `utils.normalize_unicode`. Args: text (str): The text to normalize. form: Form of normalization applied to unicode characters. For example, an \"e\" with accute accent \"\u00b4\" can be written as \"e\u00b4\" (canonical decomposition, \"NFD\") or \"\u00e9\" (canonical composition, \"NFC\"). Unicode can be normalized to NFC form without any change in meaning, so it's usually a safe bet. If \"NFKC\", additional normalizations are applied that can change characters' meanings, e.g. ellipsis characters are replaced with three periods. See Also: https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"\"\" return unicodedata . normalize ( form , text )","title":"unicode()"},{"location":"api/scrubber/normalize/#lexos.scrubber.normalize.whitespace","text":"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Parameters: Name Type Description Default text str The text to normalize. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\normalize.py def whitespace ( text : str ) -> str : \"\"\"Normalize whitespace. Replace all contiguous zero-width spaces with an empty string, line-breaking spaces with a single newline, and non-breaking spaces with a single space, then strip any leading/trailing whitespace. Args: text (str): The text to normalize. Returns: The normalized text. \"\"\" text = resources . RE_ZWSP . sub ( \"\" , text ) text = resources . RE_LINEBREAK . sub ( r \"\\n\" , text ) text = resources . RE_NONBREAKING_SPACE . sub ( \" \" , text ) return text . strip ()","title":"whitespace()"},{"location":"api/scrubber/pipeline/","text":"Pipeline \u00a4 The pipeline component of Scrubber is used to manage an ordered application of Scrubber component functions to text. lexos . scrubber . pipeline . make_pipeline ( * funcs ) \u00a4 Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. python -m pip install cytoolz is required for this function to work. Use pipe (an alias for functools.partial ) to pass arguments to preprocessors. from lexos import scrubber scrubber = Scrubber . pipeline . make_pipeline ( scrubber . replace . hashtags , scrubber . replace . emojis , pipe ( scrubber . remove . punctuation , only = [ \".\" , \"?\" , \"!\" ]) ) scrubber ( \"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\" ) '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' Parameters: Name Type Description Default *funcs dict A series of functions to be applied to the text. () Returns: Type Description Callable[[str], str] Pipeline composed of *funcs that applies each in sequential order. Source code in lexos\\scrubber\\pipeline.py def make_pipeline ( * funcs : Callable [[ str ], str ]) -> Callable [[ str ], str ]: \"\"\"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. `python -m pip install cytoolz` is required for this function to work. Use `pipe` (an alias for `functools.partial`) to pass arguments to preprocessors. ```python from lexos import scrubber scrubber = Scrubber.pipeline.make_pipeline( scrubber.replace.hashtags, scrubber.replace.emojis, pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"]) ) scrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\") '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' ``` Args: *funcs (dict): A series of functions to be applied to the text. Returns: Pipeline composed of ``*funcs`` that applies each in sequential order. \"\"\" return functoolz . compose_left ( * funcs ) lexos . scrubber . pipeline . make_pipeline_from_tuple ( funcs ) \u00a4 Return a pipeline from a tuple. Parameters: Name Type Description Default funcs tuple A tuple containing callables or string names of functions. required Returns a tuple of functions. Source code in lexos\\scrubber\\pipeline.py def make_pipeline_from_tuple ( funcs : tuple ) -> tuple : \"\"\"Return a pipeline from a tuple. Args: funcs (tuple): A tuple containing callables or string names of functions. Returns a tuple of functions. \"\"\" return make_pipeline ( * [ eval ( x ) if isinstance ( x , str ) else x for x in funcs ]) Note lexos.scrubber.pipeline.make_pipeline_from_tuple is deprecated. It should not be necessary if you are using lexos.scrubber.registry . lexos . scrubber . pipeline . pipe ( func , * args , ** kwargs ) \u00a4 Apply functool.partial and add __name__ to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Parameters: Name Type Description Default func Callable A callable. required Returns: Type Description Callable A partial function with __name__ set to the name of the function. Source code in lexos\\scrubber\\pipeline.py def pipe ( func : Callable , * args , ** kwargs ) -> Callable : \"\"\"Apply functool.partial and add `__name__` to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Args: func (Callable): A callable. Returns: A partial function with `__name__` set to the name of the function. \"\"\" if not args and not kwargs : return func else : partial_func = partial ( func , * args , ** kwargs ) update_wrapper ( partial_func , func ) return partial_func","title":"Pipeline"},{"location":"api/scrubber/pipeline/#pipeline","text":"The pipeline component of Scrubber is used to manage an ordered application of Scrubber component functions to text.","title":"Pipeline"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline","text":"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. python -m pip install cytoolz is required for this function to work. Use pipe (an alias for functools.partial ) to pass arguments to preprocessors. from lexos import scrubber scrubber = Scrubber . pipeline . make_pipeline ( scrubber . replace . hashtags , scrubber . replace . emojis , pipe ( scrubber . remove . punctuation , only = [ \".\" , \"?\" , \"!\" ]) ) scrubber ( \"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\" ) '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' Parameters: Name Type Description Default *funcs dict A series of functions to be applied to the text. () Returns: Type Description Callable[[str], str] Pipeline composed of *funcs that applies each in sequential order. Source code in lexos\\scrubber\\pipeline.py def make_pipeline ( * funcs : Callable [[ str ], str ]) -> Callable [[ str ], str ]: \"\"\"Make a callable pipeline. Make a callable pipeline that passes a text through a series of functions in sequential order, then outputs a (scrubbed) text string. This function is intended as a lightweight convenience for users, allowing them to flexibly specify scrubbing options and their order,which (and in which order) preprocessing treating the whole thing as a single callable. `python -m pip install cytoolz` is required for this function to work. Use `pipe` (an alias for `functools.partial`) to pass arguments to preprocessors. ```python from lexos import scrubber scrubber = Scrubber.pipeline.make_pipeline( scrubber.replace.hashtags, scrubber.replace.emojis, pipe(scrubber.remove.punctuation, only=[\".\", \"?\", \"!\"]) ) scrubber(\"@spacy_io is OSS for industrial-strength NLP in Python developed by @explosion_ai \ud83d\udca5\") '_USER_ is OSS for industrial-strength NLP in Python developed by _USER_ _EMOJI_' ``` Args: *funcs (dict): A series of functions to be applied to the text. Returns: Pipeline composed of ``*funcs`` that applies each in sequential order. \"\"\" return functoolz . compose_left ( * funcs )","title":"make_pipeline()"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.make_pipeline_from_tuple","text":"Return a pipeline from a tuple. Parameters: Name Type Description Default funcs tuple A tuple containing callables or string names of functions. required Returns a tuple of functions. Source code in lexos\\scrubber\\pipeline.py def make_pipeline_from_tuple ( funcs : tuple ) -> tuple : \"\"\"Return a pipeline from a tuple. Args: funcs (tuple): A tuple containing callables or string names of functions. Returns a tuple of functions. \"\"\" return make_pipeline ( * [ eval ( x ) if isinstance ( x , str ) else x for x in funcs ]) Note lexos.scrubber.pipeline.make_pipeline_from_tuple is deprecated. It should not be necessary if you are using lexos.scrubber.registry .","title":"make_pipeline_from_tuple()"},{"location":"api/scrubber/pipeline/#lexos.scrubber.pipeline.pipe","text":"Apply functool.partial and add __name__ to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Parameters: Name Type Description Default func Callable A callable. required Returns: Type Description Callable A partial function with __name__ set to the name of the function. Source code in lexos\\scrubber\\pipeline.py def pipe ( func : Callable , * args , ** kwargs ) -> Callable : \"\"\"Apply functool.partial and add `__name__` to the partial function. This allows the function to be passed to the pipeline along with keyword arguments. Args: func (Callable): A callable. Returns: A partial function with `__name__` set to the name of the function. \"\"\" if not args and not kwargs : return func else : partial_func = partial ( func , * args , ** kwargs ) update_wrapper ( partial_func , func ) return partial_func","title":"pipe()"},{"location":"api/scrubber/registry/","text":"Registry \u00a4 The registry component of Scrubber maintains a catalogue of registered functions that can be imported individually as needed. The registry enables the functions to be referenced by name using string values. The code registry is created and accessed using the catalogue library by Explosion. Registered functions can be retrieved individually using lower_case = scrubber_components.get(\"lower_case\") . Multiple functions can be loaded using the load_components function: lexos . scrubber . registry . load_component ( s ) \u00a4 Load a single component from a string. Parameters: Name Type Description Default s str The name of the function. required Source code in lexos\\scrubber\\registry.py def load_component ( s : str ): \"\"\"Load a single component from a string. Args: s: The name of the function. \"\"\" return scrubber_components . get ( s ) lexos . scrubber . registry . load_components ( t ) \u00a4 Load components from a tuple. Parameters: Name Type Description Default t tuple A tuple containing string names of functions. required Source code in lexos\\scrubber\\registry.py def load_components ( t : tuple ): \"\"\"Load components from a tuple. Args: t: A tuple containing string names of functions. \"\"\" for item in t : yield scrubber_components . get ( item ) Note Custom functions can be registered by first creating the function and then adding it to the registry. An example is given below: from lexos.scrubber.registry import scrubber_components def title_case ( text ): \"\"\"Convert text to title case using `title()`\"\"\" return text . title () scrubber_components . register ( \"title_case\" , func = title_case )","title":"Registry"},{"location":"api/scrubber/registry/#registry","text":"The registry component of Scrubber maintains a catalogue of registered functions that can be imported individually as needed. The registry enables the functions to be referenced by name using string values. The code registry is created and accessed using the catalogue library by Explosion. Registered functions can be retrieved individually using lower_case = scrubber_components.get(\"lower_case\") . Multiple functions can be loaded using the load_components function:","title":"Registry"},{"location":"api/scrubber/registry/#lexos.scrubber.registry.load_component","text":"Load a single component from a string. Parameters: Name Type Description Default s str The name of the function. required Source code in lexos\\scrubber\\registry.py def load_component ( s : str ): \"\"\"Load a single component from a string. Args: s: The name of the function. \"\"\" return scrubber_components . get ( s )","title":"load_component()"},{"location":"api/scrubber/registry/#lexos.scrubber.registry.load_components","text":"Load components from a tuple. Parameters: Name Type Description Default t tuple A tuple containing string names of functions. required Source code in lexos\\scrubber\\registry.py def load_components ( t : tuple ): \"\"\"Load components from a tuple. Args: t: A tuple containing string names of functions. \"\"\" for item in t : yield scrubber_components . get ( item ) Note Custom functions can be registered by first creating the function and then adding it to the registry. An example is given below: from lexos.scrubber.registry import scrubber_components def title_case ( text ): \"\"\"Convert text to title case using `title()`\"\"\" return text . title () scrubber_components . register ( \"title_case\" , func = title_case )","title":"load_components()"},{"location":"api/scrubber/remove/","text":"Remove \u00a4 The remove component of Scrubber contains a set of functions for removing strings and patterns from text. lexos . scrubber . remove . accents ( text , * , fast = False , accents = None ) \u00a4 Remove accents from any accented unicode characters in text , either by replacing them with ASCII equivalents or removing them entirely. Parameters: Name Type Description Default text str The text from which accents will be removed. required fast bool If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. False accents Union[str, tuple] An optional string or tuple of strings indicating the names of diacritics to be stripped. None Returns: Type Description str str fast=True can be significantly faster than fast=False , but its transformation of text is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out unidecode : https://github.com/avian2/unidecode Source code in lexos\\scrubber\\remove.py def accents ( text : str , * , fast : bool = False , accents : Union [ str , tuple ] = None ) -> str : \"\"\" Remove accents from any accented unicode characters in `text`, either by replacing them with ASCII equivalents or removing them entirely. Args: text (str): The text from which accents will be removed. fast: If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. accents: An optional string or tuple of strings indicating the names of diacritics to be stripped. Returns: str Note: `fast=True` can be significantly faster than `fast=False`, but its transformation of `text` is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out `unidecode`: https://github.com/avian2/unidecode \"\"\" if fast is False : if accents : if isinstance ( accents , str ): accents = set ( unicodedata . lookup ( accents )) elif len ( accents ) == 1 : accents = set ( unicodedata . lookup ( accents [ 0 ])) else : accents = set ( map ( unicodedata . lookup , accents )) return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if char not in accents ) else : return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if not unicodedata . combining ( char ) ) else : return ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ascii\" , errors = \"ignore\" ) . decode ( \"ascii\" ) ) lexos . scrubber . remove . brackets ( text , * , only = None ) \u00a4 Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Parameters: Name Type Description Default text str The text from which brackets will be removed. required only Optional[str | Collection[str]] Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, \"square\" removes only those contents found between square brackets, while [\"round\", \"square\"] removes those contents found between square or round brackets, but not curly. None Returns: Type Description str str Note This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. Source code in lexos\\scrubber\\remove.py def brackets ( text : str , * , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Args: text (str): The text from which brackets will be removed. only: Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, `\"square\"` removes only those contents found between square brackets, while `[\"round\", \"square\"]` removes those contents found between square or round brackets, but not curly. Returns: str Note: This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. \"\"\" only = utils . to_collection ( only , val_type = str , col_type = set ) if only is None or \"curly\" in only : text = resources . RE_BRACKETS_CURLY . sub ( \"\" , text ) if only is None or \"square\" in only : text = resources . RE_BRACKETS_SQUARE . sub ( \"\" , text ) if only is None or \"round\" in only : text = resources . RE_BRACKETS_ROUND . sub ( \"\" , text ) return text lexos . scrubber . remove . digits ( text , * , only = None ) \u00a4 Remove digits. Remove digits from text by replacing all instances of digits (or a subset thereof specified by only ) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Parameters: Name Type Description Default text str The text from which digits will be removed. required only Optional[str | Collection[str]] Remove only those digits specified here. For example, \"9\" removes only 9, while [\"1\", \"2\", \"3\"] removes 1, 2, 3; if None, all unicode digits marks are removed. None Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def digits ( text : str , * , only : Optional [ str | Collection [ str ]] = None ) -> str : \"\"\"Remove digits. Remove digits from `text` by replacing all instances of digits (or a subset thereof specified by `only`) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Args: text (str): The text from which digits will be removed. only: Remove only those digits specified here. For example, `\"9\"` removes only 9, while `[\"1\", \"2\", \"3\"]` removes 1, 2, 3; if None, all unicode digits marks are removed. Returns: str \"\"\" if only : if isinstance ( only , list ): pattern = re . compile ( f '[ { \"\" . join ( only ) } ]' ) else : pattern = re . compile ( only ) else : # Using \".\" to represent any unicode character used to indicate # a decimal number, and \"***\" to represent any sequence of # unicode digits, this pattern will match: # 1) *** # 2) ***.*** unicode_digits = \"\" for i in range ( sys . maxunicode ): if unicodedata . category ( chr ( i )) . startswith ( 'N' ): unicode_digits = unicode_digits + chr ( i ) pattern = re . compile ( r \"([+-]?[\" + re . escape ( unicode_digits ) + r \"])|((?<=\" + re . escape ( unicode_digits ) + r \")[\\u0027|\\u002C|\\u002E|\\u00B7|\" r \"\\u02D9|\\u066B|\\u066C|\\u2396][\" + re . escape ( unicode_digits ) + r \"]+)\" , re . UNICODE ) return str ( re . sub ( pattern , r \" \" , text )) lexos . scrubber . remove . new_lines ( text ) \u00a4 Remove new lines. Remove all line-breaking spaces. Parameters: Name Type Description Default text str The text from which new lines will be removed. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\remove.py def new_lines ( text : str ) -> str : \"\"\"Remove new lines. Remove all line-breaking spaces. Args: text (str): The text from which new lines will be removed. Returns: The normalized text. \"\"\" return resources . RE_LINEBREAK . sub ( \"\" , text ) . strip () lexos . scrubber . remove . pattern ( text , * , pattern ) \u00a4 Remove strings from text using a regex pattern. Parameters: Name Type Description Default text str The text from which patterns will be removed. required pattern Union[str, Collection[str]] The pattern to match. required Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def pattern ( text : str , * , pattern : Union [ str , Collection [ str ]] ) -> str : \"\"\"Remove strings from `text` using a regex pattern. Args: text (str): The text from which patterns will be removed. pattern: The pattern to match. Returns: str \"\"\" if isinstance ( pattern , list ): pattern = \"|\" . join ( pattern ) pat = re . compile ( pattern ) return re . sub ( pat , \"\" , text ) lexos . scrubber . remove . punctuation ( text , * , exclude = None , only = None ) \u00a4 Remove punctuation from text . Removes all instances of punctuation (or a subset thereof specified by only ). Parameters: Name Type Description Default text str The text from which punctuation will be removed. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\remove.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove punctuation from `text`. Removes all instances of punctuation (or a subset thereof specified by `only`). Args: text (str): The text from which punctuation will be removed. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \"\" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) else : exclude = [] # Note: We can't use the cached translation table because it replaces # the punctuation with whitespace, so we have to build a new one. translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \"\" ) return text . translate ( translation_table ) lexos . scrubber . remove . tabs ( text ) \u00a4 Remove tabs. If you want to replace tabs with a single space, use normalize.whitespace() instead. Parameters: Name Type Description Default text str The text from which tabs will be removed. required Returns: Type Description str The stripped text. Source code in lexos\\scrubber\\remove.py def tabs ( text : str ) -> str : \"\"\"Remove tabs. If you want to replace tabs with a single space, use `normalize.whitespace()` instead. Args: text (str): The text from which tabs will be removed. Returns: The stripped text. \"\"\" return resources . RE_TAB . sub ( \"\" , text ) lexos . scrubber . remove . tags ( text , sep = ' ' , remove_whitespace = True ) \u00a4 Remove tags from text . Parameters: Name Type Description Default text str The text from which tags will be removed. required sep str A string to insert between tags and text found between them. ' ' remove_whitespace bool If True, remove extra whitespace between text after tags are removed. True Returns: Type Description str A string containing just the text found between tags and other non-data elements. Note If you want to perfom selective removal of tags, use replace.tag_map instead. This function relies on the stdlib html.parser.HTMLParser . It appears to work for stripping tags from both html and xml. Using lxml or BeautifulSoup might be faster, but this is untested. This function preserves text in comments, as well as tags Source code in lexos\\scrubber\\remove.py def tags ( text : str , sep : str = \" \" , remove_whitespace : bool = True ) -> str : \"\"\"Remove tags from `text`. Args: text (str): The text from which tags will be removed. sep: A string to insert between tags and text found between them. remove_whitespace: If True, remove extra whitespace between text after tags are removed. Returns: A string containing just the text found between tags and other non-data elements. Note: - If you want to perfom selective removal of tags, use `replace.tag_map` instead. - This function relies on the stdlib `html.parser.HTMLParser`. It appears to work for stripping tags from both html and xml. Using `lxml` or BeautifulSoup might be faster, but this is untested. - This function preserves text in comments, as well as tags \"\"\" parser = resources . HTMLTextExtractor () parser . feed ( text ) text = parser . get_text ( sep = sep ) if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , sep , text , re . UNICODE ) return text","title":"Remove"},{"location":"api/scrubber/remove/#remove","text":"The remove component of Scrubber contains a set of functions for removing strings and patterns from text.","title":"Remove"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.accents","text":"Remove accents from any accented unicode characters in text , either by replacing them with ASCII equivalents or removing them entirely. Parameters: Name Type Description Default text str The text from which accents will be removed. required fast bool If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. False accents Union[str, tuple] An optional string or tuple of strings indicating the names of diacritics to be stripped. None Returns: Type Description str str fast=True can be significantly faster than fast=False , but its transformation of text is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out unidecode : https://github.com/avian2/unidecode Source code in lexos\\scrubber\\remove.py def accents ( text : str , * , fast : bool = False , accents : Union [ str , tuple ] = None ) -> str : \"\"\" Remove accents from any accented unicode characters in `text`, either by replacing them with ASCII equivalents or removing them entirely. Args: text (str): The text from which accents will be removed. fast: If False, accents are removed from any unicode symbol with a direct ASCII equivalent; if True, accented chars for all unicode symbols are removed, regardless. accents: An optional string or tuple of strings indicating the names of diacritics to be stripped. Returns: str Note: `fast=True` can be significantly faster than `fast=False`, but its transformation of `text` is less \"safe\" and more likely to result in changes of meaning, spelling errors, etc. See Also: - For a chart containing Unicode standard names of diacritics, see https://en.wikipedia.org/wiki/Combining_Diacritical_Marks#Character_table - For a more powerful (but slower) alternative, check out `unidecode`: https://github.com/avian2/unidecode \"\"\" if fast is False : if accents : if isinstance ( accents , str ): accents = set ( unicodedata . lookup ( accents )) elif len ( accents ) == 1 : accents = set ( unicodedata . lookup ( accents [ 0 ])) else : accents = set ( map ( unicodedata . lookup , accents )) return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if char not in accents ) else : return \"\" . join ( char for char in unicodedata . normalize ( \"NFKD\" , text ) if not unicodedata . combining ( char ) ) else : return ( unicodedata . normalize ( \"NFKD\" , text ) . encode ( \"ascii\" , errors = \"ignore\" ) . decode ( \"ascii\" ) )","title":"accents()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.brackets","text":"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Parameters: Name Type Description Default text str The text from which brackets will be removed. required only Optional[str | Collection[str]] Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, \"square\" removes only those contents found between square brackets, while [\"round\", \"square\"] removes those contents found between square or round brackets, but not curly. None Returns: Type Description str str Note This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. Source code in lexos\\scrubber\\remove.py def brackets ( text : str , * , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove text within curly {}, square [], and/or round () brackets, as well as the brackets themselves. Args: text (str): The text from which brackets will be removed. only: Remove only those bracketed contents as specified here: \"curly\", \"square\", and/or \"round\". For example, `\"square\"` removes only those contents found between square brackets, while `[\"round\", \"square\"]` removes those contents found between square or round brackets, but not curly. Returns: str Note: This function relies on regular expressions, applied sequentially for curly, square, then round brackets; as such, it doesn't handle nested brackets of the same type and may behave unexpectedly on text with \"wild\" use of brackets. It should be fine removing structured bracketed contents, as is often used, for instance, to denote in-text citations. \"\"\" only = utils . to_collection ( only , val_type = str , col_type = set ) if only is None or \"curly\" in only : text = resources . RE_BRACKETS_CURLY . sub ( \"\" , text ) if only is None or \"square\" in only : text = resources . RE_BRACKETS_SQUARE . sub ( \"\" , text ) if only is None or \"round\" in only : text = resources . RE_BRACKETS_ROUND . sub ( \"\" , text ) return text","title":"brackets()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.digits","text":"Remove digits. Remove digits from text by replacing all instances of digits (or a subset thereof specified by only ) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Parameters: Name Type Description Default text str The text from which digits will be removed. required only Optional[str | Collection[str]] Remove only those digits specified here. For example, \"9\" removes only 9, while [\"1\", \"2\", \"3\"] removes 1, 2, 3; if None, all unicode digits marks are removed. None Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def digits ( text : str , * , only : Optional [ str | Collection [ str ]] = None ) -> str : \"\"\"Remove digits. Remove digits from `text` by replacing all instances of digits (or a subset thereof specified by `only`) with whitespace. Removes signed/unsigned numbers and decimal/delimiter-separated numbers. Does not remove currency symbols. Some tokens containing digits will be modified. Args: text (str): The text from which digits will be removed. only: Remove only those digits specified here. For example, `\"9\"` removes only 9, while `[\"1\", \"2\", \"3\"]` removes 1, 2, 3; if None, all unicode digits marks are removed. Returns: str \"\"\" if only : if isinstance ( only , list ): pattern = re . compile ( f '[ { \"\" . join ( only ) } ]' ) else : pattern = re . compile ( only ) else : # Using \".\" to represent any unicode character used to indicate # a decimal number, and \"***\" to represent any sequence of # unicode digits, this pattern will match: # 1) *** # 2) ***.*** unicode_digits = \"\" for i in range ( sys . maxunicode ): if unicodedata . category ( chr ( i )) . startswith ( 'N' ): unicode_digits = unicode_digits + chr ( i ) pattern = re . compile ( r \"([+-]?[\" + re . escape ( unicode_digits ) + r \"])|((?<=\" + re . escape ( unicode_digits ) + r \")[\\u0027|\\u002C|\\u002E|\\u00B7|\" r \"\\u02D9|\\u066B|\\u066C|\\u2396][\" + re . escape ( unicode_digits ) + r \"]+)\" , re . UNICODE ) return str ( re . sub ( pattern , r \" \" , text ))","title":"digits()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.new_lines","text":"Remove new lines. Remove all line-breaking spaces. Parameters: Name Type Description Default text str The text from which new lines will be removed. required Returns: Type Description str The normalized text. Source code in lexos\\scrubber\\remove.py def new_lines ( text : str ) -> str : \"\"\"Remove new lines. Remove all line-breaking spaces. Args: text (str): The text from which new lines will be removed. Returns: The normalized text. \"\"\" return resources . RE_LINEBREAK . sub ( \"\" , text ) . strip ()","title":"new_lines()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.pattern","text":"Remove strings from text using a regex pattern. Parameters: Name Type Description Default text str The text from which patterns will be removed. required pattern Union[str, Collection[str]] The pattern to match. required Returns: Type Description str str Source code in lexos\\scrubber\\remove.py def pattern ( text : str , * , pattern : Union [ str , Collection [ str ]] ) -> str : \"\"\"Remove strings from `text` using a regex pattern. Args: text (str): The text from which patterns will be removed. pattern: The pattern to match. Returns: str \"\"\" if isinstance ( pattern , list ): pattern = \"|\" . join ( pattern ) pat = re . compile ( pattern ) return re . sub ( pat , \"\" , text )","title":"pattern()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.punctuation","text":"Remove punctuation from text . Removes all instances of punctuation (or a subset thereof specified by only ). Parameters: Name Type Description Default text str The text from which punctuation will be removed. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\remove.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Remove punctuation from `text`. Removes all instances of punctuation (or a subset thereof specified by `only`). Args: text (str): The text from which punctuation will be removed. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \"\" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) else : exclude = [] # Note: We can't use the cached translation table because it replaces # the punctuation with whitespace, so we have to build a new one. translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \"\" ) return text . translate ( translation_table )","title":"punctuation()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tabs","text":"Remove tabs. If you want to replace tabs with a single space, use normalize.whitespace() instead. Parameters: Name Type Description Default text str The text from which tabs will be removed. required Returns: Type Description str The stripped text. Source code in lexos\\scrubber\\remove.py def tabs ( text : str ) -> str : \"\"\"Remove tabs. If you want to replace tabs with a single space, use `normalize.whitespace()` instead. Args: text (str): The text from which tabs will be removed. Returns: The stripped text. \"\"\" return resources . RE_TAB . sub ( \"\" , text )","title":"tabs()"},{"location":"api/scrubber/remove/#lexos.scrubber.remove.tags","text":"Remove tags from text . Parameters: Name Type Description Default text str The text from which tags will be removed. required sep str A string to insert between tags and text found between them. ' ' remove_whitespace bool If True, remove extra whitespace between text after tags are removed. True Returns: Type Description str A string containing just the text found between tags and other non-data elements. Note If you want to perfom selective removal of tags, use replace.tag_map instead. This function relies on the stdlib html.parser.HTMLParser . It appears to work for stripping tags from both html and xml. Using lxml or BeautifulSoup might be faster, but this is untested. This function preserves text in comments, as well as tags Source code in lexos\\scrubber\\remove.py def tags ( text : str , sep : str = \" \" , remove_whitespace : bool = True ) -> str : \"\"\"Remove tags from `text`. Args: text (str): The text from which tags will be removed. sep: A string to insert between tags and text found between them. remove_whitespace: If True, remove extra whitespace between text after tags are removed. Returns: A string containing just the text found between tags and other non-data elements. Note: - If you want to perfom selective removal of tags, use `replace.tag_map` instead. - This function relies on the stdlib `html.parser.HTMLParser`. It appears to work for stripping tags from both html and xml. Using `lxml` or BeautifulSoup might be faster, but this is untested. - This function preserves text in comments, as well as tags \"\"\" parser = resources . HTMLTextExtractor () parser . feed ( text ) text = parser . get_text ( sep = sep ) if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , sep , text , re . UNICODE ) return text","title":"tags()"},{"location":"api/scrubber/replace/","text":"Replace \u00a4 The replace component of Scrubber contains a set of functions for replacing strings and patterns in text. Important Some functions have the same names as functions in the remove component. To distinguish them in the registry, replace functions with the same names are prefixed with re_ . When loaded into a script, they can be given any name the user desires. lexos . scrubber . replace . currency_symbols ( text , repl = '_CUR_' ) \u00a4 Replace all currency symbols in text with repl . Parameters: Name Type Description Default text str The text in which currency symbols will be replaced. required repl str The replacement value for currency symbols. '_CUR_' Returns: Type Description str The text with currency symbols replaced. Source code in lexos\\scrubber\\replace.py def currency_symbols ( text : str , repl : str = \"_CUR_\" ) -> str : \"\"\"Replace all currency symbols in `text` with `repl`. Args: text (str): The text in which currency symbols will be replaced. repl (str): The replacement value for currency symbols. Returns: str: The text with currency symbols replaced. \"\"\" return resources . RE_CURRENCY_SYMBOL . sub ( repl , text ) lexos . scrubber . replace . digits ( text , repl = '_DIGIT_' ) \u00a4 Replace all digits in text with repl . Parameters: Name Type Description Default text str The text in which digits will be replaced. required repl str The replacement value for digits. '_DIGIT_' Returns: Type Description str The text with digits replaced. Source code in lexos\\scrubber\\replace.py def digits ( text : str , repl : str = \"_DIGIT_\" ) -> str : \"\"\"Replace all digits in `text` with `repl`. Args: text (str): The text in which digits will be replaced. repl (str): The replacement value for digits. Returns: str: The text with digits replaced. \"\"\" return resources . RE_NUMBER . sub ( repl , text ) lexos . scrubber . replace . emails ( text , repl = '_EMAIL_' ) \u00a4 Replace all email addresses in text with repl . Parameters: Name Type Description Default text str The text in which emails will be replaced. required repl str The replacement value for emails. '_EMAIL_' Returns: Type Description str The text with emails replaced. Source code in lexos\\scrubber\\replace.py def emails ( text : str , repl : str = \"_EMAIL_\" ) -> str : \"\"\"Replace all email addresses in `text` with `repl`. Args: text (str): The text in which emails will be replaced. repl (str): The replacement value for emails. Returns: str: The text with emails replaced. \"\"\" return resources . RE_EMAIL . sub ( repl , text ) lexos . scrubber . replace . emojis ( text , repl = '_EMOJI_' ) \u00a4 Replace all emoji and pictographs in text with repl . Parameters: Name Type Description Default text str The text in which emojis will be replaced. required repl str The replacement value for emojis. '_EMOJI_' Returns: Type Description str The text with emojis replaced. Note If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! Source code in lexos\\scrubber\\replace.py def emojis ( text : str , repl : str = \"_EMOJI_\" ) -> str : \"\"\" Replace all emoji and pictographs in `text` with `repl`. Args: text (str): The text in which emojis will be replaced. repl (str): The replacement value for emojis. Returns: str: The text with emojis replaced. Note: If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! \"\"\" return resources . RE_EMOJI . sub ( repl , text ) lexos . scrubber . replace . hashtags ( text , repl = '_HASHTAG_' ) \u00a4 Replace all hashtags in text with repl . Parameters: Name Type Description Default text str The text in which hashtags will be replaced. required repl str The replacement value for hashtags. '_HASHTAG_' Returns: Type Description str The text with currency hashtags replaced. Source code in lexos\\scrubber\\replace.py def hashtags ( text : str , repl : str = \"_HASHTAG_\" ) -> str : \"\"\"Replace all hashtags in `text` with `repl`. Args: text (str): The text in which hashtags will be replaced. repl (str): The replacement value for hashtags. Returns: str: The text with currency hashtags replaced. \"\"\" return resources . RE_HASHTAG . sub ( repl , text ) lexos . scrubber . replace . pattern ( text , * , pattern ) \u00a4 Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which a pattern or pattern will be replaced. required pattern Union[dict, Collection[dict]] (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). required Returns: Type Description str The text with pattern(s) replaced. Source code in lexos\\scrubber\\replace.py def pattern ( text : str , * , pattern : Union [ dict , Collection [ dict ]] ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which a pattern or pattern will be replaced. pattern: (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). Returns: str: The text with pattern(s) replaced. \"\"\" pattern = utils . ensure_list ( pattern ) for pat in pattern : k = str ( * pat ) match = re . compile ( k ) text = re . sub ( match , pat [ k ], text ) return text lexos . scrubber . replace . phone_numbers ( text , repl = '_PHONE_' ) \u00a4 Replace all phone numbers in text with repl . Parameters: Name Type Description Default text str The text in which phone numbers will be replaced. required repl str The replacement value for phone numbers. '_PHONE_' Returns: Type Description str The text with phone numbers replaced. Source code in lexos\\scrubber\\replace.py def phone_numbers ( text : str , repl : str = \"_PHONE_\" ) -> str : \"\"\"Replace all phone numbers in `text` with `repl`. Args: text (str): The text in which phone numbers will be replaced. repl (str): The replacement value for phone numbers. Returns: str: The text with phone numbers replaced. \"\"\" return resources . RE_PHONE_NUMBER . sub ( repl , text ) lexos . scrubber . replace . process_tag_replace_options ( orig_text , tag , action , attribute ) \u00a4 Replace html-style tags in text files according to user options. Parameters: Name Type Description Default orig_text str The user's text containing the original tag. required tag str The particular tag to be processed. required action str A string specifying the action to be performed on the tag. required attribute str Replacement value for tag when \"replace_with_attribute\" is specified. required Action options are required - \"remove_tag\" Remove the tag required - \"remove_element\" Remove the element and contents required - \"replace_element\" Replace the tag with the specified attribute required Returns: Type Description str The text after the specified tag is processed. Source code in lexos\\scrubber\\replace.py def process_tag_replace_options ( orig_text : str , tag : str , action : str , attribute : str ) -> str : \"\"\"Replace html-style tags in text files according to user options. Args: orig_text: The user's text containing the original tag. tag: The particular tag to be processed. action: A string specifying the action to be performed on the tag. attribute: Replacement value for tag when \"replace_with_attribute\" is specified. Action options are: - \"remove_tag\": Remove the tag - \"remove_element\": Remove the element and contents - \"replace_element\": Replace the tag with the specified attribute Returns: str: The text after the specified tag is processed. Note: The replacement of a tag with the value of an attribute may not be supported. This needs a second look. \"\"\" if action == \"remove_tag\" : # searching for variants this specific tag: <tag> ... pattern = re . compile ( r '<(?:' + tag + r '(?=\\s)(?!(?:[^>\" \\' ]|\"[^\"]*\"| \\' [^ \\' ]* \\' )*?(?<=\\s)' r '\\s*=)(?!\\s*/?>)\\s+(?:\".*?\"| \\' .*? \\' |[^>]*?)+|/?' + tag + r '\\s*/?)>' , re . MULTILINE | re . DOTALL | re . UNICODE ) # substitute all matching patterns with one space processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"remove_element\" : # <[whitespaces] TAG [SPACE attributes]> contents </[whitespaces]TAG> # as applied across newlines, (re.MULTILINE), on re.UNICODE, # and .* includes newlines (re.DOTALL) pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \"( .+?>|>).+?</\\s*\" + re . escape ( tag ) + \">\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"replace_element\" : pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \".*?>.+?</\\s*\" + re . escape ( tag ) + \".*?>\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , attribute , orig_text ) else : processed_text = orig_text # Leave Tag Alone return processed_text lexos . scrubber . replace . punctuation ( text , * , exclude = None , only = None ) \u00a4 Replace punctuation from text . Replaces all instances of punctuation (or a subset thereof specified by only ) with whitespace. Parameters: Name Type Description Default text str The text in which punctuation will be replaced. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\replace.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Replace punctuation from `text`. Replaces all instances of punctuation (or a subset thereof specified by `only`) with whitespace. Args: text (str): The text in which punctuation will be replaced. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \" \" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \" \" ) else : translation_table = resources . PUNCT_TRANSLATION_TABLE return text . translate ( translation_table ) lexos . scrubber . replace . special_characters ( text , * , is_html = False , ruleset = None ) \u00a4 Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which special characters will be replaced. required is_html bool Whether to replace HTML entities. False ruleset dict A dict containing the special characters to match and their replacements. None Returns: Type Description str str Source code in lexos\\scrubber\\replace.py def special_characters ( text : str , * , is_html : bool = False , ruleset : dict = None , ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which special characters will be replaced. is_html (bool): Whether to replace HTML entities. ruleset (dict): A dict containing the special characters to match and their replacements. Returns: str \"\"\" if is_html : text = html . unescape ( text ) else : for k , v in ruleset . items (): match = re . compile ( k ) text = re . sub ( match , v , text ) return text lexos . scrubber . replace . tag_map ( text , map , remove_comments = True , remove_doctype = True , remove_whitespace = False ) \u00a4 Handle tags that are found in the text. Parameters: Name Type Description Default text str The text in which tags will be replaced. required remove_comments bool Whether to remove comments. True remove_doctype bool Whether to remove the doctype or xml declaration. True remove_whitespace bool Whether to remove whitespace. False Returns: Type Description str The text after tags have been replaced. Source code in lexos\\scrubber\\replace.py def tag_map ( text : str , # xmlhandlingoptions: List[dict], map : Dict [ str ], remove_comments : bool = True , remove_doctype : bool = True , remove_whitespace : bool = False ) -> str : \"\"\"Handle tags that are found in the text. Args: text (str): The text in which tags will be replaced. remove_comments (bool): Whether to remove comments. remove_doctype (bool): Whether to remove the doctype or xml declaration. remove_whitespace (bool): Whether to remove whitespace. Returns: str: The text after tags have been replaced. \"\"\" if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) # Remove extra white space if remove_doctype : doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations text = re . sub ( r \"(<\\?.*?>)\" , \"\" , text ) # Remove xml declarations if remove_comments : text = re . sub ( r \"(<!--.*?-->)\" , \"\" , text ) # Remove comments # This matches the DOCTYPE and all internal entity declarations doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations # Visit each tag: for tag , opts in map . items (): action = opts [ \"action\" ] attribute = opts [ \"attribute\" ] text = process_tag_replace_options ( text , tag , action , attribute ) # One last catch-all removes extra whitespace from all the removed tags if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) return text lexos . scrubber . replace . urls ( text , repl = '_URL_' ) \u00a4 Replace all URLs in text with repl . Parameters: Name Type Description Default text str The text in which urls will be replaced. required repl str The replacement value for urls. '_URL_' Returns: Type Description str The text with urls replaced. Source code in lexos\\scrubber\\replace.py def urls ( text : str , repl : str = \"_URL_\" ) -> str : \"\"\"Replace all URLs in `text` with `repl`. Args: text (str): The text in which urls will be replaced. repl (str): The replacement value for urls. Returns: str: The text with urls replaced. \"\"\" return resources . RE_SHORT_URL . sub ( repl , resources . RE_URL . sub ( repl , text )) lexos . scrubber . replace . user_handles ( text , repl = '_USER_' ) \u00a4 Replace all (Twitter-style) user handles in text with repl . Parameters: Name Type Description Default text str The text in which user handles will be replaced. required repl str The replacement value for user handles. '_USER_' Returns: Type Description str The text with user handles replaced. Source code in lexos\\scrubber\\replace.py def user_handles ( text : str , repl : str = \"_USER_\" ) -> str : \"\"\"Replace all (Twitter-style) user handles in `text` with `repl`. Args: text (str): The text in which user handles will be replaced. repl (str): The replacement value for user handles. Returns: str: The text with user handles replaced. \"\"\" return resources . RE_USER_HANDLE . sub ( repl , text )","title":"Replace"},{"location":"api/scrubber/replace/#replace","text":"The replace component of Scrubber contains a set of functions for replacing strings and patterns in text. Important Some functions have the same names as functions in the remove component. To distinguish them in the registry, replace functions with the same names are prefixed with re_ . When loaded into a script, they can be given any name the user desires.","title":"Replace"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.currency_symbols","text":"Replace all currency symbols in text with repl . Parameters: Name Type Description Default text str The text in which currency symbols will be replaced. required repl str The replacement value for currency symbols. '_CUR_' Returns: Type Description str The text with currency symbols replaced. Source code in lexos\\scrubber\\replace.py def currency_symbols ( text : str , repl : str = \"_CUR_\" ) -> str : \"\"\"Replace all currency symbols in `text` with `repl`. Args: text (str): The text in which currency symbols will be replaced. repl (str): The replacement value for currency symbols. Returns: str: The text with currency symbols replaced. \"\"\" return resources . RE_CURRENCY_SYMBOL . sub ( repl , text )","title":"currency_symbols()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.digits","text":"Replace all digits in text with repl . Parameters: Name Type Description Default text str The text in which digits will be replaced. required repl str The replacement value for digits. '_DIGIT_' Returns: Type Description str The text with digits replaced. Source code in lexos\\scrubber\\replace.py def digits ( text : str , repl : str = \"_DIGIT_\" ) -> str : \"\"\"Replace all digits in `text` with `repl`. Args: text (str): The text in which digits will be replaced. repl (str): The replacement value for digits. Returns: str: The text with digits replaced. \"\"\" return resources . RE_NUMBER . sub ( repl , text )","title":"digits()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emails","text":"Replace all email addresses in text with repl . Parameters: Name Type Description Default text str The text in which emails will be replaced. required repl str The replacement value for emails. '_EMAIL_' Returns: Type Description str The text with emails replaced. Source code in lexos\\scrubber\\replace.py def emails ( text : str , repl : str = \"_EMAIL_\" ) -> str : \"\"\"Replace all email addresses in `text` with `repl`. Args: text (str): The text in which emails will be replaced. repl (str): The replacement value for emails. Returns: str: The text with emails replaced. \"\"\" return resources . RE_EMAIL . sub ( repl , text )","title":"emails()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.emojis","text":"Replace all emoji and pictographs in text with repl . Parameters: Name Type Description Default text str The text in which emojis will be replaced. required repl str The replacement value for emojis. '_EMOJI_' Returns: Type Description str The text with emojis replaced. Note If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! Source code in lexos\\scrubber\\replace.py def emojis ( text : str , repl : str = \"_EMOJI_\" ) -> str : \"\"\" Replace all emoji and pictographs in `text` with `repl`. Args: text (str): The text in which emojis will be replaced. repl (str): The replacement value for emojis. Returns: str: The text with emojis replaced. Note: If your Python has a narrow unicode build (\"USC-2\"), only dingbats and miscellaneous symbols are replaced because Python isn't able to represent the unicode data for things like emoticons. Sorry! \"\"\" return resources . RE_EMOJI . sub ( repl , text )","title":"emojis()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.hashtags","text":"Replace all hashtags in text with repl . Parameters: Name Type Description Default text str The text in which hashtags will be replaced. required repl str The replacement value for hashtags. '_HASHTAG_' Returns: Type Description str The text with currency hashtags replaced. Source code in lexos\\scrubber\\replace.py def hashtags ( text : str , repl : str = \"_HASHTAG_\" ) -> str : \"\"\"Replace all hashtags in `text` with `repl`. Args: text (str): The text in which hashtags will be replaced. repl (str): The replacement value for hashtags. Returns: str: The text with currency hashtags replaced. \"\"\" return resources . RE_HASHTAG . sub ( repl , text )","title":"hashtags()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.pattern","text":"Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which a pattern or pattern will be replaced. required pattern Union[dict, Collection[dict]] (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). required Returns: Type Description str The text with pattern(s) replaced. Source code in lexos\\scrubber\\replace.py def pattern ( text : str , * , pattern : Union [ dict , Collection [ dict ]] ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which a pattern or pattern will be replaced. pattern: (Union[dict, Collection[dict]]): A dictionary or list of dictionaries containing the pattern(s) and replacement(s). Returns: str: The text with pattern(s) replaced. \"\"\" pattern = utils . ensure_list ( pattern ) for pat in pattern : k = str ( * pat ) match = re . compile ( k ) text = re . sub ( match , pat [ k ], text ) return text","title":"pattern()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.phone_numbers","text":"Replace all phone numbers in text with repl . Parameters: Name Type Description Default text str The text in which phone numbers will be replaced. required repl str The replacement value for phone numbers. '_PHONE_' Returns: Type Description str The text with phone numbers replaced. Source code in lexos\\scrubber\\replace.py def phone_numbers ( text : str , repl : str = \"_PHONE_\" ) -> str : \"\"\"Replace all phone numbers in `text` with `repl`. Args: text (str): The text in which phone numbers will be replaced. repl (str): The replacement value for phone numbers. Returns: str: The text with phone numbers replaced. \"\"\" return resources . RE_PHONE_NUMBER . sub ( repl , text )","title":"phone_numbers()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.process_tag_replace_options","text":"Replace html-style tags in text files according to user options. Parameters: Name Type Description Default orig_text str The user's text containing the original tag. required tag str The particular tag to be processed. required action str A string specifying the action to be performed on the tag. required attribute str Replacement value for tag when \"replace_with_attribute\" is specified. required Action options are required - \"remove_tag\" Remove the tag required - \"remove_element\" Remove the element and contents required - \"replace_element\" Replace the tag with the specified attribute required Returns: Type Description str The text after the specified tag is processed. Source code in lexos\\scrubber\\replace.py def process_tag_replace_options ( orig_text : str , tag : str , action : str , attribute : str ) -> str : \"\"\"Replace html-style tags in text files according to user options. Args: orig_text: The user's text containing the original tag. tag: The particular tag to be processed. action: A string specifying the action to be performed on the tag. attribute: Replacement value for tag when \"replace_with_attribute\" is specified. Action options are: - \"remove_tag\": Remove the tag - \"remove_element\": Remove the element and contents - \"replace_element\": Replace the tag with the specified attribute Returns: str: The text after the specified tag is processed. Note: The replacement of a tag with the value of an attribute may not be supported. This needs a second look. \"\"\" if action == \"remove_tag\" : # searching for variants this specific tag: <tag> ... pattern = re . compile ( r '<(?:' + tag + r '(?=\\s)(?!(?:[^>\" \\' ]|\"[^\"]*\"| \\' [^ \\' ]* \\' )*?(?<=\\s)' r '\\s*=)(?!\\s*/?>)\\s+(?:\".*?\"| \\' .*? \\' |[^>]*?)+|/?' + tag + r '\\s*/?)>' , re . MULTILINE | re . DOTALL | re . UNICODE ) # substitute all matching patterns with one space processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"remove_element\" : # <[whitespaces] TAG [SPACE attributes]> contents </[whitespaces]TAG> # as applied across newlines, (re.MULTILINE), on re.UNICODE, # and .* includes newlines (re.DOTALL) pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \"( .+?>|>).+?</\\s*\" + re . escape ( tag ) + \">\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , \" \" , orig_text ) elif action == \"replace_element\" : pattern = re . compile ( r \"<\\s*\" + re . escape ( tag ) + r \".*?>.+?</\\s*\" + re . escape ( tag ) + \".*?>\" , re . MULTILINE | re . DOTALL | re . UNICODE ) processed_text = re . sub ( pattern , attribute , orig_text ) else : processed_text = orig_text # Leave Tag Alone return processed_text","title":"process_tag_replace_options()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.punctuation","text":"Replace punctuation from text . Replaces all instances of punctuation (or a subset thereof specified by only ) with whitespace. Parameters: Name Type Description Default text str The text in which punctuation will be replaced. required exclude Optional[str | Collection[str]] Remove all punctuation except designated characters. None only Optional[str | Collection[str]] Remove only those punctuation marks specified here. For example, \".\" removes only periods, while [\",\", \";\", \":\"] removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. None Returns: Type Description str str Note When only=None , Python's built-in str.translate() is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. Source code in lexos\\scrubber\\replace.py def punctuation ( text : str , * , exclude : Optional [ str | Collection [ str ]] = None , only : Optional [ str | Collection [ str ]] = None , ) -> str : \"\"\"Replace punctuation from `text`. Replaces all instances of punctuation (or a subset thereof specified by `only`) with whitespace. Args: text (str): The text in which punctuation will be replaced. exclude: Remove all punctuation except designated characters. only: Remove only those punctuation marks specified here. For example, `\".\"` removes only periods, while `[\",\", \";\", \":\"]` removes commas, semicolons, and colons; if None, all unicode punctuation marks are removed. Returns: str Note: When `only=None`, Python's built-in `str.translate()` is used; otherwise, a regular expression is used. The former's performance can be up to an order of magnitude faster. \"\"\" if only is not None : only = utils . to_collection ( only , val_type = str , col_type = set ) return re . sub ( \"[ {} ]+\" . format ( re . escape ( \"\" . join ( only ))), \" \" , text ) else : if exclude : exclude = utils . ensure_list ( exclude ) translation_table = dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) and chr ( i ) not in exclude ), \" \" ) else : translation_table = resources . PUNCT_TRANSLATION_TABLE return text . translate ( translation_table )","title":"punctuation()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.special_characters","text":"Replace strings from text using a regex pattern. Parameters: Name Type Description Default text str The text in which special characters will be replaced. required is_html bool Whether to replace HTML entities. False ruleset dict A dict containing the special characters to match and their replacements. None Returns: Type Description str str Source code in lexos\\scrubber\\replace.py def special_characters ( text : str , * , is_html : bool = False , ruleset : dict = None , ) -> str : \"\"\"Replace strings from `text` using a regex pattern. Args: text (str): The text in which special characters will be replaced. is_html (bool): Whether to replace HTML entities. ruleset (dict): A dict containing the special characters to match and their replacements. Returns: str \"\"\" if is_html : text = html . unescape ( text ) else : for k , v in ruleset . items (): match = re . compile ( k ) text = re . sub ( match , v , text ) return text","title":"special_characters()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.tag_map","text":"Handle tags that are found in the text. Parameters: Name Type Description Default text str The text in which tags will be replaced. required remove_comments bool Whether to remove comments. True remove_doctype bool Whether to remove the doctype or xml declaration. True remove_whitespace bool Whether to remove whitespace. False Returns: Type Description str The text after tags have been replaced. Source code in lexos\\scrubber\\replace.py def tag_map ( text : str , # xmlhandlingoptions: List[dict], map : Dict [ str ], remove_comments : bool = True , remove_doctype : bool = True , remove_whitespace : bool = False ) -> str : \"\"\"Handle tags that are found in the text. Args: text (str): The text in which tags will be replaced. remove_comments (bool): Whether to remove comments. remove_doctype (bool): Whether to remove the doctype or xml declaration. remove_whitespace (bool): Whether to remove whitespace. Returns: str: The text after tags have been replaced. \"\"\" if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) # Remove extra white space if remove_doctype : doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations text = re . sub ( r \"(<\\?.*?>)\" , \"\" , text ) # Remove xml declarations if remove_comments : text = re . sub ( r \"(<!--.*?-->)\" , \"\" , text ) # Remove comments # This matches the DOCTYPE and all internal entity declarations doctype = re . compile ( r \"<!DOCTYPE.*?>\" , re . DOTALL ) text = re . sub ( doctype , \"\" , text ) # Remove DOCTYPE declarations # Visit each tag: for tag , opts in map . items (): action = opts [ \"action\" ] attribute = opts [ \"attribute\" ] text = process_tag_replace_options ( text , tag , action , attribute ) # One last catch-all removes extra whitespace from all the removed tags if remove_whitespace : text = re . sub ( r \"[\\n\\s\\t\\v ]+\" , \" \" , text , re . UNICODE ) return text","title":"tag_map()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.urls","text":"Replace all URLs in text with repl . Parameters: Name Type Description Default text str The text in which urls will be replaced. required repl str The replacement value for urls. '_URL_' Returns: Type Description str The text with urls replaced. Source code in lexos\\scrubber\\replace.py def urls ( text : str , repl : str = \"_URL_\" ) -> str : \"\"\"Replace all URLs in `text` with `repl`. Args: text (str): The text in which urls will be replaced. repl (str): The replacement value for urls. Returns: str: The text with urls replaced. \"\"\" return resources . RE_SHORT_URL . sub ( repl , resources . RE_URL . sub ( repl , text ))","title":"urls()"},{"location":"api/scrubber/replace/#lexos.scrubber.replace.user_handles","text":"Replace all (Twitter-style) user handles in text with repl . Parameters: Name Type Description Default text str The text in which user handles will be replaced. required repl str The replacement value for user handles. '_USER_' Returns: Type Description str The text with user handles replaced. Source code in lexos\\scrubber\\replace.py def user_handles ( text : str , repl : str = \"_USER_\" ) -> str : \"\"\"Replace all (Twitter-style) user handles in `text` with `repl`. Args: text (str): The text in which user handles will be replaced. repl (str): The replacement value for user handles. Returns: str: The text with user handles replaced. \"\"\" return resources . RE_USER_HANDLE . sub ( repl , text )","title":"user_handles()"},{"location":"api/scrubber/resources/","text":"Resources \u00a4 The resources component of Scrubber contains a set of functions for replacing strings and patterns in text. lexos.scrubber.resources.HTMLTextExtractor ( HTMLParser ) \u00a4 Simple subclass of :class: html.parser.HTMLParser . Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via HTMLTextExtractor.get_text() . Note Users probably shouldn't deal with this class directly; instead, use :func: remove.remove_html_tags()`. Source code in lexos\\scrubber\\resources.py class HTMLTextExtractor ( html . parser . HTMLParser ): \"\"\"Simple subclass of :class:`html.parser.HTMLParser`. Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via `HTMLTextExtractor.get_text()`. Note: Users probably shouldn't deal with this class directly; instead, use `:func:`remove.remove_html_tags()`. \"\"\" def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = [] def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data ) def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip () __init__ ( self ) special \u00a4 Initialize the parser. Source code in lexos\\scrubber\\resources.py def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = [] get_text ( self , sep = '' ) \u00a4 Return the collected text. Source code in lexos\\scrubber\\resources.py def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip () handle_data ( self , data ) \u00a4 Handle data elements. Source code in lexos\\scrubber\\resources.py def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data ) lexos . scrubber . resources . _get_punct_translation_table () private \u00a4 Get the punctuation translation table. Source code in lexos\\scrubber\\resources.py @functools . lru_cache ( maxsize = None ) def _get_punct_translation_table (): \"\"\"Get the punctuation translation table.\"\"\" return dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) ), \" \" ) lexos . scrubber . resources . __getattr__ ( name ) \u00a4 Call an attribute lookup from a table. Source code in lexos\\scrubber\\resources.py def __getattr__ ( name : str ) -> Any : \"\"\"Call an attribute lookup from a table.\"\"\" if name == \"PUNCT_TRANSLATION_TABLE\" : return _get_punct_translation_table () else : raise AttributeError ( f \"module { __name__ !r} has no attribute { name !r} \" ) Constants \u00a4 There are also a number of constants: QUOTE_TRANSLATION_TABLE RE_BRACKETS_CURLY RE_BRACKETS_ROUND RE_BRACKETS_SQUARE RE_BULLET_POINTS RE_CURRENCY_SYMBOL RE_EMAIL RE_EMOJI RE_HASHTAG RE_HYPHENATED_WORD RE_LINEBREAK RE_NONBREAKING_SPACE RE_NUMBER RE_PHONE_NUMBER RE_SHORT_URL RE_TAB RE_URL RE_USER_HANDLE RE_ZWSP","title":"Resources"},{"location":"api/scrubber/resources/#resources","text":"The resources component of Scrubber contains a set of functions for replacing strings and patterns in text.","title":"Resources"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor","text":"Simple subclass of :class: html.parser.HTMLParser . Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via HTMLTextExtractor.get_text() . Note Users probably shouldn't deal with this class directly; instead, use :func: remove.remove_html_tags()`. Source code in lexos\\scrubber\\resources.py class HTMLTextExtractor ( html . parser . HTMLParser ): \"\"\"Simple subclass of :class:`html.parser.HTMLParser`. Collects data elements (non-tag, -comment, -pi, etc. elements) fed to the parser, then make them available as stripped, concatenated text via `HTMLTextExtractor.get_text()`. Note: Users probably shouldn't deal with this class directly; instead, use `:func:`remove.remove_html_tags()`. \"\"\" def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = [] def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data ) def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip ()","title":"HTMLTextExtractor"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.__init__","text":"Initialize the parser. Source code in lexos\\scrubber\\resources.py def __init__ ( self ): \"\"\"Initialize the parser.\"\"\" super () . __init__ () self . data = []","title":"__init__()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.get_text","text":"Return the collected text. Source code in lexos\\scrubber\\resources.py def get_text ( self , sep : str = \"\" ) -> str : \"\"\"Return the collected text.\"\"\" return sep . join ( self . data ) . strip ()","title":"get_text()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.HTMLTextExtractor.handle_data","text":"Handle data elements. Source code in lexos\\scrubber\\resources.py def handle_data ( self , data ): \"\"\"Handle data elements.\"\"\" self . data . append ( data )","title":"handle_data()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources._get_punct_translation_table","text":"Get the punctuation translation table. Source code in lexos\\scrubber\\resources.py @functools . lru_cache ( maxsize = None ) def _get_punct_translation_table (): \"\"\"Get the punctuation translation table.\"\"\" return dict . fromkeys ( ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( \"P\" ) ), \" \" )","title":"_get_punct_translation_table()"},{"location":"api/scrubber/resources/#lexos.scrubber.resources.__getattr__","text":"Call an attribute lookup from a table. Source code in lexos\\scrubber\\resources.py def __getattr__ ( name : str ) -> Any : \"\"\"Call an attribute lookup from a table.\"\"\" if name == \"PUNCT_TRANSLATION_TABLE\" : return _get_punct_translation_table () else : raise AttributeError ( f \"module { __name__ !r} has no attribute { name !r} \" )","title":"__getattr__()"},{"location":"api/scrubber/resources/#constants","text":"There are also a number of constants: QUOTE_TRANSLATION_TABLE RE_BRACKETS_CURLY RE_BRACKETS_ROUND RE_BRACKETS_SQUARE RE_BULLET_POINTS RE_CURRENCY_SYMBOL RE_EMAIL RE_EMOJI RE_HASHTAG RE_HYPHENATED_WORD RE_LINEBREAK RE_NONBREAKING_SPACE RE_NUMBER RE_PHONE_NUMBER RE_SHORT_URL RE_TAB RE_URL RE_USER_HANDLE RE_ZWSP","title":"Constants"},{"location":"api/scrubber/scrubber/","text":"Scrubber \u00a4 The scrubber component of Scrubber contains a class for managing scrubbing pipelines. lexos.scrubber.scrubber.Scrubber \u00a4 Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) Source code in lexos\\scrubber\\scrubber.py class Scrubber : \"\"\"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) \"\"\" def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline ) def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts __init__ ( self ) special \u00a4 Initialize the Scrubber class. Source code in lexos\\scrubber\\scrubber.py def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None add_pipeline ( self , * funcs ) \u00a4 Add a pipeline. Parameters: Name Type Description Default *funcs Callable[[str], str] The functions to add to the pipeline. () Source code in lexos\\scrubber\\scrubber.py def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) get_pipeline ( self ) \u00a4 Return a tuple representation of the pipeline. Source code in lexos\\scrubber\\scrubber.py def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) scrub ( self , data ) \u00a4 Scrub a text or list of texts. Parameters: Name Type Description Default data Union[List[str], str] The text or list of texts to scrub. required Returns: Type Description list A list of scrubbed texts. Source code in lexos\\scrubber\\scrubber.py def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts set_pipeline ( self , pipeline ) \u00a4 Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of get_pipeline() , rather than passing callables. Parameters: Name Type Description Default pipeline tuple A tuple of functions. required Source code in lexos\\scrubber\\scrubber.py def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline )","title":"Scrubber"},{"location":"api/scrubber/scrubber/#scrubber","text":"The scrubber component of Scrubber contains a class for managing scrubbing pipelines.","title":"Scrubber"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber","text":"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) Source code in lexos\\scrubber\\scrubber.py class Scrubber : \"\"\"Scrubber class. Sample usage: scrubber = Scrubber() scrubber.to_lower(doc) \"\"\" def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs ) def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline ) def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline ) def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts","title":"Scrubber"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.__init__","text":"Initialize the Scrubber class. Source code in lexos\\scrubber\\scrubber.py def __init__ ( self ): \"\"\"Initialize the Scrubber class.\"\"\" self . texts = [] self . pipeline = None","title":"__init__()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.add_pipeline","text":"Add a pipeline. Parameters: Name Type Description Default *funcs Callable[[str], str] The functions to add to the pipeline. () Source code in lexos\\scrubber\\scrubber.py def add_pipeline ( self , * funcs : Callable [[ str ], str ]): \"\"\"Add a pipeline. Args: *funcs: The functions to add to the pipeline. \"\"\" self . pipeline = pipeline . make_pipeline ( funcs )","title":"add_pipeline()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.get_pipeline","text":"Return a tuple representation of the pipeline. Source code in lexos\\scrubber\\scrubber.py def get_pipeline ( self ) -> tuple : \"\"\"Return a tuple representation of the pipeline.\"\"\" pipeline = [] for f in self . pipeline : if getfullargspec ( f ) . kwonlydefaults : pipeline . append (( f . __name__ , getfullargspec ( f ) . kwonlydefaults )) else : pipeline . append ( f . __name__ ) return tuple ( pipeline )","title":"get_pipeline()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.scrub","text":"Scrub a text or list of texts. Parameters: Name Type Description Default data Union[List[str], str] The text or list of texts to scrub. required Returns: Type Description list A list of scrubbed texts. Source code in lexos\\scrubber\\scrubber.py def scrub ( self , data : Union [ List [ str ], str ]) -> List [ str ]: \"\"\"Scrub a text or list of texts. Args: data (Union[List[str], str]): The text or list of texts to scrub. Returns: list: A list of scrubbed texts. \"\"\" for text in utils . ensure_list ( data ): self . texts . append ( self . pipeline [ 0 ]( text )) return self . texts","title":"scrub()"},{"location":"api/scrubber/scrubber/#lexos.scrubber.scrubber.Scrubber.set_pipeline","text":"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of get_pipeline() , rather than passing callables. Parameters: Name Type Description Default pipeline tuple A tuple of functions. required Source code in lexos\\scrubber\\scrubber.py def set_pipeline ( self , pipeline : tuple ): \"\"\"Set the pipeline. This is a variant of add_pipeline that takes a tuple of functions. The difference is that function names are given as strings and keyword arguments as a dictionary. This is useful if you wanted to modify the pipeline after initialisation based on the output of `get_pipeline()`, rather than passing callables. Args: pipeline (tuple): A tuple of functions. \"\"\" new_pipeline = [] for x in pipeline : if isinstance ( x , tuple ): new_pipeline . append ( new_pipeline . pipe ( eval ( x [ 0 ]), ** x [ 1 ])) else : new_pipeline . append ( eval ( x )) self . pipeline = pipeline . make_pipeline ( new_pipeline )","title":"set_pipeline()"},{"location":"api/scrubber/utils/","text":"Utils \u00a4 The utils component of Scrubber contains helper functions shared by the other components. lexos . scrubber . utils . get_tags ( text ) \u00a4 Get information about the tags in a text. Parameters: Name Type Description Default text str The text to be analyzed. required Returns: Type Description dict A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. it falls back to BeautifulSoup's parser. Source code in lexos\\scrubber\\utils.py def get_tags ( text ): \"\"\"Get information about the tags in a text. Args: text (str): The text to be analyzed. Returns: dict: A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. Note: The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. \"\"\" import json import re from xml.etree import ElementTree from natsort import humansorted tags = [] attributes = [] try : root = ElementTree . fromstring ( text ) for element in root . iter (): if re . sub ( \"{.+}\" , \"\" , element . tag ) not in tags : tags . append ( re . sub ( \"{.+}\" , \"\" , element . tag )) if element . attrib != {}: attributes . append ({ re . sub ( \"{.+}\" , \"\" , element . tag ): element . attrib }) tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) except ElementTree . ParseError : import bs4 from bs4 import BeautifulSoup soup = BeautifulSoup ( text , \"xml\" ) for e in soup : if isinstance ( e , bs4 . element . ProcessingInstruction ): e . extract () [ tags . append ( tag . name ) for tag in soup . find_all ()] [ attributes . append ({ tag . name : tag . attrs }) for tag in soup . find_all ()] tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) return { \"tags\" : tags , \"attributes\" : attributes }","title":"Utils"},{"location":"api/scrubber/utils/#utils","text":"The utils component of Scrubber contains helper functions shared by the other components.","title":"Utils"},{"location":"api/scrubber/utils/#lexos.scrubber.utils.get_tags","text":"Get information about the tags in a text. Parameters: Name Type Description Default text str The text to be analyzed. required Returns: Type Description dict A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. it falls back to BeautifulSoup's parser. Source code in lexos\\scrubber\\utils.py def get_tags ( text ): \"\"\"Get information about the tags in a text. Args: text (str): The text to be analyzed. Returns: dict: A dict with the keys \"tags\" and \"attributes\". \"Tags is a list of unique tag names in the data and \"attributes\" is a list of dicts containing the attributes and values for those tags that have attributes. Note: The procedure tries to parse the markup as well-formed XML using ETree; otherwise, it falls back to BeautifulSoup's parser. \"\"\" import json import re from xml.etree import ElementTree from natsort import humansorted tags = [] attributes = [] try : root = ElementTree . fromstring ( text ) for element in root . iter (): if re . sub ( \"{.+}\" , \"\" , element . tag ) not in tags : tags . append ( re . sub ( \"{.+}\" , \"\" , element . tag )) if element . attrib != {}: attributes . append ({ re . sub ( \"{.+}\" , \"\" , element . tag ): element . attrib }) tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) except ElementTree . ParseError : import bs4 from bs4 import BeautifulSoup soup = BeautifulSoup ( text , \"xml\" ) for e in soup : if isinstance ( e , bs4 . element . ProcessingInstruction ): e . extract () [ tags . append ( tag . name ) for tag in soup . find_all ()] [ attributes . append ({ tag . name : tag . attrs }) for tag in soup . find_all ()] tags = humansorted ( tags ) attributes = json . loads ( json . dumps ( attributes , sort_keys = True )) return { \"tags\" : tags , \"attributes\" : attributes }","title":"get_tags()"},{"location":"api/tokenizer/","text":"Tokenizer \u00a4 The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used. lexos . tokenizer . make_doc ( text , model = 'xx_sent_ud_sm' , disable = []) \u00a4 Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: object: A spaCy doc object. \"\"\" nlp = spacy . load ( model ) return nlp ( text , disable = disable ) lexos . tokenizer . make_docs ( texts , model = 'xx_sent_ud_sm' , disable = []) \u00a4 Return a list of docs from a text or list of texts. Parameters: Name Type Description Default text Union[List[str], str] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description list A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> List : \"\"\"Return a list of docs from a text or list of texts. Args: text (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: list: A list of spaCy doc objects. \"\"\" nlp = spacy . load ( model ) return list ( nlp . pipe ( utils . ensure_list ( texts ), disable = disable ))","title":"Index"},{"location":"api/tokenizer/#tokenizer","text":"The Tokenizer uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual xx_sent_ud_sm model is used.","title":"Tokenizer"},{"location":"api/tokenizer/#lexos.tokenizer.make_doc","text":"Return a doc from a text. Parameters: Name Type Description Default text str The text to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description object A spaCy doc object. Source code in lexos\\tokenizer\\__init__.py def make_doc ( text : str , model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> object : \"\"\"Return a doc from a text. Args: text (str): The text to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: object: A spaCy doc object. \"\"\" nlp = spacy . load ( model ) return nlp ( text , disable = disable )","title":"make_doc()"},{"location":"api/tokenizer/#lexos.tokenizer.make_docs","text":"Return a list of docs from a text or list of texts. Parameters: Name Type Description Default text Union[List[str], str] The text(s) to be parsed. required model object The model to be used. 'xx_sent_ud_sm' disable List[str] A list of spaCy pipeline components to disable. [] Returns: Type Description list A list of spaCy doc objects. Source code in lexos\\tokenizer\\__init__.py def make_docs ( texts : Union [ List [ str ], str ], model : object = \"xx_sent_ud_sm\" , disable : List [ str ] = []) -> List : \"\"\"Return a list of docs from a text or list of texts. Args: text (Union[List[str], str]): The text(s) to be parsed. model (object): The model to be used. disable (List[str]): A list of spaCy pipeline components to disable. Returns: list: A list of spaCy doc objects. \"\"\" nlp = spacy . load ( model ) return list ( nlp . pipe ( utils . ensure_list ( texts ), disable = disable ))","title":"make_docs()"},{"location":"api/tokenizer/extensions/","text":"Extensions \u00a4 This is a set of extensions to spaCy docs allowing custom attributes and methods. Typically, they woudld be accessed with an underscore prefix like doc._.is_fruit or doc._.get(\"is_fruit\") . Extensions are set with code like fruits = [ \"apple\" , \"pear\" , \"banana\" , \"orange\" , \"strawberry\" ] is_fruit_getter = lambda token : token . text in fruits Token . set_extension ( \"is_fruit\" , getter = is_fruit_getter ) See the spaCy custom attributes documentation for full details. lexos . tokenizer . extensions . is_fruit_getter ( token ) \u00a4 Source code in lexos\\tokenizer\\extensions.py is_fruit_getter = lambda token : token . text in fruits Note This is really a proof of concept function. A better example can be added in the future.","title":"Extensions"},{"location":"api/tokenizer/extensions/#extensions","text":"This is a set of extensions to spaCy docs allowing custom attributes and methods. Typically, they woudld be accessed with an underscore prefix like doc._.is_fruit or doc._.get(\"is_fruit\") . Extensions are set with code like fruits = [ \"apple\" , \"pear\" , \"banana\" , \"orange\" , \"strawberry\" ] is_fruit_getter = lambda token : token . text in fruits Token . set_extension ( \"is_fruit\" , getter = is_fruit_getter ) See the spaCy custom attributes documentation for full details.","title":"Extensions"},{"location":"api/tokenizer/extensions/#lexos.tokenizer.extensions.is_fruit_getter","text":"Source code in lexos\\tokenizer\\extensions.py is_fruit_getter = lambda token : token . text in fruits Note This is really a proof of concept function. A better example can be added in the future.","title":"is_fruit_getter()"},{"location":"api/tokenizer/lexosdoc/","text":"LexosDoc \u00a4 A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. lexos.tokenizer.lexosdoc.LexosDoc \u00a4 A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. Source code in lexos\\tokenizer\\lexosdoc.py class LexosDoc (): \"\"\"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called `tokenizer.py` and import them. If certain functions get used commonly, they can be turned into Doc extensions. \"\"\" def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" self . doc = doc def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self: A spaCy doc. limit: The maximum number of tokens to count. start: The index of the first token to count. end: The index of the last token to count after limit is applied. filters: A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df: Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return pd . DataFrame ( term_counts , columns = columns ) else : return term_counts def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols ) def _bool_filter ( self , token : object , filters : List [ str ]) -> bool : \"\"\"Filter a token based on a list of boolean filters. Args: token (object): A spaCy token. filters (str): A list of boolean filters (the names of spaCy token attributes). Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != []: for filter in filters : if getattr ( token , filter ): return False else : return True else : return True def _dict_filter ( self , token : object , filters : List [ Dict [ str , str ]], regex : bool = False ) -> bool : \"\"\"Filter a token based on a list of dictionary filters. Args: token (object): A spaCy token. filters (List[Dict[str, str]]): A list of filter dictionaries with keys as spaCy token attributes. regex (bool): Whether to match the dictionary value using regex. Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != {}: for filter , value in filters . items (): if regex and re . search ( re . compile ( value ), getattr ( token , filter )) is not None : return False elif getattr ( token , filter ) == value : return False else : return True else : return True __init__ ( self , doc ) special \u00a4 Initialize a LexosDoc object. Source code in lexos\\tokenizer\\lexosdoc.py def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" self . doc = doc get_term_counts ( self , limit = None , start = 0 , end = None , filters = None , regex = False , normalize = False , normalize_with_filters = False , as_df = False ) \u00a4 Get a list of word counts for each token in the doc. Parameters: Name Type Description Default self A spaCy doc. required limit int The maximum number of tokens to count. None start Any The index of the first token to count. 0 end Any The index of the last token to count after limit is applied. None filters List[Union[Dict[str, str], str]] A list of Doc attributes to ignore. None regex bool Whether to match the dictionary value using regex. False normalize bool Whether to return raw counts or relative frequencies. False normalize_with_filters bool Whether to normalize based on the number of tokens after filters are applied. False as_df Whether to return a pandas dataframe. False Returns: Type Description Union[List, pd.DataFrame] A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. Source code in lexos\\tokenizer\\lexosdoc.py def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self: A spaCy doc. limit: The maximum number of tokens to count. start: The index of the first token to count. end: The index of the last token to count after limit is applied. filters: A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df: Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return pd . DataFrame ( term_counts , columns = columns ) else : return term_counts get_token_attrs ( self ) \u00a4 Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. Source code in lexos\\tokenizer\\lexosdoc.py def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } get_tokens ( self ) \u00a4 Return a list of tokens in the doc. Source code in lexos\\tokenizer\\lexosdoc.py def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] to_dataframe ( self , cols = [ 'text' ], show_ranges = True ) \u00a4 Get a pandas dataframe of the doc attributes. Parameters: Name Type Description Default cols List[str] A list of columns to include in the dataframe. ['text'] show_ranges bool Whether to include the token start and end positions in the dataframe. True Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call LexosDoc.get_token_attrs() first to check which attributes are available for the doc. Source code in lexos\\tokenizer\\lexosdoc.py def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols )","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexosdoc","text":"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions.","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc","text":"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called tokenizer.py and import them. If certain functions get used commonly, they can be turned into Doc extensions. Source code in lexos\\tokenizer\\lexosdoc.py class LexosDoc (): \"\"\"A wrapper class for a spaCy doc which allows for extra methods. A convenience that allows you to use Doc extensions without the underscore prefix. Note: There is probably no need for this class. We can just keep a library of functions in a file called `tokenizer.py` and import them. If certain functions get used commonly, they can be turned into Doc extensions. \"\"\" def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" self . doc = doc def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self: A spaCy doc. limit: The maximum number of tokens to count. start: The index of the first token to count. end: The index of the last token to count after limit is applied. filters: A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df: Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return pd . DataFrame ( term_counts , columns = columns ) else : return term_counts def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ] def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts } def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols ) def _bool_filter ( self , token : object , filters : List [ str ]) -> bool : \"\"\"Filter a token based on a list of boolean filters. Args: token (object): A spaCy token. filters (str): A list of boolean filters (the names of spaCy token attributes). Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != []: for filter in filters : if getattr ( token , filter ): return False else : return True else : return True def _dict_filter ( self , token : object , filters : List [ Dict [ str , str ]], regex : bool = False ) -> bool : \"\"\"Filter a token based on a list of dictionary filters. Args: token (object): A spaCy token. filters (List[Dict[str, str]]): A list of filter dictionaries with keys as spaCy token attributes. regex (bool): Whether to match the dictionary value using regex. Returns: bool: Whether the token passes the filters. \"\"\" if filters and filters != {}: for filter , value in filters . items (): if regex and re . search ( re . compile ( value ), getattr ( token , filter )) is not None : return False elif getattr ( token , filter ) == value : return False else : return True else : return True","title":"LexosDoc"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.__init__","text":"Initialize a LexosDoc object. Source code in lexos\\tokenizer\\lexosdoc.py def __init__ ( self , doc : object ): \"\"\"Initialize a LexosDoc object.\"\"\" self . doc = doc","title":"__init__()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_term_counts","text":"Get a list of word counts for each token in the doc. Parameters: Name Type Description Default self A spaCy doc. required limit int The maximum number of tokens to count. None start Any The index of the first token to count. 0 end Any The index of the last token to count after limit is applied. None filters List[Union[Dict[str, str], str]] A list of Doc attributes to ignore. None regex bool Whether to match the dictionary value using regex. False normalize bool Whether to return raw counts or relative frequencies. False normalize_with_filters bool Whether to normalize based on the number of tokens after filters are applied. False as_df Whether to return a pandas dataframe. False Returns: Type Description Union[List, pd.DataFrame] A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. Source code in lexos\\tokenizer\\lexosdoc.py def get_term_counts ( self , limit : int = None , start : Any = 0 , end : Any = None , filters : List [ Union [ Dict [ str , str ], str ]] = None , regex : bool = False , normalize : bool = False , normalize_with_filters : bool = False , as_df = False ) -> Union [ List , pd . DataFrame ]: \"\"\"Get a list of word counts for each token in the doc. Args: self: A spaCy doc. limit: The maximum number of tokens to count. start: The index of the first token to count. end: The index of the last token to count after limit is applied. filters: A list of Doc attributes to ignore. regex (bool): Whether to match the dictionary value using regex. normalize (bool): Whether to return raw counts or relative frequencies. normalize_with_filters (bool): Whether to normalize based on the number of tokens after filters are applied. as_df: Whether to return a pandas dataframe. Returns: Union[List, pd.DataFrame]: A list of word count tuples for each token in the doc. Alternatively, a pandas dataframe. \"\"\" tokens = [] bool_filters = [] dict_filters = {} if filters : for filter in filters : if isinstance ( filter , dict ): dict_filters [ list ( filter . keys ())[ 0 ]] = list ( filter . values ())[ 0 ] else : bool_filters . append ( filter ) tokens = [ token . text for token in self . doc if self . _bool_filter ( token , bool_filters ) and self . _dict_filter ( token , dict_filters , regex = regex ) ] term_counts = Counter ( tokens ) . most_common ( limit )[ start : end ] columns = [ \"term\" , \"count\" ] if normalize_with_filters : normalize = True num_tokens = len ( tokens ) else : num_tokens = len ( self . doc ) if normalize : term_counts = [( x [ 0 ], x [ 1 ] / num_tokens ) for x in term_counts ] columns [ 1 ] = \"frequency\" if as_df : return pd . DataFrame ( term_counts , columns = columns ) else : return term_counts","title":"get_term_counts()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_token_attrs","text":"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. Source code in lexos\\tokenizer\\lexosdoc.py def get_token_attrs ( self ): \"\"\"Get a list of attributes for each token in the doc. Returns a dict with \"spacy_attributes\" and \"extensions\". Note: This function relies on sampling the first token in a doc to compile the list of attributes. It does not check for consistency. Currently, it is up to the user to reconcile inconsistencies between docs. \"\"\" sample = self . doc [ 0 ] attrs = sorted ([ x for x in dir ( sample ) if not x . startswith ( \"__\" ) and x != \"_\" ]) exts = sorted ([ f \"_ { x } \" for x in dir ( sample . _ ) if x not in [ \"get\" , \"has\" , \"set\" ]]) return { \"spacy_attributes\" : attrs , \"extensions\" : exts }","title":"get_token_attrs()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.get_tokens","text":"Return a list of tokens in the doc. Source code in lexos\\tokenizer\\lexosdoc.py def get_tokens ( self ): \"\"\"Return a list of tokens in the doc.\"\"\" return [ token . text for token in self . doc ]","title":"get_tokens()"},{"location":"api/tokenizer/lexosdoc/#lexos.tokenizer.lexosdoc.LexosDoc.to_dataframe","text":"Get a pandas dataframe of the doc attributes. Parameters: Name Type Description Default cols List[str] A list of columns to include in the dataframe. ['text'] show_ranges bool Whether to include the token start and end positions in the dataframe. True Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call LexosDoc.get_token_attrs() first to check which attributes are available for the doc. Source code in lexos\\tokenizer\\lexosdoc.py def to_dataframe ( self , cols : List [ str ] = [ \"text\" ], show_ranges : bool = True ) -> pd . DataFrame : \"\"\"Get a pandas dataframe of the doc attributes. Args: cols: A list of columns to include in the dataframe. show_ranges: Whether to include the token start and end positions in the dataframe. Returns a pandas dataframe of the doc attributes. Note: It is a good idea to call `LexosDoc.get_token_attrs()` first to check which attributes are available for the doc. \"\"\" rows = [] for i , token in enumerate ( self . doc ): t = [] for col in cols : t . append ( getattr ( token , col )) if show_ranges : ranges = self . doc . to_json ()[ \"tokens\" ][ i ] t . append ( ranges [ \"start\" ]) t . append ( ranges [ \"end\" ]) rows . append ( t ) if show_ranges : cols = cols + [ \"start\" , \"end\" ] return pd . DataFrame ( rows , columns = cols )","title":"to_dataframe()"},{"location":"tutorial/","text":"Overview \u00a4 This page is a rough overview of the usage of the API. Important Aspects of the API may change before the tutorial is updated. At this stage, the tutorial should only be taken as a general guideline to the API's usage.","title":"Index"},{"location":"tutorial/#overview","text":"This page is a rough overview of the usage of the API. Important Aspects of the API may change before the tutorial is updated. At this stage, the tutorial should only be taken as a general guideline to the API's usage.","title":"Overview"},{"location":"tutorial/getting_started/","text":"Begin by importing in some modules in the Lexos API. from lexos.io.basic import Loader from lexos.scrubber.pipeline import make_pipeline , pipe from lexos.scrubber.registry import scrubber_components , load_components Warning If you are working in a Jupyter notebook and you cannot import the Lexos API modules, see the advice on the installation page. Here are some explanations what these modules do: The io module contains IO functions. Right now, there is a \"basic\" Loader class that takes a file path, url, list of file paths or urls, or a directory name indicating where the source data is. More sophisticated loaders can be created later. The scrubber module consists of thematic \"components\": normalize , remove , replace , and so on. Each component has a number of functions, such as converting to lower case, removing digits, stripping tags, etc. Component functions are registered in a registry. They can be loaded into memory as needed and applied to texts in any order. Loading and scrubbing comprise the main part of the \"text preparation\" portion of the Lexos workflow. Parsing and analytical functions have separate modules which will be dealt with later in this tutorial.","title":"Getting Started"},{"location":"tutorial/loading_texts/","text":"A typical workflow would create a lexos.io.basic.Loader object and call lexos.io.basic.Loader.load to load the data from disk or download it from the internet. You can access all loaded texts by calling Loader.texts . Note It is more efficient simply to use Python's open() to load texts into a list if you know the file's encoding. Currently, the main advantage of the Loader class is that it automatically coerces the data to Unicode. At this stage of development, the user or application developer is expected to maintain their data folders and know their file locations. More sophisticated project/corpus management methods could be added to the API at a later date. Here is a sample of the code for loading a single text file: # Data source data = \"tests/test_data/Austen_Pride.txt\" # Create the loader and load the data loader = Loader () loader . load ( data ) # Print the first text in the Loader text = loader . texts [ 0 ] print ( text ) lexos.io.basic.Loader accepts filepaths, urls, or lists of either. If urls are submitted, the content will be downloaded automatically.","title":"Loading Texts"},{"location":"tutorial/scrubbing_texts/","text":"About Scrubber \u00a4 Scrubber can be defined as a destructive preprocessor. In other words, it changes the text as loaded in ways that potentially make mapping the results onto the original text potentially impossible. It is therefore best used before other procedures so that the scrubbed text is essentially treated as the \"original\" text. The importance of this will be seen below when we see the implementation of the tokeniser. But, to be short, the Lexos API differs from the web app in that Scrubber does not play a role in tokenisation by separating tokens by whitespace. Scrubbing works by applying a single function or a pipeline of functions to the text. As a reminder, we need to load the scrubber components registry with from lexos.scrubber.registry import scrubber_components, load_components . Loading Scrubber Components \u00a4 We can now load the components we want. We can load them individually, as in the first example below, or we can specify multiple components in a tuple, as in the second example. In both cases, the returned variable is a function, which we can then feed to a scrubbing pipeline. # Load a component from the registry lower_case = scrubber_components . get ( \"lower_case\" ) # Or, if you want to do several at once... title_case , remove_digits = load_components (( \"title_case\" , \"remove_digits\" )) In the first example, a component is loaded using the registry's built-in get method. It is also possible to load a single component with the the lexos.scrubber.registry.load_component helper method. This parallels lexos.scrubber.registry.load_components for multiple components and is possibly easier to remember. Making a Pipeline \u00a4 Now let's make the pipeline. We simply feed our component function names into the make_pipeline() function in the order we want them to be implemented. Notice that remove_digits has to be passed through the pipe() function. This is because lexos.scrubber.remove.digits requires extra arguments, and pipe() allows those arguments to be passed to the main pipeline function. # Make the pipeline scrub = make_pipeline ( lower_case , title_case , pipe ( remove_digits , only = [ \"1\" ]) ) The value returned is a function that implements the full pipeline when called on a text, as shown below. # Scrub the text scrubbed_text = scrub ( \"Lexos is the number 12 text analysis tool.\" ) This will return \"Lexos Is The Number 2 Text Analysis Tool\". Note You can also call component functions without a pipeline. For instance, scrubbed_text = remove_digits(\"Lexos123\", only=[\"2\", \"3\"]) will return \"Lexos1\". Custom Scrubbing Components \u00a4 The title_case function in the example above will not work because title_case is a custom component. To use it, we need to add it to the registry. # Define the custom function def title_case ( text : str ) -> str : \"\"\"Our custom function to convert text to title case.\"\"\" return text . title () # Register the custom function scrubber_components . register ( \"title_case\" , func = title_case ) Users can add whatever scrubbing functions they want. For development purposes, we can start by creating custom functions, and, if we use them a lot, migrate them to the permanent registry. Important To use a custom scrubbing function, you must register it before you call lexos.scrubber.registry.load_component or lexos.scrubber.registry.load_components .","title":"Scrubbing Texts"},{"location":"tutorial/scrubbing_texts/#about-scrubber","text":"Scrubber can be defined as a destructive preprocessor. In other words, it changes the text as loaded in ways that potentially make mapping the results onto the original text potentially impossible. It is therefore best used before other procedures so that the scrubbed text is essentially treated as the \"original\" text. The importance of this will be seen below when we see the implementation of the tokeniser. But, to be short, the Lexos API differs from the web app in that Scrubber does not play a role in tokenisation by separating tokens by whitespace. Scrubbing works by applying a single function or a pipeline of functions to the text. As a reminder, we need to load the scrubber components registry with from lexos.scrubber.registry import scrubber_components, load_components .","title":"About Scrubber"},{"location":"tutorial/scrubbing_texts/#loading-scrubber-components","text":"We can now load the components we want. We can load them individually, as in the first example below, or we can specify multiple components in a tuple, as in the second example. In both cases, the returned variable is a function, which we can then feed to a scrubbing pipeline. # Load a component from the registry lower_case = scrubber_components . get ( \"lower_case\" ) # Or, if you want to do several at once... title_case , remove_digits = load_components (( \"title_case\" , \"remove_digits\" )) In the first example, a component is loaded using the registry's built-in get method. It is also possible to load a single component with the the lexos.scrubber.registry.load_component helper method. This parallels lexos.scrubber.registry.load_components for multiple components and is possibly easier to remember.","title":"Loading Scrubber Components"},{"location":"tutorial/scrubbing_texts/#making-a-pipeline","text":"Now let's make the pipeline. We simply feed our component function names into the make_pipeline() function in the order we want them to be implemented. Notice that remove_digits has to be passed through the pipe() function. This is because lexos.scrubber.remove.digits requires extra arguments, and pipe() allows those arguments to be passed to the main pipeline function. # Make the pipeline scrub = make_pipeline ( lower_case , title_case , pipe ( remove_digits , only = [ \"1\" ]) ) The value returned is a function that implements the full pipeline when called on a text, as shown below. # Scrub the text scrubbed_text = scrub ( \"Lexos is the number 12 text analysis tool.\" ) This will return \"Lexos Is The Number 2 Text Analysis Tool\". Note You can also call component functions without a pipeline. For instance, scrubbed_text = remove_digits(\"Lexos123\", only=[\"2\", \"3\"]) will return \"Lexos1\".","title":"Making a Pipeline"},{"location":"tutorial/scrubbing_texts/#custom-scrubbing-components","text":"The title_case function in the example above will not work because title_case is a custom component. To use it, we need to add it to the registry. # Define the custom function def title_case ( text : str ) -> str : \"\"\"Our custom function to convert text to title case.\"\"\" return text . title () # Register the custom function scrubber_components . register ( \"title_case\" , func = title_case ) Users can add whatever scrubbing functions they want. For development purposes, we can start by creating custom functions, and, if we use them a lot, migrate them to the permanent registry. Important To use a custom scrubbing function, you must register it before you call lexos.scrubber.registry.load_component or lexos.scrubber.registry.load_components .","title":"Custom Scrubbing Components"},{"location":"tutorial/the_document_term_matrix/","text":"About the Document-Term Matrix \u00a4 A document-term matrix (DTM) is the standard interface for analysis and information of document data. It consists in its raw form of a list of token counts per document in the corpus. Each unique token form is called a term. Thus it is really a list of term counts per document, arranged as matrix. In the Lexos App, sklearn's CountVectorizer is used to produce the DTM. In the Lexos API, Textacy's Vectorizer is used. Here is a vanilla implication to get a DTM containing the raw term counts for each document. from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = \"linear\" , idf_type = None , norm = None ) tokenised_docs = ( LexosDoc ( doc ) . get_tokens () for doc in docs ) doc_term_matrix = vectorizer . fit_transform ( tokenised_docs ) The main advantage of this procedures is that sklearn's CountVectorizer employs a regular expression pattern to tokenise the text and has very limited functionality to implement the kind of language-specific knowledge available in a document tokenised with a language model. Getting Term Counts and Frequencies \u00a4 The Lexos API provides an easy method of using the Vectorizer to retrieve term counts or frequencies from a single document and returning the results in a pandas dataframe. from lexos.dtm import get_doc_term_counts df = get_doc_term_counts ( docs , as_df = True ) Setting normalize=True will return relative frequencies instead of raw counts. lexos.dtm.get_doc_term_counts has various parameters for limiting and filtering the output based on token labels or regex patterns. The DTM Class \u00a4 Lexos also wraps Textacy's Vectorizer in the lexos.dtm.DTM class with greater functionality. You can import it with from lexos.dtm import DTM Most work will leverage the lexos.dtm.DTM class to builds a document-term matrix and provide methods for manipulating the information held therein. The standard method of creating a DTM object is as follows: labels = [ \"Pride_and_Prejudice\" , \"Sense_and_Sensibility\" ] dtm = DTM ( docs , labels ) The labels are human-readable names for the documents which would otherwise be referenced by numeric indices. Instantiating a lexos.dtm.DTM object creates a vectorizer. By default, this is a Textacy Vectorizer object with parameters set to produce raw counts. The vectorizer settings can be viewed by calling lexos.dtm.vectorizer_settings and they can be adjusted by calling lexos.dtm.DTM.set_vectorizer . The vectorizer is an object, so you can also inspect individual vectorizer settings with calls like lexos.dtm.vectorizer.idf_type . Important After changing the settings of an object, you must call lexos.dtm.DTM.build to rebuild the document-term matrix. Getting a Term Counts Table \u00a4 The lexos.dtm.DTM class method for getting a table of raw term counts is lexos.dtm.DTM.get_table . You can also call lexos.dtm.table , which will return a table based on state after the last time lexos.dtm.DTM.build was called. The options are as follows: # Get a table of counts with documents as columns and terms as rows df = dtm . get_table () # Get a table of counts with terms as columns and documents as rows df = dtm . get_table ( transpose = True ) The second option is equivalent to calling dtm.get_table().T , using pandas notation. The lexos.dtm.DTM.get_table output is generally intended to allow you to use the pandas API once you have the data in the form of a pandas dataframe. If you change vectorizer settings, remember to rebuild the DTM. For instance, you want to use the Lexos app's implementation of TF-IDF, you would use the following (I think): dtm . set_vectorizer ( tf_type = \"log\" , idf_type = \"smooth\" , norm = \"l2\" ) dtm . build () df = dtm . get_table () Important Currently, lexos.dtm.DTM.build resets dtm.table=None , so you will need to call lexos.dtm.DTM.get_table to use the new vectorizer. This is intended to reduce overhead if an app only needs to interact directly with the vectorizer. Perhaps down the line, it might be advisable to give lexos.dtm.DTM.build a boolean parameter to allow the user to decide whether the table gets regenerated automatically. Note The Lexos culling function is now handled by the min_df parameter and extended by the max_df parameter in the vectorizer. The Lexos most frequent words function is handled by max_n_terms . But see the section below. Getting Most Frequent Terms \u00a4 In addition to max_n_terms , it is possible to use pandas to sort and slice the DTM.table after it has been built in order to get the most frequent terms. The lexos.dtm.DTM.most_frequent method is a helper function for this purpose: most_frequent = dtm . most_frequent ( max_n_terms = 25 ) It is possible to take slices (start counting from a particular index) in the DTM.table : most_frequent = dtm . most_frequent ( max_n_terms = 25 , start = 25 ) This will return terms 25 to 50. There is also an equivalent function lexos.dtm.DTM.least_frequent to get the least frequent terms in the table. Important lexos.dtm.DTM.most_frequent and lexos.dtm.DTM.least_frequent should not be used if min_df or max_df are set in the vectorizer, as this will cause the document-term matrix to be reduced twice. Getting Statistics from the DTM \u00a4 Pandas has methods for calculating the sum, mean, and median of rows in the table. However, to save users from Googling, the DTM class has the lexos.dtm.DTM.get_stats_table method that calculates these statistics and adds them to the columns in the default DTM table. stats_table = dtm . get_stats_table ([ \"sum\" , \"mean\" , \"median\" ]) Once the new dataframe is generated, it is easy to extract the data to a list with standard pandas syntax like stats_table[\"sum\"].values.tolist() . Getting Relative Frequencies \u00a4 lexos.dtm.DTM.get_freq_table converts the raw counts in the default DTM table to relative frequencies. Since the resulting values are typically floats, there is an option to set the number of digits used for rounding. frequency_table = dtm . get_freq_table ( rounding = 2 , as_percent = True ) The setting as_percent=True multiples the frequencies by 100. The default is False . Getting Lists of Terms and Term Counts \u00a4 By default, most of the lexos.dtm.DTM methods return a pandas dataframe. Two methods provide output in the form of lists. lexos.dtm.DTM.get_terms provides a simple, alphabetised list of terms in the document-term matrix. lexos.dtm.DTM.get_term_counts returns a list of tuples with terms as the first element and sums (the total number of occurrences of the term in all documents) as the second element. This method has parameters for sorting by column and direction. By default, terms are sorted by natsort.ns.LOCALE (i.e. the computer's locale is used for the sorting algorithm). This can be configured using the options in the Python natsort reference . Visualising the DTM \u00a4 Once a document-term matrix table has been generated as a pandas dataframe, it becomes possible to use any of the pandas.DataFrame.plot methods, or to export the data for use with other tools. However, the Lexos API has two built-in visualisations: word clouds and bubble charts. Word clouds can be generated for the entire DTM or for individual documents. Multiple word clouds arrange for comparison are referred to as multiclouds. Note A primitive function exists for generating word clouds in Plotly as proof of concept. However, this function is not documented because Plotly (and other interactive graphing libraries) are really not good for word clouds. It is better to output the DTM as json and parse it in d3.s for a pretty, interactive visualisation. Single Word Clouds \u00a4 It is easiest to make a word cloud from lexos.dtm.DTM.get_stats_table . Start by getting a table with the sums (total term count per document). data = dtm . get_stats_table ( \"sum\" ) Next rename the columns to the header labels expected by lexos.dtm.wordcloud.make_wordcloud . data = data . rename ({ \"terms\" : \"term\" , \"sum\" : \"count\" }, axis = 1 ) Wordclouds are generated by the Python Wordcloud library. It has various option which can be defined in a dictionary and passed to lexos.dtm.wordcloud.make_wordcloud . Figures are generated using Python's matplotlib , and its options can also be passed to lexos.dtm.wordcloud.make_wordcloud . wordcloud_opts = { \"max_words\" : 2000 , \"background_color\" : \"white\" , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" } matplotlib_opts = { \"figsize\" : ( 15 , 8 )} wordcloud = make_wordcloud ( data , wordcloud_ , figure_opts = matplotlib_opts show = True , round = 150 ) The round parameter (normally between 100 and 300) will add various degrees of rounded corners to the word cloud. By default, show=True , and the wordcloud will be plotted to the screen if the environment is appropriate. If show=False , the WordCloud object will be returned, and it can be saved or further manipulated. For instance, the word cloud above could be saved by calling wordcloud.to_file(filename) . There are various ways to plot a word cloud of a single document. For instance, you could make a copy of the DTM table with only the counts for the desired document. doc1 = dtm . table [[ \"terms\" , \"doc1\" ]] . copy () data = doc1 . rename ({ \"terms\" : \"term\" , \"doc1\" : \"count\" }, axis = 1 ) You can now submit the data to lexos.dtm.wordcloud.make_wordcloud as above. For a subset of documents, you might do the following: # Copy the table with only the desired document columns some_docs = dtm . table [[ \"terms\" , \"doc1\" , \"doc2\" ]] . copy () # Create a new column with the total for each row some_docs [ \"count\" ] = some_docs . sum ( axis = 1 ) # Make a copy with only the terms and counts data = some_docs . table [[ \"terms\" , \"count\" ]] . copy () Note lexos.dtm.wordcloud.make_wordcloud takes a number of other input formats, including raw text, but a lexos.dtm.DTM.table is by far the easiest method to generate data from pre-tokenised texts. Multiclouds \u00a4 Multiclouds are grid-organised word clouds of individual documents, which allow you to compare the document clouds side by side. The method of generating multiclouds is similar to word clouds. The basic input is a lexos.dtm.DTM.table , where one word cloud will be generated for each document. If a subset of documents is required, a similar method to that shows above can be used to limit the documents in the table. Once the data is prepared, the multiclouds are generated with a lexos.dtm.wordcloud.make_multiclouds . multiclouds = make_multiclouds ( data , wordcloud_opts , matplotlib_opts , show = True , title = \"My Title\" , labels = labels , ncols = 3 round = 150 , ) Since multicloud produce multiple subplots, there is a title parameter to give the entire figure a title and a labels parameter, which includes a list labels to be assigned to each subplot. The ncols parameter sets the number of subplots per row. If show=False , lexos.dtm.wordcloud.make_multiclouds returns a list of word clouds. These can be saved individually by calling to_file() on them, as shown above. There is not currently a method of saving the entire plot. As with word clouds, lexos.dtm.wordcloud.make_multiclouds takes a number of different input formats, but pandas dataframes are the easiest to work with. Bubble Charts \u00a4 Bubble charts (\"bubbleviz\" in the Lexos app) are produce by the following imports: from lexos.dtm.bubbleviz import BubbleChart , make_bubble_chart To generate the visualisation: # Get a table with the sums of each row sums = dtm . get_stats_table ( \"sum\" ) terms = sums [ \"term\" ] . values . tolist () area = sums [ \"count\" ] . values . tolist () colors = [ \"#5A69AF\" , \"#579E65\" , \"#F9C784\" , \"#FC944A\" , \"#F24C00\" , \"#00B825\" ] make_bubble_chart ( terms , area , limit = 100 , title = \"My Title\" , bubble_spacing = 0.1 , colors = colors , figsize = ( 15 , 15 ), show = True , filename ) lexos.dtm.bubbleviz.make_bubble_chart has a slightly simpler interface than the other visualisations. It takes a list of terms and a list of counts or frequencies (easily obtainable from a dataframe as shown), as well as other parameters that affect the appearance of the image. If a filename is provided, the image will be saved automatically.","title":"The Document-Term Matrix"},{"location":"tutorial/the_document_term_matrix/#about-the-document-term-matrix","text":"A document-term matrix (DTM) is the standard interface for analysis and information of document data. It consists in its raw form of a list of token counts per document in the corpus. Each unique token form is called a term. Thus it is really a list of term counts per document, arranged as matrix. In the Lexos App, sklearn's CountVectorizer is used to produce the DTM. In the Lexos API, Textacy's Vectorizer is used. Here is a vanilla implication to get a DTM containing the raw term counts for each document. from textacy.representations.vectorizers import Vectorizer vectorizer = Vectorizer ( tf_type = \"linear\" , idf_type = None , norm = None ) tokenised_docs = ( LexosDoc ( doc ) . get_tokens () for doc in docs ) doc_term_matrix = vectorizer . fit_transform ( tokenised_docs ) The main advantage of this procedures is that sklearn's CountVectorizer employs a regular expression pattern to tokenise the text and has very limited functionality to implement the kind of language-specific knowledge available in a document tokenised with a language model.","title":"About the Document-Term Matrix"},{"location":"tutorial/the_document_term_matrix/#getting-term-counts-and-frequencies","text":"The Lexos API provides an easy method of using the Vectorizer to retrieve term counts or frequencies from a single document and returning the results in a pandas dataframe. from lexos.dtm import get_doc_term_counts df = get_doc_term_counts ( docs , as_df = True ) Setting normalize=True will return relative frequencies instead of raw counts. lexos.dtm.get_doc_term_counts has various parameters for limiting and filtering the output based on token labels or regex patterns.","title":"Getting Term Counts and Frequencies"},{"location":"tutorial/the_document_term_matrix/#the-dtm-class","text":"Lexos also wraps Textacy's Vectorizer in the lexos.dtm.DTM class with greater functionality. You can import it with from lexos.dtm import DTM Most work will leverage the lexos.dtm.DTM class to builds a document-term matrix and provide methods for manipulating the information held therein. The standard method of creating a DTM object is as follows: labels = [ \"Pride_and_Prejudice\" , \"Sense_and_Sensibility\" ] dtm = DTM ( docs , labels ) The labels are human-readable names for the documents which would otherwise be referenced by numeric indices. Instantiating a lexos.dtm.DTM object creates a vectorizer. By default, this is a Textacy Vectorizer object with parameters set to produce raw counts. The vectorizer settings can be viewed by calling lexos.dtm.vectorizer_settings and they can be adjusted by calling lexos.dtm.DTM.set_vectorizer . The vectorizer is an object, so you can also inspect individual vectorizer settings with calls like lexos.dtm.vectorizer.idf_type . Important After changing the settings of an object, you must call lexos.dtm.DTM.build to rebuild the document-term matrix.","title":"The DTM Class"},{"location":"tutorial/the_document_term_matrix/#getting-a-term-counts-table","text":"The lexos.dtm.DTM class method for getting a table of raw term counts is lexos.dtm.DTM.get_table . You can also call lexos.dtm.table , which will return a table based on state after the last time lexos.dtm.DTM.build was called. The options are as follows: # Get a table of counts with documents as columns and terms as rows df = dtm . get_table () # Get a table of counts with terms as columns and documents as rows df = dtm . get_table ( transpose = True ) The second option is equivalent to calling dtm.get_table().T , using pandas notation. The lexos.dtm.DTM.get_table output is generally intended to allow you to use the pandas API once you have the data in the form of a pandas dataframe. If you change vectorizer settings, remember to rebuild the DTM. For instance, you want to use the Lexos app's implementation of TF-IDF, you would use the following (I think): dtm . set_vectorizer ( tf_type = \"log\" , idf_type = \"smooth\" , norm = \"l2\" ) dtm . build () df = dtm . get_table () Important Currently, lexos.dtm.DTM.build resets dtm.table=None , so you will need to call lexos.dtm.DTM.get_table to use the new vectorizer. This is intended to reduce overhead if an app only needs to interact directly with the vectorizer. Perhaps down the line, it might be advisable to give lexos.dtm.DTM.build a boolean parameter to allow the user to decide whether the table gets regenerated automatically. Note The Lexos culling function is now handled by the min_df parameter and extended by the max_df parameter in the vectorizer. The Lexos most frequent words function is handled by max_n_terms . But see the section below.","title":"Getting a Term Counts Table"},{"location":"tutorial/the_document_term_matrix/#getting-most-frequent-terms","text":"In addition to max_n_terms , it is possible to use pandas to sort and slice the DTM.table after it has been built in order to get the most frequent terms. The lexos.dtm.DTM.most_frequent method is a helper function for this purpose: most_frequent = dtm . most_frequent ( max_n_terms = 25 ) It is possible to take slices (start counting from a particular index) in the DTM.table : most_frequent = dtm . most_frequent ( max_n_terms = 25 , start = 25 ) This will return terms 25 to 50. There is also an equivalent function lexos.dtm.DTM.least_frequent to get the least frequent terms in the table. Important lexos.dtm.DTM.most_frequent and lexos.dtm.DTM.least_frequent should not be used if min_df or max_df are set in the vectorizer, as this will cause the document-term matrix to be reduced twice.","title":"Getting Most Frequent Terms"},{"location":"tutorial/the_document_term_matrix/#getting-statistics-from-the-dtm","text":"Pandas has methods for calculating the sum, mean, and median of rows in the table. However, to save users from Googling, the DTM class has the lexos.dtm.DTM.get_stats_table method that calculates these statistics and adds them to the columns in the default DTM table. stats_table = dtm . get_stats_table ([ \"sum\" , \"mean\" , \"median\" ]) Once the new dataframe is generated, it is easy to extract the data to a list with standard pandas syntax like stats_table[\"sum\"].values.tolist() .","title":"Getting Statistics from the DTM"},{"location":"tutorial/the_document_term_matrix/#getting-relative-frequencies","text":"lexos.dtm.DTM.get_freq_table converts the raw counts in the default DTM table to relative frequencies. Since the resulting values are typically floats, there is an option to set the number of digits used for rounding. frequency_table = dtm . get_freq_table ( rounding = 2 , as_percent = True ) The setting as_percent=True multiples the frequencies by 100. The default is False .","title":"Getting Relative Frequencies"},{"location":"tutorial/the_document_term_matrix/#getting-lists-of-terms-and-term-counts","text":"By default, most of the lexos.dtm.DTM methods return a pandas dataframe. Two methods provide output in the form of lists. lexos.dtm.DTM.get_terms provides a simple, alphabetised list of terms in the document-term matrix. lexos.dtm.DTM.get_term_counts returns a list of tuples with terms as the first element and sums (the total number of occurrences of the term in all documents) as the second element. This method has parameters for sorting by column and direction. By default, terms are sorted by natsort.ns.LOCALE (i.e. the computer's locale is used for the sorting algorithm). This can be configured using the options in the Python natsort reference .","title":"Getting Lists of Terms and Term Counts"},{"location":"tutorial/the_document_term_matrix/#visualising-the-dtm","text":"Once a document-term matrix table has been generated as a pandas dataframe, it becomes possible to use any of the pandas.DataFrame.plot methods, or to export the data for use with other tools. However, the Lexos API has two built-in visualisations: word clouds and bubble charts. Word clouds can be generated for the entire DTM or for individual documents. Multiple word clouds arrange for comparison are referred to as multiclouds. Note A primitive function exists for generating word clouds in Plotly as proof of concept. However, this function is not documented because Plotly (and other interactive graphing libraries) are really not good for word clouds. It is better to output the DTM as json and parse it in d3.s for a pretty, interactive visualisation.","title":"Visualising the DTM"},{"location":"tutorial/the_document_term_matrix/#single-word-clouds","text":"It is easiest to make a word cloud from lexos.dtm.DTM.get_stats_table . Start by getting a table with the sums (total term count per document). data = dtm . get_stats_table ( \"sum\" ) Next rename the columns to the header labels expected by lexos.dtm.wordcloud.make_wordcloud . data = data . rename ({ \"terms\" : \"term\" , \"sum\" : \"count\" }, axis = 1 ) Wordclouds are generated by the Python Wordcloud library. It has various option which can be defined in a dictionary and passed to lexos.dtm.wordcloud.make_wordcloud . Figures are generated using Python's matplotlib , and its options can also be passed to lexos.dtm.wordcloud.make_wordcloud . wordcloud_opts = { \"max_words\" : 2000 , \"background_color\" : \"white\" , \"contour_width\" : 0 , \"contour_color\" : \"steelblue\" } matplotlib_opts = { \"figsize\" : ( 15 , 8 )} wordcloud = make_wordcloud ( data , wordcloud_ , figure_opts = matplotlib_opts show = True , round = 150 ) The round parameter (normally between 100 and 300) will add various degrees of rounded corners to the word cloud. By default, show=True , and the wordcloud will be plotted to the screen if the environment is appropriate. If show=False , the WordCloud object will be returned, and it can be saved or further manipulated. For instance, the word cloud above could be saved by calling wordcloud.to_file(filename) . There are various ways to plot a word cloud of a single document. For instance, you could make a copy of the DTM table with only the counts for the desired document. doc1 = dtm . table [[ \"terms\" , \"doc1\" ]] . copy () data = doc1 . rename ({ \"terms\" : \"term\" , \"doc1\" : \"count\" }, axis = 1 ) You can now submit the data to lexos.dtm.wordcloud.make_wordcloud as above. For a subset of documents, you might do the following: # Copy the table with only the desired document columns some_docs = dtm . table [[ \"terms\" , \"doc1\" , \"doc2\" ]] . copy () # Create a new column with the total for each row some_docs [ \"count\" ] = some_docs . sum ( axis = 1 ) # Make a copy with only the terms and counts data = some_docs . table [[ \"terms\" , \"count\" ]] . copy () Note lexos.dtm.wordcloud.make_wordcloud takes a number of other input formats, including raw text, but a lexos.dtm.DTM.table is by far the easiest method to generate data from pre-tokenised texts.","title":"Single Word Clouds"},{"location":"tutorial/the_document_term_matrix/#multiclouds","text":"Multiclouds are grid-organised word clouds of individual documents, which allow you to compare the document clouds side by side. The method of generating multiclouds is similar to word clouds. The basic input is a lexos.dtm.DTM.table , where one word cloud will be generated for each document. If a subset of documents is required, a similar method to that shows above can be used to limit the documents in the table. Once the data is prepared, the multiclouds are generated with a lexos.dtm.wordcloud.make_multiclouds . multiclouds = make_multiclouds ( data , wordcloud_opts , matplotlib_opts , show = True , title = \"My Title\" , labels = labels , ncols = 3 round = 150 , ) Since multicloud produce multiple subplots, there is a title parameter to give the entire figure a title and a labels parameter, which includes a list labels to be assigned to each subplot. The ncols parameter sets the number of subplots per row. If show=False , lexos.dtm.wordcloud.make_multiclouds returns a list of word clouds. These can be saved individually by calling to_file() on them, as shown above. There is not currently a method of saving the entire plot. As with word clouds, lexos.dtm.wordcloud.make_multiclouds takes a number of different input formats, but pandas dataframes are the easiest to work with.","title":"Multiclouds"},{"location":"tutorial/the_document_term_matrix/#bubble-charts","text":"Bubble charts (\"bubbleviz\" in the Lexos app) are produce by the following imports: from lexos.dtm.bubbleviz import BubbleChart , make_bubble_chart To generate the visualisation: # Get a table with the sums of each row sums = dtm . get_stats_table ( \"sum\" ) terms = sums [ \"term\" ] . values . tolist () area = sums [ \"count\" ] . values . tolist () colors = [ \"#5A69AF\" , \"#579E65\" , \"#F9C784\" , \"#FC944A\" , \"#F24C00\" , \"#00B825\" ] make_bubble_chart ( terms , area , limit = 100 , title = \"My Title\" , bubble_spacing = 0.1 , colors = colors , figsize = ( 15 , 15 ), show = True , filename ) lexos.dtm.bubbleviz.make_bubble_chart has a slightly simpler interface than the other visualisations. It takes a list of terms and a list of counts or frequencies (easily obtainable from a dataframe as shown), as well as other parameters that affect the appearance of the image. If a filename is provided, the image will be saved automatically.","title":"Bubble Charts"},{"location":"tutorial/tokenising_texts/","text":"Language Models \u00a4 The tokenizer module is a big change for Lexos, as it formally separates tokenisation from preprocessing. In the Lexos app, the user employs Scrubber to massage the text into shape using his or her implicit knowledge about the text's language. Tokenisation then takes place by splitting the text according to a regular expression pattern (normally whitespace). By contrast, the Lexos API uses a language model that formalises the implicit rules and thus automates the tokenisation process. Language models can implement both rule-based and probabilistic strategies for separating document strings into tokens. Because they have built-in procedures appropriate to specific languages, language models can often do a better job of tokenisation than the approach used in the Lexos app. Important There are some trade-offs to using language models. Because the algorithm does more than split strings, processing times can be greater. In addition, tokenisation is no longer (explicitly) language agnostic. A language model is \"opinionated\" and it may overfit the data. At the same time, if no language model exist for the language being tokenised, the results may not be satisfactory. The Lexos API strategy for handling this situation is described below. Tokenised Documents \u00a4 A tokenised document can be defined as a text split into tokens in which each token is stored with any number of annotations assigned by the model. These annotations are token \"attributes\". The structure of a tokenised document can then be conceived in theory as a list of tuples like [ ( text = \"The\" , part_of_speech = \"noun\" , is_stopword = \"True\" ), ( text = \"end\" , part_of_speech = \"noun\" , is_stopword = \"False\" ), ] It is then a simple matter to iterate through the document and retrieve all the tokens that are not stopwords using a Python list comprehension. Many filtering procedures are easy to implement in this way. For languages such as Modern English, language models exist that can automatically annotate tokens with information like part of speech, lemmas, stop words, and other information. However, token attributes can also be set after the text has been tokenised. If no language model exists for the text's language, it will only be possible to tokenise using general rules, and it will not be possible to add other labels (at the tokenisation stage). But new language models, including models for historical languages, are being produced all the time, and this is a growing area of interest in DH. spaCy Docs \u00a4 The Lexos API wraps the spaCy NLP library for loading language models and tokenising texts. Because spaCy has excellent documentation and fairly wide acceptance in the DH community, it is a good tool to use under the bonnet. spaCy has a growing number of language models in a number of languages, as well as wrappers for loading models from other common NLP libraries such as Stanford Stanza. Note The architecture of the Scrubber module is partially built on top of the preprocessing functions in Textacy , which also accesses and extends spaCy. In spaCy, texts are parsed into spacy.Doc objects consisting of sequences of tokens. Note In order to formalise the difference between a text string that has been scrubbed and one that has been tokenised, we refer wherever possible to the string as a \"text\" and to the tokenised spacy.Doc object as a \"document\" (or just \"doc\"). We continue to refer to the individual items as \"documents\" if we are not concerned with their data type. Each token is spacy.Token object which stores all the token's attributes. The Lexos API wraps this procedure in the lexos.tokenizer.make_doc function: from lexos import tokenizer doc = tokenizer . make_doc ( text ) This returns a spacy.Doc object. By default the tokenizer uses spaCy's \"xx_sent_ud_sm\" language model, which has been trained for tokenisation and sentence segmentation on multiple languages. This model performs statistical sentence segmentation and possesses general rules for token segmentation. If you were making a document from a text in a language for which a more language-specific model, you would specify the model to be used. For instance, to use spaCy's small English model trained on web texts, you would call doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" ) Note tokenizer also has a lexos.tokenizer.make_docs function to parse a list of texts into spaCy docs. A list of individual tokens can be obtained by iterating over the spaCy doc: # Get a list of tokens tokens = [ token . text for token in doc ] Here the text attribute stores the original text form of the token. SpaCy docs are non-destructive because they preserve the original text alongside the list of tokens and their attributes. You can access the original text of the entire doc by calling doc.text (assuming you have assigned the Doc object to the doc variable). Indeed, calling doc.to_json() will return a JSON representation which gives the start and end position of each token in the original text! As mentioned above, you can use a Python list comprehension to filter the the contents of the doc using information in the document's attributes. For instance: # Get a list of non-punctuation tokens non_punct_tokens = [ token . text for token in doc if not token . is_punct ] The example above leverages the built-in is_punct attribute to indicate whether the token is defined as (or predicted to be) a punctuation mark in the language model. SpaCy docs have a number of built-in attributes, which are described in the spaCy API reference . Note It is possible to extend spaCy's Doc object with its extension attribute. The Lexos API has a sample is_fruit extension, which is illustrated below. Note that extensions are accessed via the underscore prefix, as shown. # Indicate whether the token is labelled as fruit for token in doc : print ( token . _ . is_fruit ) The sample extension can be found in lexos.tokenizer.extensions . LexosDocs \u00a4 The Lexos API also has a LexosDoc class, which provides a wrapper for spaCy docs. Its use is illustrated below. from lexos.tokenizer.lexosdoc import LexosDoc lexos_doc = LexosDoc ( doc ) tokens = lexos_doc . get_tokens () This example just returns [token.text for token in doc] , so it is not strictly necessary. But using the LexosDoc wrapper can be useful for producing clean code. In other cases, it might be useful to manipulate spaCy docs with methods that do not access their built-in or extended attributes or method. For instance, lexos.tokenizer.lexosdoc.LexosDoc.get_token_attrs shows what attributes are available for tokens in the doc and lexos.tokenizer.lexosdoc.LexosDoc.to_dataframe exports the tokens and their attributes to a pandas dataframe. Ngrams \u00a4 Both texts and documents can be parsed into sequences of two or more tokens called ngrams. Many spaCy models can identify syntactic units such as noun chunks. These capabilities are not covered here since they are language specific. Instead, the section below describe how to obtain more general ngram sequences. Generating Word Ngrams \u00a4 The easiest method of obtaining ngrams from a text is to create a spaCy doc and then call Textacy's textacy.extract.basics.ngrams method: import spacy import textacy.extract.basics.ngrams as ng nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = list ( ng ( doc , 2 , min_freq = 1 )) This will produce [The end, end is, is nigh] . The output is a list of spaCy tokens. (An additional [token.text for token in ngrams] is required to ensure that you have quoted strings: [\"The end\", \"end is\", \"is nigh\"] ). Textacy has a lot of additional options, which are documented in the Textacy API reference under textacy.extract.basics.ngrams . However, if you do not need these options, you can use Lexos' helper function lexos.tokenizer.ngrams_from_doc : import spacy nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = ngrams_from_doc ( doc , size = 2 ) Notice that in both cases, the output will be a list of overlapping ngrams generated by a rolling window across the pre-tokenised document. If you want your document to contain ngrams as tokens, you will need to create a new document using the lexos.tokenizer.doc_from_ngrams function: doc = doc_from_ngrams ( ngrams , strict = True ) Note Setting strict=False will preserve all the whitespace in the ngrams; otherwise, your language model may modify the output by doing things like splitting punctuation into separate tokens. There is also a lexos.tokenizer.docs_from_ngrams function to which you can feed multiple lists of ngrams. A possible workflow might call Textacy directly to take advantage of some its filters, when generating ngrams and then calling lexos.tokenizer.doc_from_ngrams to pipe the extracted tokens back into a doc. textacy.extract.basics.ngrams has sister functions that do things like extract noun chunks (if available in the language model), making this a very powerful approach generating ngrams with semantic information. Generating Character Ngrams \u00a4 Character ngrams at their most basic level split the untokenised string every N characters. So \"The end is nigh.\" would produce something like [\"Th\", \"e \", \"nd\", \" i\", \"s \", \"ni\", \"gh\", \".\"] (if we wanted to preserve the whitespace). Lexos does this with the lexos.tokenizer.generate_ngrams function: text = \"The end is nigh.\" ngrams = generate_character_ngrams ( text , 2 , drop_whitespace = False ) This will produce the output shown above. If we wanted to output [\"Th\", \"en\", \"di\", \"sn\", \"ig\", \"h.\"] , we would use drop_whitespace=True (which is the default). Note lexos.tokenizer.generate_character_ngrams is a wrapper for Python's textwrap.wrap method, which can also be called directly. Once you have produced a list of ngrams, you can create a doc from them using lexos.tokenizer.ngrams_from_doc , as shown above. Use lexos.tokenizer.generate_character_ngrams (a) when you simply want a list of non-overlapping ngrams, or (b) when you want to produce docs with non-overlapping ngrams as tokens. Note that your language model may not be able apply labels effectively to ngram tokens, so working with character ngrams is primarily useful if you are planning to work with the token forms only, or if the ngram size you use maps closely to character lengths of words in the language you are working in. Summary of Workflow \u00a4 Tokenisation can be considered the end of the text preparation workflow, although, in its use language models, it already begins to introduce some elements of analysis. Before proceeding to the analysis stage, it is useful to have summary of what a minimal text preparation workflow might look like: # Create the loader and load the data loader = Loader () loader . load ( data ) # Load Scrubber components, make a pipeline, and scrub the texts lower_case , remove_digits = load_components ( ( \"lower_case\" , \"remove_digits\" ) ) scrub = make_pipeline ( lower_case , pipe ( remove_digits , only = [ \"1\" ]) ) scrubbed_texts = [ scrub ( text ) for text in loader . texts ] # Tokenise the texts docs = tokenizer . make_docs ( scrubbed_texts )","title":"Tokenising Texts"},{"location":"tutorial/tokenising_texts/#language-models","text":"The tokenizer module is a big change for Lexos, as it formally separates tokenisation from preprocessing. In the Lexos app, the user employs Scrubber to massage the text into shape using his or her implicit knowledge about the text's language. Tokenisation then takes place by splitting the text according to a regular expression pattern (normally whitespace). By contrast, the Lexos API uses a language model that formalises the implicit rules and thus automates the tokenisation process. Language models can implement both rule-based and probabilistic strategies for separating document strings into tokens. Because they have built-in procedures appropriate to specific languages, language models can often do a better job of tokenisation than the approach used in the Lexos app. Important There are some trade-offs to using language models. Because the algorithm does more than split strings, processing times can be greater. In addition, tokenisation is no longer (explicitly) language agnostic. A language model is \"opinionated\" and it may overfit the data. At the same time, if no language model exist for the language being tokenised, the results may not be satisfactory. The Lexos API strategy for handling this situation is described below.","title":"Language Models"},{"location":"tutorial/tokenising_texts/#tokenised-documents","text":"A tokenised document can be defined as a text split into tokens in which each token is stored with any number of annotations assigned by the model. These annotations are token \"attributes\". The structure of a tokenised document can then be conceived in theory as a list of tuples like [ ( text = \"The\" , part_of_speech = \"noun\" , is_stopword = \"True\" ), ( text = \"end\" , part_of_speech = \"noun\" , is_stopword = \"False\" ), ] It is then a simple matter to iterate through the document and retrieve all the tokens that are not stopwords using a Python list comprehension. Many filtering procedures are easy to implement in this way. For languages such as Modern English, language models exist that can automatically annotate tokens with information like part of speech, lemmas, stop words, and other information. However, token attributes can also be set after the text has been tokenised. If no language model exists for the text's language, it will only be possible to tokenise using general rules, and it will not be possible to add other labels (at the tokenisation stage). But new language models, including models for historical languages, are being produced all the time, and this is a growing area of interest in DH.","title":"Tokenised Documents"},{"location":"tutorial/tokenising_texts/#spacy-docs","text":"The Lexos API wraps the spaCy NLP library for loading language models and tokenising texts. Because spaCy has excellent documentation and fairly wide acceptance in the DH community, it is a good tool to use under the bonnet. spaCy has a growing number of language models in a number of languages, as well as wrappers for loading models from other common NLP libraries such as Stanford Stanza. Note The architecture of the Scrubber module is partially built on top of the preprocessing functions in Textacy , which also accesses and extends spaCy. In spaCy, texts are parsed into spacy.Doc objects consisting of sequences of tokens. Note In order to formalise the difference between a text string that has been scrubbed and one that has been tokenised, we refer wherever possible to the string as a \"text\" and to the tokenised spacy.Doc object as a \"document\" (or just \"doc\"). We continue to refer to the individual items as \"documents\" if we are not concerned with their data type. Each token is spacy.Token object which stores all the token's attributes. The Lexos API wraps this procedure in the lexos.tokenizer.make_doc function: from lexos import tokenizer doc = tokenizer . make_doc ( text ) This returns a spacy.Doc object. By default the tokenizer uses spaCy's \"xx_sent_ud_sm\" language model, which has been trained for tokenisation and sentence segmentation on multiple languages. This model performs statistical sentence segmentation and possesses general rules for token segmentation. If you were making a document from a text in a language for which a more language-specific model, you would specify the model to be used. For instance, to use spaCy's small English model trained on web texts, you would call doc = tokenizer . make_doc ( text , model = \"en_core_web_sm\" ) Note tokenizer also has a lexos.tokenizer.make_docs function to parse a list of texts into spaCy docs. A list of individual tokens can be obtained by iterating over the spaCy doc: # Get a list of tokens tokens = [ token . text for token in doc ] Here the text attribute stores the original text form of the token. SpaCy docs are non-destructive because they preserve the original text alongside the list of tokens and their attributes. You can access the original text of the entire doc by calling doc.text (assuming you have assigned the Doc object to the doc variable). Indeed, calling doc.to_json() will return a JSON representation which gives the start and end position of each token in the original text! As mentioned above, you can use a Python list comprehension to filter the the contents of the doc using information in the document's attributes. For instance: # Get a list of non-punctuation tokens non_punct_tokens = [ token . text for token in doc if not token . is_punct ] The example above leverages the built-in is_punct attribute to indicate whether the token is defined as (or predicted to be) a punctuation mark in the language model. SpaCy docs have a number of built-in attributes, which are described in the spaCy API reference . Note It is possible to extend spaCy's Doc object with its extension attribute. The Lexos API has a sample is_fruit extension, which is illustrated below. Note that extensions are accessed via the underscore prefix, as shown. # Indicate whether the token is labelled as fruit for token in doc : print ( token . _ . is_fruit ) The sample extension can be found in lexos.tokenizer.extensions .","title":"spaCy Docs"},{"location":"tutorial/tokenising_texts/#lexosdocs","text":"The Lexos API also has a LexosDoc class, which provides a wrapper for spaCy docs. Its use is illustrated below. from lexos.tokenizer.lexosdoc import LexosDoc lexos_doc = LexosDoc ( doc ) tokens = lexos_doc . get_tokens () This example just returns [token.text for token in doc] , so it is not strictly necessary. But using the LexosDoc wrapper can be useful for producing clean code. In other cases, it might be useful to manipulate spaCy docs with methods that do not access their built-in or extended attributes or method. For instance, lexos.tokenizer.lexosdoc.LexosDoc.get_token_attrs shows what attributes are available for tokens in the doc and lexos.tokenizer.lexosdoc.LexosDoc.to_dataframe exports the tokens and their attributes to a pandas dataframe.","title":"LexosDocs"},{"location":"tutorial/tokenising_texts/#ngrams","text":"Both texts and documents can be parsed into sequences of two or more tokens called ngrams. Many spaCy models can identify syntactic units such as noun chunks. These capabilities are not covered here since they are language specific. Instead, the section below describe how to obtain more general ngram sequences.","title":"Ngrams"},{"location":"tutorial/tokenising_texts/#generating-word-ngrams","text":"The easiest method of obtaining ngrams from a text is to create a spaCy doc and then call Textacy's textacy.extract.basics.ngrams method: import spacy import textacy.extract.basics.ngrams as ng nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = list ( ng ( doc , 2 , min_freq = 1 )) This will produce [The end, end is, is nigh] . The output is a list of spaCy tokens. (An additional [token.text for token in ngrams] is required to ensure that you have quoted strings: [\"The end\", \"end is\", \"is nigh\"] ). Textacy has a lot of additional options, which are documented in the Textacy API reference under textacy.extract.basics.ngrams . However, if you do not need these options, you can use Lexos' helper function lexos.tokenizer.ngrams_from_doc : import spacy nlp = spacy . load ( \"xx_sent_ud_sm\" ) text = \"The end is nigh.\" doc = nlp ( text ) ngrams = ngrams_from_doc ( doc , size = 2 ) Notice that in both cases, the output will be a list of overlapping ngrams generated by a rolling window across the pre-tokenised document. If you want your document to contain ngrams as tokens, you will need to create a new document using the lexos.tokenizer.doc_from_ngrams function: doc = doc_from_ngrams ( ngrams , strict = True ) Note Setting strict=False will preserve all the whitespace in the ngrams; otherwise, your language model may modify the output by doing things like splitting punctuation into separate tokens. There is also a lexos.tokenizer.docs_from_ngrams function to which you can feed multiple lists of ngrams. A possible workflow might call Textacy directly to take advantage of some its filters, when generating ngrams and then calling lexos.tokenizer.doc_from_ngrams to pipe the extracted tokens back into a doc. textacy.extract.basics.ngrams has sister functions that do things like extract noun chunks (if available in the language model), making this a very powerful approach generating ngrams with semantic information.","title":"Generating Word Ngrams"},{"location":"tutorial/tokenising_texts/#generating-character-ngrams","text":"Character ngrams at their most basic level split the untokenised string every N characters. So \"The end is nigh.\" would produce something like [\"Th\", \"e \", \"nd\", \" i\", \"s \", \"ni\", \"gh\", \".\"] (if we wanted to preserve the whitespace). Lexos does this with the lexos.tokenizer.generate_ngrams function: text = \"The end is nigh.\" ngrams = generate_character_ngrams ( text , 2 , drop_whitespace = False ) This will produce the output shown above. If we wanted to output [\"Th\", \"en\", \"di\", \"sn\", \"ig\", \"h.\"] , we would use drop_whitespace=True (which is the default). Note lexos.tokenizer.generate_character_ngrams is a wrapper for Python's textwrap.wrap method, which can also be called directly. Once you have produced a list of ngrams, you can create a doc from them using lexos.tokenizer.ngrams_from_doc , as shown above. Use lexos.tokenizer.generate_character_ngrams (a) when you simply want a list of non-overlapping ngrams, or (b) when you want to produce docs with non-overlapping ngrams as tokens. Note that your language model may not be able apply labels effectively to ngram tokens, so working with character ngrams is primarily useful if you are planning to work with the token forms only, or if the ngram size you use maps closely to character lengths of words in the language you are working in.","title":"Generating Character Ngrams"},{"location":"tutorial/tokenising_texts/#summary-of-workflow","text":"Tokenisation can be considered the end of the text preparation workflow, although, in its use language models, it already begins to introduce some elements of analysis. Before proceeding to the analysis stage, it is useful to have summary of what a minimal text preparation workflow might look like: # Create the loader and load the data loader = Loader () loader . load ( data ) # Load Scrubber components, make a pipeline, and scrub the texts lower_case , remove_digits = load_components ( ( \"lower_case\" , \"remove_digits\" ) ) scrub = make_pipeline ( lower_case , pipe ( remove_digits , only = [ \"1\" ]) ) scrubbed_texts = [ scrub ( text ) for text in loader . texts ] # Tokenise the texts docs = tokenizer . make_docs ( scrubbed_texts )","title":"Summary of Workflow"}]}