{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe73f29a",
   "metadata": {},
   "source": [
    "# Rolling Windows Tutorial\n",
    "\n",
    "This notebook will guide you through analyzing text patterns using the Lexos `rolling_windows` module. Whether you want to track word frequencies, compare character usage, or visualize how themes change throughout a document, this guide will show you exactly what to do.\n",
    "\n",
    "First, let's load a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e500e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS: Replace \"A_Scandal_in_Bohemia.txt\" with your filename\n",
    "filename = \"A_Scandal_in_Bohemia.txt\"\n",
    "\n",
    "# Import the Lexos Loader class\n",
    "from lexos.io import Loader\n",
    "\n",
    "# Create a Loader instance and load the file\n",
    "loader = Loader()\n",
    "loader.load([filename])\n",
    "\n",
    "# Remove extra whitespace and newlines\n",
    "text = loader.texts[0]\n",
    "text = \" \".join(text.split())\n",
    "print(f\"✓ Loaded {len(text)} characters from {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fc666",
   "metadata": {},
   "source": [
    "## Creating Windows\n",
    "\n",
    "Windows are overlapping segments of your text. Think of it like a sliding frame that moves through your document, creating overlapping \"snapshots\" of your text.\n",
    "\n",
    "Before beginning your analysis, you need to decide on your window settings. The first decision is how to segment your text, by *n* characters or *n* tokens. Segmenting by characters is good for analysing patterns of symbols such as punctuation marks or individual letters. Segmenting by tokens is good for studying word patterns or patterns of larger semantic units.\n",
    "\n",
    "In the cell below, we'll create windows of 10,000 characters each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f91c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Windows class\n",
    "from lexos.rolling_windows import Windows\n",
    "\n",
    "# Create a Windows object\n",
    "windows = Windows()\n",
    "\n",
    "# Define your window settings and generate windows\n",
    "windows(input=text, n=10000, window_type=\"characters\")\n",
    "\n",
    "# Print the number of windows\n",
    "print(f\"Generated windows: {windows.length}\")\n",
    "\n",
    "# Iterate through the first five windows and print the first 20 characters of each\n",
    "for window in list(windows)[:5]:\n",
    "    print(window[:20])\n",
    "\n",
    "print(f\"Remaining windows: {windows.length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd9e83",
   "metadata": {},
   "source": [
    "Some important observations:\n",
    "\n",
    "1. Each window is 10,000 characters long (we're only displaying the first twenty), and each window start index advances one character from the last.\n",
    "2. The `windows` instance produces a generator. If you need to know the number of windows, use the `length` property.\n",
    "3. The generator will be exhausted whenever you iterate through it (In the cell above, the generator is exhausted when it is converted to a list). If you need to access your windows a second time, you will have to regenerate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ce7a2",
   "metadata": {},
   "source": [
    "We can similarly generate windows of tokens if we have a spaCy `Doc` object. We'll create one below using our text and spaCy's small English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lexos Tokenizer class\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "# Create a Tokenizer instance and make a spaCy Doc\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e454a4",
   "metadata": {},
   "source": [
    " We can now generate windows from our spaCy doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d00baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your window settings and generate windows\n",
    "windows(input=doc, n=100, window_type=\"tokens\")\n",
    "\n",
    "# Iterate through the first five windows and print the first 20 characters of each\n",
    "for window in list(windows)[:5]:\n",
    "    print(window[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54beb0c",
   "metadata": {},
   "source": [
    "Here we can see that our windows consist of tokens, in this case words (100 per window). Each window advances one word.\n",
    "\n",
    "Larger structural divisions can be used as tokens. For instance, we can split our text on line breaks and submit the list of lines as our input. The lines will be counted like tokens. This can be useful for studying texts like poetry. Note that, in the code below, we do some manipulations of the text to get rid of extra white space but keep single line breaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Strip and leading/trailing whitespace\n",
    "lines = loader.texts[0].strip()\n",
    "# Replace multiple spaces with a single space\n",
    "lines = re.sub(' +', ' ', lines)\n",
    "# Replace multiple newlines with a single newline\n",
    "lines = re.sub('\\n+', '\\n', lines)\n",
    "# Split into lines and strip each line\n",
    "lines = lines.split('\\n')\n",
    "lines = [line.strip() for line in lines]\n",
    "\n",
    "# Generate windows of lines (treating each line as a token)\n",
    "windows(input=lines, n=5, window_type=\"tokens\")\n",
    "for window in list(windows)[:5]:\n",
    "    # print the first 20 characters of each window\n",
    "    print(window[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f3596",
   "metadata": {},
   "source": [
    "## Rolling Windows Calculators\n",
    "\n",
    "Once we have created our windows, we want to calculate the changes of frequency in which patterns occur in these windows. By \"patterns\", we mean characters or character sequences, tokens such as words, or perhaps large spans such as lines or sentences. \"Frequency\" can be understood in many ways. At a bare minimum, it can be a raw count of the number of occurrences. However, we can also apply further calculations in order to normalize for window or document length, or to compare frequencies of different patterns. The `rolling_windows` module allows you to select a calculator class to decide which kind of calculation you wish to apply. Lexos comes with three different calculator classes: `Counts`, `Averages`, and `Ratios`. (You can also design your own and have it inherit from the `BaseCalculator` class.) In the section below, we will go over the usage of each of the built-in classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72673af",
   "metadata": {},
   "source": [
    "## The `Counts` Class\n",
    "\n",
    "The `Counts` class counts the raw number of occurrences for each your desired patterns in your windows, without normalization. It is a good way to learn how to implement each of the other classes. A sample implementation is given in the cell below. Try changing the `patterns_to_count` (some examples are given in comments).\n",
    "\n",
    "Other settings to experiment with:\n",
    "\n",
    "- Try changing the number of windows and the `window_type` when you generate your windows.\n",
    "- Try changing the case sensitivity option in the calculator. This determines whether the counter matches patterns regardless of case or distinguishes upper and lower case when matching patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Counts calculator\n",
    "from lexos.rolling_windows.calculators.counts import Counts\n",
    "\n",
    "# CHANGE THESE: Your patterns to search for\n",
    "patterns_to_count = [\"a\", \"e\"]  # Example: Common letters\n",
    "# patterns_to_count = [\"love\", \"hate\"]  # Example: Emotional words\n",
    "patterns_to_count = [\"Sherlock\", \"Watson\"]  # Example: Character names\n",
    "\n",
    "# Create a new set of windows for counting\n",
    "windows = Windows()\n",
    "windows(input=doc, n=5000, window_type=\"tokens\")\n",
    "\n",
    "# Create the calculator and run the analysis\n",
    "counter = Counts()\n",
    "counter(\n",
    "    patterns=patterns_to_count,\n",
    "    windows=windows,\n",
    "    mode=\"exact\",\n",
    "    case_sensitive=True\n",
    ")\n",
    "\n",
    "# Get results as a DataFrame\n",
    "count_results = counter.to_df()\n",
    "print(f\"✓ Counted patterns in {len(count_results)} windows\")\n",
    "print(\"\\nSample results (5 random windows):\")\n",
    "print(count_results.sample(5))\n",
    "print(f\"\\nTotal occurrences across all windows:\")\n",
    "print(count_results.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a2d55",
   "metadata": {},
   "source": [
    "> Remember that your windows are a generator, which will be exhausted when you run it through a calculator. If you change your calculator settings, you will need to rebuild your windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a913af8",
   "metadata": {},
   "source": [
    "Notice that the calculator's `mode` parameter is set to \"exact\". This sets the calculator to find exact (or case-insensitive) matches to the string patterns you pass to it. You can also pass regular expressions, using `mode=\"regex\"`. An example is given below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new set of windows for counting\n",
    "windows = Windows()\n",
    "windows(input=text, n=5000, window_type=\"characters\")\n",
    "\n",
    "# Create the calculator and run the analysis matching tokens starting with \"sh\"\n",
    "counter = Counts()\n",
    "counter(\n",
    "    patterns=[r\"\\bsh\\w*\"],  # \\b = word boundary, \\w* = any word characters\n",
    "    windows=windows,\n",
    "    mode=\"regex\",\n",
    "    case_sensitive=False\n",
    ")\n",
    "\n",
    "# Get results as a DataFrame\n",
    "count_results = counter.to_df()\n",
    "print(\"REGEX EXAMPLE - Words starting with 'sh':\\n\")\n",
    "print(f\"✓ Counted patterns in {len(count_results)} windows\")\n",
    "print(f\"✓ Total occurrences across all windows: {count_results[r'\\bsh\\w*'].sum()}\")\n",
    "print(\"✓ Sample results (5 random windows):\")\n",
    "count_results.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bba20",
   "metadata": {},
   "source": [
    "You can also search for multi-word phrases in spaCy `Doc` objects. Set your window output to `spans` and your calculator's `mode` to \"multi_token\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new windows instance for counting\n",
    "windows = Windows()\n",
    "windows = windows(\n",
    "    input=doc,\n",
    "    n=5000,\n",
    "    window_type=\"tokens\",\n",
    "    output=\"spans\"\n",
    ")\n",
    "\n",
    "# Create the calculator and run the analysis matching the token pattern \"sherlock holmes\"\n",
    "counter = Counts()\n",
    "counter(\n",
    "    patterns=[\"sherlock holmes\"],\n",
    "    windows=windows,\n",
    "    mode=\"multi_token\",\n",
    "    case_sensitive=False\n",
    ")\n",
    "\n",
    "results = counter.to_df()\n",
    "print(\"MULTI-WORD PHRASE EXAMPLE - 'sherlock holmes':\")\n",
    "print(f\"Found in {len(results)} windows\")\n",
    "print(f\"Total occurrences: {results.sum().values[0] if len(results) > 0 else 0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19fd754",
   "metadata": {},
   "source": [
    "If you have token windows derived from a spaCy `Doc`, you can search for linguistic patterns. In the example below, we search for instances of a proper noun using spaCy's small English-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Windows instance for counting\n",
    "windows = windows(\n",
    "    input=doc,\n",
    "    n=5000,\n",
    "    window_type=\"tokens\",\n",
    "    output=\"spans\"\n",
    ")\n",
    "\n",
    "# Create the calculator and run the analysis matching proper nouns\n",
    "counter = Counts()\n",
    "counter(\n",
    "    patterns=[[{\"POS\": \"PROPN\"}]],\n",
    "    windows=windows,\n",
    "    mode=\"spacy_rule\",\n",
    "    model=\"en_core_web_sm\"\n",
    ")\n",
    "\n",
    "results = counter.to_df()\n",
    "print(\"LINGUISTIC PATTERN EXAMPLE - Proper nouns:\")\n",
    "print(f\"Found in {len(results)} windows\")\n",
    "print(f\"Total occurrences: {results.sum().values[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0cf09",
   "metadata": {},
   "source": [
    "### The `Averages` Class\n",
    "\n",
    "The `Averages` calculator is used when you want know **how often** patterns appear. Calculating the average frequency per unit is a method of normalizing the frequency, which can be helpful when comparing across window sizes or texts of different lengths.\n",
    "\n",
    "The usage is very similar to the `Counts` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fcc962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Averages calculator\n",
    "from lexos.rolling_windows.calculators.averages import Averages\n",
    "\n",
    "# CHANGE THESE: Your patterns to analyze\n",
    "patterns_to_average = [\"a\", \"e\"]  # Same format as counting\n",
    "\n",
    "# Create fresh windows for averaging\n",
    "windows = windows(\n",
    "    input=text,\n",
    "    n=5000,\n",
    "    window_type=\"characters\",\n",
    "    output=\"strings\"\n",
    ")\n",
    "\n",
    "# Create and call the calculator\n",
    "calculator = Averages()\n",
    "calculator(\n",
    "    patterns=patterns_to_average,\n",
    "    windows=windows,\n",
    "    mode=\"exact\",\n",
    "    case_sensitive=False\n",
    ")\n",
    "\n",
    "# Get results\n",
    "results = calculator.to_df()\n",
    "print(f\"✓ Calculated average frequencies in {len(results)} windows\")\n",
    "print(\"\\nRandom 5 windows:\")\n",
    "print(results.sample(5))\n",
    "print(f\"\\nOverall average frequency:\")\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4143a1e",
   "metadata": {},
   "source": [
    "### The `Ratios` Class\n",
    "\n",
    "The `Ratios` calculator class is used when you compare the **balance between exactly two patterns**. The following notes are helpful in interpreting the results:\n",
    "\n",
    "- **0.0** = Only the second pattern appears\n",
    "- **0.5** = Both patterns appear equally\n",
    "- **1.0** = Only the first pattern appears\n",
    "- Values closer to 0 favor the second pattern\n",
    "- Values closer to 1 favor the first pattern\n",
    "\n",
    "> **Important:** The `Ratios` calculator requires **exactly** 2 patterns. For comparing more than two patterns, use `Counts` or `Averages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Ratios calculator\n",
    "from lexos.rolling_windows.calculators.ratios import Ratios\n",
    "\n",
    "# CHANGE THESE: Exactly 2 patterns to compare\n",
    "patterns_to_compare = [\"a\", \"e\"]  # Will calculate a / (a + e)\n",
    "# patterns_to_compare = [\"positive\", \"negative\"]  # Example: sentiment balance\n",
    "# patterns_to_compare = [\"past\", \"present\"]  # Example: tense analysis\n",
    "\n",
    "# Create fresh windows for ratios\n",
    "windows = Windows()\n",
    "windows = windows(input=text, n=5000, window_type=\"characters\", output=\"strings\")\n",
    "\n",
    "# Create and run the calculator\n",
    "calculator = Ratios()\n",
    "calculator(\n",
    "    patterns=patterns_to_compare, windows=windows, mode=\"exact\", case_sensitive=False\n",
    ")\n",
    "\n",
    "# Get results\n",
    "results = calculator.to_df()\n",
    "print(f\"✓ Calculated ratios in {len(results)} windows\")\n",
    "print(\"\\nSample results (5 random windows):\")\n",
    "print(results.sample(5))\n",
    "\n",
    "# Interpret the results\n",
    "avg_ratio = results.iloc[:, 0].mean()\n",
    "print(f\"\\nAverage ratio: {avg_ratio:.3f}\")\n",
    "print(\n",
    "    f\"→ '{patterns_to_compare[0]}' appears more frequently overall\"\n",
    "    if avg_ratio > 0.5\n",
    "    else f\"→ '{patterns_to_compare[1]}' appears more frequently overall\"\n",
    "    if avg_ratio < 0.5\n",
    "    else \"→ Both patterns appear equally overall\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029f74a",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "Now let's create visualizations of your analysis. You can create either static plots (for reports, publications, and presentations) using the `SimplePlotter` class or interactive plots (for exploration) using the `PlotlyPlotter` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a86fe",
   "metadata": {},
   "source": [
    "### Using `SimplePlotter` for Static Plots\n",
    "\n",
    "For illustration, we'll use the averages we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Averages calculator\n",
    "from lexos.rolling_windows.calculators.averages import Averages\n",
    "\n",
    "# Create fresh windows for averaging\n",
    "windows = Windows()\n",
    "windows = windows(\n",
    "    input=text,\n",
    "    n=5000,\n",
    "    window_type=\"characters\",\n",
    "    output=\"strings\"\n",
    ")\n",
    "\n",
    "# Create and call the calculator\n",
    "calculator = Averages()\n",
    "calculator(\n",
    "    patterns=[\"holmes\", \"watson\"],\n",
    "    windows=windows,\n",
    "    mode=\"exact\",\n",
    "    case_sensitive=False\n",
    ")\n",
    "\n",
    "# Get results\n",
    "results = calculator.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569dece",
   "metadata": {},
   "source": [
    "Now import the `SimplePlotter` class and generate the plot. You can uncomment the code at the bottom of the cell to save the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SimplePlotter class\n",
    "from lexos.rolling_windows.plotters.simple_plotter import SimplePlotter\n",
    "\n",
    "# CHANGE THIS: Use your results from above\n",
    "data_to_plot = results\n",
    "\n",
    "# Create a static plot\n",
    "plotter = SimplePlotter(\n",
    "    df=data_to_plot,\n",
    "    title=\"Pattern Frequency Analysis\",  # CHANGE THIS\n",
    "    xlabel=\"Window Position\",\n",
    "    ylabel=\"Frequency\",  # CHANGE THIS based on your analysis type\n",
    "    figsize=(10, 6),  # (width, height) in inches\n",
    "    show_legend=True,\n",
    "    show_grid=True\n",
    ")\n",
    "\n",
    "# Generate the plot\n",
    "plotter.plot()\n",
    "\n",
    "# Optional: Save the plot\n",
    "# plotter.save(\"my_analysis.png\", dpi=300)\n",
    "# plotter.save(\"my_analysis.pdf\")  # For publications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0831573",
   "metadata": {},
   "source": [
    "By default, the `plotter.plot` function displays the plot. If you do not wish to do this, such as if you just want to save the file, use `plotter.plot(show=False)`. If you wish to show the figure later, you can call `plotter.show()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769921c",
   "metadata": {},
   "source": [
    "#### Adding Milestones\n",
    "\n",
    "Milestones mark important points in your text (like chapter breaks, scene changes, etc.), which can help you interpret the graph. You can pass a dictionary of milestone labels and location indexes. For instance, \"Chapter 1\" might occur at the beginnning of the document, (index 0) and \"Chapter 2\" might begin at character or token 5000. In this case, your dictionary would look like `{\"Chapter 1\": 0, \"Chapter 2\": 5000}`.\n",
    "\n",
    "You can construct a milestone dictionary on your own, but it is easier to use the Lexos `milestones` module to calculate the milestone locations. A simple example is given below (see the Lexos `milestones` documentation for all the options).\n",
    "\n",
    "We know that our source document has three chapters with the word \"CHAPTER\" all in capital letters and the chapter numbers in Roman numerals. It is therefore possible to use a fairly simple regular expression to detect all three chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.milestones.string_milestones import StringMilestones\n",
    "\n",
    "milestones = StringMilestones(doc=text, patterns=\"CHAPTER I+\")\n",
    "milestone_dict = {milestone.text: milestone.start for milestone in milestones}\n",
    "milestone_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d45072d",
   "metadata": {},
   "source": [
    "We're now ready to create our plot with milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with calculated milestones\n",
    "plotter = SimplePlotter(\n",
    "    df=data_to_plot,\n",
    "    title=\"Analysis with Calculated Markers\",\n",
    "    figsize=(12, 6),\n",
    "    show_milestones=True,\n",
    "    milestone_labels=milestone_dict,\n",
    "    milestone_colors=\"red\",\n",
    "    milestone_style=\"--\",  # Dashed lines\n",
    "    show_milestone_labels=True,\n",
    "    milestone_labels_rotation=45\n",
    ")\n",
    "\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5b755",
   "metadata": {},
   "source": [
    "### Using `PlotlyPlotter` for Interactive Plots\n",
    "\n",
    "It can sometimes be useful to generate an interactive plot for exploration or web presentation because you can pan and zoom around the image, as well as access detailed information when hovering over coordinates in the plot. Lexos has the `PlotlyPlotter` class, which uses the Python <a href=\"https://plotly.com/python/\" target=\"_blank\">Plotly</a> library to generate a graph from Rolling Windows data. Most of the parameters parallel those in the `SimplePlotter` class, but see the API documentation for a list of all options. The example below illustrates a typical usage similar to the previous example. Here is a quick list of the interactive features in the toolbar that appears when you hover over the plot:\n",
    "\n",
    "- **Hover** over points to see exact values\n",
    "- **Zoom** in/out with mouse wheel or zoom controls\n",
    "- **Pan** by clicking and dragging\n",
    "- **Toggle** lines on/off by clicking legend items\n",
    "- **Download** plot as PNG using the camera icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PlotlyPlotter class\n",
    "from lexos.rolling_windows.plotters.plotly_plotter import PlotlyPlotter\n",
    "\n",
    "# Create an interactive plot\n",
    "plotter = PlotlyPlotter(\n",
    "    df=data_to_plot,\n",
    "    title=\"Interactive Pattern Analysis\",  # CHANGE THIS\n",
    "    xlabel=\"Window Position\",\n",
    "    ylabel=\"Frequency\",  # CHANGE THIS based on your analysis type\n",
    "    width=800,  # pixels\n",
    "    height=600,  # pixels\n",
    "    showlegend=True,\n",
    "    show_milestones=True,\n",
    "    milestone_labels=milestone_dict,\n",
    "    show_milestone_labels=True,\n",
    ")\n",
    "\n",
    "# Generate the plot\n",
    "plotter.plot()\n",
    "\n",
    "# Optional: Save as interactive HTML\n",
    "# plotter.save(\"my_analysis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d2a7e",
   "metadata": {},
   "source": [
    "## Saving Your Work\n",
    "\n",
    "In the cells above, we have commented out code to save the plotter output. For the `SimplePlotter` class, this is the equivalent of matplotlib's <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\" target=\"_blank\">pyplot.savefig</a> function and will accept any of its keyword arguments. The `PlotlyPlotter` class accepts any keyword arguments for the <a href=\"https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_html.html\" target=\"_blank\">plotly.io.write_html</a> and <a href=\"https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html\" target=\"_blank\">plotly.io.write_image</a> functions. If your filename ends in `.html`, Lexos will call `write_html`; otherwise, it calss `write_image`.\n",
    "\n",
    "To save your data, the easiest method is to create a pandas DataFrame and use its built-in methods. We have already repeatedly used `results = calculator.to_df()` to obtain our data. If we want to save it as a CSV file, we can use `pandas.DataFrame.to_csv`. This is illustrated in the cell below using the data we plotted last. For good measure, we'll generate some summary statistics and save them to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your output file path here\n",
    "output = \"sample_outputs/sample_analysis_data.csv\"\n",
    "summary_output = \"sample_outputs/sample_analysis_summary.csv\"\n",
    "\n",
    "# Save your numerical results as CSV\n",
    "data_to_plot.to_csv(output, index=True)\n",
    "print(f\"✓ Saved data to {output}\")\n",
    "\n",
    "# Create a summary report\n",
    "summary = pd.DataFrame({\n",
    "    'Pattern': data_to_plot.columns,\n",
    "    'Total': data_to_plot.sum().values,\n",
    "    'Average': data_to_plot.mean().values,\n",
    "    'Max': data_to_plot.max().values,\n",
    "    'Min': data_to_plot.min().values,\n",
    "    'Std_Dev': data_to_plot.std().values\n",
    "})\n",
    "\n",
    "print(\"\\n=== SUMMARY REPORT ===\")\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "summary.to_csv(summary_output, index=False)\n",
    "print(\"✓ Saved summary to sample_analysis_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ef27b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting Common Issues\n",
    "\n",
    "**Problem:** \"Windows are consumed\" error  \n",
    "**Solution:** Create fresh windows for each calculator call\n",
    "\n",
    "**Problem:** spaCy patterns don't work  \n",
    "**Solution:** Ensure `window_type=\"tokens\"` and `output=\"tokens\"` for spaCy rules\n",
    "\n",
    "**Problem:** No matches found  \n",
    "**Solution:** Check case sensitivity, try `case_sensitive=False`\n",
    "\n",
    "**Problem:** Regex not working  \n",
    "**Solution:** Use raw strings (`r\"pattern\"`) and escape special characters\n",
    "\n",
    "**Problem:** Memory issues with large texts  \n",
    "**Solution:** Reduce window size or limit text length (e.g., `text[:1000000]`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040d3fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
