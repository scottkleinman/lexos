{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd2892c",
   "metadata": {},
   "source": [
    "# Milestones Tutorial\n",
    "\n",
    "Milestones are specified locations in the text that designate structural or sectional divisions. A milestone can be either a designated unit *within* the text or a placemarker inserted between sections of text. The Lexos `milestones` module provides methods for identifying milestone locations by searching for patterns you designate. There three separate classes for identifying milestones in different ways: `StringMilestones`, `TokenMilestones`, and `SpanMilestones`. We will look at each of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af705399",
   "metadata": {},
   "source": [
    "## `StringMilestones`\n",
    "\n",
    "The `StringMilestones` class is used for extracting and storing milestones in strings or spaCy Doc objects. It uses regular expressions to find patterns and returns their locations and text.\n",
    "\n",
    "Here is a basic example in which we will search for the word \"Chapter\" followed by one or more digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StringMilestones class\n",
    "from lexos.milestones.string_milestones import StringMilestones\n",
    "\n",
    "# Sample text\n",
    "text = \"Chapter 1\\nThis is a sample text.\\nChapter 2: The Journey Begins\\nThis is the second chapter.\"\n",
    "\n",
    "# Create StringMilestones object with the text and pattern to search for\n",
    "milestones = StringMilestones(doc=text, patterns=\"Chapter \\\\d+\")\n",
    "\n",
    "# Print the start character, end character, and text of each milestone\n",
    "for milestone in milestones:\n",
    "    print(milestone.start, milestone.end, milestone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f54b4",
   "metadata": {},
   "source": [
    "You can supply a list of patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d55744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StringMilestones object with the text and pattern to search for\n",
    "milestones = StringMilestones(doc=text, patterns=[\"Chapter\", \"chapter\"])\n",
    "\n",
    "# Print the start character, end character, and text of each milestone\n",
    "for milestone in milestones:\n",
    "    print(milestone.start, milestone.end, milestone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63735f",
   "metadata": {},
   "source": [
    "You can make your search case insensitive if we set `case_sensitive=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14789384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StringMilestones object with the text and pattern to search for\n",
    "milestones = StringMilestones(doc=text, patterns=\"Chapter\", case_sensitive=False)\n",
    "\n",
    "# Print the start character, end character, and text of each milestone\n",
    "for milestone in milestones:\n",
    "    print(milestone.start, milestone.end, milestone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b7dae",
   "metadata": {},
   "source": [
    "You can use the `set` method to change any previously assigned milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091aa65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "milestones.set(\"The\", case_sensitive=False)\n",
    "\n",
    "for milestone in milestones:\n",
    "    print(milestone.start, milestone.end, milestone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a747c",
   "metadata": {},
   "source": [
    "The `StringMilestone` class also accepts a spaCy `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lexos Tokenizer class\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "text = \"the Chapter 1: Introduction. Chapter 2: Methods.\"\n",
    "\n",
    "# Create a Tokenizer instance and create a spaCy Doc object\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(text)\n",
    "\n",
    "# Create StringMilestones object with the spaCy Doc and print the milestones\n",
    "milestones = StringMilestones(doc=doc, patterns=\"Chapter\", case_sensitive=False)\n",
    "\n",
    "for milestone in milestones:\n",
    "    print(milestone.start, milestone.end, milestone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec9f4b",
   "metadata": {},
   "source": [
    "## `TokenMilestones`\n",
    "\n",
    "The `TokenMilestones` class is used for extracting and storing milestones tokenized text, such as spaCy Doc objects. It differs from `StringMilestones` in that it matches against full tokens. Furthermore, the process has two steps. First you must generate a list of matches using the `get_matches` methods. Next, you must commit those passages to the `Milestones` object and the `Doc` object by passing this list to the `set_milestones` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lexos TokenMilestones class\n",
    "from lexos.milestones.token_milestones import TokenMilestones\n",
    "\n",
    "# Create a spaCy Doc object\n",
    "text = \"Chapter 1: Introduction. Chapter 2: Methods.\"\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(text)\n",
    "\n",
    "# Create TokenMilestones object with the spaCy Doc and print the milestones\n",
    "milestones = TokenMilestones(doc=doc)\n",
    "matches = milestones.get_matches(patterns=\"Chapter\")\n",
    "\n",
    "# Set new milestones using the matches found\n",
    "milestones.set_milestones(matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcabfb",
   "metadata": {},
   "source": [
    "The `set_milestones` method creates two custom attributes in the document's tokens: `milestone_iob` and `milestone_label`. The first is an indication of whether the token is inside the milestone (\"I\"), outside the milestone (\"O\"), or at the beginning of the milestone (\"B\"). Tokens at the beginning of a milestone contain the complete text of the milestone as the `milestone_label` value; otherwise, it is an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cce353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tokens with their milestone information\n",
    "for token in doc:\n",
    "    print(token.text, token.i, token._.milestone_iob, token._.milestone_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f372e",
   "metadata": {},
   "source": [
    "By setting `mode` to \"phrase\", you can match multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91126bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TokenMilestones object with the spaCy Doc and print the milestones\n",
    "milestones = TokenMilestones(doc=doc)\n",
    "matches = milestones.get_matches(patterns=\"Chapter 1\", mode=\"phrase\")\n",
    "\n",
    "# Set new milestones using the matches found\n",
    "milestones.set_milestones(matches)\n",
    "\n",
    "# Display the tokens with their milestone information\n",
    "for token in doc:\n",
    "    print(token.text, token.i, token._.milestone_iob, token._.milestone_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895821b4",
   "metadata": {},
   "source": [
    "Set `mode` to \"rule\" to use more complex spaCy rule matching patterns. See the [spaCy documentation](https://spacy.io/usage/rule-based-matching) for a full description of the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59788420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": \"Chapter\"}, {\"IS_DIGIT\": True}]\n",
    "milestones = TokenMilestones(doc=doc)\n",
    "matches = milestones.get_matches(patterns=[pattern], mode=\"rule\")\n",
    "milestones.set_milestones(matches)\n",
    "for token in doc:\n",
    "    print(token.text, token._.milestone_iob, token._.milestone_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76df071",
   "metadata": {},
   "source": [
    "## SpanMilestones\n",
    "\n",
    "Span milestones are used to group spans together for analysis or visualization. Span milestones differ from normal milestones in that milestones are \"invisible\" structural boundaries between spans or groups of spans (e.g. sentence or line breaks). Thus, instead of storing a list of patterns representing milestones, span milestones store the groups of spans themselves.\n",
    "\n",
    "There are three subclasses that inherit from `SpanMilestones`: `LineMilestones`, `SentenceMilestones`, and `CustomMilestones`.\n",
    "\n",
    "### LineMilestones\n",
    "\n",
    "The `LineMilestones` class is the easiest to understand. It splits the text on line breaks and generates a list of spaCy `Span` objects. These can be accessed through the `spans` of both the `Milestones` and the `Doc` objects:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb50eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lexos LineMilestones class\n",
    "from lexos.milestones.span_milestones import LineMilestones\n",
    "\n",
    "# Create a spaCy Doc object\n",
    "text = \"Chapter 1: Introduction.\\nChapter 2: Methods.\"\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(text)\n",
    "\n",
    "# Create LineMilestones object with the spaCy Doc and set the milestones\n",
    "milestones = LineMilestones(doc=doc)\n",
    "milestones.set()\n",
    "\n",
    "# Print the milestone span text in both the milestones and Doc objects\n",
    "print(milestones.spans)\n",
    "print(doc.spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ffc203",
   "metadata": {},
   "source": [
    "You can iterate through the `milestones.spans` list directly, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ded40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for milestone in milestones:\n",
    "    print(milestone.start, milestone.end, milestone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67467657",
   "metadata": {},
   "source": [
    "There is also a `to_list()` method, which returns a list of dictionaries providing additional indexing information, should you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(milestones.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a28a4",
   "metadata": {},
   "source": [
    "By default, the pattern used to identify line breaks is \"\\n\", but this can be customed with the `pattern` keyword when calling `set`. By default, all line breaks are not included in the milestone spans, but this can be disabled with `remove_linebreak= False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed958f64",
   "metadata": {},
   "source": [
    "### SentenceMilestones\n",
    "\n",
    "The `SentenceMilestones` class works in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lexos SentenceMilestones class\n",
    "from lexos.milestones.span_milestones import SentenceMilestones\n",
    "\n",
    "# Create a spaCy Doc object using a model with a sentence segmenter\n",
    "text = \"This is sentence 1. This is sentence 2.\"\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(text)\n",
    "print(f\"Doc Sentences: {list(doc.sents)}\")\n",
    "\n",
    "# Create SentenceMilestones object with the spaCy Doc and set the milestones\n",
    "milestones = SentenceMilestones(doc=doc)\n",
    "milestones.set()\n",
    "\n",
    "# Print the text of the spans in a list\n",
    "print(f\"Milestone spans: {milestones.spans}\")\n",
    "print(f\"Doc spans: {doc.spans}\")\n",
    "print(f\"Milestones list: {milestones.to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5f91f",
   "metadata": {},
   "source": [
    "Note that the `Doc` object already has a `sents` attribute that contains a generator sentence spans. This is generated automatically *if and only if* your language model has a sentence segmenter. If it does not, you cannot use the `SentenceMilestones` class and will need to rely on the custom approach discussed below. See the [spaCy documentation](https://spacy.io/usage/linguistic-features#sbd) for further information on creating `Doc` objects with sentence segmentation in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0c024",
   "metadata": {},
   "source": [
    "### CustomMilestones\n",
    "\n",
    "The `CustomMilestones` class can be used to generate milestones based on arbitrary spans. A good way to demonstrate this is to reproduce the sentence segments shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ac424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lexos CustomMilestones class\n",
    "from lexos.milestones.span_milestones import CustomMilestones\n",
    "\n",
    "# Create two spaCy Span objects from the existing Doc\n",
    "spans = [doc[0:5], doc[5:10]]\n",
    "\n",
    "# Create CustomMilestones object with the spaCy Doc and set the milestones\n",
    "milestones = CustomMilestones(doc=doc)\n",
    "milestones.set(spans)\n",
    "\n",
    "# Print the text of the spans in a list\n",
    "print(f\"Milestone spans: {milestones.spans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3586c14",
   "metadata": {},
   "source": [
    "Here we have manually set our spans to include the first and last five tokens, which happen to coincide with sentence boundaries. But we could easily create spans separated in other ways.\n",
    "\n",
    "Note that, unlike the previous two classes, `CustomMilestones` requires you to pass a list of `Span` objects to the `set` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32613a",
   "metadata": {},
   "source": [
    "### Additional Settings and Methods\n",
    "\n",
    "All three classes have additional `max_label_length` and `step` parameters. The `max_label_length` is the maximum number of characters in a token's `milestones_label` attribute (the default is 20). The `step` parameter takes an integer indicating the number of spans per item in the milestones list. For instance, if you wanted to have a milestone every tenth sentence, setting `step=10` would mean that every item in the `milestones.spans` list would consist of ten sentences. This parameter can similarly be used to group lines or custom spans.\n",
    "\n",
    "All three classes have a `reset` method, which will remove all spans from both the `Milestones` and `Doc` objects.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
