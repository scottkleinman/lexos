{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite Database Integration Tutorial\n",
    "\n",
    "This tutorial demonstrates the SQLite database integration for the Lexos `corpus` module. The database integration provides:\n",
    "\n",
    "- **Full-text search**: Efficient search across document content in the corpus\n",
    "- **Metadata queries**: Filter and aggregate corpus statistics\n",
    "- **Flexible deployment options**: In-memory, file-based, or hybrid storage\n",
    "- **Data Integrity**: Hash verification and transaction safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the database-enabled corpus classes\n",
    "from lexos.corpus.sqlite import create_corpus\n",
    "\n",
    "print(\"Database integration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Basic Database-Enabled Corpus\n",
    "\n",
    "Create a corpus with database integration enabled alongside file storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database-enabled corpus with dual storage\n",
    "corpus = create_corpus(\n",
    "    corpus_dir=\"demo_corpus\",\n",
    "    sqlite_path=\"demo_corpus.db\",\n",
    "    name=\"Demo Corpus\",\n",
    "    sqlite_only=False  # Use both files and database\n",
    ")\n",
    "\n",
    "print(f\"Created corpus: {corpus.name}\")\n",
    "print(f\"Database enabled: {corpus.use_sqlite}\")\n",
    "print(f\"Database path: {corpus.db.database_path if corpus.db else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created a database-enabled corpus, all the corpus attributes, properties, and methods of the `Corpus` class are available for your use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Documents\n",
    "\n",
    "Add documents that will be stored in both the file system and database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for demonstration\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"The quick brown fox jumps over the lazy dog. This pangram contains every letter of the alphabet.\",\n",
    "        \"name\": \"pangram_sample\",\n",
    "        \"metadata\": {\"genre\": \"example\", \"language\": \"english\", \"type\": \"pangram\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Machine learning is a powerful tool for analyzing large datasets and extracting meaningful patterns.\",\n",
    "        \"name\": \"ml_description\",\n",
    "        \"metadata\": {\"genre\": \"technical\", \"language\": \"english\", \"type\": \"description\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Shakespeare wrote many famous plays including Hamlet, Romeo and Juliet, and Macbeth during the Elizabethan era.\",\n",
    "        \"name\": \"shakespeare_info\",\n",
    "        \"metadata\": {\"genre\": \"literature\", \"language\": \"english\", \"type\": \"biographical\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Climate change represents one of the most significant challenges facing humanity in the 21st century.\",\n",
    "        \"name\": \"climate_statement\",\n",
    "        \"metadata\": {\"genre\": \"science\", \"language\": \"english\", \"type\": \"statement\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add documents to the corpus\n",
    "for doc in documents:\n",
    "    corpus.add(\n",
    "        content=doc[\"content\"],\n",
    "        name=doc[\"name\"],\n",
    "        metadata=doc[\"metadata\"]\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(documents)} documents to corpus\")\n",
    "print(f\"Total records: {corpus.num_docs}\")\n",
    "print(f\"Active records: {corpus.num_active_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Full-Text Search Capabilities\n",
    "\n",
    "Use the `search()` method to leverage SQLite's FTS5 full-text search functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for documents containing specific terms\n",
    "search_queries = [\n",
    "    \"machine learning\",\n",
    "    \"Shakespeare\",\n",
    "    \"climate change\",\n",
    "    \"fox jumps\",\n",
    "    \"alphabet OR patterns\"\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    results = corpus.search(query, limit=10)\n",
    "    print(f\"\\nSearch: '{query}'\")\n",
    "    print(f\"Found {len(results)} results:\")\n",
    "\n",
    "    for record in results:\n",
    "        print(f\"  - {record.name}: {record.preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform more complex searches using SQL syntax or wildcards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex boolean search\n",
    "complex_query = '(\"machine learning\" OR \"artificial intelligence\") AND dataset*'\n",
    "complex_results = corpus.search(complex_query)\n",
    "print(f\"Complex query results: {len(complex_results)}\")\n",
    "\n",
    "# Prefix search\n",
    "prefix_results = corpus.search('Shakespear*')\n",
    "print(f\"Prefix search results: {len(prefix_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Filtering\n",
    "\n",
    "You can use database queries to filter records by various criteria. For this, use the `filter_records()` method. Parameters are:\n",
    "\n",
    "- `is_active`: A boolean indicating whether the record is active\n",
    "- `is_parsed`: A boolean indicating whether the record is active\n",
    "- `model`: A string indicating the record's language model\n",
    "- `min_tokens`: An integer indicating the minimum number of tokens in the record's content\n",
    "- `max_tokens`: An integer indicating the maximum number of tokens in the record's content\n",
    "- `limit`: An integer indicating the maximum number of results to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by token count range\n",
    "short_docs = corpus.filter_records(min_tokens=1, max_tokens=15)\n",
    "print(f\"Short documents (1-15 tokens): {len(short_docs)}\")\n",
    "for doc in short_docs:\n",
    "    print(f\"  - {doc.name}: {doc.num_tokens() if doc.is_parsed else 'not parsed'} tokens\")\n",
    "\n",
    "# Filter by active status\n",
    "active_docs = corpus.filter_records(is_active=True)\n",
    "print(f\"\\nActive documents: {len(active_docs)}\")\n",
    "\n",
    "# Filter by parsing status\n",
    "parsed_docs = corpus.filter_records(is_parsed=True)\n",
    "print(f\"Parsed documents: {len(parsed_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Statistics\n",
    "\n",
    "You can get comprehensive statistics directly from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database-derived statistics\n",
    "db_stats = corpus.get_stats()\n",
    "\n",
    "print(\"Database Statistics:\")\n",
    "print(f\"  Total records: {db_stats['total_records']}\")\n",
    "print(f\"  Active records: {db_stats['active_records']}\")\n",
    "print(f\"  Parsed records: {db_stats['parsed_records']}\")\n",
    "print(f\"  Total tokens: {db_stats['total_tokens']}\")\n",
    "print(f\"  Total terms: {db_stats['total_terms']}\")\n",
    "print(f\"  Average vocab density: {db_stats['average_vocab_density']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database-Only Mode\n",
    "\n",
    "If you want to store corpus records only in the database, rather than in separate files, you can create a corpus that uses only database storage by setting `sqlite_only=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database-only corpus\n",
    "db_only_corpus = create_corpus(\n",
    "    corpus_dir=\"memory_corpus\",\n",
    "    sqlite_path=\":memory:\",  # In-memory database\n",
    "    name=\"Memory Corpus\",\n",
    "    sqlite_only=True  # No file storage\n",
    ")\n",
    "\n",
    "# Add some documents\n",
    "test_docs = [\n",
    "    \"This is a test document for the memory-based corpus.\",\n",
    "    \"Another example showing database-only storage capabilities.\",\n",
    "    \"Fast in-memory operations for temporary analysis workflows.\"\n",
    "]\n",
    "\n",
    "for i, content in enumerate(test_docs):\n",
    "    db_only_corpus.add(\n",
    "        content=content,\n",
    "        name=f\"memory_doc_{i+1}\",\n",
    "        metadata={\"source\": \"memory_test\", \"doc_id\": i+1}\n",
    "    )\n",
    "\n",
    "print(f\"Database-only corpus created with {db_only_corpus.num_docs} documents\")\n",
    "\n",
    "# Search in memory corpus\n",
    "memory_results = db_only_corpus.search(\"memory OR operations\")\n",
    "print(f\"Search results in memory corpus: {len(memory_results)}\")\n",
    "for result in memory_results:\n",
    "    print(f\"  - {result.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Synchronization\n",
    "\n",
    "You can also synchronize records between file-based and database storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.corpus import Corpus\n",
    "from lexos.corpus.sqlite import SQLiteCorpus\n",
    "\n",
    "# Create a traditional file-based corpus first\n",
    "file_corpus = Corpus(corpus_dir=\"sync_test_corpus\", name=\"File Corpus\")\n",
    "\n",
    "# Add some documents to file corpus\n",
    "sync_docs = [\n",
    "    \"Document one for synchronization testing.\",\n",
    "    \"Document two with different content for sync.\",\n",
    "    \"Final document to test the sync process.\"\n",
    "]\n",
    "\n",
    "for i, content in enumerate(sync_docs):\n",
    "    file_corpus.add(\n",
    "        content=content,\n",
    "        name=f\"sync_doc_{i+1}\",\n",
    "        metadata={\"sync_test\": True, \"file_id\": i+1}\n",
    "    )\n",
    "\n",
    "print(f\"Created file corpus with {file_corpus.num_docs} documents\")\n",
    "\n",
    "# Now create database-enabled corpus and sync from files\n",
    "sync_corpus = SQLiteCorpus(\n",
    "    corpus_dir=\"sync_test_corpus\",  # Same directory\n",
    "    sqlite_path=\"sync_test.db\",\n",
    "    name=\"File Corpus\",  # Same name\n",
    "    use_sqlite=True\n",
    ")\n",
    "\n",
    "# Load existing file-based records\n",
    "# sync_corpus.load(corpus_dir=\"sync_test_corpus\")\n",
    "\n",
    "# Synchronize to database\n",
    "synced_count = sync_corpus.sync()\n",
    "print(f\"Synchronized {synced_count} records to database\")\n",
    "\n",
    "# Test search on synchronized data\n",
    "sync_results = sync_corpus.search(\"synchronization OR sync\")\n",
    "print(f\"Search results in synchronized corpus: {len(sync_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "If you have been using this notebook to test the functionality of the SQLite integration, now is a good time to delete the database and record files you created. Run this cell to perform a clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of test directories and files to clean up\n",
    "cleanup_items = [\n",
    "    \"demo_corpus\",\n",
    "    \"demo_corpus.db\",\n",
    "    \"sync_test_corpus\",\n",
    "    \"sync_test.db\",\n",
    "    \"memory_corpus\"\n",
    "]\n",
    "\n",
    "print(\"Cleaning up test files and directories:\")\n",
    "for item in cleanup_items:\n",
    "    try:\n",
    "        if os.path.isdir(item):\n",
    "            shutil.rmtree(item)\n",
    "            print(f\"  - Removed directory: {item}\")\n",
    "        elif os.path.isfile(item):\n",
    "            os.remove(item)\n",
    "            print(f\"  - Removed file: {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Could not remove {item}: {str(e)}\")\n",
    "\n",
    "print(\"\\nCleanup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
