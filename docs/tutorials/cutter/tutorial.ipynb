{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e1d73",
   "metadata": {},
   "source": [
    "# Cutting Documents\n",
    "\n",
    "The Lexos `cutter` module is used to split documents into smaller, more manageable pieces, variously called \"segments\" or \"chunks.\" This is particularly useful for processing large texts, enabling more efficient analysis and manipulation. If your documents are raw text files or strings, you can use the `TextCutter` class. If your documents are tokenized spaCy `Doc` objects, you can use the `TokenCutter` class. Documents can be cut based on byte size, number of tokens, number of sentences, line breaks, or custom-defined spans called milestones. The different cutting methods are described below.\n",
    "\n",
    "## Cutting Text Strings with `TextCutter`\n",
    "\n",
    "Let's say that you have a  long text string that you wanted to break into smaller chunks every *n* characters. The cell below demonstrates a simple way you can do this with the `TextCutter` class. We to begin, we'll use a very short sample text for demonstration purposes, but you can replace the `text` variable with any long string of your choice. You can also change the `chunksize` parameter to specify how many characters you want in each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba16890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TextCutter class\n",
    "from lexos.cutter.text_cutter import TextCutter\n",
    "\n",
    "# Create a sample text\n",
    "text1 = \"It is a truth universally acknowledged, that a single  man in possession of a good fortune must be in want of a wife.\"\n",
    "\n",
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Split text into chunks of 20 characters each\n",
    "cutter.split(text1, chunksize=20)\n",
    "\n",
    "# Print the resulting chunks\n",
    "for chunk in cutter.chunks[0]:\n",
    "    print(f\"- {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315470cd",
   "metadata": {},
   "source": [
    "The first parameter (for which you can use the keyword `docs`) is the text string to be cut. You can also supply a list of text strings (e.g., multiple documents) to the `docs` parameter, and each document will be cut separately.\n",
    "\n",
    "Once the text has been split, its chunks are stored in a list of lists (one list per document), which can be accessed with `cutter.chunks`. This `split()` method can also save this to a variable, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Split text into chunks of 20 characters each and store the result in a variable\n",
    "chunks = cutter.split(docs=text1, chunksize=20)\n",
    "\n",
    "for chunk in chunks[0]:\n",
    "    print(f\"- {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d66a6",
   "metadata": {},
   "source": [
    "Notice that we need to reference the first item in the `chunks` list because it is the first (and in this case only) document. Here is an example with multiple documents.\n",
    "\n",
    "We can supply an optional `names` parameter to name each document for easier identification later. This allows us to view the chunks for each document separately in a dictionary using the `to_dict()` method. To try it out, uncomment the second `split()` method in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another text\n",
    "text2 = \"However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\"\n",
    "\n",
    "# Combine texts into a list\n",
    "texts = [text1, text2]\n",
    "\n",
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Split texts into chunks of 50 characters each and store the result in a variable\n",
    "chunks = cutter.split(docs=texts, chunksize=50)\n",
    "\n",
    "# Use custom document names\n",
    "# chunks = cutter.split(docs=texts, names=[\"text1\", \"text2\"], chunksize=50)\n",
    "\n",
    "# Print chunks for each document\n",
    "for i, doc_chunks in enumerate(chunks):\n",
    "    print(f\"Document {i + 1} chunks:\")\n",
    "    for chunk in doc_chunks:\n",
    "        print(f\"- {chunk}\")\n",
    "    print()\n",
    "\n",
    "# Display chunks as a dictionary\n",
    "print(\"Chunks as dictionary:\")\n",
    "print(cutter.to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a3987",
   "metadata": {},
   "source": [
    "You can also use the `len()` function to check how many documents are in your cutter instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11237e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of docs in the cutter instance\n",
    "print(f\"Number of docs in cutter instance: {len(cutter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d029d",
   "metadata": {},
   "source": [
    "If your texts are stored as files, you can easily read them into strings using Python's built-in file handling methods before passing them to the `TextCutter`. However, you can also pass the file paths directly to the `TextCutter` using the `docs` parameter and set `file=True`. We'll demonstrate this below using the full text of Jane Austen's *Pride and Prejudice*. In this example, we use only one file, but you can pass a list of file paths to the `docs` parameter to cut multiple files at once.\n",
    "\n",
    "Note that the `names` will be auto-generated from your filenames unless you provide a list of custom names using the `names` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Path to text file\n",
    "path = \"Austen_Pride.txt\"\n",
    "\n",
    "# Split text into chunks of 500 characters each and store the result in a variable\n",
    "chunks = cutter.split(docs=path, chunksize=500, file=True)\n",
    "\n",
    "# Print the names of the documents\n",
    "print(f\"Document names: {cutter.names}\\n\")\n",
    "\n",
    "# Get the second chunk\n",
    "chunk2 = chunks[0][1]\n",
    "\n",
    "# Print the first 100 characters of the second chunk\n",
    "print(chunk2[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a1c45",
   "metadata": {},
   "source": [
    "Cutting texts can often leave small dangling pieces at the end. To address this, you can use the `merge_threshold` and `merge_final` parameters to control whether the last two chunks should be merged based on their size.\n",
    "\n",
    "- `merge_threshold`: The threshold for merging the last two chunks. The default is 0.5 (50%).\n",
    "- `merge_final`: Whether to merge the last two chunks. The default is `False`.\n",
    "\n",
    "**Always inspect your chunks to see if the merging behaviour meets your expectations.**\n",
    "\n",
    "In the cell below, try changing the `chunksize`, `merge_threshold`, and `merge_final` parameters to see how they affect the resulting chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd949598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Split text into chunks and specify merge behaviour\n",
    "chunks = cutter.split(text1, chunksize=25, merge_final=False, merge_threshold=0.5)\n",
    "\n",
    "# Print chunks for each document\n",
    "for i, doc_chunks in enumerate(chunks):\n",
    "    for chunk in doc_chunks:\n",
    "        print(f\"- {chunk} ({len(chunk)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f785389",
   "metadata": {},
   "source": [
    "You can also generate chunks that overlap with each other by using the `overlap` parameter. This parameter specifies the number of characters that should be repeated at the start of each chunk (except the first one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8039ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Split text into chunks and specify merge behaviour\n",
    "chunks = cutter.split(text1, chunksize=25, overlap=5, merge_threshold=0.0)\n",
    "\n",
    "# Print chunks for each document overlapping chunks by 5 characters\n",
    "for i, doc_chunks in enumerate(chunks):\n",
    "    for chunk in doc_chunks:\n",
    "        print(f\"- {chunk} ({len(chunk)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a46f3e",
   "metadata": {},
   "source": [
    "### Cutting Documents into a Fixed Number of Chunks\n",
    "\n",
    "The `n` parameter allows you to specify the number of chunks to split the text into. When you provide a value for `n`, the text will be divided into that many approximately equal parts. This is useful when you want to ensure that the text is split into a specific number of segments, regardless of their size. The `n` parameter overrides the `chunksize` parameter if both are provided.\n",
    "\n",
    "In the example below, we split the text of *Pride and Prejudice* into 5 chunks. Try changing the value of `merge_threshold`, for instance, to the default 0.0, to see how it affects the number of chunks created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Split the text into a fixed number of chunks\n",
    "cutter.split(path, file=True, n=5, merge_threshold=0.0)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Number of chunks created: {len(cutter.chunks[0])}\")\n",
    "\n",
    "# Print the chunk excerpts with chunk lengths\n",
    "for i, chunk in enumerate(cutter.chunks[0]):\n",
    "    chunk = chunk.strip().replace(\"\\n\", \" \")\n",
    "    print(f\"- Chunk {i + 1} ({len(chunk)} characters): {chunk[0:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26705f9b",
   "metadata": {},
   "source": [
    "### Cutting Documents by Line\n",
    "\n",
    "The `newline` parameter allows you to split the text based on line breaks instead of character count. In other words, the cutter will create chunks that respect line boundaries. This is particularly useful for texts where line structure is important, such as poetry, scripts, or any text where lines represent meaningful units.\n",
    "\n",
    "When using `newline=True`, both the `chunksize` and `n` parameters refer to line counts rather than character counts.\n",
    "\n",
    "The example below creates chunks with 3 lines each. The last chunk will contain any remaining lines and will depend on your merge settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "# Create a text with line breaks\n",
    "lineated_text = \"\"\"It is a truth universally acknowledged,\n",
    "that a single man in possession of a good fortune,\n",
    "must be in want of a wife.\n",
    "However little known the feelings or views of such a man may be\n",
    "on his first entering a neighbourhood,\n",
    "this truth is so well fixed in the minds of the surrounding families,\n",
    "that he is considered the rightful property\n",
    "of some one or other of their daughters.\"\"\"\n",
    "\n",
    "# Split the text into chunks of 3 lines each\n",
    "cutter.split(lineated_text, chunksize=3, newline=True)\n",
    "\n",
    "# Print the lines for each chunk\n",
    "for i, chunk in enumerate(cutter.chunks[0]):\n",
    "    print(f\"Chunk {i + 1} lines:\")\n",
    "    lines = chunk.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        print(f\"- {line}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffef15",
   "metadata": {},
   "source": [
    "### Cutting Documents on Milestones\n",
    "\n",
    "Milestones are specified locations in the text that designate structural or sectional divisions. A milestone can be either a designated unit *within* the text or a placemarker inserted between sections of text. The Lexos `milestones` module provides methods for identifying milestone locations by searching for patterns you designate. You can use the `StringMilestones` class in the `milestones` module to generate a list of `StringSpan` objects that mark the locations of milestones in your text. The `TextCutter.split_on_milestones()` method can then use these spans to split the text into chunks at the specified locations. Here is a quick example of how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StringMilestones class\n",
    "from lexos.milestones.string_milestones import StringMilestones\n",
    "\n",
    "# A sample doc\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create a String Milestones instance and search for the pattern \"quick\"\n",
    "# milestones.spans is a list of StringSpan objects\n",
    "milestones = StringMilestones(doc=text, patterns=\"quick\")\n",
    "\n",
    "# Create a TextCutter instance and split on the found milestones\n",
    "cutter = TextCutter()\n",
    "chunks = cutter.split_on_milestones(docs=text, milestones=milestones.spans)\n",
    "print(chunks[0][0])  # The\n",
    "print(chunks[0][1])  # brown fox jumps over the lazy dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd0292",
   "metadata": {},
   "source": [
    "You will notice that the milestone itself (\"quick\") is not included in either chunk. By default, the milestone text is removed during the split. You can control this behaviour with the `keep_spans` parameter set to \"preceding\" or \"following\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74850d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preceding: milestone included at end of previous chunk\n",
    "cutter_preceding = TextCutter()\n",
    "chunks_preceding = cutter_preceding.split_on_milestones(\n",
    "    docs=text, milestones=milestones.spans, keep_spans=\"preceding\"\n",
    ")\n",
    "print(\"Preceding:\", chunks_preceding[0])\n",
    "\n",
    "# Following: milestone included at start of next chunk\n",
    "cutter_following = TextCutter()\n",
    "chunks_following = cutter_following.split_on_milestones(\n",
    "    docs=text, milestones=milestones.spans, keep_spans=\"following\"\n",
    ")\n",
    "print(\"Following:\", chunks_following[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609cda9",
   "metadata": {},
   "source": [
    "If you do not want to use the `milestones` module to find milestones, you can also create your own list of `StringSpan` objects manually and pass them to the `split_on_milestones()` method.\n",
    "\n",
    "This tutorial does not cover all the options for using the `milestones` module to find milestones in your text. For more information, please refer to the Lexos `milestones` documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35cd36",
   "metadata": {},
   "source": [
    "### Saving Chunks to Disk\n",
    "\n",
    "Once you have split your documents into chunks, you may want to save these chunks to disk for later use. The `save()` method of the `TextCutter` class allows you to do this easily. You can specify an output directory where the chunks will be saved, and optionally provide names for each document to create more meaningful filenames for the chunks.\n",
    "\n",
    "In the example below, we save the chunks to the output directory specified with `output_dir`. Each chunk will be saved as a separate text file, with filenames based on the document names and chunk indices. You can specify your own names for the documents using the `names` parameter. If you do not provide names (or they are not already defined in your `TextCutter` instance), default names will be used These have the format  `doc001_001.txt`; however, you can modify the `_` delimiter and the zero padding with the `delimiter` and `padding` parameters.\n",
    "\n",
    "In the cell below, you can uncomment parameters in the `save()` method to change filenames. Make sure to specify a valid output directory path on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6767ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextCutter\n",
    "cutter = TextCutter()\n",
    "\n",
    "texts = [\n",
    "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood,\",\n",
    "    \"this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\",\n",
    "]\n",
    "\n",
    "# Spread the texts into chunks\n",
    "cutter.split(texts, chunksize=50)\n",
    "\n",
    "# Configure the output directory\n",
    "output_dir = (\"ADD_YOUR_OUTPUT_DIRECTORY_PATH_HERE\",)\n",
    "\n",
    "# Save the chunks to disk -- uncomment lines to change the settings\n",
    "cutter.save(\n",
    "    output_dir=output_dir,\n",
    "    # names = [\"text1\", \"text2\"],\n",
    "    # delimiter = \"-\",\n",
    "    # pad = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e925b9",
   "metadata": {},
   "source": [
    "## Cutting spaCy Documents with `TokenCutter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TextCutter class\n",
    "from lexos.cutter.token_cutter import TokenCutter\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "\n",
    "# Create a sample doc\n",
    "text1 = \"It is a truth universally acknowledged, that a single  man in possession of a good fortune, must be in want of a wife.\"\n",
    "doc1 = tokenizer.make_doc(text1)\n",
    "\n",
    "# Initialize TokenCutter\n",
    "cutter = TokenCutter()\n",
    "\n",
    "# Split text into chunks of 10 tokens each\n",
    "cutter.split(doc1, chunksize=10)\n",
    "\n",
    "# Print the resulting chunks\n",
    "for chunk in cutter.chunks[0]:\n",
    "    print(f\"- {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac3011",
   "metadata": {},
   "source": [
    "Cutting multiple `Doc` objects is similar to cutting multiple texts. Uncomment the second `split()` method in the cell below to see how to use custom document names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second document\n",
    "text2 = \"However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\"\n",
    "doc2 = tokenizer.make_doc(text2)\n",
    "\n",
    "\n",
    "# Initialize TokenCutter\n",
    "cutter = TokenCutter()\n",
    "\n",
    "# Split text into chunks of 10 characters each and store the result in a variable\n",
    "chunks = cutter.split(docs=[doc1, doc2], chunksize=10)\n",
    "\n",
    "# Use custom document names\n",
    "# chunks = cutter.split(docs=texts, names=[\"text1\", \"text2\"], chunksize=10)\n",
    "\n",
    "# Print chunks for each document\n",
    "for i, doc_chunks in enumerate(chunks):\n",
    "    print(f\"Document {i + 1} chunks:\")\n",
    "    for chunk in doc_chunks:\n",
    "        print(f\"- {chunk}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f2a75",
   "metadata": {},
   "source": [
    "You can also display the number of documents in the cutter instance using the `len()` function, and export the chunks to a dictionary using the `to_dict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of docs in the cutter instance\n",
    "print(f\"Number of docs in cutter instance: {len(cutter)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Display chunks as a dictionary\n",
    "print(\"Chunks as dictionary:\")\n",
    "print(cutter.to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be531803",
   "metadata": {},
   "source": [
    "Cutting spaCy `Doc` objects directly from files requires a slightly different approach compared to cutting raw text files. The files must be in spaCy's binary format, which can be created using the `Doc.to_bytes()` or `Doc.to_disk()` methods. You must also specify the spaCy model used to create the `Doc` objects so that they can be deserialised correctly. For example:\n",
    "\n",
    "```python\n",
    "path = \"path/to/spacy_doc.spacy\"\n",
    "cutter.split(docs=path, file=True, spacy_model=\"en_core_web_sm\", chunksize=100)\n",
    "```\n",
    "\n",
    "If your files are in raw text format, you will need to read and tokenize them into `Doc` objects before passing them to the `TokenCutter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc15d33",
   "metadata": {},
   "source": [
    "### Cutting Tokens on Line Breaks\n",
    "\n",
    "Using line breaks to cut tokenized documents works similarly to cutting raw text documents. By setting the `newline` parameter to `True`, the `TokenCutter` will create chunks that respect line boundaries based on token positions. Both the `chunksize` and `n` parameters will refer to line counts rather than token counts when `newline=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456cfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TokenCutter\n",
    "cutter = TokenCutter()\n",
    "\n",
    "lineated_text = \"\"\"It is a truth universally acknowledged,\n",
    "that a single man in possession of a good fortune,\n",
    "must be in want of a wife.\n",
    "However little known the feelings or views of such a man may be\n",
    "on his first entering a neighbourhood,\n",
    "this truth is so well fixed in the minds of the surrounding families,\n",
    "that he is considered the rightful property\n",
    "of some one or other of their daughters.\"\"\"\n",
    "\n",
    "lineated_doc = tokenizer.make_doc(lineated_text)\n",
    "\n",
    "cutter.split(docs=[lineated_doc], chunksize=3, newline=True)\n",
    "for chunk in cutter.chunks[0]:\n",
    "    print(f\"- {chunk}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a69962",
   "metadata": {},
   "source": [
    "### Cutting on Sentences Breaks\n",
    "\n",
    "Some spaCy language models (such as `en_core_web_sm`) include sentence boundary detection. If your `Doc` objects have sentence boundaries defined, you can use the `split_on_sentences()` method to cut the documents into chunks based on a specified number of sentences. For instances, assume that your spaCy `Doc` object has ten sentences. You can split it into chunks of 5 sentences each as shown below. The last chunk will contain any remaining sentences and will depend on your merge settings. If a `Doc` does not have sentence boundaries defined, Lexos will raise an error.\n",
    "\n",
    "In the example below, we will cut a spaCy `Doc` object created from the first 2000 characters of *Pride and Prejudice* into chunks of 5 sentences each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have both TokenCutter and Tokenizer imported\n",
    "from lexos.cutter.token_cutter import TokenCutter\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "# Initialize Tokenizer with the English-language model\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "\n",
    "# Initialize TokenCutter\n",
    "cutter = TokenCutter()\n",
    "\n",
    "# Read a text file\n",
    "path = \"Austen_Pride.txt\"\n",
    "with open(path, \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Convert the first 2000 characters of the text into a spaCy Doc\n",
    "doc = tokenizer.make_doc(text[:2000])\n",
    "\n",
    "chunks = cutter.split_on_sentences(docs=doc, n=5)\n",
    "for chunk in chunks[0]:\n",
    "    print(f\"- {chunk.text.strip().replace('\\n', ' ')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee598d",
   "metadata": {},
   "source": [
    "### Cutting Tokens on Milestones\n",
    "\n",
    "Cutting on milestones works similarly for `TokenCutter` objects, except that the milestones are specified as lists of spaCy `Span` objects rather than `StringSpan` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38760e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the TokenMilestones, TokenCutter, and Tokenizer are imported\n",
    "from lexos.milestones.token_milestones import TokenMilestones\n",
    "from lexos.cutter.token_cutter import TokenCutter\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "# Initialize Tokenizer with the English-language model\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "\n",
    "# Create a sample doc\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = tokenizer.make_doc(text)\n",
    "\n",
    "# Create a TokenMilestones instance and search for the pattern \"quick\"\n",
    "milestones = TokenMilestones(doc=doc)\n",
    "\n",
    "# Get a list of Span matches for the pattern \"quick\"\n",
    "spans = milestones.get_matches(patterns=\"quick\")\n",
    "\n",
    "# Create a TokenCutter instance and split on the found milestones\n",
    "cutter = TokenCutter()\n",
    "\n",
    "# Split the doc on the found milestones\n",
    "chunks = cutter.split_on_milestones(docs=doc, milestones=spans, merge_threshold=0.0)\n",
    "print(chunks[0][0])  # The\n",
    "print(chunks[0][1])  # brown fox jumps over the lazy dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306ed46",
   "metadata": {},
   "source": [
    "As with `TextCutter`, you can modify the handling of the milestone text during the split using the `keep_spans` parameter set to \"preceding\" or \"following\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preceding: milestone included at end of previous chunk\n",
    "cutter_preceding = TokenCutter()\n",
    "chunks_preceding = cutter_preceding.split_on_milestones(\n",
    "    docs=doc, milestones=spans, keep_spans=\"preceding\", merge_threshold=0.0\n",
    ")\n",
    "print(\"Preceding:\", chunks_preceding[0])\n",
    "\n",
    "# Following: milestone included at start of next chunk\n",
    "cutter_following = TokenCutter()\n",
    "chunks_following = cutter_following.split_on_milestones(\n",
    "    docs=doc, milestones=spans, keep_spans=\"following\", merge_threshold=0.0\n",
    ")\n",
    "print(\"Following:\", chunks_following[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f239d26",
   "metadata": {},
   "source": [
    "###  Saving spaCy `Doc` Files to Disk\n",
    "\n",
    "By default, the `save()` method saves the chunk text strings, rather than the spaCy `Doc` objects. If you would like to store the spaCy `Doc` objects themselves, set the `as_text` parameter to `False`. This is the equivalent of calling spaCy's `Doc.to_bytes()` method on each chunk and saving the resulting bytes to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db633d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the output directory\n",
    "output_dir = (\"ADD_YOUR_OUTPUT_DIRECTORY_PATH_HERE\",)\n",
    "\n",
    "# Save the chunks to disk -- uncomment lines to change the settings\n",
    "cutter.save(\n",
    "    output_dir=output_dir,\n",
    "    # names = [\"text1\", \"text2\"],\n",
    "    # delimiter = \"-\",\n",
    "    # pad = 2,\n",
    "    # as_text = False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
