# Overview

The `Tokenizer` uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual `xx_sent_ud_sm` model is used.

### ::: lexos.tokenizer.make_doc
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer.make_docs
    rendering:
      show_root_heading: true
      heading_level: 3
