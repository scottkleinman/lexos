{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11178670",
   "metadata": {},
   "source": [
    "# Filter Module Tutorial\n",
    "\n",
    "The `filter` module provides a set of tools for filtering and identifying tokens within spaCy `Doc` objects. Filters allow you to identify specific types of tokens (such as words, Roman numerals, or stop words) and work with the matched results. This is useful for preprocessing, analysis, and text transformation tasks.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "Lexos filters are built around the concept of matching tokens based on specific criteria. Each filter:\n",
    "\n",
    "1. Accepts a spaCy `Doc` object\n",
    "2. Uses a spaCy `Matcher` to identify tokens matching certain criteria\n",
    "3. Returns a modified document or provides access to matched tokens\n",
    "\n",
    "The standard output is either a filtered spaCy `Doc` or lists of matched token IDs, allowing flexible use of the results.\n",
    "\n",
    "We'll demonstrate the basic procedure for using filters below by importing the `IsWordFilter` class. This class identifies tokens that are words (as opposed to other symbols) and creates a new `Token` attribute `._.is_word` indicating whether the token is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de857b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the desired filter class\n",
    "from lexos.filter import IsWordFilter\n",
    "\n",
    "# Create a spaCy doc from your text\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(\"Hello, world! This is a test.\")\n",
    "\n",
    "# Create an instance of a filter\n",
    "word_filter = IsWordFilter()\n",
    "\n",
    "# Apply the filter to the doc\n",
    "filtered_doc = word_filter(doc)\n",
    "\n",
    "# Display whether tokens are word\n",
    "for token in filtered_doc:\n",
    "    if token._.is_word:\n",
    "        print(f\"'{token.text}' is a word\")\n",
    "    else:\n",
    "        print(f\"'{token.text}' is not a word\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0eb19",
   "metadata": {},
   "source": [
    "The `IsWordFilter` has built in a specific definition of what constitutes a \"word\". The token must not be a space, a punctuation mark, a digit, or a Roman numeral. This definition may not meet your requirements, so you can customise the behaviour with the following parameters.\n",
    "\n",
    "- `exclude` (list[str] | str, optional): Patterns to exclude from being considered words (default: [\" \", \"\\n\"])\n",
    "- `exclude_digits` (bool, optional): If True, numeric tokens will not be treated as words (default: False)\n",
    "- `exclude_roman_numerals` (bool, optional): If True, Roman numerals (capital letters only) will not be treated as words (default: False)\n",
    "- `exclude_pattern` (list[str] | str, optional): Additional regex patterns to exclude\n",
    "\n",
    "Here is a more complex example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import classes if needed\n",
    "from lexos.tokenizer import Tokenizer\n",
    "from lexos.filter import IsWordFilter\n",
    "\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(\"Hello, world! 123 and IV are not words.\")\n",
    "\n",
    "# Create a filter with specific options\n",
    "word_filter = IsWordFilter(exclude_digits=True, exclude_roman_numerals=False)\n",
    "filtered_doc = word_filter(doc)\n",
    "\n",
    "# Use matched_tokens instead of matches\n",
    "print(f\"Matched tokens: {[token.text for token in word_filter.matched_tokens]}\")\n",
    "\n",
    "# Or use matched_token_ids\n",
    "print(\"\\nMatched token IDs:\", word_filter.matched_token_ids)\n",
    "\n",
    "# Or use custom extensions:\n",
    "print()\n",
    "for token in filtered_doc:\n",
    "    if token._.is_word == True:\n",
    "        print(f\"'{token.text}' is a word\")\n",
    "    else:\n",
    "        print(f\"'{token.text}' is not a word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc084728",
   "metadata": {},
   "source": [
    "## The `BaseFilter` Class\n",
    "\n",
    "The `IsWordFilter` inherits from `BaseFilter`, which has the following properties and methods:\n",
    "\n",
    "### Properties\n",
    "\n",
    "- `matched_tokens`: Returns a list of tokens that matched the filter criteria\n",
    "- `matched_token_ids`: Returns a set of token IDs that matched the filter criteria\n",
    "- `filtered_token_ids`: Returns a set of token IDs that did NOT match the filter criteria\n",
    "- `filtered_tokens`: Returns a list of tokens that did NOT match the filter criteria\n",
    "\n",
    "### Methods\n",
    "\n",
    "- `get_matched_doc()`: Creates a new spaCy `Doc` containing only the matched tokens\n",
    "- `get_filtered_doc()`: Creates a new spaCy `Doc` containing only the filtered (non-matched) tokens\n",
    "\n",
    "You can create your own filters that inherit from `BaseFilter` with:\n",
    "\n",
    "```python\n",
    "class MyCustomFilter(BaseFilter):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__(...)\n",
    "        # Custom initialization code here\n",
    "```\n",
    "\n",
    "Filtering tokens with `BaseFilter` methods can cause the neighbouring tokens to run together in the new document. You can use the `add_spaces` parameter to insert spaces between tokens in the new document to prevent this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_filter.get_matched_doc(add_spaces=True))\n",
    "print(word_filter.get_filtered_doc(add_spaces=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136211f",
   "metadata": {},
   "source": [
    "The `filters` module provides two other classes deriving from `BaseFilter`: `IsRomanFilter` and `IsStopWordFilter`. The first identifies stop words, while the second identifies Roman numerals. Their use is demonstrated below.\n",
    "\n",
    "`IsRomanFilter` identifies Roman numerals based on a specific pattern of capital letters (a limitation is that it only works on capital letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c912d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "from lexos.filter import IsRomanFilter\n",
    "\n",
    "# Create a spaCy doc\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(\"Chapter IV begins here. Not Roman: iv, but Roman: IV.\")\n",
    "\n",
    "# Create a Roman numeral filter\n",
    "roman_filter = IsRomanFilter(attr=\"is_roman\")\n",
    "roman_filter(doc)\n",
    "\n",
    "# Access matched Roman numerals\n",
    "roman_numerals = roman_filter.matched_tokens\n",
    "print(f\"Roman Numerals: {[token.text for token in roman_numerals]}\")\n",
    "print(doc[1], doc[1]._.is_roman)  # Should be True for \"IV\"\n",
    "print(doc[8], doc[8]._.is_roman)  # Should be False for \"iv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d6f28",
   "metadata": {},
   "source": [
    "The `IsStopwordFilter` class manages stop words in a spaCy model. Stop words are common words that are often filtered out during text processing (such as \"the\", \"a\", \"is\", etc.).\n",
    "\n",
    "!!! important\n",
    "    This filter modifies the model's default stop words. Changes will apply to any document created with that model unless the model is reloaded.\n",
    "\n",
    "The class has the following attributes:\n",
    "\n",
    "- `stopwords`: A list or string containing the stop word(s) to add or remove\n",
    "- `remove`: If True, the specified stop words will be removed from the model. If False, they will be added (default: False)\n",
    "- `case_sensitive`: If False (default), stop word changes apply to all case variations (lowercase, original, and capitalized). If True, only the exact case provided is modified (default: False)\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Stop Words:\n",
    "\n",
    "# Python imports\n",
    "from lexos.filter import IsStopwordFilter\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "# Create a spaCy doc from your text\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "# Add custom stop words\n",
    "stopword_filter = IsStopwordFilter()\n",
    "print(\"Adding custom stop words 'quick', 'brown' to stop words...\")\n",
    "stopword_filter(doc, stopwords=[\"quick\", \"brown\"], remove=False)\n",
    "\n",
    "# Now \"quick\" and \"brown\" are marked as stop words\n",
    "doc2 = tokenizer.make_doc(\"The quick brown fox\")\n",
    "for token in doc2:\n",
    "    if token.is_stop:\n",
    "        print(f\"-'{token.text}' is a stop word\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Removing Stop Words:\n",
    "\n",
    "# Remove \"the\" from stop words -- try change case sensitivity\n",
    "print(\"Removing 'the' from stop words...\")\n",
    "stopword_filter(doc, stopwords=\"the\", remove=True, case_sensitive=False)\n",
    "\n",
    "# Now \"the\" is no longer marked as a stop word\n",
    "doc2 = tokenizer.make_doc(\"The quick brown fox\")\n",
    "for token in doc2:\n",
    "    if token.is_stop:\n",
    "        print(f\"- '{token.text}' is a stop word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bd06f",
   "metadata": {},
   "source": [
    "You can set stop words back to their original state by reloading the spaCy model or by instantiating a new `Tokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer.make_doc(\"The quick brown fox jumps over the lazy dog.\")\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        print(f\"- '{token.text}' is a stop word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92078f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
