{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51938d95",
   "metadata": {},
   "source": [
    "# Tokenizing Texts\n",
    "\n",
    "Lexos uses the `Tokenizer` module to split text into meaningful units called tokens.  \n",
    "Unlike Lexos  which splits text using simple rules like whitespace, the API uses **spaCy** language models. These models work across many languages and add useful annotations like parts of speech, lemmas, and stop word flags.\n",
    "\n",
    "This tutorial shows how to use the `Tokenizer` class to process one or more texts with pre-trained language models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dd63b",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "Lexos wraps **spaCy**, a Natural Language Processing library, to load and apply language models for tokenization. These models offer rule-based and statistical approaches, giving annotated token objects.\n",
    "\n",
    "> **Important:**  \n",
    "> While more accurate, language models can consume more memory and may not work well for underrepresented languages.  \n",
    "> Lexos defaults to `\"xx_sent_ud_sm\"`(a multilingual model) for broad compatibility.\n",
    "\n",
    "You can also customize the model used by specifying its name (e.g., `\"en_core_web_sm\"`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35abe86",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca78abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "#default tokenizer using the multilingual model\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# specific language model that you can use\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc40bd2",
   "metadata": {},
   "source": [
    "# ðŸ“š Table of Contents\n",
    "\n",
    "- [Key Terms](#key-terms)\n",
    "- [Importing Tokenizer](#importing-tokenizer)\n",
    "- [Loading Data](#loading-data)\n",
    "- [Tokenizing a Single Text](#tokenizing-a-single-text)\n",
    "- [Tokenizing Multiple Texts](#tokenizing-multiple-texts)\n",
    "- [Changing Language Models](#changing-language-models)\n",
    "- [ Adding/Removing Stop Words](#adding-or-removing-stop-words)\n",
    "- [ Filtering Docs](#filtering-docs)\n",
    "- [ Adjusting spaCy Pipelines](#adjusting-spacy-pipelines)\n",
    "- [ Simple Tokenizers](#simple-tokenizers)\n",
    "- [Working with Ngrams](#ngrams)\n",
    "  - [From Text Input](#generating-ngrams-from-text)\n",
    "  - [From Token List](#from-token-list)\n",
    "  - [From spaCy Doc](#from-spacy-doc)\n",
    "  - [Filtering Ngrams](#filtering-options)\n",
    "  - [Character Ngrams](#character-ngrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2f852",
   "metadata": {},
   "source": [
    "## Key Terms\n",
    "\n",
    "Before proceeding, here are some key terms:\n",
    "\n",
    "- **Text**: A string of characters before any preprocessing.\n",
    "\n",
    "- **Token**: A unit of text used in natural language processing. Most commonly a token is a word, but it can also be punctuation, a number, or even whitespace, depending on how the text is split.  \n",
    "  **Example**:  \n",
    "  Sentence: `\"I like NLP!\"`  \n",
    "  Tokens: `['I', 'like', 'NLP', '!']`\n",
    "\n",
    "- **Document (doc)**: A parsed text returned by the tokenizer. This is a `spacy.Doc` object containing tokens and their attributes.\n",
    "\n",
    "- **Language Model**: A model that defines how text is segmented into tokens and what annotations are applied.\n",
    "\n",
    "- **Pipeline**: A series of NLP tasks applied to the text after tokenization (e.g., part-of-speech tagging, dependency parsing).\n",
    "\n",
    "- **N-gram**: A sequence of *n* tokens used to analyze patterns or context within texts.  \n",
    "  - Unigrams: single tokens  \n",
    "  - Bigrams: 2-token sequences â†’ `['I like', 'like NLP']`  \n",
    "  - Trigrams: 3-token sequences â†’ `['I like NLP']`\n",
    "\n",
    "- **Stopword**: A commonly used word often filtered out before processing because it adds little semantic value.  \n",
    "  **Examples**: `['the', 'is', 'and', 'of']`  \n",
    "  **Use case**: Remove stopwords to focus on more meaningful words.\n",
    "\n",
    "- **Filtering**: The process of removing certain types of tokens before generating n-grams or running analysis.\n",
    "\n",
    "  | Term             | Definition                                                                                   | Example / Notes                                         |\n",
    "  |------------------|----------------------------------------------------------------------------------------------|---------------------------------------------------------|\n",
    "  | `filter_stops`   | Removes tokens that are stopwords.                                                           | `\"This is good\"` â†’ `['This', 'good']` (removes `'is'`)  |\n",
    "  | `filter_punct`   | Removes punctuation-only tokens.                                                             | `\"Great!\"` â†’ `['Great']`                                |\n",
    "  | `filter_digits`  | Removes tokens that are only numeric digits.                                                 | `['test', '2023']` â†’ `['test']`                         |\n",
    "  | `filter_nums`    | Removes tokens that look like numbers, including decimals and formatted numbers.             | `['3.14', '100', 'ten']` â†’ `['ten']`                    |\n",
    "  | `min_freq`       | Removes n-grams that appear fewer times than a specified threshold.                          | `min_freq=2` removes rare n-grams                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a1c05",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "You can either input text manually or load it from an external file.  \n",
    "Here, we load in a text file.\n",
    "\n",
    "To load data to tokenize we'll use the Loader module to load in a text file from Github. The file we'll be using is a small portion of \"Pride and Prejudice\" by Jane Austen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaedd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.io import loader\n",
    "loader = loader.Loader()\n",
    "loader.load([\"https://raw.githubusercontent.com/scottkleinman/lexos/refs/heads/main/tests/test_data/txt/Austen_Pride_sm.txt\"])\n",
    "# use a file path instead if wanting to load in a file ex. loader.load([\"./data/local_text.txt\"]) \n",
    "text = loader.texts[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6bdc7",
   "metadata": {},
   "source": [
    "> **Tip**  \n",
    "> The `load()` method accepts a list of file paths or URLs. You can mix and match as long as the file type is supported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110883a9",
   "metadata": {},
   "source": [
    "### Manual Text Input\n",
    "Alternatively, you can define a text directly in your code, which is useful for quick testing or demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
    "\n",
    "#once defined you can tokenize this text using:\n",
    "doc = tokenizer.make_doc(text)\n",
    "[token.text for token in doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1032c",
   "metadata": {},
   "source": [
    "> **Tip**  \n",
    "> Manual text input is ideal for experimenting with pipeline behavior or for debugging small examples before scaling up to batch processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd12fd5",
   "metadata": {},
   "source": [
    "## Selecting a Model <br>\n",
    "As mentioned previously, you can select the model that tokenizer uses in order to get more information from or text, or to better fit the language the text is in. In order to do this, you can use the `model` parameter in the `make_doc()` function to override the default model. For this example, we'll use the `'en_web_core_sm'` model, since \"Pride and Prejudice\" is written in english. This model tags parts of speech to each token, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(model=\"en_core_web_sm\")\n",
    "doc = tokenizer_en.make_doc(text)\n",
    "print(\"\\nTokens with parts of speech:\")\n",
    "for token in doc[0:50]:\n",
    "    print(f\"<{token.text}> : {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2063c07",
   "metadata": {},
   "source": [
    "## Adjusting spaCy Pipelines <br>\n",
    "Sometimes, the size of a text and the amount of information being collected by a language model can severly slow down the processing speed of a tokenization call. If one would like to disable a component of a model in order to speed up processing speed, then one can use the `remove_extension()` method to disable a component of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.remove_extension(\"tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2803beb",
   "metadata": {},
   "source": [
    "Similarly, if one would like to add or re-enable a component of a model, then they can use the `add_extension()` method to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.add_extension(\"tagger\", default=\"default_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5de91d",
   "metadata": {},
   "source": [
    "## Tokenizing a Single Text\n",
    "\n",
    "Once your text is loaded, you can tokenize it using the `Tokenizer` class.\n",
    "\n",
    "The recommended method is to use the `make_doc()` function, which takes a string and returns a `spaCy.Doc` object. This object contains the original text and a sequence of annotated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d459a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_def = Tokenizer()\n",
    "doc = tokenizer_def.make_doc(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d2cfa",
   "metadata": {},
   "source": [
    "Alternatively, you can call the `Tokenizer` instance directly, just like you would with a `spaCy` `Language` object.  \n",
    "This automatically routes input to either `make_doc()` or `make_docs()` depending on whether a single string or a list of strings is passed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alernatively call Tokenizer object directly:\n",
    "doc = tokenizer_def(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c149a14",
   "metadata": {},
   "source": [
    "After tokenization, you can access the tokens in the returned `Doc` object.  \n",
    "Here's an example that prints the first 50 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51691281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTokens:\")\n",
    "for token in doc[0:50]:\n",
    "    print(f\"<{token.text}>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ddbb5",
   "metadata": {},
   "source": [
    "You can access the original text by referencing `doc.text`, and by using the bracket operators, you can access a substring of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_text = doc.text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0ed79",
   "metadata": {},
   "source": [
    "## Tokenizing Multiple Texts\n",
    "\n",
    "The `Tokenizer` class provides the `make_docs()` method to convert a list of raw strings into a list of `spaCy.Doc` objects.  \n",
    "Each string is tokenized using the language model and returned with full annotations.\n",
    "\n",
    "> **Note**  \n",
    "> This method is ideal for batch processing documents.  \n",
    "> It is functionally similar to calling the `Tokenizer` object directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sub1 = text[0:100]\n",
    "text_sub2 = text[100:200]\n",
    "text_list = [text_sub1, text_sub2]\n",
    "docs = list(tokenizer_def.make_docs(text_list))\n",
    "\n",
    "# Alternatively, call Tokenizer object directly:\n",
    "docs = list(tokenizer_def(text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c017bc0",
   "metadata": {},
   "source": [
    "> **Note**  \n",
    "> Some tokens may include punctuation or newline characters. To avoid this, you can either:  \n",
    "> - Scrub the text using `Scrubber` before tokenization  \n",
    "> - Filter out unwanted tokens after tokenization using token attributes (e.g., `is_punct`, `is_space`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d8db4",
   "metadata": {},
   "source": [
    "## Scrub the text before tokenization\n",
    "Use the `Scrubber` module to clean the text by removing unwanted characters like extra spaces, line breaks, and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8da55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.scrubber.scrubber import Scrubber\n",
    "scrubber = Scrubber()\n",
    "scrubber.remove_whitespace = True\n",
    "scrubber.remove_punctuation = True\n",
    "\n",
    "cleaned_text = scrubber.scrub(text)\n",
    "doc = tokenizer.make_doc(cleaned_text)\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f4604",
   "metadata": {},
   "source": [
    "### Filter tokens after tokenization\n",
    "You can also leave the text as-is and filter the tokens using spaCy's built-in attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizer.make_doc(text)\n",
    "\n",
    "#remove punctuation and whitespace tokens\n",
    "filtered_tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a18de",
   "metadata": {},
   "source": [
    "## Adding or Removing Stop Words\n",
    "Stop words are words that are to be excluded from the list of tokens. These are generally words like \"the\" and \"and\", but can be anything that serves the purpose of the desired analysis. <br> <br>\n",
    "To add stopwords to a `tokenizer` instance, use the `add_stopwords()` function to pass a list of words that will act as stopwords before tokenizing. <br><br>\n",
    "> **Note:** <br>\n",
    "> Some models, such as `\"en_web_core_sm\"`, have built-in stopwords. To remove these, or any other stop words, use the `remove_stopwords()` function before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e83517",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"This is a test of stopwords in the tokenizer class.\"\n",
    "\n",
    "# Adding stopwords\n",
    "tokenizer_def.add_stopwords([\"is\", \"the\", \"of\"])\n",
    "stop_doc = tokenizer_def.make_doc(test_text)\n",
    "for token in stop_doc[0:50]:\n",
    "    # Print token text formatted with padding for alignment\n",
    "    # and whether it is a stopword\n",
    "    print(f\"Token: {token.text:<12}    Stopword: {token.is_stop}\")\n",
    "print(\"\\n================================\\n\")\n",
    "\n",
    "# Removing stopwords\n",
    "tokenizer_def.remove_stopwords([\"is\", \"the\", \"of\"])\n",
    "stop_doc = tokenizer_def.make_doc(test_text)\n",
    "for token in stop_doc[0:50]:\n",
    "    # Print token text formatted with padding for alignment\n",
    "    # and whether it is a stopword\n",
    "    print(f\"Token: {token.text:<12}    Stopword: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2cdbb",
   "metadata": {},
   "source": [
    "## Simple Tokenizers <br>\n",
    "Along with the language model based tokenizer, the `tokenizer` class also contains two simple tokenizers: `SliceTokenizer` and `WhitespaceTokenizer`. <br><br>\n",
    "`SliceTokenizer` slices the text into tokens of n characters. The constructor takes two arguments: `n`, which is the number of characters that each token will be, and `drop_ws`, a modifier that controls whether to drop whitespace or keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.tokenizer import SliceTokenizer\n",
    "test_text = \"Cut me up into tiny pieces!\"\n",
    "slicer = SliceTokenizer(n = 4, drop_ws=True)\n",
    "slices = slicer(test_text)\n",
    "print(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b025afe",
   "metadata": {},
   "source": [
    "`WhitespaceTokenizer` simply slices a text into tokens on whitespace, similarly to the built-in `split()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6313f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.tokenizer import WhitespaceTokenizer\n",
    "test_text = \"Split me up by whitespace!\"\n",
    "neatSlicer = WhitespaceTokenizer()\n",
    "slices = neatSlicer(test_text)\n",
    "print(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d93e27",
   "metadata": {},
   "source": [
    "## Ngrams <br>\n",
    "\n",
    "The tokenizer module contains a subclass named `ngrams`, which allows you to generate ngrams, which are sequences of consecutive tokensfrom either raw text or tokenized documents.  \n",
    "These are useful for analyzing patterns, building frequency models, or studying frequent phrases in your texts.\n",
    "\n",
    "Ngrams can be created using:\n",
    "\n",
    "- A pre-tokenized `spaCy.Doc` object  \n",
    "- Raw text input  \n",
    "- Character-based slicing\n",
    "\n",
    "To import this subclass, import the `ngrams` class from the `tokenizer` module.\n",
    "\n",
    "> **Note**  \n",
    "> An ngram is a contiguous sequence of *n* items from a given text.  \n",
    "> For example, 2-grams (bigrams) from `\"I like to eat\"` would be:  \n",
    "> `\"I like\"`, `\"like to\"`, `\"to eat\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd029266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.tokenizer.ngrams import Ngrams\n",
    "\n",
    "#initialize Ngrams object\n",
    "ngrams = Ngrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9e599",
   "metadata": {},
   "source": [
    "### Generating Ngrams from Text\n",
    "\n",
    "To generate ngrams directly from a raw string (before tokenization), use the `ngrams.from_text()` function.  \n",
    "This function automatically tokenizes the input and returns ngrams as strings or tokens depending on the `output` parameter.<br>\n",
    "> **Note:** <br>\n",
    "> The default `n` value is 2, and the default `output` value is 'text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ngrams = ngrams.from_text(text=text, output=\"text\", n=3, tokenizer=tokenizer_def, drop_ws=True)\n",
    "ngrams_list = list(out_ngrams)\n",
    "for ngram in ngrams_list[0:10]:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46994fdc",
   "metadata": {},
   "source": [
    "If you have a list of texts, you can use `from_texts()` to generate a list of docs corresponding to each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ngrams = ngrams.from_texts(texts=text_list, output=\"text\", n=3, tokenizer=tokenizer_def, drop_ws=True)\n",
    "for doc in out_ngrams:\n",
    "    doc_list = list(doc)\n",
    "    for ngram in doc_list[0:10]:\n",
    "        print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5f2ed",
   "metadata": {},
   "source": [
    "### From Token List <br>\n",
    "\n",
    "If you already have a list of tokens (e.g. from simple tokenization), you can use from_tokens(). <br>\n",
    "> **Note:** <br>\n",
    "> `from_tokens` does not support 'spans' as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d404010",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [token.text for token in doc]\n",
    "out_ngrams = ngrams.from_tokens(tokens=tokens_list, output=\"tuples\", n=3, drop_ws=True)\n",
    "ngrams_list = list(out_ngrams)\n",
    "for ngram in ngrams_list[0:10]:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52915c3",
   "metadata": {},
   "source": [
    "If you have a list of token lists, you can use `from_token_lists()` to generate a list of of the selected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list_1 = [token.text for token in docs[0]]\n",
    "tokens_list_2 = [token.text for token in docs[1]]\n",
    "out_ngrams = ngrams.from_token_lists([tokens_list_1, tokens_list_2], output=\"tuples\", n=3, drop_ws=True)\n",
    "for doc in out_ngrams:\n",
    "    doc_list = list(doc)\n",
    "    for ngram in doc_list[0:10]:\n",
    "        print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049350c",
   "metadata": {},
   "source": [
    "### From a spaCy Doc <br>\n",
    "\n",
    "When using spaCy documents (produced by Tokenizer), you can generate spans, tuples, or text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfad3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizer_def.make_doc(text)\n",
    "out_ngrams = ngrams.from_doc(doc=doc, output=\"tuples\", n=3)\n",
    "ngrams_list = list(out_ngrams)\n",
    "for ngram in ngrams_list[0:10]:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb5ff5",
   "metadata": {},
   "source": [
    "If you have a list of spaCy docs, you can use `from_docs()` to create ngrams from each doc in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded06b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ngrams = ngrams.from_docs(docs=docs, output=\"tuples\", n=3)\n",
    "for doc in out_ngrams:\n",
    "    doc_list = list(doc)\n",
    "    for ngram in doc_list[0:10]:\n",
    "        print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c8027",
   "metadata": {},
   "source": [
    "### Filtering Options <br>\n",
    "\n",
    "To remove unwanted tokens such as stopwords, punctuation, or digits you can either manually filter tokens using their attributes or use built-in filters available in the ngrams module.\n",
    "\n",
    "Both approaches help clean your text before further analysis, such as generating ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b78e38",
   "metadata": {},
   "source": [
    "##### Manual Filtering with Token Attributes\n",
    "\n",
    "You can use a list comprehension to build a filtered version of the text using spaCy token attributes, and then re-tokenize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1819231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out stopwords,punctuation, and whitespace\n",
    "filtered_tokens = [\n",
    "    token.text for token in doc \n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "# join tokens and create a new doc\n",
    "filtered_text = \" \".join(filtered_tokens)\n",
    "filtered_doc = tokenizer_def.make_doc(filtered_text)\n",
    "\n",
    "#view the first 38 tokens\n",
    "for token in filtered_doc[0:38]:\n",
    "    print(f\"<{token.text}>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f59e2",
   "metadata": {},
   "source": [
    "> **Note**  \n",
    "> Besides the `text` attribute and boolean `is_` attributes like `is_stop`, `is_punct`, or `is_space`, other attributes such as `pos` and `ent_type` require a trailing underscore (`_`) to get human-readable values.  \n",
    "> \n",
    "> For example:  \n",
    "> - `token.pos` â†’ returns a numerical ID  \n",
    "> - `token.pos_` â†’ returns the actual part of speech (e.g., `'NOUN'`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01012fe5",
   "metadata": {},
   "source": [
    "##### Filtering Ngrams with Built in Parameters\n",
    "\n",
    "If you're generating ngrams using the ngrams.from_doc() helper, you can apply filters directly by setting boolean parameters.\n",
    "\n",
    "The Ngrams class includes filters to clean the text:\n",
    "\n",
    "- **filter_stops**: removes stopwords\n",
    "\n",
    "- **filter_digits**: removes tokens that are digits\n",
    "\n",
    "- **filter_nums**: removes tokens that are numbers or number-like\n",
    "\n",
    "- **filter_punct**: removes punctuation\n",
    "\n",
    "- **drop_ws**: removes whitespace\n",
    "\n",
    "- **min_freq**: removes tokens that occur less than specified frequency \n",
    "\n",
    "<br>\n",
    "\n",
    ">**Note:**<br>\n",
    "> Although they seem similar, `filter-nums` and `filter_digits` filter out two seperate types of tokens. `filter_digits` filters out numerical digits (e.g. 0, 3, 8, etc.), while `filter_nums` filters out spelled-out numerical values (e.g. 'zero', 'three', 'eight', etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizer_def.make_doc(\"This is test ten of 10.\")\n",
    "print(list(ngrams.from_doc(doc, output=\"text\", filter_digits=True)))\n",
    "# Output: ['This is', 'is test', 'test ten', 'ten of']\n",
    "\n",
    "doc = tokenizer_def.make_doc(\"This test includes 100%, punctuation, and stopwords.\")\n",
    "print(list(ngrams.from_doc(doc, output=\"text\", filter_digits=True, filter_punct=True, filter_stops=True)))\n",
    "# Output: ['This test', 'test includes', 'and stopwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135c63d",
   "metadata": {},
   "source": [
    "To customize the size of the ngrams, use the `n` parameter to adjust. For example, `n = 2` would produce bigrams, `n = 3`, would produce trigrams, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigrams (n=3)\n",
    "ngrams = Ngrams()\n",
    "doc = tokenizer_def.make_doc(\"It is a beautiful day outside.\")\n",
    "print(list(ngrams.from_doc(doc, n=3, output=\"text\")))\n",
    "# Output: ['It is a', 'is a beautiful', 'a beautiful day', 'beautiful day outside']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_lexos_team",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
