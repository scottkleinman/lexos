{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ba4e9c",
   "metadata": {},
   "source": [
    "# Topwords Tutorial\n",
    "\n",
    "This tutorial will guide on how to use this module for extracting keyterms using the `KeyTerms`, `ZTest`, and `MannWhitney` classes of the `topwords` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf986b",
   "metadata": {},
   "source": [
    "## What's the Difference Between `KeyTerms`, `ZTest`, and `MannWhitney`?\n",
    "\n",
    "Imagine you have a large collection of books. You can use `KeyTerms` like an index generator for a single book. It looks inside the book, identifies the most important words or phrases and highlights which ones are most statistically significant in the book, which may correlate to significant topics or themes. The book is not compared to any other book.\n",
    "\n",
    "By contrast, you can use `ZTest` to examine what distinguishes term significance in groups of books. One group of books represents your area of interest (your \"target\" documents), and the rest, or a portion of the rest, represents the \"comparison\" documents to which you will compare the ones that interest you. `ZTest` compares words and phrases in your target documents to the comparison documents. Its goal is to find words and phrases that are unusually common in your target group or vice versa. This helps to identify what terms mark your target documents as unique.\n",
    "\n",
    "The `MannWhitney` class implements a procedure similar to `ZTest` but provides a measure of confidence in *how* significant the differences between documents are. More information about when to use `MannWhitney` over `ZTest` is provided below.\n",
    "\n",
    "In short, use `KeyTerms` to identify what an individual document is about. Use `ZTest` or `MannWhitney` to identify what terms make a specific group of documents different from another group of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d83e54",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Run the cell below to load the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.io.loader import Loader\n",
    "from lexos.tokenizer import Tokenizer\n",
    "\n",
    "paths = [\n",
    "    \"txt_files/Dracula by Bram Stoker.txt\",\n",
    "    \"txt_files/Alices Adventures in Wonderland by Lewis Carroll.txt\",\n",
    "    \"txt_files/Romeo and Juliet by William Shakespeare.txt\",\n",
    "]\n",
    "loader = Loader()\n",
    "loader.load(paths)\n",
    "\n",
    "# We take only the beginning of the text to speed up processing for this tutorial\n",
    "texts = [text[0:10000] for text in loader.texts]\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "docs = list(tokenizer.make_docs(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a518d",
   "metadata": {},
   "source": [
    "### The `KeyTerms` Class\n",
    "\n",
    "This class wraps [Textacy's `extract.keyterms`](https://textacy.readthedocs.io/en/latest/api_reference/extract.html#keyterms) method, the user is able to extract representative keyterms from a single document, using the library's algorithms `textrank`, `sgrank`, `scake`, or `yake`. Choose your algorithm with the `method` parameter. See the Lexos documentation for advice on which algorithm to choose.\n",
    "\n",
    "The `topn` parameter is used to configure the number of topwords to output (the default is 10). You can also choose the count ngrams by setting `ngrams=2` (or 3, 4, etc.). If you wish to count multiple types of ngrams, use a tuple like `ngrams=(1, 2)`, which will count single tokens and bigrams.\n",
    "\n",
    "The `normalize` parameter is used to determine whether variant forms of tokens should be counted separately. The default setting \"lemma\" counts together all morphological forms of a word. You can turn this off by setting `normalize=\"orth\"`. You can also make counts case insensitive by setting `normalize=\"lower\"`.\n",
    "\n",
    "Note that you need to preprocess your documents into spaCy `Doc` objects with a language model that generates lemmas for your tokens (e.g. `en_core_web_sm` for English). You may also pass a such a model to the `KeyTerms` class to preprocess string inputs in place, but this will slow down processing times.\n",
    "\n",
    "Run the cell below to perform a basic experiment on *Dracula*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.topwords.keyterms import KeyTerms\n",
    "\n",
    "kt = KeyTerms(\n",
    "        document=docs[0],\n",
    "        method=\"textrank\",\n",
    "        topn=5,\n",
    "        ngrams=1,\n",
    "        normalize=\"lemma\",\n",
    "    )\n",
    "kt.keyterms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4ca70",
   "metadata": {},
   "source": [
    "You can output the results as a list of tuples or pandas DataFrame with the helper methods shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a list of tuples\n",
    "print(kt.to_list())\n",
    "\n",
    "# Output a pandas DataFrame\n",
    "kt.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4dc9a",
   "metadata": {},
   "source": [
    "### Understanding `KeyTerms` Output\n",
    "The `KeyTerms` output, whether as a DataFrame or a list of tuples, provides a term and a score.\n",
    "\n",
    "For example, say you had the following output:\n",
    "\n",
    "```txt\n",
    "term score\n",
    "0 poor old man 0.003206\n",
    "1 dear old man 0.003025\n",
    "2 good one 0.002898\n",
    "3 little one 0.002791\n",
    "4 good brave man 0.002725\n",
    "```\n",
    "\n",
    "The `term` represents the keyterm or keyphrase extracted from the document. `KeyTerms` uses algorithms like TextRank, which are graph-based methods that identify important words or phrases by analyzing their relationships within the text. This is why you often see multi-word phrases like \"poor old man\" or \"good brave man\" as keyterms.\n",
    "\n",
    "The `score` is a numerical value indicating the estimated importance or salience of the term within the single document being analyzed. A higher score means the algorithm considers that term more central or relevant to the document's content. For example, 'poor old man' is more relevant to the document's content than 'good brave man'. The exact range and meaning of the score depend on the specific Textacy algorithm used (`textrank` or `sgrank`), but generally, they are normalized values, not counts or frequencies. They reflect the term's position and connections in the text's semantic network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd82daa",
   "metadata": {},
   "source": [
    "### The `ZTest` Class\n",
    "\n",
    "`ZTest` identifies statistically over-represented words in target documents compared to comparison documents. It calculates the proportion of each term in target vs. comparison documents, computes a z-score for the difference in proportions, and returns terms with the highest z-scores. These terms are deemed to be the most distinctive of the target documents.\n",
    "\n",
    "In the process, it also sets a `._.topwords` attribute on all spaCy `Doc` objects so that the topwords for that document can be retrieved easily using `doc._.topwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.topwords.ztest import ZTest\n",
    "\n",
    "target_docs = [docs[0]] # Dracula\n",
    "comparison_docs = [docs[1], docs[2]]  # Alice and Romeo\n",
    "\n",
    "ztest = ZTest(target_docs=target_docs, comparison_docs=comparison_docs, topn=5)\n",
    "\n",
    "# Print the top words as a list of tuples\n",
    "print(\"All Topwords from ZTest:\")\n",
    "print(ztest.topwords)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Each doc in docs now has doc._.topwords set to the top words:\")\n",
    "print(f\"Topwords for *Dracula*: {docs[0]._.topwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696be8c2",
   "metadata": {},
   "source": [
    "The following keyword parameters are useful for configuring the output:\n",
    "\n",
    "- `topn`: Number of top words to return (default: 10).\n",
    "- `ngrams`: An integer representing the number of ngrams to count or a tuple specifying more than one type, e.g., `(1, 2)` for single tokens and bigrams.\n",
    "- `case_sensitive`, `remove_stopwords`, `remove_punct`, `remove_digits`: Preprocessing options. The default is `True`, except for `remove_digits`.\n",
    "- `model`: spaCy model name to use for tokenization if the input documents are strings. The default is `\"xx_sent_ud_sm\"`.\n",
    "\n",
    "You can try these out in the code above to see how the output changes.\n",
    "\n",
    "Additionally, there are `to_dict()`, `to_df()`, and `to_list_of_dicts methods()`, which allow you to output the data as a dictionary, a pandas DataFrame, or a list of dictionaries. In the example below, we display the results as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztest.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b910615",
   "metadata": {},
   "source": [
    "### Understanding `ZTest` Output\n",
    "The `ZTest` output provides a term and a `z-score`.\n",
    "\n",
    "For example:\n",
    "\n",
    "```txt\n",
    "term z_score\n",
    "0 Alice -30.694384\n",
    "1 thou -23.555131\n",
    "2 ROMEO -19.807056\n",
    "3 Romeo -18.895006\n",
    "4 O -18.895006\n",
    "5 thy -18.832643\n",
    "6 thee -17.936969\n",
    "7 JULIET -16.925139\n",
    "8 CAPULET -15.321415\n",
    "9 NURSE -14.614962\n",
    "```\n",
    "The `term` is a word or phrase (n-gram) that has a statistically significant difference in its relative frequency between your target documents and your comparison documents.\n",
    "\n",
    "The `z_score` is the result of a z-test, which measures the statistical significance of the difference in proportions of a given term between the two document sets. The larger the absolute value of the z-score, the more statistically significant the difference in frequency. A z-score further from zero (either very high positive or very high negative) indicates a stronger distinction.\n",
    "\n",
    "A negative z-score (e.g., \"Alice\", \"ROMEO\", \"thou\") indicates that the term is more characteristic of the comparison documents (in this case, *Alice in Wonderland* and *Romeo and Juliet*) compared to the target documents (in this case, *Dracula*). The higher the absolute negative value, the more strongly that term belongs to the background set.\n",
    "\n",
    "A positive z-score would indicate that the term is more characteristic of the target documents than the comparison documents. This is often what users are looking for when identifying \"top distinguishing words.\" (Note: You might not see many in this example if *Dracula* is truly distinct from the comparison documents.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a0a63",
   "metadata": {},
   "source": [
    "### The `MannWhitney` Class\n",
    "\n",
    "The Mann-Whitney U test (also called the Wilcoxon rank-sum test) is a statistical method that compares two groups to determine if they differ significantly. Unlike the z-test, it doesn't make assumptions about how the data is distributed.\n",
    "\n",
    "Instead of comparing proportions directly (like the z-test), the Mann-Whitney test:\n",
    "\n",
    "1. Ranks all term frequencies across both document sets\n",
    "2. Compares the ranks between target and comparison documents\n",
    "3. Calculates a U-statistic that measures how different the rankings are\n",
    "4. Provides a p-value indicating the probability the difference occurred by chance\n",
    "\n",
    "The `MannWhitney` class takes as its input a pandas DataFrame of term frequencies with docs in rows and terms in columns. Any filtering of your terms must be done in advance. The easiest way to produce the input DataFrames is with the Lexos DTM module. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17333a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.dtm import DTM\n",
    "from lexos.tokenizer import Tokenizer\n",
    "from lexos.topwords.mann_whitney import MannWhitney\n",
    "\n",
    "texts = [\n",
    "    \"This is a sample text for testing.\",\n",
    "    \"Here is another example of a text to analyze.\",\n",
    "    \"This text is different from the others.\",\n",
    "    \"Yet another sample text for comparison.\",\n",
    "    \"This text is similar to the first one.\",\n",
    "    \"A completely different text for the analysis.\",\n",
    "]\n",
    "\n",
    "# Process the sample texts with spaCy to create documents\n",
    "docs = list(tokenizer.make_docs(texts))\n",
    "\n",
    "# Create labels for the documents\n",
    "labels = [f\"Doc{i + 1}\" for i in range(len(docs))]\n",
    "\n",
    "# Create a Document-Term Matrix (DTM) using the sample documents\n",
    "# Limit to terms occurring in at least 2 documents\n",
    "dtm = DTM()\n",
    "dtm(docs=docs, labels=labels, min_df=2)\n",
    "df = dtm.to_df(transpose=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357242b6",
   "metadata": {},
   "source": [
    "In the example above, we'll split the DataFrame into target and comparison data based on whether the label has an even or odd number. We'll uses these as the targe and comparison data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949764d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into target and comparison groups based on even/odd document labels\n",
    "even_df = df[df.index.isin([\"Doc2\", \"Doc4\", \"Doc6\"])]\n",
    "odd_df = df[df.index.isin([\"Doc1\", \"Doc3\", \"Doc5\"])]\n",
    "\n",
    "mw = MannWhitney(target=even_df, comparison=odd_df)\n",
    "mw.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c785585",
   "metadata": {},
   "source": [
    "The output will show the terms ranked by their distinctiveness, along with their U statistic and p-value.\n",
    "\n",
    "The p-value is the probability that a test statistic is extreme or more extreme than the one observed, assuming that the two samples come from the same distribution. A small p-value (typically less than 0.05) indicates that the observed difference between the two samples is statistically significant, and we conclude that the two samples do not come from the same distribution.\n",
    "\n",
    "By default, the table displays the average frequency of terms in the control group along with the increase in frequency in the comparison group. This provides us with another view of how important the word is to the sample and its relative over- or under-usage in comparison to the other sample. You can suppress the average frequency and difference columns with `add_freq=False`.\n",
    "\n",
    "The following points provide a useful guide to interpreting the results:\n",
    "\n",
    "- **u_statistic**: Higher values indicate the term appears more in target documents\n",
    "- **p_value**: Lower values (< 0.05) indicate statistically significant differences\n",
    "  - p < 0.05: Statistically significant (95% confident)\n",
    "  - p < 0.01: Highly significant (99% confident)\n",
    "  - p < 0.001: Very highly significant (99.9% confident)\n",
    "\n",
    "#### When to Use Mann-Whitney vs. Z-Test\n",
    "\n",
    "There are a number of key differences between the two types of tests:\n",
    "\n",
    "- Z-test assumes terms are normally distributed (that is, data has a rough bell curve shape with frequencies decreasing evenly on both sides of the central mean). Mann-Whitney makes no such assumption.\n",
    "- Z-test is more powerful with large, well-behaved data; Mann-Whitney works better with small or irregular data.\n",
    "- Z-test gives a z-score (can be positive or negative); Mann-Whitney gives a U-statistic and p-value. Lower p-values (< 0.05) indicate more significant differences.\n",
    "\n",
    "Here are some rules of thumb for choosing a method:\n",
    "\n",
    "- **Large corpus (100+ documents)** → Use `ZTest` for faster, more powerful results\n",
    "- **Small corpus (< 30 documents)** → Use `MannWhitney` for more reliable results\n",
    "- **Unsure about your data** → Use `MannWhitney` to be safe\n",
    "- **Need fast computation** → Use `ZTest` (it's computationally simpler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2936492e",
   "metadata": {},
   "source": [
    "### The `Compare` Class\n",
    "\n",
    "The `Compare` class provides powerful methods for analyzing and comparing documents using statistical measures. This class wraps around `ZTest` or `MannWhitney` to enable three comparison strategies:\n",
    "\n",
    "1. **`document_to_corpus()`** - Compare each document to all other documents. Use this method when you want to find what terms make each document unique.\n",
    "2. **`documents_to_classes()`** - Compare each document to documents in other classes. Use this method when you want to find outliers or representative terms within classes.\n",
    "3. **`classes_to_classes()`** - Compare entire classes to each other. Use this method when you want to find the signature vocabulary of particular categories.\n",
    "\n",
    "All methods support three output formats: `dict`, `dataframe`, and `list_of_dicts`.\n",
    "\n",
    "As a basic example, we will take four short texts. Although you can perform experiments with raw strings, they will generally be converted to spaCy `Doc` objects internally. So, for efficiency, we will preprocess the texts into spaCy `Docs`. We'll then create an instance of the `ZTest` class for our example. We provide it with no docs because these will be passed to it when we choose what we want to compare. The `ZTest` instance is our calculator. We can swap it out for other classes in the `topwords` module or with our own custom classes. Finally, we create an instance of the `Compare` class and pass it our calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexos.tokenizer import Tokenizer\n",
    "from lexos.topwords.ztest import ZTest\n",
    "from lexos.topwords.compare import Compare\n",
    "\n",
    "# Load spaCy model\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "\n",
    "# Prepare sample documents\n",
    "docs = list(tokenizer.make_docs([\n",
    "    \"Dracula was a vampire who lived in Transylvania. He had sharp fangs and drank blood.\",\n",
    "    \"Frankenstein created a monster in his laboratory. The creature was terrifying and misunderstood.\",\n",
    "    \"Alice fell down the rabbit hole into Wonderland. She met the Cheshire Cat and Mad Hatter.\",\n",
    "    \"Peter Pan could fly and never wanted to grow up. He lived in Neverland with the Lost Boys.\"\n",
    "]))\n",
    "\n",
    "# Create a calculator instance (ZTest in this example)\n",
    "calculator = ZTest(target_docs=[], comparison_docs=[])\n",
    "\n",
    "# Create Compare instance\n",
    "compare = Compare(calculator=calculator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4049024",
   "metadata": {},
   "source": [
    "By creating the calculator class first, we are also able to configure it with any parameters relevant to the class. For instance, we might want to set the `case_sensitive` parameter with `calculator = ZTest(target_docs=[], comparison_docs=[], case_sensitive=False)`.\n",
    "\n",
    "We're now ready to perform our comparison. We'll start by using the `document_to_corpus()` method to find what terms make each document unique compared to all other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each document to the rest of the corpus\n",
    "results = compare.document_to_corpus(docs)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1403356",
   "metadata": {},
   "source": [
    "Running one of the `Compare` class methods populates its `data` and `results` attributes, which you can access separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "print(\"Topwords Data:\")\n",
    "print(compare.data)\n",
    "\n",
    "# View the results\n",
    "print(\"\\nTopwords Results:\")\n",
    "print(compare.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d2448",
   "metadata": {},
   "source": [
    "You can also change the output with the with the `output_format` parameter (the default is \"dict\"). You can also return a pandas DataFrame or a list of dicts by setting this parameter to \"dataframe\" or \"list_of_dicts\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare.document_to_corpus(docs, output_format=\"dataframe\")\n",
    "display(results)\n",
    "\n",
    "results = compare.document_to_corpus(docs, output_format=\"list_of_dicts\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16760c1",
   "metadata": {},
   "source": [
    "The `documents_to_classes()` method is used to find what makes each document distinctive compared to documents in other categories. In addition to `docs` and `doc_labels`, it accepts a `class_labels` list that supplies categories for each document (class labels indices must correspond to document indices).\n",
    "\n",
    "When you call `documents_to_classes()`, each document is compared to **all documents in other classes**:\n",
    "\n",
    "- \"Dracula\" (gothic) is compared to [\"Alice\", \"Peter Pan\"] (whimsy)\n",
    "- \"Frankenstein\" (gothic) is compared to [\"Alice\", \"Peter Pan\"] (whimsy)\n",
    "- \"Alice\" (whimsy) is compared to [\"Dracula\", \"Frankenstein\"] (gothic)\n",
    "- \"Peter Pan\" (whimsy) is compared to [\"Dracula\", \"Frankenstein\"] (gothic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels for each document\n",
    "doc_labels = [\"Dracula\", \"Frankenstein\", \"Alice\", \"Peter Pan\"]\n",
    "class_labels = [\"gothic\", \"gothic\", \"whimsy\", \"whimsy\"]\n",
    "\n",
    "results = compare.documents_to_classes(\n",
    "    docs=docs,\n",
    "    doc_labels=doc_labels,\n",
    "    class_labels=class_labels\n",
    ")\n",
    "print(results['Dracula'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4de38d",
   "metadata": {},
   "source": [
    "You can access data and results with the `data` and `results` attributes (class labels are added to the `data` dict). You can also output DataFrames and lists of dicts as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5f0d0",
   "metadata": {},
   "source": [
    "The `classes_to_classes()` method is used to find what what terms characterize entire categories/genres compared to other categories. It also takes a `class_labels` parameter.\n",
    "\n",
    "Each class is treated as a unified group:\n",
    "\n",
    "- \"gothic\" class: [\"Dracula\", \"Frankenstein\"] combined vs. [\"Alice\", \"Peter Pan\"] combined\n",
    "- \"whimsy\" class: [\"Alice\", \"Peter Pan\"] combined vs. [\"Dracula\", \"Frankenstein\"] combined\n",
    "\n",
    "This is different from `documents_to_classes()` which compares individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed094e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare.classes_to_classes(\n",
    "    docs=docs,\n",
    "    doc_labels=doc_labels,\n",
    "    class_labels=class_labels,\n",
    "    output_format=\"dict\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d7e50",
   "metadata": {},
   "source": [
    "As with the other classes, you can output the results as a pandas DataFrame or as a list of dicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ffc5c",
   "metadata": {},
   "source": [
    "#### Using Dictionary Input\n",
    "\n",
    "Instead of passing separate `doc_label` and `class_label` lists, you can pass documents as a list of dictionaries if you already have your data in that format. Just pass the dictionary with the `docs` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dicts = [\n",
    "    {\"doc\": docs[0], \"doc_label\": \"Dracula\", \"class_label\": \"gothic\"},\n",
    "    {\"doc\": docs[1], \"doc_label\": \"Frankenstein\", \"class_label\": \"gothic\"},\n",
    "    {\"doc\": docs[2], \"doc_label\": \"Alice\", \"class_label\": \"whimsy\"},\n",
    "    {\"doc\": docs[3], \"doc_label\": \"Peter Pan\", \"class_label\": \"whimsy\"}\n",
    "]\n",
    "\n",
    "results = compare.documents_to_classes(docs=doc_dicts)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c437d6",
   "metadata": {},
   "source": [
    "Note that only the `doc` and `doc_label` keys are used for `document_to_corpus()`. The `class_label` key (if present) will be ignored.\n",
    "\n",
    "If `doc_label` values are not available in the dict, `documents_to_classes()` and `classes_to_classes()` will supply \"Doc 1\", \"Doc 2\", \"Doc 3\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351bb8a",
   "metadata": {},
   "source": [
    "#### Using spaCy `Doc` Extensions\n",
    "\n",
    "If given a list of spaCy `Doc` objects, the `Compare` class will attempt to extract class values from custom extensions before trying other methods. For instance, if you supply the class label \"author\", `Compare` will first try to assign values for each `Doc` from its `._.author` extension. If that fails, the value \"author\" will be assigned as the class label for the doc.\n",
    "\n",
    "!!! Note\n",
    "    The class does not support nested dictionaries like `{\"metadata\": \"author\": \"Shakespeare\", \"language\": \"en\"}` If you have metadata in this form, you can convert it to an class instance. Here is a simple way to do this:\n",
    "\n",
    "    ```python\n",
    "    from dataclasses import dataclass\n",
    "\n",
    "    @dataclass\n",
    "    class Metadata:\n",
    "        author: str\n",
    "        language: str\n",
    "\n",
    "    doc._.metadata = Metadata(\"Shakespeare\", \"en\")\n",
    "    ```\n",
    "\n",
    "    You can now use dot notation for the nested attributes:\n",
    "\n",
    "    ```python\n",
    "    results = comparison.documents_to_classes(\n",
    "          docs=docs,\n",
    "          class_labels=[\"_.metadata.author\"]\n",
    "      )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349da331",
   "metadata": {},
   "source": [
    "#### Helper Methods\n",
    "\n",
    "The `Compare` class provides two helper methods for getting information about your data once it has been populated.\n",
    "\n",
    "The `get_class()` method takes a document label and returns the name of the class to which the document it belongs (if available).\n",
    "\n",
    "The `get_docs_by_class()` method returns a dict containing all documents grouped by class (the dictionary key). If you supply a `class_label` the output will be restricted to only documents with that label.\n",
    "\n",
    "You can run some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class for a specific document\n",
    "doc_class = compare.get_class(\"Dracula\")\n",
    "print(f\"Class for Dracula: {doc_class}\")\n",
    "\n",
    "# Get all documents grouped by class\n",
    "docs_by_class = compare.get_docs_by_class()\n",
    "print(f\"All classes: {list(docs_by_class.keys())}\")\n",
    "\n",
    "# Get documents for a specific class\n",
    "gothic_docs = compare.get_docs_by_class(class_label=\"gothic\")\n",
    "print(f\"Documents in 'gothic' class: {gothic_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bf7fa",
   "metadata": {},
   "source": [
    "### 3. Example Experiments with the `topwords` Module\n",
    "\n",
    "\n",
    "- **Extracting representative keyterms from a single literary work**  \n",
    "    Use `KeyTerms` to identify the most important terms or phrases in a novel, article, or essay.\n",
    "\n",
    "- **Comparing distinguishing words between two or more texts**  \n",
    "    Use `ZTest` to find words that are statistically significant in one document (or set) compared to a background corpus.\n",
    "\n",
    "- **Analyzing topic drift across chapters or sections**  \n",
    "    Apply `KeyTerms` or `ZTest` to different segments of a text to observe how key terms change over time.\n",
    "\n",
    "- **Identifying authorial style or signature vocabulary**  \n",
    "    Compare works by different authors to find words or phrases that are characteristic of each.\n",
    "\n",
    "- **Studying the effect of preprocessing choices**  \n",
    "    Experiment with parameters like `remove_stopwords`, `remove_punct`, `ngrams`, and `case_sensitive` to see how they affect the output.\n",
    "\n",
    "- **Building custom keyterm extraction pipelines**  \n",
    "    Integrate the module with other NLP tools (e.g., sentiment analysis, entity recognition) for richer text analysis.\n",
    "\n",
    "- **Exploring genre or period-specific vocabulary**  \n",
    "    Use the Z-test approach to compare texts from different genres or historical periods.\n",
    "\n",
    "- **Evaluating the impact of different spaCy models**  \n",
    "    Swap out the `model` parameter to see how different language models affect tokenization and keyterm extraction.\n",
    "<a id='exp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93524fd",
   "metadata": {},
   "source": [
    "### Visualizing Topwords with Word Clouds \n",
    "\n",
    "A word cloud visually represents the frequency or importance of words. Larger words indicate higher significance. This is particularly useful for quickly grasping the most prominent keyterms from `KeyTerms` or the distinguishing terms from `ZTest`. The code samples below use the Lexos `cloud` module to generate word clouds of terms calculated by Topwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11968c76",
   "metadata": {},
   "source": [
    "#### Word Cloud from `KeyTerms` Output\n",
    "\n",
    "You can directly use the output (as a list of tuples with term and score) from `KeyTerms` to generate a word cloud. The wordcloud() function can interpret this format. The 'score' from Textacy will determine the size of the words in the cloud. See the Lexos `cloud` module documentation for information on how to customize the word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reload some resources in case they are not available\n",
    "import pandas as pd\n",
    "from lexos.io.loader import Loader\n",
    "from lexos.tokenizer import Tokenizer\n",
    "from lexos.visualization.cloud import WordCloud\n",
    "\n",
    "paths = [\"txt_files/Dracula by Bram Stoker.txt\"]\n",
    "loader = Loader()\n",
    "loader.load(paths)\n",
    "\n",
    "# We take only the beginning of the text to speed up processing for this tutorial\n",
    "texts = [text[0:10000] for text in loader.texts]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "docs = list(tokenizer.make_docs(texts))\n",
    "\n",
    "# Generate the key terms\n",
    "kt = KeyTerms(\n",
    "    document=docs[0],\n",
    "    method=\"textrank\",\n",
    "    topn=10,\n",
    "    ngrams=1,\n",
    "    normalize=\"lemma\",\n",
    ")\n",
    "\n",
    "# Convert the key terms to a pandas DataFrame\n",
    "df = pd.DataFrame(kt.to_list(), columns=[\"term\", \"score\"]).set_index(\n",
    "    \"term\"\n",
    ")\n",
    "\n",
    "wc = WordCloud(data=df, title=\"Key Terms from Dracula\", height=200, width=200, round=150)\n",
    "wc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996dfb8",
   "metadata": {},
   "source": [
    "##### Word Cloud from `ZTest` Output\n",
    "\n",
    "Similarly, you can create a word cloud from the `ZTest` output. In this case, the absolute value of the Z-score can determine the size of the words, indicating how strongly the term distinguishes the target from the background corpus.\n",
    "\n",
    "Note that a couple of hacks are required in the code below. First, we convert the z-scores to absolute values (no negative numbers), which the word cloud function needs to generate term sizes. Second, we strip any line breaks that inadvertently crept into our docs since these generate an error in the word cloud function. For convenience, we perform both of these hacks when converting the z-test output to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reload some resources in case they are not available\n",
    "import pandas as pd\n",
    "from lexos.io.loader import Loader\n",
    "from lexos.tokenizer import Tokenizer\n",
    "from lexos.topwords.ztest import ZTest\n",
    "from lexos.visualization.cloud import WordCloud\n",
    "\n",
    "# Load all three texts\n",
    "loader = Loader()\n",
    "loader.load([\n",
    "    \"txt_files/Dracula by Bram Stoker.txt\",\n",
    "    \"txt_files/Alices Adventures in Wonderland by Lewis Carroll.txt\",\n",
    "    \"txt_files/Romeo and Juliet by William Shakespeare.txt\",\n",
    "])\n",
    "\n",
    "# We take only the beginning of the text to speed up processing for this tutorial\n",
    "texts = [text[0:10000] for text in loader.texts]\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer(model=\"en_core_web_sm\")\n",
    "docs = list(tokenizer.make_docs(texts))\n",
    "\n",
    "# Split the docs into target and comparison sets\n",
    "target_docs = [docs[0]]\n",
    "comparison_docs = docs[1:]\n",
    "\n",
    "# Perform the Z-Test\n",
    "ztest = ZTest(target_docs=target_docs, comparison_docs=comparison_docs, topn=15)\n",
    "\n",
    "# Convert list of tuples to a Pandas DataFrame\n",
    "df = pd.DataFrame(\n",
    "    [(term, abs(score)) for term, score in ztest.topwords if '\\n' not in term],\n",
    "    columns=[\"term\", \"abs_z_score\"],\n",
    ").set_index(\"term\")\n",
    "\n",
    "# Create and show the word cloud\n",
    "wc = WordCloud(data=df, title=\"Key Terms from Dracula\", height=200, width=200, round=150)\n",
    "wc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72749985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
