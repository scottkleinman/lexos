# Tokenizer

The `Tokenizer` uses spaCy to convert texts to documents using one of the functions below. If no language model is specified, spaCy's multi-lingual `xx_sent_ud_sm` model is used.

### ::: lexos.tokenizer._add_remove_stopwords
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer._get_disabled_components
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer._get_excluded_components
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer._load_model
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer._validate_input
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer.doc_from_ngrams
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer.docs_from_ngrams
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer.generate_character_ngrams
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer.make_doc
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer.make_docs
    rendering:
      show_root_heading: true
      heading_level: 3

### ::: lexos.tokenizer.ngrams_from_doc
    rendering:
      show_root_heading: true
      heading_level: 3
